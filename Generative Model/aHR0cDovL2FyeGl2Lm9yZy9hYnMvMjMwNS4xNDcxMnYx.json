{
  "url": "http://arxiv.org/abs/2305.14712v1",
  "title": "On the Generalization of Diffusion Model",
  "authors": "Mingyang Yi, Jiacheng Sun, Zhenguo Li",
  "year": 2023,
  "abstract": "The diffusion probabilistic generative models are widely used to generate\nhigh-quality data. Though they can synthetic data that does not exist in the\ntraining set, the rationale behind such generalization is still unexplored. In\nthis paper, we formally define the generalization of the generative model,\nwhich is measured by the mutual information between the generated data and the\ntraining set. The definition originates from the intuition that the model which\ngenerates data with less correlation to the training set exhibits better\ngeneralization ability. Meanwhile, we show that for the empirical optimal\ndiffusion model, the data generated by a deterministic sampler are all highly\nrelated to the training set, thus poor generalization. This result contradicts\nthe observation of the trained diffusion model's (approximating empirical\noptima) extrapolation ability (generating unseen data). To understand this\ncontradiction, we empirically verify the difference between the sufficiently\ntrained diffusion model and the empirical optima. We found, though obtained\nthrough sufficient training, there still exists a slight difference between\nthem, which is critical to making the diffusion model generalizable. Moreover,\nwe propose another training objective whose empirical optimal solution has no\npotential generalization problem. We empirically show that the proposed\ntraining objective returns a similar model to the original one, which further\nverifies the generalization ability of the trained diffusion model.",
  "citation": 23
}