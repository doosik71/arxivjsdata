# Learning how to Active Learn: A Deep Reinforcement Learning Approach

Meng Fang, Yuan Li, Trevor Cohn

## 🧩 Problem to Solve

- 기존 능동 학습(Active Learning, AL) 방법론은 대부분 휴리스틱 기반으로, 데이터셋에 따라 성능 편차가 크고 최적의 데이터 선택 전략을 학습하기 어렵다는 한계가 있습니다.
- 특히 저자원 언어(low-resource languages)의 경우 주석(annotation) 데이터 확보가 어려워 기존 AL 방법론 적용이 비현실적일 수 있습니다.
- 또한, 한 언어에서 학습된 능동 학습 전략을 다른 언어에 전이(transfer)하여 활용하는 교차 언어(cross-lingual) 환경에서의 능동 학습 전략 학습 및 적용 방안이 부족합니다.

## ✨ Key Contributions

- 능동 학습을 강화 학습(Reinforcement Learning, RL) 문제로 재정의하여 데이터 선택 정책(policy)을 명시적으로 학습하는 새로운 프레임워크인 PAL(Policy based Active Learning)을 제안했습니다.
- 학습된 데이터 선택 정책을 다른 언어 및 데이터 설정으로 전이할 수 있음을 보였으며, 특히 교차 언어 이름 개체명 인식(Named Entity Recognition, NER) 태스크에서 효과를 입증했습니다.
- 딥 Q-네트워크(Deep Q-Network, DQN)를 사용하여 동적이고 풍부한 주석 정책을 학습하는 방법을 제시했습니다.
- 교차 언어 워드 임베딩을 활용하여 학습된 정책을 저자원 타겟 언어에 효과적으로 전이할 수 있음을 보였습니다.
- 정책 및 모델 전이만을 허용하고 온라인 피드백이나 추가 모델 업데이트가 없는 '콜드 스타트(cold-start)' 시나리오에서도 우수한 성능을 달성했습니다.

## 📎 Related Works

- **기존 능동 학습 알고리즘**: 불확실성 샘플링(Uncertainty Sampling), 위원회 질의(Query by Committee), 예상 오류 감소(Expected Error Reduction) 등의 휴리스틱 기반 방법이 주로 사용되었습니다.
- **심층 강화 학습(Deep Reinforcement Learning, DRL)**: 딥 Q-러닝(Deep Q-Learning), 딥 시각 운동 정책 등 의사 결정 및 표현 학습에 널리 사용되지만, 능동 학습 전략 학습, 특히 교차 언어 환경에서 DRL을 적용한 연구는 거의 없었습니다.
- **전이 학습(Transfer Learning)**: 정책 파라미터 재사용이나 새로운 신경망 아키텍처를 통한 정책 전이 연구는 있었으나, 공유 특징 공간을 가진 언어 간 능동 학습 전략 전이에 초점을 맞춘 연구는 부족했습니다.

## 🛠️ Methodology

본 논문은 능동 학습을 마르코프 결정 과정(Markov Decision Process, MDP)으로 공식화하고 딥 강화 학습을 통해 데이터 선택 정책을 학습합니다.

1. **능동 학습의 MDP 재구성**:

   - **상태($s_i$)**: 주석 후보 문장 $x_i$의 콘텐츠 정보, 학습된 모델 $\phi$의 예측 주변 분포, 모델의 순차 예측 신뢰도 $C = \sqrt[n]{\max_y p_{\phi}(y|x_i)}$를 연속 벡터로 결합하여 표현합니다. 콘텐츠 및 주변 분포 표현은 각각 별도의 컨볼루션 신경망(CNN)을 통해 추출됩니다.
   - **행동($a_i$)**: 에이전트가 문장 $x_i$를 주석할지($a_i=1$) 말지($a_i=0$)를 결정하는 이진 선택입니다.
   - **보상($r_i$)**: 에피소드 종료 시 최종 모델의 F1 점수를 보상으로 사용하며, 학습 속도 향상을 위해 단계별 held-out 데이터셋 성능 변화($\text{Acc}(\phi_i) - \text{Acc}(\phi_{i-1})$)를 중간 보상으로 부여하는 보상 셰이핑(reward shaping)을 적용합니다.
   - **예산($B$)**: 주석할 수 있는 최대 인스턴스 수로, 에피소드 종료 조건 중 하나입니다.

2. **정책 학습**:

   - 딥 Q-러닝(Deep Q-Learning, DQL)을 사용하여 최적의 Q 함수 $Q_{\pi}(s,a)$를 학습합니다.
   - Q 함수는 상태 표현을 입력으로 받아 각 행동($a \in \{0,1\}$)에 대한 Q 값을 출력하는 단일 은닉층 신경망으로 구현됩니다.
   - 벨만 방정식 $Q_{\pi}(s,a) = E[R_i | s_i=s, a_i=a, \pi]$과 경험 리플레이(experience replay) 메모리 $M$를 사용하여 신경망 파라미터를 업데이트합니다.

3. **교차 언어 정책 전이**:
   - **Bilingual / Multilingual Policy Transfer (Algorithm 2)**: 고자원(source) 언어에서 학습된 정책 $\pi$를 교차 언어 워드 임베딩을 통해 저자원(target) 언어에 적용합니다. 타겟 언어에서는 held-out 성능을 기반으로 정책을 미세 조정할 수 있습니다.
   - **Cold-start Transfer (Algorithm 3)**: held-out 데이터가 없고 온라인 정책 업데이트가 불가능한 환경을 위해, 소스 언어에서 사전 학습된 모델 $\phi$와 함께 정책 $\pi$를 타겟 언어로 전이합니다. 에이전트는 피드백 없이 배치로 주석을 요청하고, 사전 학습된 모델의 예측 및 신뢰도 정보를 활용하여 결정합니다.

## 📊 Results

CoNLL 2002/2003 NER 데이터셋(영어, 독일어, 스페인어, 네덜란드어)을 사용하여 실험을 수행했습니다.

- **Bilingual 및 Multilingual 설정**: 제안된 PAL 방법(PAL$_{b}$ 및 PAL$_{m}$)은 무작위 샘플링 및 불확실성 샘플링 기준선을 모든 타겟 언어(독일어, 네덜란드어, 스페인어)에서 일관되게 능가했습니다. 특히 여러 소스 언어에서 정책을 학습한 PAL$_{m}$이 단일 소스 언어에서 학습한 PAL$_{b}$보다 우수한 성능을 보였습니다.
- **Cold-start 설정**: 정책이나 모델 업데이트가 없는 어려운 콜드 스타트 환경에서도 PAL$_{c}$는 사전 학습된 NER 태거로 초기화된 상태에서 무작위 및 불확실성 기준선을 모두 능가하는 성능을 보였습니다.
- **최종 성능 및 비용 절감**: 200개 문장 주석 시 모든 PAL 방법이 기준선보다 뛰어났습니다. PAL$_{m}$은 PAL$_{b}$보다 우수했으며, PAL$_{c}$가 정적 정책 및 모델 사용에도 불구하고 전반적으로 가장 좋은 F1 점수(독일어 70.7%)를 달성하여 모델 사전 학습의 중요성을 강조했습니다. PAL 방법은 무작위 샘플링 대비 주석 비용을 최대 10%까지 절감할 수 있음을 보였습니다.

## 🧠 Insights & Discussion

- **능동 학습 전략의 학습 가능성**: 강화 학습을 통해 데이터 선택 정책 자체를 데이터로부터 직접 학습하는 것이 가능하고 기존 휴리스틱 기반 방식보다 효과적임을 입증했습니다.
- **교차 언어 전이의 유용성**: 교차 언어 워드 임베딩을 활용하여 학습된 능동 학습 정책이 다른 언어에 성공적으로 전이될 수 있음을 보여주어, 저자원 언어의 데이터 부족 문제를 완화하는 중요한 방법론을 제시합니다.
- **콜드 스타트 시나리오에서의 강건성**: held-out 데이터와 온라인 피드백이 없는 극히 제한적인 콜드 스타트 환경에서도 유의미한 성능 향상을 달성하여 현실적인 필드 언어학 환경에 대한 실용적인 함의를 가집니다.
- **정책 및 모델 사전 학습의 중요성**: 특히 콜드 스타트 환경에서의 우수한 성능은 저자원 환경에서 사전 학습된 모델과 정책이 얼마나 중요한 역할을 하는지 강조합니다.

## 📌 TL;DR

이 논문은 능동 학습(Active Learning)을 심층 강화 학습(Deep Reinforcement Learning) 문제로 재정의하여 데이터 선택 정책을 학습하는 PAL(Policy based Active Learning)을 제안합니다. PAL은 문장 콘텐츠, 모델 예측 주변 분포, 신뢰도 등을 상태로 활용하고, 딥 Q-네트워크를 통해 주석 여부를 결정하는 정책을 학습합니다. 교차 언어 워드 임베딩을 사용하여 학습된 정책을 다른 언어로 성공적으로 전이시킬 수 있으며, 심지어 온라인 피드백 없는 '콜드 스타트' 설정에서도 사전 학습된 모델과 정책 전이만으로 기존 방법보다 뛰어난 성능을 달성하여 Named Entity Recognition 태스크에서 주석 비용을 크게 절감할 수 있음을 입증했습니다.
