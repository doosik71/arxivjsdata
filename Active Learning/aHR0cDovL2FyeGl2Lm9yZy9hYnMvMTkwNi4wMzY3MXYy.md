# DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS

Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal

## 🧩 Problem to Solve

최근 딥러닝은 다양한 감독 학습 태스크에서 뛰어난 성능을 보였지만, 많은 양의 레이블링된 데이터가 필요합니다. 능동 학습(Active Learning)은 레이블링 노력을 최소화하기 위해 가장 정보 가치가 높은 샘플만 선별적으로 레이블링하도록 요청하는 방법입니다. 그러나 심층 신경망과 같은 표현력이 풍부한 모델에 대한 기존 능동 학습 이론(버전 공간 기반)은 효과적이지 못합니다.

특히 배치(batch) 설정에서는 다음과 같은 문제가 발생합니다:

- **불확실성 기반 샘플링의 비효율성**: 모델이 가장 불확실한 샘플만 선택하면, 배치 내의 데이터들이 거의 동일하여 정보적 중복이 발생합니다.
- **다양성 기반 샘플링의 비정보성**: 다양성을 최대화하는 샘플만 선택하면, 모델에 새로운 정보를 거의 제공하지 않는 샘플을 선택할 수 있습니다.
- **견고성 부족**: 기존 방법들은 모델 아키텍처, 배치 크기 또는 데이터셋에 따라 성능이 일관되지 않으며, 하이퍼파라미터 튜닝이 필요하지만 능동 학습에서 하이퍼파라미터 튜닝은 레이블링 비용이 많이 듭니다.

이 논문은 예측 불확실성과 샘플 다양성을 모두 통합하면서, 하이퍼파라미터 튜닝 없이 다양한 환경에서 견고하게 작동하는 실용적이고 일반적인 심층 배치 능동 학습 알고리즘을 설계하는 것을 목표로 합니다.

## ✨ Key Contributions

- **BADGE (Batch Active learning by Diverse Gradient Embeddings) 알고리즘 제안**: 딥러닝 모델을 위한 새로운 배치 능동 학습 알고리즘을 개발했습니다.
- **불확실성 및 다양성 통합**: '환각 그레디언트 공간(hallucinated gradient space)'이라는 개념을 도입하여, 각 샘플의 예측 불확실성(그레디언트 크기)과 샘플 다양성(그레디언트 방향)을 동시에 고려하여 배치 내에 포함되도록 설계했습니다.
- **하이퍼파라미터 없는 균형**: 불확실성과 다양성 사이의 균형을 명시적인 하이퍼파라미터 튜닝 없이 자동으로 조절합니다.
- **효율적인 샘플링 전략**: **k-MEANS++** 초기화 알고리즘을 그레디언트 임베딩에 적용하여, **k-DPP** (Determinantal Point Process)와 유사한 통계적 성능을 유지하면서 훨씬 효율적인 계산 성능을 달성했습니다.
- **광범위한 견고성 입증**: 다양한 신경망 아키텍처, 배치 크기, 데이터셋에 걸쳐 기존 최고 성능의 베이스라인보다 일관되게 우수하거나 동등한 성능을 보임을 실험적으로 증명했습니다.
- **불확실성 측정에 대한 이론적 근거**: 모델의 현재 예측 레이블로 계산된 손실 함수 그레디언트의 크기가 어떤 다른 레이블에 의해 유도되는 그레디언트 크기에 대한 하한(lower bound)임을 보였습니다 (Proposition 1). 이는 이 그레디언트 크기가 모델의 불확실성을 보수적으로 추정함을 의미합니다.

## 📎 Related Works

- **대표성 샘플링 (Representative Sampling)**: unlabeled 데이터셋을 대표하는 샘플을 선택하여 모델의 일반화 성능을 향상시키는 방법.
  - **CORESET** (Sener & Savarese, 2018; Geifman & El-Yaniv, 2017): 코어셋 구성 기반.
  - **GAN**-inspired (Gissin & Shalev-Shwartz, 2019): unlabeled 풀과 구별할 수 없는 샘플 선택.
- **불확실성 샘플링 (Uncertainty Sampling)**: 모델이 가장 불확실한 샘플을 선택하여 알고리즘의 불확실성을 최대한 줄이는 방법.
  - **선형 분류**: 결정 경계에 가장 가까운 예시 쿼리 (Tong & Koller, 2001; Schohn & Cohn, 2000; Tur et al., 2005).
  - **심층 학습**: **Dropout**을 사후 확률 근사로 사용 (Gal et al., 2017), 적대적 예시와의 거리 사용 (Ducoffe & Precioso, 2018), 앙상블 사용 (Beluch et al., 2018).
  - **CONF** (Confidence Sampling), **MARG** (Margin Sampling), **ENTROPY** (Entropy Sampling) 등이 이 범주에 속합니다.
- **하이브리드 접근 (Hybrid Approaches)**: 대표성 샘플링과 불확실성 샘플링의 장점을 결합.
  - **ALBL** (Active Learning by Learning, Hsu & Lin, 2015): 매 라운드마다 다양성 기반 또는 불확실성 기반 알고리즘 중 하나를 선택하는 메타 능동 학습.
  - 정보성과 대표성 균형 (Huang et al., 2010).
- **배치 모드 능동 학습 (Batch Mode Active Learning)**: 한 라운드에 여러 샘플을 선택하는 것에 초점.
  - 목적 함수를 최적화 문제로 공식화 (Guo & Schuurmans, 2008; Wang & Ye, 2015; Chen & Krause; Wei et al., 2015; Kirsch et al., 2019).
  - **예상 그레디언트 길이 (Expected Gradient Length, EGL)** (Settles et al., 2008): 그레디언트 길이에 기반한 쿼리 기준. **BADGE**는 배치 내 다양성을 고려한다는 점에서 EGL과 차별화됩니다.
- **결정점 과정 (Determinantal Point Processes, DPP)**: 품질과 다양성 사이의 균형을 맞추는 확률적 샘플링 도구 (Kulesza & Taskar, 2011). **BADGE**는 **k-DPP**에서 영감을 받았지만, 계산 복잡성 때문에 **k-MEANS++**를 사용합니다.

## 🛠️ Methodology

**BADGE**는 초기 무작위 샘플링으로 시작하여, 다음의 두 가지 주요 계산을 반복적으로 수행합니다:

1. **그레디언트 임베딩 계산 (Gradient Embedding Computation)**

   - 현재 모델 $\theta_t$를 사용하여 레이블이 지정되지 않은 모든 샘플 $x$에 대해 모델이 선호하는 가상 레이블 $\hat{y}(x) = h_{\theta_t}(x)$을 예측합니다.
   - 이 가상 레이블 $(\text{x}, \hat{y}(x))$에 대한 손실 함수의 그레디언트 $g_x$를 신경망의 마지막 (출력) 레이어 파라미터 $\theta_{\text{out}}$에 대해 계산합니다:
     $$g_x = \frac{\partial}{\partial \theta_{\text{out}}} \mathcal{L}_{\text{CE}}(f(x;\theta), \hat{y}(x))|_{\theta=\theta_t}$$
   - 이 그레디언트 벡터 $g_x$는 샘플 $x$에 대한 모델의 불확실성(그레디언트의 크기)과 잠재적인 모델 업데이트 방향(그레디언트의 방향) 정보를 모두 담고 있습니다. 논문은 $\text{Proposition 1}$을 통해 이 그레디언트 크기가 실제 레이블에 대한 그레디언트 크기의 하한임을 보이며, 모델의 높은 확신은 작은 그레디언트 크기로 이어진다는 것을 설명합니다.

2. **샘플링 계산 (Sampling Computation)**

   - 계산된 그레디언트 임베딩 벡터 $\{g_x\}$를 사용하여, **k-MEANS++ 시딩 알고리즘**을 통해 배치 크기 $B$에 해당하는 샘플을 선택합니다.
   - **k-MEANS++**는 **k-평균** 클러스터링의 좋은 초기화를 위해 고안된 알고리즘으로, 이미 선택된 중심(centroid)으로부터의 제곱 거리에 비례하여 다음 점을 샘플링합니다. 이 방식은 높은 크기(불확실성)와 다양성(이미 선택된 점들로부터의 거리)을 동시에 선호하는 경향이 있어 **k-DPP**와 유사한 특성을 가집니다.
   - **k-DPP**는 통계적으로 유리하지만 계산 비용이 매우 높아, **k-MEANS++**가 실용적인 대안으로 선택되었습니다.

3. **반복 및 모델 재학습**
   - 선택된 샘플들의 레이블을 쿼리하고, 이를 기존 레이블링된 데이터셋에 추가합니다.
   - 새로운 레이블링된 데이터셋으로 모델 $\theta_{t+1}$을 처음부터 다시 학습시킵니다.
   - 이 과정을 원하는 반복 횟수만큼 또는 레이블링 예산이 소진될 때까지 반복합니다.

## 📊 Results

**BADGE**는 여러 기준 모델 및 능동 학습 알고리즘(**CORESET**, **CONF**, **MARG**, **ENTROPY**, **ALBL**, **RAND**)과 비교 평가되었습니다. 실험은 세 가지 신경망 아키텍처(MLP, ResNet, VGG), 일곱 가지 데이터셋(SVHN, CIFAR10, MNIST, 네 가지 OpenML 데이터셋), 다양한 배치 크기(100, 1000, 10000)를 포함하여 총 231개의 시나리오에서 수행되었습니다.

- **견고성 및 일관된 성능**: **BADGE**는 아키텍처, 배치 크기, 데이터셋에 관계없이 모든 실험에서 기존 베이스라인 중 최고 성능과 동등하거나 더 우수한 성능을 일관되게 달성했습니다.
- **학습 곡선**: **BADGE**는 레이블링 예산의 초기 단계(다양성 샘플링이 유리한 경우)와 후기 단계(불확실성 샘플링이 유리한 경우) 모두에서 강점을 보이며, 전반적인 레이블링 예산에 관계없이 좋은 선택임을 시사했습니다.
- **쌍별 비교 (Penalty Matrix)**: 전반적인 실험 결과에서 **BADGE**는 가장 낮은 평균 페널티를 기록하여 다른 알고리즘들을 압도적으로 능가하는 성능을 보였습니다.
- **정규화된 오류의 누적 분포 함수 (CDFs)**: **BADGE**는 정규화된 오류 CDF에서 가장 높은 값을 보이며 전반적으로 가장 우수한 성능을 입증했습니다.
  - 작은 배치 크기(100 또는 1000)나 MLP 모델 사용 시에는 **BADGE**와 **MARG**가 가장 좋았습니다.
  - 큰 배치 크기(10000)에서는 **MARG**의 성능이 저하되는 반면, **BADGE**, **ALBL**, **CORESET**가 가장 좋은 성능을 보였습니다.
- **k-MEANS++** vs. **k-DPP**: 그레디언트 임베딩을 사용한 샘플 선택에서 **k-MEANS++**는 **k-DPP**와 거의 동일한 통계적 분류 정확도 성능을 보였지만, 계산 시간이 훨씬 짧아 효율성 측면에서 우수했습니다.
- **배치 다양성 및 불확실성**: **BADGE**는 그레디언트 임베딩의 Gram 행렬 로그 판별식(다양성 측정)과 평균 **L2** 노름(불확실성 측정) 모두에서 높은 값을 보이며, 다른 샘플링 방식(예: FF-k-CENTER)보다 더 다양하고 불확실성이 높은 배치를 선택하는 경향이 있었습니다.

## 🧠 Insights & Discussion

**BADGE**의 뛰어난 성능과 견고성 뒤에는 몇 가지 핵심 직관이 있습니다:

1. **불확실성의 효과적인 정의**: 모델의 현재 예측 레이블에 대한 마지막 레이어 그레디언트의 크기를 불확실성 지표로 사용합니다. 이 크기는 어떤 실제 레이블에 대한 그레디언트 크기의 하한을 제공하므로, 반드시 파라미터 업데이트를 유도하는 정보적인 샘플을 선택하게 됩니다.
2. **하이퍼파라미터 없는 불확실성-다양성 균형**: **k-MEANS++** 시딩 알고리즘은 본질적으로 그레디언트의 크기(불확실성)와 기존 배치 내 샘플들과의 거리(다양성)를 동시에 고려하여 샘플을 선택합니다. 이로써 불확실성이 높지만 중복되는 샘플을 선택하는 비효율적인 상황을 방지하며, 수동 튜닝 없이 이러한 균형을 달성합니다.
3. **랜덤화의 이점**: **k-MEANS++** 초기화의 랜덤화 특성은 잠재적으로 적대적으로 구성된 데이터셋에서도 결국 좋은 솔루션으로 수렴할 수 있는 확률적 보증을 제공합니다.

이러한 특성들의 결합이 **BADGE**에서 관찰된 실험적 견고성을 생성하는 것으로 보입니다. 비록 딥러닝의 이론적 분석은 어렵지만, 이 알고리즘은 실용적인 심층 능동 학습 문제에 대한 강력하고 신뢰할 수 있는 해결책을 제시합니다.

## 📌 TL;DR

**문제**: 딥러닝은 대량의 레이블링된 데이터를 필요로 하지만, 기존 능동 학습 기법은 모델 아키텍처나 배치 크기에 따라 성능이 불안정하며, 불확실성과 다양성 사이의 균형을 하이퍼파라미터 없이 잡기 어렵습니다.

**제안 방법**: **BADGE**는 레이블링되지 않은 샘플들을 모델의 마지막 레이어 파라미터에 대한 '환각 그레디언트(hallucinated gradient)' 벡터로 임베딩합니다. 이 그레디언트 벡터의 크기는 불확실성을, 방향은 다양성을 나타냅니다. **BADGE**는 **k-MEANS++** 시딩 알고리즘을 이 그레디언트 임베딩에 적용하여, 불확실성이 높고 서로 다른 방향성을 가진 샘플들로 구성된 배치를 선택합니다.

**주요 발견**: **BADGE**는 MLP, ResNet, VGG와 같은 다양한 신경망 아키텍처, 다양한 배치 크기 및 여러 데이터셋에 걸쳐 기존 능동 학습 베이스라인들과 비교했을 때, 일관되게 우수하거나 동등한 성능을 보이며 뛰어난 견고성을 입증했습니다. 특히, **k-DPP**와 유사한 통계적 이점을 유지하면서 훨씬 효율적인 계산 성능을 제공하여 실제 환경에서의 활용 가치를 높였습니다.
