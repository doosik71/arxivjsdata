# REVISITING FINE-TUNING FOR FEW-SHOT LEARNING

Akihiro Nakamura, Tatsuya Harada

## 🧩 Problem to Solve

소수의 예제만으로 새로운 클래스를 학습하는 소수 학습(Few-shot Learning)은 기계 학습에서 여전히 어려운 과제입니다. 기존에는 심층 신경망을 소수의 예제로 단순히 미세 조정(fine-tuning)할 경우 쉽게 과적합(overfit)될 수 있다는 가정 하에 복잡한 소수 학습 알고리즘들이 다수 제안되었습니다. 본 연구는 이러한 가설에 의문을 제기하며, 단순 미세 조정 방법의 성능이 제대로 평가되지 않았을 수 있음을 지적하고, 이 방법이 실제로 기존의 복잡한 알고리즘들보다 더 나은 성능을 보일 수 있는지 탐구합니다.

## ✨ Key Contributions

* 일반적으로 사용되는 저해상도 Mini-ImageNet 데이터셋의 1-shot 학습에서 본 논문의 미세 조정 방법이 일반적인 소수 학습 알고리즘보다 높은 정확도를 달성하며, 5-shot 학습에서는 최신 알고리즘과 거의 동등한 정확도를 보임을 입증했습니다.
* 고해상도 단일 도메인(single-domain) 및 교차 도메인(cross-domain)과 같은 보다 실용적인 작업에서 본 방법이 일반적인 소수 학습 알고리즘보다 높은 정확도를 달성함을 보여주었습니다.
* 실험 결과를 분석하여 미세 조정 과정에 대한 세 가지 중요한 통찰력을 제시했습니다:
    1. 낮은 학습률($\text{learning rate}$)을 사용하여 재학습 과정을 안정화할 수 있습니다.
    2. 미세 조정 시 적응형 경사 최적화기($\text{adaptive gradient optimizer}$)를 사용하면 테스트 정확도를 높일 수 있습니다.
    3. 기존 클래스와 새로운 클래스 간에 큰 도메인 이동(domain-shift)이 있는 경우 전체 네트워크를 업데이트하면 테스트 정확도가 향상될 수 있습니다.

## 📎 Related Works

* **Metric-based Approaches**: MatchingNet (Vinyals et al., 2016), ProtoNet (Snell et al., 2017), RelationNet (Sung et al., 2018) 등은 특징 임베딩을 학습하고 유사도 측정을 통해 분류합니다. Weight imprinting (Qi et al., 2018)은 마지막 FC 레이어 초기화에 사용됩니다.
* **Meta-learning-based Approaches**: MAML (Finn et al., 2017), Ravi & Larochelle (2017) 등은 네트워크가 새로운 클래스를 "학습하는 방법을 학습"하도록 훈련합니다.
* **Data-augmentation-based Approaches**: Hariharan & Girshick (2017), Schwartz et al. (2018), Wang et al. (2018) 등은 합성 데이터를 생성하여 데이터 부족 문제를 해결합니다.
* **Previous Fine-tuning Analysis**: Chen et al. (2019)은 미세 조정 방법의 성능이 과소평가되었음을 지적하며, 고해상도 및 교차 도메인 데이터셋의 중요성을 강조했습니다.
* **Related Fine-tuning Techniques**: MTL (Sun et al., 2019) 및 Noguchi & Harada (2019)는 특정 네트워크 파라미터만 업데이트하여 과적합을 방지하고 효율적인 적응을 달성하는 방법을 제시합니다.

## 🛠️ Methodology

1. **네트워크 구조**: ResNet-18/34/50/101/152 또는 VGG-16 (FC 레이어 제외)을 특징 추출기로 사용합니다. VGG-16의 마지막 MaxPool2d 레이어는 GlobalAveragePool2d로 대체됩니다.
2. **분류기**: 일반적인 FC 레이어를 사용하는 `Simple classifier`와 `weight imprinting` (Qi et al., 2018) 기법을 활용한 `Normalized classifier`를 사용합니다. 새로운 클래스에 대한 초기 분류기 가중치($W$)는 `weight imprinting`을 통해 얻어지거나(Normalized classifier), 다중 클래스 선형 SVM을 통해 얻어집니다(Simple classifier).
3. **데이터셋**:
    * **저해상도 단일 도메인 태스크**: 일반적인 84x84 이미지의 Mini-ImageNet (Vinyals et al., 2016; Ravi & Larochelle, 2017 분할).
    * **고해상도 단일 도메인 태스크**: 224x224 이미지의 고해상도 Mini-ImageNet (Chen et al., 2019).
    * **교차 도메인 태스크**: Mini-ImageNet을 기본 클래스로, CUB-200-2011 데이터셋 (Wah et al., 2011)에서 샘플링된 클래스를 새로운 클래스로 사용하여 더 큰 도메인 이동을 포함합니다 (Chen et al., 2019).
4. **사전 훈련 (Pretraining)**:
    * 기본 클래스를 사용하여 600 에폭 동안 네트워크를 사전 훈련합니다.
    * Adam optimizer를 사용하고 학습률($\text{lr}$)은 $0.001$로 설정합니다.
    * 데이터 전처리: 224x224 (저해상도 태스크는 84x84) 랜덤 크롭핑, 색상 지터링, 랜덤 수평 뒤집기, ImageNet 평균/표준편차 정규화.
5. **미세 조정 (Fine-tuning)**:
    * 새로운 클래스를 사용하여 네트워크를 재훈련합니다.
    * 세 가지 미세 조정 전략 비교:
        * `All`: 전체 네트워크 업데이트.
        * `BN & FC`: 배치 정규화(Batch Normalization, BN) 통계 및 FC 레이어 가중치 업데이트.
        * `FC`: FC 레이어 가중치만 업데이트 (과적합 방지를 위한 일반적인 방법).
    * 미니 배치 기반 학습을 사용하며, 배치 크기는 $N \times K$ (N-way K-shot 학습).
    * 미세 조정을 위한 학습률과 에폭 수는 검증 클래스(validation classes)를 사용하여 결정합니다.
    * 평가: 무작위 샘플링된 클래스와 예제를 사용하여 600회 시도의 평균 정확도와 95% 신뢰 구간을 계산합니다.
    * 테스트/초기화 단계에서의 입력 이미지 전처리: 256x256 리사이즈, 224x224 중앙 크롭핑, ImageNet 평균/표준편차 정규화.

## 📊 Results

* **저해상도 단일 도메인 태스크 (Table 1)**:
  * 1-shot 학습에서 VGG-16과 Normalized classifier를 사용했을 때 약 6%의 정확도 향상을 보였으며, 일반적인 소수 학습 방법보다 우수했습니다.
  * 5-shot 학습에서는 전체 네트워크를 업데이트했을 때 정확도가 더욱 향상되었으며, 최신 알고리즘과 거의 동등한 성능을 달성했습니다.
* **고해상도 단일 도메인 태스크 (Table 2)**:
  * 저해상도 태스크 대비 전반적으로 높은 정확도를 보였습니다.
  * VGG-16이 ResNet-152보다 저해상도 입력에 대한 강건성(robustness)이 더 우수함을 시사했습니다 (5-shot "Normalized All"에서 VGG-16은 4.3% 감소, ResNet-152는 11.0% 감소). 이는 저해상도 Mini-ImageNet 데이터셋의 유효성에 대한 재고를 시사합니다.
* **교차 도메인 태스크 (Table 3)**:
  * 고해상도 단일 도메인 태스크 대비 성능이 감소했는데, 이는 기본 클래스와 새로운 클래스 간의 큰 도메인 이동 때문으로 분석됩니다.
  * 전체 네트워크를 미세 조정함으로써 단일 도메인과 교차 도메인 태스크 간의 정확도 차이를 줄일 수 있었습니다 (예: VGG-16 5-shot에서 미세 조정 없이 15.9% 차이, 전체 미세 조정 시 6.9% 차이로 감소). 이는 네트워크가 큰 도메인 이동에 적응할 수 있음을 의미합니다.
* **기존 방법과의 비교 (Table 4)**:
  * 1-shot 저해상도 단일 도메인 태스크에서는 최신 알고리즘보다 낮았지만, MatchingNet 및 ProtoNet과 같은 다른 일반적인 소수 학습 방법보다 높았습니다.
  * 5-shot 저해상도 단일 도메인 태스크에서는 최신 방법과 거의 동일한 정확도를 달성했습니다.
  * 고해상도 단일 도메인 및 교차 도메인 태스크 모두에서 기존 방법보다 높은 분류 정확도를 달성했습니다.

## 🧠 Insights & Discussion

* **낮은 학습률의 중요성**: 미세 조정 시 학습률을 사전 훈련 단계보다 낮게 설정($0.0001$ 대 $0.001$)하면 재학습 과정이 더 안정적으로 진행됨을 확인했습니다. (Figure 2 참조)
* **적응형 경사 최적화기의 효과**: Adam, Adamax, Adadelta 등 적응형 경사 최적화기가 Momentum-SGD, ASGD와 같은 다른 최적화기보다 높은 분류 정확도를 달성했으며, 특히 Normalized classifier와 함께 사용될 때 더욱 두드러졌습니다. 이는 소수 학습 환경에서 적응형 최적화기의 일반화 능력이 우수할 수 있음을 시사합니다. (Figure 3 참조)
* **도메인 이동 시 전체 네트워크 업데이트**: 큰 도메인 이동이 발생하는 교차 도메인 태스크에서는 전체 네트워크를 업데이트하는 것이 BN 및 FC 레이어만 업데이트하거나 FC 레이어만 업데이트하는 것보다 더 높은 정확도를 달성했습니다. 이는 도메인 이동에 대응하기 위해 특징 추출기를 포함한 네트워크 전체의 조정이 필요함을 의미합니다. (Figure 4 참조)
* **저해상도 데이터셋의 유효성 재고**: 저해상도 Mini-ImageNet 데이터셋이 빠른 실험에 유용하지만, 다양한 특징 추출기의 저해상도 입력에 대한 강건성 차이가 소수 학습 성능 평가를 어렵게 만들 수 있음을 지적하며, 해당 데이터셋의 평가 유효성에 대한 재고를 제안합니다.

## 📌 TL;DR

소수 학습에서 과적합 문제 때문에 단순 미세 조정이 비효율적이라는 통념과 달리, 본 연구는 미세 조정이 놀랍도록 높은 성능을 달성할 수 있음을 보여줍니다. 특히 저해상도 Mini-ImageNet의 1-shot에서는 일반적인 방법보다, 5-shot에서는 최신 방법과 필적하는 성능을 내며, 고해상도 및 교차 도메인과 같은 실용적인 환경에서도 우수한 결과를 보였습니다. 낮은 학습률, Adam과 같은 적응형 최적화기 사용, 그리고 도메인 이동이 큰 경우 전체 네트워크를 업데이트하는 것이 미세 조정 성능을 극대화하는 핵심 요소임을 밝혀냈습니다.
