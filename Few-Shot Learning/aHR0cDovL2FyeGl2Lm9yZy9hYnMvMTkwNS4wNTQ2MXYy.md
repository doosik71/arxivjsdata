# Learning Generative Models across Incomparable Spaces

Charlotte Bunne, David Alvarez-Melis, Andreas Krause, Stefanie Jegelka

## 🧩 Problem to Solve

기존 생성 모델(예: GAN)은 생성 분포와 참조 분포가 동일하거나 적어도 비교 가능한 공간(예: 동일한 차원)에 존재해야 한다는 제약이 있습니다. 이는 생성된 샘플이 참조 분포와 구분할 수 없게 만드는 것이 목표일 때 적합합니다. 그러나 때로는 데이터의 매니폴드 구조나 클러스터링과 같은 관계적/위상적 측면만 학습하고, 동시에 차원, 스타일, 방향 등 다른 특성(예: 이미지의 두께 변경, 그래프 데이터를 유클리드 공간으로 매핑)을 변경해야 하는 경우가 있습니다. 이러한 **비교 불가능한 공간(incomparable spaces)** 간에 생성 모델을 학습하는 것이 기존 방법론으로는 어렵다는 문제가 있습니다.

## ✨ Key Contributions

* **비교 불가능한 공간을 위한 새로운 생성 모델 클래스 제안**: 차원이나 데이터 유형(예: 그래프에서 유클리드 공간)이 다른 공간 간에 분포를 학습할 수 있는 Gromov-Wasserstein GAN (GWGAN)을 소개합니다.
* **다양한 응용 분야 입증**: 매니폴드 학습, 관계형 학습, 도메인 간 학습 작업에 GWGAN을 성공적으로 적용하여 모델의 유용성을 시연합니다.
* **Gromov-Wasserstein 불일치(discrepancy)의 손실 함수 활용 개선**: 강력하고 효율적인 학습을 위해 Gromov-Wasserstein 거리의 정식화(formulation)를 수정하여 다양한 기계 학습 응용 프로그램에서 손실 함수로 사용할 수 있도록 합니다.
* **신경망에서 직교성(orthogonality)을 근사적으로 강제하는 새로운 접근 방식 제시**: 직교 프로크루스테스(Orthogonal Procrustes) 문제를 기반으로 하는 정규화 기법을 제안하며, 이는 GWGAN뿐만 아니라 다른 신경망 모델에도 적용될 수 있습니다.

## 📎 Related Works

* **Generative Adversarial Networks (GANs)**: Goodfellow et al. (2014)가 제안한 생성 모델로, 판별자와 생성자의 제로섬 게임을 통해 분포를 학습합니다. 불안정한 학습 문제가 있어 다양한 확장 연구(MMD, IPM, Optimal Transport 기반 GAN)가 진행되었습니다.
* **GANs and Optimal Transport (OT)**: Wasserstein GAN (Arjovsky et al., 2017)과 같은 OT 기반 GAN들은 샘플 간의 직접적인 비교를 통해 분포를 학습하지만, 비교 가능한 공간을 전제합니다 (Salimans et al., 2018; Genevay et al., 2018). 본 논문은 이보다 더 일반적인 OT 형태인 Gromov-Wasserstein 거리를 사용합니다.
* **GANs for Cross-Domain Learning**: 이미지 스타일 전이 (Zhu et al., 2017), 텍스트-이미지 생성 (Reed et al., 2016) 등 도메인 간 학습에 GAN이 활용되었으나, 일반적으로 조건부 변수, 정렬된 데이터 쌍 또는 사이클 일관성 제약(cycle consistency constraints)에 의존합니다.
* **Gromov-Wasserstein Learning**: Mémoli (2011)가 소개한 이래로 모양 매칭 (Solomon et al., 2016), 워드 임베딩 공간 정렬 (Alvarez-Melis & Jaakkola, 2018), 이기종 도메인 적응 (Yan et al., 2018) 등 다른 거리 공간 간의 연결에 의존하는 문제에 적용되었습니다. 본 연구는 이를 생성 모델링에 적용하는 새로운 틀을 제시합니다.

## 🛠️ Methodology

본 논문은 비교 불가능한 공간 간에 생성 모델을 학습하기 위해 **Gromov-Wasserstein Generative Adversarial Network (GWGAN)**를 제안합니다. 핵심 방법론은 다음과 같습니다.

1. **Gromov-Wasserstein (GW) 거리 활용**:
    * 기존 GAN과 달리, GWGAN은 샘플 간의 **공간 내 거리(intra-space distances)**를 비교하여 분포를 학습합니다. 이는 공간의 특정 특성이나 차원에 구애받지 않고, 관계적 정보를 보존합니다.
    * 주어진 두 분포 $(D,p)$와 $(\bar{D},q)$에 대해, GW 거리는 다음과 같이 정의됩니다:
        $$ GW(D, \bar{D}, p, q) := \min_{T \in U_{p,q}} \sum_{ijkl} L(D_{ik}, \bar{D}_{jl}) T_{ij} T_{kl} $$
        여기서 $D_{ik}$는 참조 공간 내 샘플 $i,k$ 간의 거리이고, $\bar{D}_{jl}$는 생성 공간 내 샘플 $j,l$ 간의 거리입니다. $T$는 두 분포 $p,q$ 사이의 커플링입니다.

2. **적대적 학습 프레임워크**:
    * 생성자 $g_{\theta}: Z \rightarrow Y$는 노이즈 $z$를 생성 공간 $Y$의 샘플로 매핑하는 신경망입니다.
    * 적대자 $f_{\omega}: X \rightarrow \mathbb{R}^s$는 데이터 공간 $X$와 생성 공간 $Y$의 샘플을 특징 공간 $\mathbb{R}^s$로 매핑하여 공간 내 거리 행렬 $D_{\check{\omega}}$와 $D_{\hat{\omega}}$를 계산하는 데 사용됩니다.
    * 생성자의 손실 최소화 문제는 미니맥스 문제로 확장됩니다:
        $$ \min_{\theta} \max_{\omega=(\check{\omega},\hat{\omega})} GW(D_{\check{\omega}}, D_{\hat{\omega}}, p, q) $$

3. **훈련 알고리즘 개선**:
    * **엔트로피 정규화된 GW**: 계산 비용이 높은 GW 문제를 효율적으로 해결하기 위해 엔트로피 정규화 (Peyré et al., 2016)를 적용합니다. 이를 통해 Sinkhorn-Knopp 알고리즘을 사용한 빠른 최적화와 미분 가능한 손실 함수를 얻습니다.
    * **GW 정규화**: 동일한 공간에서도 불일치 값이 0이 아니게 되는 엔트로피 정규화의 편향을 줄이기 위해 Wasserstein 거리와 유사하게 GW 값을 정규화합니다:
        $$ GW_{\epsilon}(D, \bar{D}, p, q) := 2 \times GW_{\epsilon}(D, \bar{D}, p, q) - GW_{\epsilon}(D, D, p, p) - GW_{\epsilon}(\bar{D}, \bar{D}, q, q) $$
    * **수치적 안정성 개선**: Sinkhorn 알고리즘의 안정화된 버전 (Schmitzer, 2016)과 로그 도메인 계산을 사용하여 수치적 불안정성을 방지합니다. 또한, Sinkhorn 반복에서는 정규화된 공간 내 거리를 사용하고, 최종 손실 계산에는 원본 거리를 사용하여 스케일 정보를 유지합니다.

4. **적대자 정규화 (Orthogonal Procrustes-based Regularization)**:
    * 적대자 $f_{\omega}$가 공간을 임의로 늘려 GW 거리를 최대화하는 것을 방지하기 위해, $f_{\omega}$가 근사적으로 유니타리(unitary) 변환을 정의하도록 강제합니다.
    * 이를 위해 새로운 직교 정규화 항을 도입합니다:
        $$ R_{\beta}(f_{\omega}(X), X) := \beta \|f_{\omega}(X) - XP^{*\top}\|_{F}^2 $$
        여기서 $P^{*}$는 $X$를 $f_{\omega}(X)$에 가장 가깝게 매핑하는 직교 행렬이며, 직교 프로크루스테스 문제를 풀어 구할 수 있습니다.

5. **생성자 제약 (Steering the Generator)**:
    * GW 손실은 관계적/기하학적 특성만 학습하므로, 생성된 분포의 다른 전역적 측면(예: 중심 위치, 이미지의 자연스러운 모양, 스타일)은 미결정 상태로 남습니다.
    * **$\ell_1$-정규화**: 생성된 샘플의 노름(norm)에 페널티를 부과하여 분포가 원점을 중심으로 하도록 강제할 수 있습니다.
    * **Total Variation 정규화**: 컴퓨터 비전 작업에서 생성된 이미지가 자연스러워 보이도록 돕습니다.
    * **스타일 적대자 (Style Adversary)**: 추가적인 스타일 참조가 주어질 때, 생성된 샘플의 스타일적 특성을 제어하기 위해 스타일 적대자 $c(g_{\theta}(z))$를 통합합니다. 생성자 목표는 $ \min_{\theta} \max_{\omega=(\check{\omega},\hat{\omega})} GW(D_{\check{\omega}}, D_{\hat{\omega}}, p, q) - \lambda \times c(g_{\theta}(z)) $가 됩니다.

## 📊 Results

* **동일 공간 학습**: 2D 가우시안 혼합 분포를 성공적으로 재현하며, $\ell_1$-정규화를 통해 분포의 중심을 원점에 맞출 수 있음을 보여줍니다. MNIST, Fashion-MNIST, CIFAR10 그레이스케일 이미지 생성에서도 성공적인 결과를 보였습니다. 이는 제안된 직교 프로크루스테스 정규화가 복잡한 분포 학습에 효과적임을 입증합니다.
* **차원 간 학습**: 2차원에서 3차원, 3차원 S-커브에서 2차원으로의 변환과 같이, 참조 분포와 다른 차원의 공간에서 성공적으로 분포를 학습하여 글로벌 구조와 모드 간의 상대적 거리를 보존함을 보여줍니다.
* **데이터 모달리티 및 매니폴드 간 학습**: 3차원 S-커브 데이터셋의 매니폴드 구조를 2차원 공간에서 성공적으로 재현했습니다. 또한, 절대적인 샘플 표현 없이 가중 그래프(관계 정보만 제공)에서 2D 유클리드 공간으로의 매핑을 통해 그래프의 이웃 구조를 근사적으로 재현하는 능력을 보여줍니다.
* **학습된 분포의 스타일 조정**: MNIST 숫자 생성 시, 추가적인 '스타일 적대자'를 통해 숫자의 두께를 굵게 변경하는 것을 성공적으로 시연합니다. 이는 GWGAN이 위상적 정보와 표면적 특성을 분리하여 학습할 수 있음을 보여줍니다.

## 🧠 Insights & Discussion

* **GWGAN의 유연성**: Gromov-Wasserstein 거리를 활용함으로써, GWGAN은 기존 GAN의 제약(동일 공간에서의 분포 비교)을 넘어섰습니다. 이는 차원이 다르거나 데이터 유형이 다른(예: 그래프 대 유클리드 공간) '비교 불가능한 공간'에서도 관계적 정보를 기반으로 분포를 학습할 수 있게 합니다.
* **새로운 생성 모델링 가능성**: 데이터 공간과 생성자 공간을 분리함으로써, 생성 모델링의 응용 분야가 크게 확장됩니다. 매니폴드 학습, 차원 축소, 관계형 학습과 같은 전통적인 문제에 대한 새로운 접근 방식을 제공하며, 데이터의 확률 분포까지 함께 학습합니다.
* **스타일 제어의 가능성**: GW 손실의 불변성(invariance)은 생성된 샘플의 스타일적 특성을 활발하게 조작할 수 있는 여지를 제공합니다. '스타일 적대자'와 같은 모듈형 프레임워크를 통해 이미지 두께 조절과 같은 특정 스타일 특성을 강제할 수 있으며, 이는 기존 GAN으로는 어려웠던 작업입니다.
* **적대자 정규화의 중요성**: 학습된 적대자가 공간을 왜곡하여 GW 거리를 인위적으로 최대화하는 것을 방지하기 위해 제안된 직교 프로크루스테스 기반 정규화는 모델의 안정적인 학습에 필수적입니다. 이는 다른 신경망 훈련에도 적용될 수 있는 일반적인 원칙을 제공합니다.
* **제한 및 미래 연구**: 본 논문은 숫자 두께 제어와 같은 간단한 실험을 통해 스타일 제어의 잠재력을 보여주었지만, 더 복잡한 스타일적 특성을 제어하기 위한 정교한 정규화 손실의 개발은 흥미로운 미래 연구 방향입니다.

## 📌 TL;DR

이 논문은 기존 GAN이 비교 불가능한 공간(예: 다른 차원, 다른 데이터 유형)에서 분포를 학습하기 어렵다는 문제를 해결하기 위해 **Gromov-Wasserstein GAN (GWGAN)**을 제안합니다. GWGAN은 샘플 간의 절대적인 비교 대신 **Gromov-Wasserstein 거리**를 사용하여 **공간 내 관계적/위상적 특성**을 학습합니다. 강력한 학습을 위해 엔트로피 정규화, GW 정규화, 수치적 안정화 기법을 도입했으며, 적대자가 공간을 왜곡하는 것을 막기 위해 **직교 프로크루스테스 기반 정규화**를 개발했습니다. 결과적으로 GWGAN은 동일/다른 차원 공간, 그래프 데이터, 이미지 스타일 변경 등 다양한 시나리오에서 성공적으로 분포를 학습하여 생성 모델링의 적용 범위를 크게 확장했습니다.
