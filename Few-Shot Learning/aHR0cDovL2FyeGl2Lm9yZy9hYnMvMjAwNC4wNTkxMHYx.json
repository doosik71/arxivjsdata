{
  "title": "Training few-shot classification via the perspective of minibatch and\n  pretraining",
  "authors": "Meiyu Huang, Xueshuang Xiang, Yao Xu",
  "year": 2020,
  "url": "http://arxiv.org/abs/2004.05910v1",
  "abstract": "Few-shot classification is a challenging task which aims to formulate the\nability of humans to learn concepts from limited prior data and has drawn\nconsiderable attention in machine learning. Recent progress in few-shot\nclassification has featured meta-learning, in which a parameterized model for a\nlearning algorithm is defined and trained to learn the ability of handling\nclassification tasks on extremely large or infinite episodes representing\ndifferent classification task, each with a small labeled support set and its\ncorresponding query set. In this work, we advance this few-shot classification\nparadigm by formulating it as a supervised classification learning problem. We\nfurther propose multi-episode and cross-way training techniques, which\nrespectively correspond to the minibatch and pretraining in classification\nproblems. Experimental results on a state-of-the-art few-shot classification\nmethod (prototypical networks) demonstrate that both the proposed training\nstrategies can highly accelerate the training process without accuracy loss for\nvarying few-shot classification problems on Omniglot and miniImageNet."
}