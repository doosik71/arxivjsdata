{
  "url": "http://arxiv.org/abs/1803.00676v1",
  "title": "Meta-Learning for Semi-Supervised Few-Shot Classification",
  "authors": "Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel",
  "year": 2018,
  "abstract": "In few-shot classification, we are interested in learning algorithms that\ntrain a classifier from only a handful of labeled examples. Recent progress in\nfew-shot classification has featured meta-learning, in which a parameterized\nmodel for a learning algorithm is defined and trained on episodes representing\ndifferent classification problems, each with a small labeled training set and\nits corresponding test set. In this work, we advance this few-shot\nclassification paradigm towards a scenario where unlabeled examples are also\navailable within each episode. We consider two situations: one where all\nunlabeled examples are assumed to belong to the same set of classes as the\nlabeled examples of the episode, as well as the more challenging situation\nwhere examples from other distractor classes are also provided. To address this\nparadigm, we propose novel extensions of Prototypical Networks (Snell et al.,\n2017) that are augmented with the ability to use unlabeled examples when\nproducing prototypes. These models are trained in an end-to-end way on\nepisodes, to learn to leverage the unlabeled examples successfully. We evaluate\nthese methods on versions of the Omniglot and miniImageNet benchmarks, adapted\nto this new framework augmented with unlabeled examples. We also propose a new\nsplit of ImageNet, consisting of a large set of classes, with a hierarchical\nstructure. Our experiments confirm that our Prototypical Networks can learn to\nimprove their predictions due to unlabeled examples, much like a\nsemi-supervised algorithm would."
}