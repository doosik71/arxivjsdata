{
  "title": "Meta-Learning Neural Procedural Biases",
  "authors": "Christian Raymond, Qi Chen, Bing Xue, Mengjie Zhang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2406.07983v1",
  "abstract": "The goal of few-shot learning is to generalize and achieve high performance\non new unseen learning tasks, where each task has only a limited number of\nexamples available. Gradient-based meta-learning attempts to address this\nchallenging task by learning how to learn new tasks by embedding inductive\nbiases informed by prior learning experiences into the components of the\nlearning algorithm. In this work, we build upon prior research and propose\nNeural Procedural Bias Meta-Learning (NPBML), a novel framework designed to\nmeta-learn task-adaptive procedural biases. Our approach aims to consolidate\nrecent advancements in meta-learned initializations, optimizers, and loss\nfunctions by learning them simultaneously and making them adapt to each\nindividual task to maximize the strength of the learned inductive biases. This\nimbues each learning task with a unique set of procedural biases which is\nspecifically designed and selected to attain strong learning performance in\nonly a few gradient steps. The experimental results show that by meta-learning\nthe procedural biases of a neural network, we can induce strong inductive\nbiases towards a distribution of learning tasks, enabling robust learning\nperformance across many well-established few-shot learning benchmarks."
}