# Reinforcement Learning for Reasoning in Large Language Models with One Training Example

Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen

## 🧩 Problem to Solve

이 논문은 **검증 가능한 보상 기반 강화 학습(RLVR, Reinforcement Learning with Verifiable Reward)**을 사용하여 대규모 언어 모델(LLM)의 추론 능력을 향상시키는 과정에서 **훈련 데이터셋을 얼마나 극단적으로 줄일 수 있는지**를 탐구합니다. 특히, 기존의 수천 개 데이터셋을 사용하는 RLVR과 비교하여, 최소한의 훈련 데이터로도 동등한 성능을 유지할 수 있는지에 대한 질문을 던집니다. 이는 RLVR의 데이터 효율성을 이해하고, LLM의 잠재된 추론 능력 활성화 메커니즘을 밝히는 데 목적이 있습니다.

## ✨ Key Contributions

- **1-shot RLVR의 놀라운 효과 입증**: 단 하나의 훈련 예제(1-shot RLVR)만으로도 수천 개의 예제를 사용한 RLVR과 동등한 수준의 수학적 추론 성능 향상을 달성했습니다.
  - Qwen2.5-Math-1.5B 모델의 MATH500 벤치마크 점수를 36.0%에서 73.6%로 향상(형식 보정 외 8.6% 개선).
  - 6개 수학 추론 벤치마크 평균 성능을 17.6%에서 35.7%로 향상(형식 보정 외 7.0% 개선).
  - 2개의 예제(2-shot)를 사용했을 때는 MATH500 74.8%, 평균 36.6%로 더 나은 결과를 보였습니다.
- **다양한 모델 및 알고리즘으로의 일반화**: 1(few)-shot RLVR의 효과는 Qwen2.5-Math-7B, Llama3.2-3B-Instruct 등 다양한 LLM과 GRPO, PPO 등 다양한 RL 알고리즘에서 일관되게 관찰되었습니다. 심지어 수학 예제로 훈련된 1-shot RLVR이 비수학적 추론 태스크(예: ARC)에서도 성능을 향상시켰습니다.
- **포화 후 일반화(Post-saturation Generalization) 현상 발견**: 단일 훈련 예제에 대한 훈련 정확도가 100%에 빠르게 도달한 후에도 모델의 테스트 정확도는 꾸준히 향상되는 현상을 발견했습니다. 이 현상은 수백만 번의 롤아웃 이후에야 과적합이 발생합니다.
- **교차 범주 일반화 및 자기 성찰 증가**: 단일 예제 훈련이 해당 범주 외의 다른 수학 범주에서도 성능을 향상시키는 교차 범주 일반화 능력을 보였습니다. 또한, 훈련이 진행됨에 따라 모델 응답의 길이와 'rethink', 'recheck'와 같은 자기 성찰(self-reflection) 용어의 사용 빈도가 증가했습니다.
- **정책 경사 손실의 주요 기여 및 탐색의 중요성**: 1-shot RLVR의 성능 개선은 주로 정책 경사 손실(policy gradient loss)에서 비롯되며, 이는 '그로킹(grokking)' 현상과 구별됩니다. 또한, 적절한 계수의 엔트로피 손실(entropy loss)을 추가하여 모델 출력의 탐색(exploration)을 촉진하는 것이 성능 향상에 필수적임을 보였습니다.
- **엔트로피 손실 단독 훈련의 부분적 효과**: 보상 신호 없이 엔트로피 손실만으로도 기본 모델 대비 약간의 성능 향상이 있었지만, 이는 주로 형식 보정(format correction) 효과에 기인하며 정책 경사 손실보다 효과가 미미했습니다.
- **레이블 견고성 및 프롬프트 수정에 대한 통찰**: 미미한 레이블 부정확성은 1-shot RLVR 성능에 큰 영향을 미치지 않지만, 잘못된 레이블이 너무 많을 경우 성능이 저하될 수 있음을 보여주었습니다.

## 📎 Related Works

- **RLVR의 발전**: LLM의 추론 능력을 향상시키는 데 RLVR이 효과적임이 입증되었으며, 주로 규칙 기반 검증을 통한 이진 보상(정답 여부)을 사용합니다. OpenAI-o1, DeepSeek-R1, Kimi-1.5 등이 이 분야의 주요 기여자입니다.
- **RL 알고리즘 개선**: PPO [7], GRPO [8]와 같은 RL 알고리즘의 성능과 안정성을 높이기 위한 많은 연구(예: VinePPO [9], VAPO [12], DAPO [11])가 진행되었습니다.
- **데이터 중심 RLVR 연구의 부족**: 고품질 수학 추론 데이터셋 구축 노력은 있었으나(예: DeepScaleR-Preview-Dataset [18]), RLVR 훈련 데이터의 역할에 대한 심층적인 탐구는 상대적으로 적었습니다.
- **데이터 선택 연구**: LLM 후처리 훈련을 위한 데이터 선택에 대한 선행 연구들은 주로 지도 미세 조정(supervised fine-tuning)이나 RLHF (Reinforcement Learning from Human Feedback) [55]에 중점을 두었습니다.
- **LIMR [19]**: RLVR 훈련 데이터셋을 1/6으로 줄이면서도 성능을 유지하는 방법을 제시했지만, 1-shot과 같은 극단적인 데이터 축소는 탐구하지 않았습니다.
- **동시 연구**: 4개의 예제로 PPO RLVR을 수행하여 상당한 개선을 보인 연구 [56]가 있었으나, 1-shot RLVR이 전체 데이터셋과 필적하는 성능을 낼 수 있다는 점을 체계적으로 탐구하지는 않았습니다.

## 🛠️ Methodology

1. **강화 학습 알고리즘**: 기본적으로 GRPO (Group-Normalized Policy Optimization) [8, 2]를 사용하며, PPO [7]로도 유효성을 검증합니다.
2. **손실 함수**: 세 가지 주요 구성 요소로 이루어집니다.
   - **정책 경사 손실($L'_{\text{PG-GRPO}}$)**: 모델이 더 높은 보상을 받는 응답을 생성하도록 유도합니다. 수학 문제의 경우, 정답 시 1, 오답 시 0의 이진 보상($r_i$)이 부여됩니다. 이 보상은 응답 그룹 내에서 정규화된 이점($A_i$)을 통해 가중됩니다.
     $$ L'_{\text{PG-GRPO}}(q, \{o_i\}_{i=1}^G, \theta) = - \frac{1}{G} \sum*{i=1}^G \min \left( \frac{\pi*{\theta}(o*i|q)}{\pi*{\theta}^{\text{old}}(o*i|q)} A_i, \text{clip} \left( \frac{\pi*{\theta}(o*i|q)}{\pi*{\theta}^{\text{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon \right) A_i \right) $$
        여기서 $A_i = \frac{r_i - \text{mean}(\{r_1, ..., r_G\})}{\text{std}(\{r_1, ..., r_G\})}$.
   - **KL 손실($L'_{\text{KL}}$)**: 현재 모델과 참조 모델 간의 KL 발산(Kullback-Leibler divergence)을 측정하여 언어 품질의 급격한 저하를 방지합니다.
   - **엔트로피 손실($L'_{\text{Entropy}}$)**: 음수 계수($\alpha < 0$)와 함께 적용되어 모델 출력의 토큰당 엔트로피를 높여 다양한 추론 경로를 탐색하도록 장려합니다.
3. **데이터 선택**: 1-shot RLVR에 사용할 "최적의" 단일 예제를 찾기 위해, 전체 데이터셋에 대한 RLVR 훈련 중 각 예제의 훈련 정확도 이력 분산(historical variance score)을 계산하고, 분산이 높은 예제들을 잠재적인 후보로 선정합니다. 본 연구에서는 $\pi_1$과 $\pi_{13}$ 예제가 높은 성능을 보였습니다.
4. **실험 설정**:
   - **기본 모델**: Qwen2.5-Math-1.5B를 사용하며, 다양한 규모의 Qwen 및 Llama 모델로 확장 검증합니다.
   - **훈련 데이터**: DeepScaleR-Preview-Dataset [18]에서 추출한 1209개 예제의 "DSR-sub"를 주로 사용합니다. 1-shot 훈련 시에는 단일 예제를 배치 크기에 맞춰 복제하여 사용합니다.
   - **평가 벤치마크**: MATH500 [27, 29], AIME 2024/2025 [30], AMC 2023 [31], Minerva Math [32], OlympiadBench [33] 등 6가지 수학 벤치마크와 ARC-Easy/Challenge [34] 등의 비수학적 추론 벤치마크를 활용합니다. AIME, AMC와 같이 문항 수가 적은 벤치마크는 $\text{avg}@8$ (온도 0.6에서 8회 샘플링 후 평균)을 보고하여 안정성을 확보합니다.

## 📊 Results

- **기본 성능 향상**: Qwen2.5-Math-1.5B 모델이 MATH500에서 36.0%에서 73.6%로, 6개 벤치마크 평균에서 17.6%에서 35.7%로 크게 향상되었습니다. 이는 1209개 예제를 사용한 RLVR의 성능과 거의 일치하며, 2개 예제만으로는 오히려 더 높은 성능을 달성했습니다.
- **범용성 입증**: Qwen2.5-Math-7B, Llama3.2-3B-Instruct 등 다른 모델과 PPO 알고리즘에서도 1(few)-shot RLVR이 효과적임을 확인했습니다.
- **포화 후 일반화(Post-saturation Generalization)**: 단일 예제 훈련에서 훈련 정확도가 100%에 도달한 후에도 테스트 정확도는 수백, 수천 스텝 동안 계속해서 개선되었습니다.
- **교차 범주 일반화**: 기하학 문제로 훈련된 모델이 대수학, 정수론 등 다른 수학 범주에서도 성능이 향상되는 것을 관찰했습니다.
- **자기 성찰 행동 증가**: 훈련이 진행됨에 따라 모델의 응답 길이가 길어지고 'rethink', 'recheck', 'recalculate'와 같은 자기 성찰 용어 사용 빈도가 증가했습니다.
- **손실 함수의 기여**: Ablation study 결과, 정책 경사 손실이 1-shot RLVR 성능 개선의 주요 원동력이며, 엔트로피 손실이 탐색을 촉진하여 추가적인 성능 향상을 가져왔습니다.

## 🧠 Insights & Discussion

- **LLM 내재 추론 능력의 활성화**: 1-shot RLVR의 성공은 LLM이 이미 강력한 추론 능력을 내재하고 있으며, 극히 적은 양의 데이터로도 이 잠재력을 효과적으로 '점화(ignite)'할 수 있음을 강력히 시사합니다. 이는 RL 단계에서 데이터 효율적인 모델 활성화 전략의 중요성을 부각합니다.
- **RLVR의 암묵적 정규화 효과**: '포화 후 일반화' 현상은 RLVR이 단순한 암기를 넘어 모델이 새로운 추론 전략을 탐색하고 일반화하는 능력을 촉진한다는 것을 보여줍니다. 정책 경사 손실과 엔트로피 손실이 결합하여, 모델이 학습된 예제의 정확성을 유지하면서도 다양한 출력을 탐색하도록 장려하는 '암묵적 정규화' 역할을 할 수 있습니다.
- **데이터 선택 및 큐레이션의 중요성**: 모든 단일 예제가 동일한 효과를 내는 것은 아니므로, RLVR 훈련에 가장 유용한 예제를 식별하고 큐레이션하는 것이 미래 연구의 중요한 방향임을 강조합니다.
- **탐색(Exploration)의 핵심 역할**: 엔트로피 손실이 1-shot RLVR에서 중요한 기여를 하는 것은 모델이 솔루션 공간 내에서 더 다양한 출력을 탐색하도록 장려하는 것이 LLM의 다운스트림 태스크 일반화에 중대한 영향을 미친다는 통찰을 제공합니다.
- **한계**: 대규모 모델(Qwen2.5-32B)에 대한 미적용, 수학 영역에만 집중, 1-shot RLVR이 반드시 계산 비용을 절약하지 않을 수 있다는 점, 그리고 $\pi_1$ 예제가 모든 모델에 최적의 1-shot 예제가 아닐 수 있다는 점 등이 한계로 지적되었습니다.

## 📌 TL;DR

이 논문은 **단 하나의 훈련 예제(1-shot)만으로도 LLM의 수학적 추론 능력을 강화 학습(RLVR)을 통해 대폭 향상**시킬 수 있음을 보여줍니다. Qwen2.5-Math-1.5B 모델은 MATH500에서 36.0% $\to$ 73.6%로, 6개 벤치마크 평균에서 17.6% $\to$ 35.7%로 성능이 향상되었는데, 이는 수천 개의 예제를 사용한 RLVR과 동등하거나 심지어 능가하는 결과입니다. 저자들은 훈련 정확도가 포화된 후에도 테스트 성능이 개선되는 '포화 후 일반화' 현상과 교차 범주 일반화를 발견했습니다. 이러한 개선은 주로 정책 경사 손실과 엔트로피 손실을 통한 탐색 촉진에서 비롯되며, LLM이 이미 강력한 추론 능력을 내재하고 있으며, 아주 적은 데이터로도 이를 활성화할 수 있음을 시사합니다.
