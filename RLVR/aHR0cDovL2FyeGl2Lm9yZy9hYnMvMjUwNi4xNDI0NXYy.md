# REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS IMPLICITLY INCENTIVIZES CORRECT REASONING IN BASE LLMS

Xumeng Wen, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, Xiao Liang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang

## 🧩 Problem to Solve

최근 검증 가능한 보상을 사용하는 강화 학습 (RLVR)이 대규모 언어 모델(LLM)의 장기 연쇄 사고(CoT) 추론 능력을 향상시킬 수 있다는 가능성으로 큰 관심을 받고 있습니다. 그러나 RLVR이 LLM의 추론 능력을 진정으로 향상시키는지, 아니면 단순히 샘플링 효율성만 높이는지에 대한 논쟁이 지속되고 있습니다. 특히, 기존 모델에 모든 올바른 추론 경로가 이미 존재하며 RLVR이 이러한 경로의 샘플링 확률만 조절한다는 가설이 제기되었습니다. 이 연구는 RLVR이 LLM 추론에 미치는 영향을 체계적으로 조사하여 이러한 논쟁을 해결하고자 합니다.

## ✨ Key Contributions

- **확장된 추론 능력 경계 입증**: RLVR이 코드 및 수학 문제 모두에서 LLM의 추론 능력 경계를 확장함을 체계적인 평가를 통해 밝혀냈습니다.
  - 특히 수학 문제의 경우, 최종 답변뿐만 아니라 중간 추론 단계의 정확성까지 평가하는 새로운 지표인 CoT-Pass@K를 도입했습니다.
- **RLVR의 이론적 이해**: 답변 정확성만을 보상으로 사용할 때도 RLVR이 어떻게 올바른 추론을 암묵적으로 장려하는지에 대한 이론적 프레임워크를 제시했습니다. 이는 LLM이 사전 학습을 통해 확립한 강력한 "논리 사전 지식(Logic Prior)"에 기반합니다.
- **RLVR 학습 동역학 분석**: RLVR의 학습 과정을 분석하여 최적화 효과, 일반화 행동 및 현재 한계를 파악했습니다.
- **추론 CoT 품질 향상 확인**: 학습 관점에서 생성된 CoT의 품질이 RLVR 후에 근본적으로 개선되었음을 확인했으며, 이는 지도 미세 조정(SFT)을 통해 RLVR 모델의 일반화 능력을 재현할 수 있음을 보여줍니다.

## 📎 Related Works

- **RLVR**: DeepSeek-R1 (Guo et al., 2025)이 Group Relative Policy Optimization (GRPO) 알고리즘을 사용하여 긴 CoT 추론을 성공적으로 재현한 이후, RLVR 패러다임에 대한 연구가 급증했습니다. 이는 LLM이 경험을 통해 학습하고 무한한 지능을 가질 수 있는 잠재력을 제시합니다.
- **RLVR의 효과에 대한 논쟁**: Yue et al. (2025)은 RLVR이 Pass@1 지표는 개선하지만, 특정 K 값 이상에서 Pass@K 지표가 기본 모델에 비해 향상되지 않거나 심지어 뒤처지는 현상을 발견했습니다. 이는 RLVR이 단순히 샘플링 효율성을 개선할 뿐 추론 능력을 감소시킨다는 가설로 이어졌습니다. 일부 연구는 이 가설을 지지했지만(Zhu et al., 2025; Zhang et al., 2025), 다른 연구들은 모순되는 결과를 보고했습니다(Liu et al., 2025a; Chen et al., 2025b).
- **CoT의 정확성 중요성**: Arcuschin et al. (2025), McGinness & Baumgartner (2025), Shojaee et al. (2025) 등 최근 연구들은 CoT의 정확성을 검증하는 것의 중요성을 강조했습니다. 본 연구는 수학 및 코드와 같은 비구조적 추론 시나리오에서 LLM을 CoT 검증자로 사용하는 패러다임을 제안합니다.

## 🛠️ Methodology

1. **Pass@K 실험 재검토**:
   - 기존 Pass@K 실험을 재현하고, 특히 수학 문제에서 기본 LLM이 부정확한 CoT로도 우연히 정답을 맞힐 수 있는 경우를 식별했습니다.
   - 코드 추론의 경우, 실행 기반 검증으로 인해 추측 가능성이 적어 Pass@K가 신뢰할 수 있는 지표임을 확인했습니다.
2. **CoT-Pass@K 지표 도입 (수학)**:
   - 최종 답변과 중간 추론 CoT 모두가 정확할 때만 성공으로 평가하는 CoT-Pass@K를 도입했습니다.
   - **LLM-as-a-CoT-Judge**: DeepSeek-R1-0528-Qwen3-8B와 같은 특화된 LLM을 CoT 검증자로 활용했습니다.
   - **다중 검증 전략**: 개별 검증의 오류를 완화하기 위해 `n=3`회의 독립적인 검증 시도를 수행하고, "any-correct", "all-correct", "majority-correct" 세 가지 집계 전략을 사용했습니다.
3. **RLVR의 이론적 프레임워크 구축**:
   - **문제 설정**: 질문 프롬프트 $q$에 대해 정책 $\pi_{\theta}$로부터 $G$개의 응답 $Y = \{y^1, \dots, y^G\}$를 샘플링하고, 각 응답 $y_i$는 CoT $c_i$와 최종 답변 $a_i$를 포함합니다.
   - **정확성 지표**: CoT 정확성 $I_{\text{CoT}}(c_i)$와 답변 정확성 $I_{\text{Ans}}(a_i)$를 정의합니다. 보상 $R(y_i)$는 $I_{\text{Ans}}(a_i)$에 의해서만 결정되는 이진값으로 가정합니다.
   - **논리 사전 지식(Logic Prior) 가정**: 올바른 CoT는 틀린 CoT보다 올바른 답변을 유도할 확률이 높다는 가정 ($P(I_{\text{Ans}}(a_i) = 1|I_{\text{CoT}}(c_i) = 1) = \alpha > P(I_{\text{Ans}}(a_i) = 1|I_{\text{CoT}}(c_i) = 0) = \beta$)을 도입했습니다.
   - **이론 증명**: GRPO 기울기(gradient)가 올바른 CoT($p_{\theta}^c$)를 생성할 확률을 증가시킨다는 Theorem 1을 증명했습니다.
4. **RLVR 학습 동역학 분석**:
   - 오픈 소스 DAPO 레시피(Yu et al., 2025)를 사용하여 GRPO 스타일 학습을 재현하고 검증했습니다.
   - 학습 과정 중 올바른 답변 생성 확률 $P(\text{CA})^{(q)}$와 올바른 답변 내에서 올바른 CoT 생성 확률 $P(\text{CC|CA})^{(q)}$를 추적하여 최적화 효과와 일반화 행동을 분석했습니다.
5. **추론 CoT의 품질 평가**:
   - 다른 모델에 의해 생성된 CoT로 동일한 기본 LLM에서 SFT를 수행했습니다.
   - SFT 후 모델의 테스트 세트 일반화 성능을 (Pass@1) CoT 데이터의 품질을 측정하는 대리 지표로 사용했습니다.

## 📊 Results

- **수학 추론 (CoT-Pass@K)**: 일반 Pass@K는 기본 LLM이 RLVR 모델(DAPO-Qwen-32B)에 근접하거나 능가하는 경향을 보였으나, CoT-Pass@K를 적용한 결과 AIME 2024 및 AIME 2025에서 RLVR 모델이 기본 LLM보다 지속적으로 유의미한 성능 격차를 보였습니다. 이는 RLVR이 추론 능력 경계를 확장함을 시사합니다. MATH-500, AMC23에서는 효과가 덜 두드러졌고, Minerva에서는 도메인 불일치로 인해 개선이 없었습니다.
- **코드 추론 (Pass@K)**: AceReason-Nemotron-7B (RLVR 적용 모델)는 DeepSeek-R1-Distill-Qwen-7B (정제된 기본 모델)에 비해 LiveCodeBench 여러 버전에서 명확한 Pass@K 향상을 보여주며, RLVR이 경쟁 코딩 작업에서 추론 능력 경계를 확장할 수 있음을 입증했습니다.
- **학습 동역학**: RLVR 학습 초기에 올바른 추론 CoT($P(\text{CC|CA})^{(q)}$) 생성이 증가하기 시작하고, 이러한 능력은 보지 못한 테스트 문제에도 잘 일반화됨을 확인했습니다. 이는 RLVR이 답변 정확성뿐만 아니라 암묵적으로 올바른 추론을 장려한다는 이론적 예측과 일치합니다.
- **CoT 품질 향상**: RLVR을 거친 모델에서 생성된 CoT 데이터로 SFT를 수행한 결과, SFT 모델이 RLVR 모델과 거의 동일한 Pass@1 성능을 보였습니다. 이는 RLVR이 계산 비용이 많이 드는 RLVR 훈련 없이도 모델의 추론 능력을 복제할 수 있는 고품질의 CoT를 생성함을 나타냅니다.

## 🧠 Insights & Discussion

- **RLVR의 진정한 가치**: CoT-Pass@K와 이론적 분석은 RLVR이 단순히 샘플링 효율성을 넘어 LLM의 추론 능력을 근본적으로 향상시킨다는 강력한 증거를 제공합니다. 이는 이전 연구들의 상반된 결론을 해소하는 데 도움이 됩니다.
- **암묵적인 추론 장려**: LLM이 올바른 CoT가 올바른 답변으로 이어질 가능성이 더 높다는 "논리 사전 지식"을 갖추고 있다면, GRPO는 답변 정확성만을 보상으로 사용하더라도 올바른 추론을 암묵적으로 장려할 수 있습니다. 이는 전통적인 강화 학습과의 중요한 차이점입니다.
- **한계점 및 개선 방향**:
  - **LLM 검증자의 한계**: CoT 검증을 위해 LLM-as-a-CoT-Judge를 사용하는 것은 유용하지만 완벽하지 않으며, 거짓 양성/음성 문제가 발생할 수 있습니다. 향후에는 더욱 가볍고 신뢰할 수 있는 CoT 검증자 개발이 필요합니다.
  - **학습 질문 최적화의 한계**: DAPO 학습 질문의 경우, 최종 답변 정확성이 1.0에 도달하더라도 완벽하지 않은 CoT가 상당 부분 남아있을 수 있으며, 이는 순전히 답변 정확성만으로는 모든 추론 결함을 완화하기 어렵다는 것을 시사합니다.
- **RLVR의 확장**: LLM의 사전 학습 규모를 확장하는 것만큼 RLVR의 규모를 확장하는 것이 인공 일반 지능(AGI) 시대로 나아가는 데 중요할 수 있습니다. 경험을 통한 학습은 다음 도약이 될 수 있습니다.
- **새로운 RLVR 알고리즘**: RLVR이 올바른 추론을 암묵적으로 장려한다는 통찰력을 바탕으로, 기본 LLM의 내재된 논리적 편향을 완화하고 올바른 추론 경로를 직접적으로 장려하는 새로운 알고리즘 패러다임 개발이 기대됩니다.

## 📌 TL;DR

- **문제**: RLVR이 LLM의 추론 능력을 실제로 향상시키는지 아니면 샘플링 효율성만 높이는지에 대한 논쟁이 있었습니다.
- **방법**: 이 연구는 Pass@K 실험을 재검토하고, 수학 문제의 경우 답변과 CoT 정확성을 모두 평가하는 CoT-Pass@K 지표(LLM을 CoT 검증자로 활용)를 도입하여 RLVR의 효과를 측정했습니다. 또한, "논리 사전 지식" 가정을 통해 답변 기반 보상이 어떻게 올바른 추론을 암묵적으로 장려하는지 이론적 프레임워크를 제시하고, 학습 동역학을 분석하며, SFT를 통해 RLVR 생성 CoT의 품질을 평가했습니다.
- **결과**: RLVR은 CoT-Pass@K에서 수학 문제의 추론 경계를, Pass@K에서 코드 문제의 추론 경계를 유의미하게 확장시킴을 실증적으로 보여주었습니다. 이론적으로는 LLM이 강력한 사전 지식(Logic Prior)을 가질 때 답변 정확성을 통한 보상이 올바른 추론을 암묵적으로 장려하며, 학습 분석 결과 CoT 품질이 초기부터 개선되어 일반화됨을 확인했습니다. RLVR로 생성된 고품질 CoT는 비용이 많이 드는 RLVR 훈련 없이도 유사한 성능을 재현하는 데 사용될 수 있습니다.
