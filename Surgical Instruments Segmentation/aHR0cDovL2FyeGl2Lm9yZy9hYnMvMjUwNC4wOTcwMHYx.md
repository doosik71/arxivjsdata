# ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical Instrument Tip Detection
Zijian Wu, Shuojue Yang, Yueming Jin, and Septimiu E. Salcudean

## Problem to Solve
로봇 보조 복강경 전립선 절제술(RALP)에서 수술 도구 끝(tip)의 정확한 위치를 파악하는 것은 초음파 프레임과 복강경 카메라 프레임을 정합하는 데 중요합니다. 기존 다빈치(da Vinci) API를 통해 얻는 도구 끝 위치는 부정확하며 수동 보정이 필요하다는 한계가 있었습니다. 따라서 시각 기반(vision-based) 방법을 통해 카메라 프레임에서 직접 도구 끝 위치를 계산하는 것이 매력적인 해결책으로 부상했습니다. 또한, 수술 AI(인공지능) 분야에서 수술 도구 끝 감지는 자세 추정, 기술 평가, 수술 자동화 등 많은 다른 작업의 핵심 구성 요소입니다. 그러나 도구 끝의 **작은 크기**와 수술 도구의 **관절 움직임(articulation)**으로 인해 이 작업은 매우 어렵습니다. 기존 딥러닝 방법들은 특정 그리퍼 끝에 초점을 맞추지 않았습니다.

## Key Contributions
*   수술 도구의 부분별(part-level) 분할 마스크를 입력으로 사용하여 딥러닝 기반 수술 도구 끝 감지 접근 방식인 **ToolTipNet**을 제안했습니다.
*   **Segment Anything**과 같은 분할 파운데이션 모델의 발전을 활용하여 수술 도구 분할이 비교적 쉬워진 점에 착안, 마스크 정보만으로 도구 끝 위치를 추정하는 새로운 패러다임을 탐구했습니다.
*   성능 향상을 위해 그리퍼 부분 마스크를 사용하여 **마스크 가이드 어텐션(mask-guided attention)** 맵을 생성하고 이를 융합된 특징 맵에 곱하여 도구 끝의 기하학적 특성을 활용했습니다.
*   모의(simulated) 및 실제(real) 데이터셋에서 수작업 이미지 처리 접근 방식(`SVD-based method`)과의 비교 실험을 통해 제안된 방법의 우수성을 입증했습니다.

## Methodology
### ToolTipNet 아키텍처
*   **입력:** 도구의 부분별 분할 마스크.
*   **백본:** `HRNet-w32` (PyTorch Image Models, `timm`에서 사전 학습된 모델 사용). 이는 키포인트 감지와 같은 위치 민감(position-sensitive) 작업에 필수적인 고해상도 표현을 유지하는 데 적합합니다.
*   **특징 추출:** 다중 해상도(1/16, 1/8, 1/4, 1/2) 특징 피라미드를 추출합니다.
*   **특징 융합(Feature Fusion):** 추출된 특징들을 `(H/4, W/4)` 크기의 단일 특징으로 융합합니다.
*   **마스크 가이드 어텐션(Mask-Guided Attention):** 그리퍼 영역의 기하학적 속성(예: 모서리 점)을 활용하기 위해 그리퍼 마스크에서 어텐션 맵을 생성하고 이를 융합된 특징 맵에 곱합니다.
*   **예측 헤드(Prediction Head):** 최종 특징 맵을 입력받아 도구 끝의 히트맵(heatmap)을 출력하여 위치를 예측합니다.

### 비교를 위한 이미지 처리 기반 도구 끝 감지
*   분할 마스크를 기반으로 수작업 특징을 사용하여 수술 도구 끝을 결정하는 방법으로, `Singular Value Decomposition (SVD)`를 사용하여 그리퍼 마스크의 주축(principal axis)을 계산합니다. 손목에서 가장 멀리 떨어진 픽셀을 도구 끝으로 간주합니다.

### 구현 세부 사항
*   **손실 함수:** $L = 0.1 \cdot L_1 + 100 \cdot L_{MSE}$를 사용하며, 여기서 $L_1$은 도구 끝 좌표의 L1 손실, $L_{MSE}$는 히트맵의 MSE 손실입니다.
*   **데이터 전처리:** 모든 마스크는 512x640으로 크기 조정됩니다.
*   **데이터 증강:** 무작위 뒤집기(random flip) 및 스케일(scale) 변환을 적용합니다.
*   **훈련:** Adam 옵티마이저를 사용하여 100 에포크 동안 훈련하며, 초기 학습률은 1e-4, 배치 크기는 12입니다. 학습률은 `Cosine Annealing` 스케줄에 따라 변경됩니다.

## Results
*   **데이터셋:** 8,092개의 모의 마스크와 266개의 실제 마스크로 구성된 혼합 데이터셋으로 훈련했습니다. 제안된 `ToolTipNet`과 SVD 기반 방법은 모의 데이터(1,823 마스크) 및 실제 데이터(96 마스크)에서 테스트되었습니다.
*   **평가 지표:**
    1.  예측값과 실제 도구 끝 위치 간의 `Root Mean Squared Error (RMSE)`
    2.  수술 도구 끝 예측의 **정확도(Accuracy)** (RMSE가 10픽셀 미만일 경우 정확하다고 정의)
*   **정량적 결과 (Table I 요약):**
    | Methods        | Sim. Dataset (RMSE↓) | Sim. Dataset (Accuracy↑) | Real Dataset (RMSE↓) | Real Dataset (Accuracy↑) |
    | :------------- | :------------------- | :----------------------- | :------------------- | :----------------------- |
    | Yang et al. [7] | 28.14                | 0.287                    | 27.64                | 0.583                    |
    | Ours (`ToolTipNet`) | **3.73**             | **0.959**                | **9.04**             | **0.813**                |
*   **정성적 관찰:** 수작업 이미지 처리 방법은 두 그리퍼가 서로 가까이 있을 때 종종 잘못된 위치를 예측했습니다. 이는 좌우 그리퍼 마스크를 두 개의 연결된 영역으로 분리하기 어렵기 때문으로 추정됩니다. 이 상황에서 SVD는 정확한 주축을 계산할 수 없어 올바른 위치를 예측하지 못했습니다. `ToolTipNet`은 이러한 어려운 상황에서도 SVD 기반 방법에 비해 훨씬 **낮은 RMSE**와 **높은 정확도**를 보여주며 우수한 성능을 입증했습니다.