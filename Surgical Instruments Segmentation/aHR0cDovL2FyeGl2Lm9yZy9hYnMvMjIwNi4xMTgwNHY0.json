{
  "title": "Rethinking Surgical Instrument Segmentation: A Background Image Can Be\n  All You Need",
  "authors": "An Wang, Mobarakol Islam, Mengya Xu, Hongliang Ren",
  "year": 2022,
  "url": "http://arxiv.org/abs/2206.11804v4",
  "abstract": "Data diversity and volume are crucial to the success of training deep\nlearning models, while in the medical imaging field, the difficulty and cost of\ndata collection and annotation are especially huge. Specifically in robotic\nsurgery, data scarcity and imbalance have heavily affected the model accuracy\nand limited the design and deployment of deep learning-based surgical\napplications such as surgical instrument segmentation. Considering this, we\nrethink the surgical instrument segmentation task and propose a one-to-many\ndata generation solution that gets rid of the complicated and expensive process\nof data collection and annotation from robotic surgery. In our method, we only\nutilize a single surgical background tissue image and a few open-source\ninstrument images as the seed images and apply multiple augmentations and\nblending techniques to synthesize amounts of image variations. In addition, we\nalso introduce the chained augmentation mixing during training to further\nenhance the data diversities. The proposed approach is evaluated on the real\ndatasets of the EndoVis-2018 and EndoVis-2017 surgical scene segmentation. Our\nempirical analysis suggests that without the high cost of data collection and\nannotation, we can achieve decent surgical instrument segmentation performance.\nMoreover, we also observe that our method can deal with novel instrument\nprediction in the deployment domain. We hope our inspiring results will\nencourage researchers to emphasize data-centric methods to overcome demanding\ndeep learning limitations besides data shortage, such as class imbalance,\ndomain adaptation, and incremental learning. Our code is available at\nhttps://github.com/lofrienger/Single_SurgicalScene_For_Segmentation."
}