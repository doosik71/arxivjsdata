{
  "title": "Rethinking Text-Promptable Surgical Instrument Segmentation with Robust\n  Framework",
  "authors": "Tae-Min Choi, Juyoun Park",
  "year": 2024,
  "url": "http://arxiv.org/abs/2411.12199v3",
  "abstract": "Surgical instrument segmentation is an essential component of\ncomputer-assisted and robotic surgery systems. Vision-based segmentation models\ntypically produce outputs limited to a predefined set of instrument categories,\nwhich restricts their applicability in interactive systems and robotic task\nautomation. Promptable segmentation methods allow selective predictions based\non textual prompts. However, they often rely on the assumption that the\ninstruments present in the scene are already known, and prompts are generated\naccordingly, limiting their ability to generalize to unseen or dynamically\nemerging instruments. In practical surgical environments, where instrument\nexistence information is not provided, this assumption does not hold\nconsistently, resulting in false-positive segmentation. To address these\nlimitations, we formulate a new task called Robust text-promptable Surgical\nInstrument Segmentation (R-SIS). Under this setting, prompts are issued for all\ncandidate categories without access to instrument presence information. R-SIS\nrequires distinguishing which prompts refer to visible instruments and\ngenerating masks only when such instruments are explicitly present in the\nscene. This setting reflects practical conditions where uncertainty in\ninstrument presence is inherent. We evaluate existing segmentation methods\nunder the R-SIS protocol using surgical video datasets and observe substantial\nfalse-positive predictions in the absence of ground-truth instruments. These\nfindings demonstrate a mismatch between current evaluation protocols and\nreal-world use cases, and support the need for benchmarks that explicitly\naccount for prompt uncertainty and instrument absence."
}