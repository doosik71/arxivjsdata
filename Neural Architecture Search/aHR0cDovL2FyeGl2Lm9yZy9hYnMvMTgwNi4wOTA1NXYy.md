# DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH

Hanxiao Liu, Karen Simonyan, Yiming Yang

## 🧩 Problem to Solve

기존 신경망 아키텍처 탐색(NAS) 방법론(예: 강화 학습 또는 진화 알고리즘 기반)은 이산적이고 미분 불가능한 탐색 공간에서 작동하여 막대한 계산 자원(수천 GPU-일)을 필요로 하는 확장성 문제가 있었습니다. 이러한 방법은 아키텍처 탐색을 블랙박스 최적화 문제로 취급하여 수많은 아키텍처 평가가 필요했습니다. 이 논문은 이러한 비효율성을 극복하고 계산 비용을 크게 줄이면서 고성능 아키텍처를 찾을 수 있는 방법을 제안합니다.

## ✨ Key Contributions

- **미분 가능한 아키텍처 탐색 알고리즘 제안:** 컨볼루션 및 순환 아키텍처 모두에 적용 가능한 이중 수준 최적화(bilevel optimization) 기반의 미분 가능한 신경망 아키텍처 탐색(DARTS) 알고리즘을 소개합니다.
- **경쟁력 있는 성능 및 효율성:** CIFAR-10 이미지 분류에서 최고 수준의 결과를 달성하고 Penn Treebank (PTB) 언어 모델링에서 기존 방법들을 능가하는 성능을 보였습니다. 이는 기존 비미분적 탐색 방법(강화 학습, 진화)에 비해 수백, 수천 배 빠른 계산 효율성(단 몇 GPU-일)으로 달성되었습니다.
- **전이 가능성 입증:** CIFAR-10에서 학습된 컨볼루션 셀과 PTB에서 학습된 순환 셀이 각각 ImageNet 및 WikiText-2로 성공적으로 전이되어 좋은 성능을 보임을 입증했습니다.
- **공개된 구현:** 추가 연구를 촉진하기 위해 DARTS의 구현을 공개했습니다.

## 📎 Related Works

- **강화 학습(RL) 기반 NAS:** Zoph & Le (2017), Zoph et al. (2018), Pham et al. (2018b), Baker et al. (2017) 등은 컨트롤러를 사용하여 아키텍처를 생성하고, 검증 성능을 보상으로 받아 학습합니다. 높은 성능을 보였으나 엄청난 계산 비용이 단점입니다.
- **진화 알고리즘 기반 NAS:** Real et al. (2018), Liu et al. (2018b) 등은 진화 전략을 통해 아키텍처를 탐색합니다. 역시 높은 계산 비용이 필요합니다.
- **기타 비미분적 탐색:** MCTS (Negrinho & Gordon, 2017), SMBO (Liu et al., 2018a), Bayesian optimization (Kandasamy et al., 2018) 등의 방법도 이산적 탐색 공간 문제를 겪습니다.
- **효율성 개선 노력:** 특정 탐색 공간 구조 (Liu et al., 2018b;a), 가중치/성능 예측 (Brock et al., 2018; Baker et al., 2018), 가중치 공유/상속 (Elsken et al., 2017; Pham et al., 2018b; Cai et al., 2018; Bender et al., 2018) 등 아키텍처 탐색 속도 개선을 위한 여러 시도가 있었습니다. ENAS (Pham et al., 2018b)는 가중치 공유를 통해 효율성을 높인 대표적인 예시입니다.
- **연속적 탐색 공간 아이디어:** Saxena & Verbeek (2016), Ahmed & Torresani (2017), Veniat & Denoyer (2017), Shin et al. (2018) 등은 필터 모양이나 브랜칭 패턴과 같은 특정 아키텍처 측면을 미세 조정하기 위해 연속적 도메인에서 아키텍처를 탐색하는 아이디어를 시도했습니다. DARTS는 더 복잡한 그래프 토폴로지의 빌딩 블록 학습이 가능합니다.

## 🛠️ Methodology

DARTS는 아키텍처 탐색 문제를 미분 가능하게 공식화하여 경사 하강법으로 효율적인 탐색을 가능하게 합니다.

1. **탐색 공간 정의 (Search Space):**

   - 최종 아키텍처의 빌딩 블록인 '계산 셀(computational cell)'을 탐색합니다.
   - 셀은 $N$개의 노드로 구성된 방향성 비순환 그래프(DAG)로 표현됩니다. 각 노드 $x^{(j)}$는 잠재 표현(예: 특성 맵)이며, 각 엣지 $(i,j)$는 $x^{(i)}$를 변환하는 연산 $o^{(i,j)}$와 연관됩니다.
   - 셀은 두 개의 입력 노드와 하나의 출력 노드를 가집니다. 출력은 모든 중간 노드의 출력을 연결하여 얻습니다.
   - 각 중간 노드는 이전 노드들의 출력에 연산을 적용하여 계산됩니다: $x^{(j)} = \sum_{i \lt j} o^{(i,j)}(x^{(i)})$.
   - 특수 '제로(zero)' 연산은 연결이 없음을 나타냅니다. 따라서 셀 학습은 엣지 상의 연산을 학습하는 것으로 귀결됩니다.

2. **연속적인 이완 및 최적화 (Continuous Relaxation and Optimization):**

   - 후보 연산 집합 $\mathcal{O}$ (예: 컨볼루션, 맥스 풀링, 제로)에 대해, 각 엣지 $(i,j)$ 상의 연산 선택을 모든 가능 연산에 대한 소프트맥스(softmax)로 이완합니다.
   - 연속적인 혼합 연산 $\bar{o}^{(i,j)}(x)$는 다음과 같이 정의됩니다:
     $$\bar{o}^{(i,j)}(x) = \sum_{o \in \mathcal{O}} \frac{\exp(\alpha^{(i,j)}_o)}{\sum_{o' \in \mathcal{O}} \exp(\alpha^{(i,j)}_{o'})} o(x)$$
     여기서 $\alpha^{(i,j)}$는 노드 쌍 $(i,j)$에 대한 연산 혼합 가중치 벡터입니다.
   - 아키텍처 탐색은 연속 변수 집합 $\alpha = \{\alpha^{(i,j)}\}$를 학습하는 문제로 변환됩니다. 탐색 종료 시, 이산 아키텍처는 각 혼합 연산을 가장 확률이 높은 연산 $\arg\max_{o \in \mathcal{O}} \alpha^{(i,j)}_o$으로 대체하여 얻습니다.
   - 아키텍처 $\alpha$와 네트워크 가중치 $w$를 함께 학습하는 것을 목표로 합니다. 이는 검증 손실 $L_{\text{val}}(w^*(\alpha),\alpha)$을 최소화하는 이중 수준 최적화(bilevel optimization) 문제로 공식화됩니다:
     $$\min_{\alpha} L_{\text{val}}(w^*(\alpha),\alpha)$$
     $$\text{s.t. } w^*(\alpha) = \argmin_{w} L_{\text{train}}(w,\alpha)$$
     여기서 $w^*(\alpha)$는 주어진 아키텍처 $\alpha$에 대해 훈련 손실 $L_{\text{train}}$을 최소화하여 얻은 최적의 가중치입니다.

3. **근사 아키텍처 그래디언트 (Approximate Architecture Gradient):**

   - 내부 최적화 문제를 정확히 푸는 것은 비용이 많이 들기 때문에, $w^*(\alpha)$를 현재 가중치 $w$를 한 단계 훈련하여 업데이트한 것으로 근사합니다: $w' = w - \xi \nabla_w L_{\text{train}}(w,\alpha)$.
   - 아키텍처 그래디언트는 다음과 같이 근사됩니다:
     $$\nabla_{\alpha} L_{\text{val}}(w^*(\alpha),\alpha) \approx \nabla_{\alpha} L_{\text{val}}(w - \xi \nabla_w L_{\text{train}}(w,\alpha),\alpha)$$
   - 체인 룰을 적용하면 2차 미분 항이 포함되는데, 이 행렬-벡터 곱은 유한 차분 근사를 통해 효율적으로 계산됩니다.
     $$\nabla^2_{\alpha,w} L_{\text{train}}(w,\alpha) \nabla_{w'} L_{\text{val}}(w',\alpha) \approx \frac{\nabla_{\alpha} L_{\text{train}}(w^+, \alpha) - \nabla_{\alpha} L_{\text{train}}(w^-, \alpha)}{2\epsilon}$$
     여기서 $w^{\pm} = w \pm \epsilon \nabla_{w'} L_{\text{val}}(w',\alpha)$ 입니다. 이 근사는 $O(|\alpha| + |w|)$의 복잡도를 가집니다.
   - **1차 근사:** $\xi = 0$인 경우, 2차 미분 항이 사라지며 $\nabla_{\alpha} L_{\text{val}}(w,\alpha)$로 단순화됩니다. 이는 더 빠르지만 경험적으로 성능이 저하됩니다. 이 논문에서는 $\xi > 0$인 경우를 2차 근사라고 지칭합니다.

4. **이산 아키텍처 도출 (Deriving Discrete Architectures):**
   - 학습된 $\alpha$를 바탕으로 각 노드에 대해 '제로(zero)' 연산을 제외한 모든 이전 노드에서 오는 연산 중 상위 $k$개의 '강력한' 연산(softmax 확률 기준)을 유지하여 이산 아키텍처를 구성합니다. 컨볼루션 셀의 경우 $k=2$, 순환 셀의 경우 $k=1$을 사용합니다.

## 📊 Results

DARTS는 이미지 분류 및 언어 모델링에서 최고 수준의 성능을 달성했으며, 기존 비미분적 탐색 방법보다 훨씬 높은 효율성을 보였습니다.

- **CIFAR-10 이미지 분류:**

  - **성능:** Cutout을 사용한 2차 근사 DARTS는 $2.76 \pm 0.09\%$의 테스트 에러율을 달성했습니다. 이는 NASNet-A ($2.65\%$) 및 AmoebaNet-B ($2.55\%$)와 경쟁력 있는 수준입니다.
  - **효율성:** 탐색 비용은 4 GPU-일(NASNet-A는 2000 GPU-일, AmoebaNet-A는 3150 GPU-일)로, 기존 최첨단 방법보다 3 자릿수 이상 빠릅니다. ENAS (0.5 GPU-일)보다는 약간 길지만 더 나은 성능을 달성합니다.
  - **파라미터 수:** 3.3M 파라미터로 효율적인 모델 크기를 유지합니다.
  - **1차 근사:** 1차 근사 DARTS는 $3.00 \pm 0.14\%$의 에러율을 기록하여 2차 근사보다 성능이 낮았습니다.

- **Penn Treebank (PTB) 언어 모델링:**

  - **성능:** 2차 근사 DARTS는 55.7의 테스트 퍼플렉시티(perplexity)를 달성했습니다. 이는 기존의 수동 또는 자동 탐색된 모든 아키텍처(예: LSTM + skip connections 58.3, NAS 64.0, ENAS 58.6)를 능가하는 최고 수준의 결과입니다.
  - **효율성:** 총 4회 실행에 1 GPU-일 이내의 탐색 비용으로, ENAS와 비슷하고 NAS (1만 CPU-일)보다 훨씬 빠릅니다.
  - **파라미터 수:** 23M 파라미터로 효율적인 모델 크기를 유지합니다.

- **ImageNet으로의 전이 (모바일 설정):**

  - CIFAR-10에서 학습된 셀을 ImageNet에 전이했을 때, $26.7\%$의 top-1 에러율과 $8.7\%$의 top-5 에러율을 달성했습니다.
  - 이는 NASNet-A ($26.0\%$)와 AmoebaNet-A ($25.5\%$)와 경쟁력 있는 성능으로, 여전히 3 자릿수 낮은 계산 비용으로 달성되었습니다.

- **WikiText-2 (WT2)로의 전이:**
  - PTB에서 학습된 셀을 WT2에 전이했을 때, $69.6$의 테스트 퍼플렉시티를 달성하여 ENAS ($70.4$)보다 좋은 전이 성능을 보였습니다.

## 🧠 Insights & Discussion

- **경사 기반 최적화의 힘:** DARTS의 놀라운 효율성 향상은 아키텍처 탐색을 연속적인 공간에서 경사 기반 최적화로 접근함으로써 달성되었습니다. 이는 기존 블랙박스 최적화 방식의 한계를 명확히 보여줍니다.
- **이중 수준 최적화의 중요성:** 아키텍처 $\alpha$와 가중치 $w$를 훈련 및 검증 데이터셋에 걸쳐 단순히 공동으로 최적화하는 단순한 전략은 무작위 탐색보다도 나쁜 성능을 보였습니다. 이는 아키텍처를 하이퍼파라미터처럼 취급하여 훈련 데이터에 과적합될 수 있음을 시사하며, 이중 수준 최적화의 필요성을 뒷받침합니다. DARTS는 훈련 세트에 $\alpha$를 직접 최적화하지 않습니다.
- **탐색 공간 설계의 중요성:** 무작위 탐색도 경쟁력 있는 결과를 보여, 탐색 공간 설계의 중요성을 강조합니다. 그럼에도 불구하고 DARTS는 무작위 탐색보다 훨씬 나은 성능을 달성합니다.
- **아키텍처 전이성:** CIFAR-10에서 학습된 셀은 ImageNet으로 잘 전이되었고, PTB에서 학습된 셀은 WT2로 전이되었습니다. 다만, PTB와 WT2 간의 전이성은 CIFAR-10과 ImageNet보다 약했는데, 이는 소스 데이터셋(PTB)의 상대적으로 작은 크기 때문일 수 있습니다.
- **한계 및 향후 연구:**
  - 연속적인 아키텍처 인코딩과 도출된 이산 아키텍처 간의 불일치 문제가 발생할 수 있습니다. 소프트맥스 온도를 점진적으로 낮춰(annealing) 원-핫(one-hot) 선택을 강제하는 방식으로 완화될 수 있습니다.
  - 탐색 과정에서 학습된 공유 파라미터를 기반으로 성능을 고려한 아키텍처 도출 방안을 연구하는 것도 흥미로운 방향입니다.

## 📌 TL;DR

DARTS는 기존 신경망 아키텍처 탐색(NAS) 방법의 높은 계산 비용 문제를 해결하기 위해 **이산적인 탐색 공간을 연속적으로 이완하고 이중 수준 최적화를 통해 경사 하강법으로 아키텍처를 탐색**하는 방법을 제안합니다. 이 방법은 CIFAR-10 이미지 분류 및 Penn Treebank 언어 모델링에서 기존의 최고 수준 아키텍처들과 **경쟁하거나 능가하는 성능**을 보이면서도 **수백, 수천 배 더 효율적**인 탐색 비용(단 몇 GPU-일)으로 아키텍처를 발견합니다. 또한, 학습된 아키텍처 셀은 ImageNet과 WikiText-2와 같은 다른 데이터셋으로도 성공적으로 **전이**됨을 입증하여, 아키텍처 탐색의 효율성과 실용성을 크게 향상시켰습니다.
