# Bayesian Neural Networks: A Min-Max Game Framework

Junping Hong, Ercan Engin Kuruoglu,Senior Member, IEEE

## Problem to Solve

이 논문은 딥러닝에서 **베이지안 신경망(BNN)**의 견고성(robustness) 분석에 초점을 맞추며, 전통적인 베이지안 분야에서 보수적인 선택으로 사용되는 **미니맥스(minimax) 방법**을 BNN에 적용하는 것을 연구합니다. 구체적으로, 이 연구는 다음을 목표로 합니다:

* 미니맥스 방법을 사용하여 **보수적인 BNN**을 공식화합니다.
* **확정적 신경망** $f$와 **샘플링 확률적 신경망** $f + r*ξ$ 간의 **두 플레이어 게임**을 제안합니다.
* 이러한 관점을 통해 미니맥스 손실을 가진 **폐쇄 루프(closed-loop) 신경망**을 이해하고 BNN과의 연결을 밝힙니다.
* 노이즈 교란(noise perturbation)에 대한 모델의 견고성을 평가하고, 주요 파라미터 $r$을 찾는 데 발생하는 문제점을 탐구합니다.

## Key Contributions

이 논문의 주요 기여는 다음과 같습니다:

* 제안된 프레임워크는 미니맥스 손실을 가진 **폐쇄 루프 신경망**을 명확하게 설명하고, 이를 **BNN**과 연결 지을 수 있음을 보여줍니다.
* 이 공식화는 고전적인 BNN보다 본질적으로 **더 보수적**이며, 고전적인 BNN의 분산 설정을 위한 **참고 자료**를 제공할 수 있습니다.
* 이 공식화는 기존 방법들보다 복잡할 수 있지만, **분포 외(out-of-distribution, OOD) 감지**를 위한 대안적인 방법을 제공합니다.
* 모델 구현 시 $r$을 찾는 데 발생하는 몇 가지 작은 문제점을 보고하고 분석합니다.

## Methodology

이 연구는 **확정적 신경망 $f$**와 **샘플링 확률적 신경망 $g$** 간의 **두 플레이어 미니맥스 게임**으로 BNN을 공식화합니다.

* **게임 공식화**: $min_g max_f τ(f,g)$
  * $f$: 입력 $X$에 대해 $f(X)$를 출력하는 **확정적 신경망**입니다.
  * $g$: $g = f + r*ξ$로 정의되는 **샘플링 확률적 신경망**입니다. 여기서 $r$은 **반경(radius)** 또는 교란 수준을 나타내는 스칼라 값이며, $ξ$는 **샘플링 노이즈**(예: $N(0, 0.01)$)입니다. $f$는 평균 또는 중심점을, $g$는 적도(equator)를 나타낸다고 비유됩니다.
* **손실 함수 $τ(f,g)$**:
  * **최대 코딩률 축소(Maximal Coding Rate Reduction, MCR) 손실**을 기반으로 합니다:
    $$ \tau(f,g) = \Delta R(f(X)) + \Delta R(g(X)) + \sum_{i=1}^k \Delta(R(f(X),g(X))) $$
    (여기서 $ΔR$은 MCR 손실을 나타내며, 세 번째 항은 $f$와 $g$ 사이의 간격을 제어합니다.)
  * 더 작은 간격을 위해 $log(1 + Σ_k Δ(R(Z, Z_hat)))$ 형태의 $log$ 케이스도 사용됩니다.
  * 지도 학습을 위한 등가 공식화도 제안됩니다:
    $$ \min_{f,g} loss(f(X)) + loss(g(X)) \quad s.t. \quad |pre(f(X)) - pre(g(X))| = c $$
    (여기서 $pre$는 최종 예측을 나타내며 $c$는 $f$와 $g$ 간의 허용된 간격 상수입니다.)
* **$r$ 값 탐색**: 모델 학습 과정에서 적절한 $r$ 값을 찾는 것이 중요합니다. 이상적으로 $r$에 대한 손실 함수는 볼록(concave)해야 골든 서치(golden search)와 같은 방법을 사용할 수 있지만, 실제로는 차원이 충분하지 않거나 바이어스(bias) 또는 배치 정규화(Batch Normalization, BN) 레이어를 추가할 경우 함수의 형태가 복잡해져 $r$ 탐색에 어려움이 발생할 수 있습니다.

## Results

* **주요 성능**: 제안된 $MinMax BNN$은 $MNIST$ 및 $FMNIST$ 데이터셋에서 테스트되었습니다. $MinMax BNN$의 결과는 기존의 폐쇄 루프 방식 [23]보다 약간 나빴는데, 이는 학습 과정에서 확률적 요소로 인해 추가적인 노이즈가 도입되기 때문으로 분석됩니다. $MNIST$ 데이터셋에 대한 $log$ 케이스에서 $r=0.2$일 때 가장 좋은 샘플링 결과가 나타났으며, 이는 고전적인 BNN의 분산 설정에 대한 참고가 될 수 있습니다.
* **반경 $r$ 탐색 문제**:
  * 모델에 대한 교란 수준은 성능에 영향을 미치며, $BN$ 레이어가 있는 모델은 성능이 저조했습니다. 이는 $Leakyrelu$ 활성화 함수를 사용할 때 $BN$이 원래의 목적(경사 소실/폭발 방지)에 부합하지 않고, 교란의 영향을 증폭시키는 경향이 있기 때문입니다.
  * $r$을 찾기 위해서는 충분한 차원이 중요하며, 차원이 부족하거나 바이어스/$BN$ 레이어를 추가하면 함수가 볼록하지 않아 골든 서치 방식으로는 $r$을 정확히 찾기 어렵습니다. 그럼에도 불구하고, 적절한 탐색 범위와 손실 간격을 설정하고 여러 번 샘플링함으로써 $r$을 효과적으로 찾을 수 있습니다.
* **노이즈 교란에 대한 견고성**: 데이터에 가우시안 랜덤 노이즈를 추가했을 때, 노이즈 비율이 증가함에 따라 $r$ 값도 증가하는 경향을 보였습니다. 이는 작은 노이즈가 모델에 미치는 영향을 $r$이 조정함으로써 견고성을 유지하려는 특성을 보여줍니다.
* **데이터셋 유사성 및 OOD 감지**: $MinMax BNN$ 프레임워크는 훈련된 모델의 $r$ 값을 통해 다른 데이터셋과의 유사성을 평가하여 $OOD$ 감지에 활용될 수 있음을 보여주었습니다. 훈련 데이터셋($MNIST$ 또는 $FMNIST$)과 다른 데이터셋($CIFAR-10$, 가우시안, 라플라스, 코시 노이즈)에 대한 $r$ 값을 비교했을 때, 다른 분포의 데이터셋에 대해 더 큰 $r$ 값이 관찰되었습니다. 이는 고전적인 BNN 또는 데이터에 노이즈를 추가하는 방식과 유사한 $OOD$ 감지 능력을 시사합니다.
