# Restricted Bayesian Neural Network
Sourav Ganguly, Saprativa Bhattacharjee

## 해결해야 할 문제

기존 딥러닝 모델은 예측의 **불확실성**이 높고, 대규모 네트워크에서 **막대한 저장 공간**을 필요로 하며, **과적합**, **과소적합**, **기울기 소실/폭주** 등의 문제에 직면합니다. 베이지안 신경망(BNN)은 불확실성을 모델링하지만, 각 연결의 가중치 분포에 대한 평균과 표준 편차를 저장해야 하므로 여전히 **높은 저장 공간 복잡도**를 가집니다. 이 연구는 이러한 BNN의 저장 공간 문제를 완화하고, **비볼록(non-convex) 목적 함수**에서도 지역 최적해에 갇히지 않고 강건하게 수렴하며 불확실성을 효율적으로 처리하는 새로운 아키텍처와 알고리즘을 제안합니다.

## 주요 기여

*   네트워크의 가중치(정확히는 가중치 분포의 매개변수) 저장량을 제한하여 **저장 공간 복잡도를 크게 줄이는** 효율적인 학습 방법을 제안합니다.
*   가중치가 샘플링되는 분포의 매개변수(특히 **평균**)를 조정하기 위해 **교차 엔트로피 최적화(Cross Entropy Optimization, CEO)** 방법을 활용하는 알고리즘을 제시합니다.
*   제안된 방법이 표준 신경망(FFNN) 및 기존 BNN에 비해 갖는 이점을 강조하기 위해 **포괄적인 비교 결과**를 제공합니다.

## 방법론

본 연구는 **제한된 베이지안 신경망 (Restricted Bayesian Neural Network, $RBNN$)** 이라는 새로운 아키텍처와 **교차 엔트로피 최적화 (CEO)** 기반 학습 알고리즘을 제안합니다.

1.  **가중치 행렬 구성**:
    *   기존 BNN이 각 연결 가중치 $W_{l_i},j$에 대해 개별적인 확률 분포를 정의하는 것과 달리, $RBNN$은 특정 레이어 $l+1$의 특정 뉴런 $j$에 들어오는 모든 연결 가중치(즉, $W_{l_i},j$의 열 벡터)가 **단일 정규 분포** $N(μ_l(j), σ)$에서 샘플링된다고 가정합니다.
    *   이는 각 뉴런 헤드(즉, 다음 레이어의 뉴런)에 대해 **하나의 평균 $μ$** (그리고 고정된 표준 편차 $σ$)만 저장하면 되므로, 저장해야 하는 매개변수 수를 혁신적으로 줄입니다. ($σ$는 실험에서 고정된 값으로 사용됩니다.)
    *   가중치는 $[-1, 1]$ 범위로 바인딩될 수 있다고 설명됩니다.

2.  **가우시안 매개변수 최적화 (CEO 기반 학습)**:
    *   $RBNN$은 **알고리즘 4: Restricted BNN**에 명시된 교차 엔트로피 최적화(CEO) 방법을 사용하여 가중치 분포의 평균 $μ$를 업데이트합니다.
    *   **단계별 과정**:
        *   각 에포크에서, 현재 저장된 $μ$ 값(각 뉴런 헤드에 대한 평균)과 고정된 $σ$를 사용하여 각 레이어의 가중치 행렬 $W_l$을 구성하기 위해 가중치들을 샘플링합니다.
        *   샘플링된 가중치를 사용하여 입력 데이터에 대한 **순방향 전파($forward$ 서브루틴)**를 수행하고 예측 $ˆy$를 얻습니다.
        *   참 값 $y$와 $ˆy$ 사이의 오차/손실 값을 계산합니다.
        *   이 가중치 샘플링 및 오차 계산 과정을 $N$번 반복하여 여러 개의 오차 값을 수집합니다.
        *   수집된 오차 값들을 **오름차순으로 정렬**하고, 오차가 가장 낮은 상위 $⌊ρN⌋$개 (예: 10%)의 샘플(가중치 세트)을 **"엘리트 샘플"**로 선택합니다.
        *   이 엘리트 샘플들을 사용하여 각 뉴런 헤드에 대한 **분포의 평균 $μ$를 업데이트**합니다. (수식: $$v^{(j+1)} = \alpha \frac{\Sigma_{k \in M} f(X_k)X_k}{\Sigma_{k \in M} f(X_k)} + (1-\alpha)v_j$$, 여기서 $f(X_k)$는 오차의 역수와 같이 최적화 목표에 맞춰 변형될 수 있습니다).
        *   이 과정은 오차가 최소값에 도달할 때까지 반복됩니다.

## 결과

제안된 $RBNN$ 모델은 Pulsar-star 및 Iris 데이터셋(모두 분류 문제)에 대해 표준 신경망(FFNN) 및 기존 베이지안 신경망(BNN)과 비교 평가되었습니다.

*   **저장 공간 복잡도**: $RBNN$은 FFNN 및 BNN에 비해 **매개변수 저장량이 현저히 적습니다**. (예: Pulsar 데이터셋에서 $RBNN$ 6개, FFNN 12개, BNN 24개 매개변수; Iris 데이터셋에서 $RBNN$ 8개, FFNN 16개, BNN 32개 매개변수).
*   **정확도**:
    *   **Pulsar-star 데이터셋**: $RBNN$이 **92.865%**의 정확도로 FFNN(90.7625%) 및 BNN(90.124%)보다 우수한 성능을 보였습니다.
    *   **Iris 데이터셋**: $RBNN$이 **99.3%**의 정확도를 달성하며 FFNN(95.8%) 및 BNN(92.6%)을 능가했습니다.
*   **수렴 속도 및 손실 감소**:
    *   Pulsar 데이터셋에서 $RBNN$의 손실은 약 10 에포크 이내에 거의 0으로 떨어지며 FFNN 및 BNN보다 빠르게 수렴했습니다.
    *   Iris 데이터셋에서는 $RBNN$이 초기에 더 높은 오차를 보였지만, 약 10 에포크 학습 후 FFNN 및 BNN보다 훨씬 효율적인 결과를 보여주었습니다.
*   **장점**: 가중치 저장이 필요 없어 공간 복잡도를 크게 줄이고, 기울기 계산이 필요 없는 **영차 최적화(zero-order optimization)** 방법으로 기울기 소실/폭주 문제를 해결합니다. 또한, 본질적인 무작위 샘플링으로 드롭아웃(dropout)이 필요 없습니다.
*   **한계**: 샘플링된 값이 최적 가중치에서 멀리 떨어져 있거나 모델이 커질수록 최적 솔루션으로 **수렴하는 데 상당한 시간**이 소요될 수 있습니다.