{
  "title": "Multi-Task Learning on Networks",
  "authors": "Andrea Ponti",
  "year": 2021,
  "url": "http://arxiv.org/abs/2112.04891v1",
  "abstract": "The multi-task learning (MTL) paradigm can be traced back to an early paper\nof Caruana (1997) in which it was argued that data from multiple tasks can be\nused with the aim to obtain a better performance over learning each task\nindependently. A solution of MTL with conflicting objectives requires modelling\nthe trade-off among them which is generally beyond what a straight linear\ncombination can achieve. A theoretically principled and computationally\neffective strategy is finding solutions which are not dominated by others as it\nis addressed in the Pareto analysis. Multi-objective optimization problems\narising in the multi-task learning context have specific features and require\nadhoc methods. The analysis of these features and the proposal of a new\ncomputational approach represent the focus of this work. Multi-objective\nevolutionary algorithms (MOEAs) can easily include the concept of dominance and\ntherefore the Pareto analysis. The major drawback of MOEAs is a low sample\nefficiency with respect to function evaluations. The key reason for this\ndrawback is that most of the evolutionary approaches do not use models for\napproximating the objective function. Bayesian Optimization takes a radically\ndifferent approach based on a surrogate model, such as a Gaussian Process. In\nthis thesis the solutions in the Input Space are represented as probability\ndistributions encapsulating the knowledge contained in the function\nevaluations. In this space of probability distributions, endowed with the\nmetric given by the Wasserstein distance, a new algorithm MOEA/WST can be\ndesigned in which the model is not directly on the objective function but in an\nintermediate Information Space where the objects from the input space are\nmapped into histograms. Computational results show that the sample efficiency\nand the quality of the Pareto set provided by MOEA/WST are significantly better\nthan in the standard MOEA."
}