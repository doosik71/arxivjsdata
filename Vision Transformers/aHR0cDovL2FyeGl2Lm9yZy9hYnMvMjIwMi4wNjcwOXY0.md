# HOW DO VISION TRANSFORMERS WORK?

Namuk Park, Songkuk Kim

## Problem to Solve

컴퓨터 비전 분야에서 **Multi-Head Self-Attention**(**MSA**)의 성공에도 불구하고, MSA가 **어떻게 작동하는지**에 대한 근본적인 이해는 부족합니다. 특히, **Vision Transformer**(**ViT**)의 성공 요인으로 널리 알려진 **약한 귀납적 편향**(**weak inductive bias**) 및 **장거리 의존성**(**long-range dependency**) 포착이라는 설명이 충분히 뒷받침되지 않으며, 오히려 ViT가 **소규모 데이터셋**에서 **과적합** 경향을 보여 **예측 성능이 저조**하다는 문제가 제기됩니다. 본 논문은 다음과 같은 세 가지 핵심 질문에 답하여 MSA의 본질을 설명하고자 합니다.

1. 어떤 MSA 특성이 **최적화를 개선**하는 데 필요한가? MSA의 장거리 의존성이 신경망 학습에 도움이 되는가?
2. MSA는 **Convolution**(**Conv**)처럼 작동하는가? 그렇지 않다면 어떻게 다른가?
3. MSA와 Conv를 **어떻게 조화롭게** 사용하여 각 모듈의 장점만을 활용할 수 있는가?

## Key Contributions

- **MSA의 손실 함수 평탄화 효과**: MSA는 **손실 지형**(**loss landscape**)을 **평탄화**(**flattening**)하여 정확도뿐만 아니라 **일반화**(**generalization**) 성능도 향상시킵니다. 이러한 개선은 주로 **데이터 특이성**(**data specificity**)에 기인하며, 장거리 의존성 때문이 아닙니다. ViT는 비볼록(**non-convex**) 손실의 문제에 직면하지만, 대규모 데이터셋과 **손실 지형 평탄화 방법**을 통해 완화될 수 있습니다.
- **MSA와 Conv의 상반된 특성**: MSA와 Conv는 **대조적인 행동**을 보입니다. MSA는 **저주파 필터**(**low-pass filter**)로 고주파 신호를 감소시키는 반면, Conv는 **고주파 필터**(**high-pass filter**)로 고주파 신호를 증폭시킵니다. 또한, MSA는 특징 맵을 **집계**(**aggregate**)하여 분산을 줄이는 반면, Conv는 특징 맵을 **다양화**(**diversify**)하여 분산을 증가시킵니다. 따라서 이들은 **상호 보완적**입니다.
- **다단계 신경망의 동작 및 MSA의 역할**: **다단계 신경망**(**multi-stage neural networks**)은 작은 개별 모델의 **직렬 연결**처럼 작동합니다. 특히, **각 단계의 끝**에 위치한 MSA가 **예측에 핵심적인 역할**을 합니다.
- **AlterNet 제안**: 위의 통찰력을 바탕으로, 각 단계의 끝에 있는 Conv 블록을 MSA 블록으로 대체하는 **`AlterNet`** 모델을 제안합니다. `AlterNet`은 **대규모 데이터셋**뿐만 아니라 **소규모 데이터셋**에서도 **CNN**(**Convolutional Neural Network**)을 능가하는 성능을 보이며, 이는 기존 ViT가 소규모 데이터에서 저조한 성능을 보이는 것과 대조됩니다.

## Methodology

- **손실 지형 분석**: **Hessian 최대 고유값 스펙트럼**을 시각화하여 손실 함수의 평탄도와 볼록성을 평가했습니다. `L2` 정규화가 적용된 `NLL`(**Negative Log-Likelihood**) 손실을 사용했으며, **음의 최대 고유값 비율**(`NEP`)과 **양의 최대 고유값 평균**(`APE`)으로 정량화했습니다.
- **푸리에 분석**(**Fourier Analysis**) **및 특징 맵 분산 분석**: 특징 맵을 **푸리에 변환**하여 MSA와 Conv가 각각 **저주파 필터** 또는 **고주파 필터**로 작동하는지 분석했습니다. 또한, 각 신경망 레이어에서 특징 맵의 **분산**을 측정하여 MSA가 특징 맵을 집계하는 경향이 있음을 보였습니다.
- **표현 유사성**(**Representational Similarity**) **및 손상 연구**(**Lesion Study**): **CKA**(**Centered Kernel Alignment**)를 사용하여 ResNet, PiT, Swin과 같은 다단계 신경망의 특징 맵 유사성에서 **블록 구조**가 나타남을 확인했습니다. 또한, 학습된 모델에서 특정 단위를 제거하여 예측 정확도에 미치는 영향을 분석하는 손상 연구를 수행하여 각 모듈의 중요성을 파악했습니다.
- **모델 및 학습 설정**: `ViT-Ti`, `PiT-Ti`, `Swin-Ti`, `ResNet-50` 등을 포함한 다양한 Vision Transformer 및 CNN 모델을 평가했습니다. 모든 모델은 DeiT 스타일의 **강력한 데이터 증강**(**strong data augmentation**) 기법(RandAugment, Random Erasing, Label Smoothing, Mixup, CutMix, Stochastic Depth)과 AdamW 옵티마이저를 사용하여 300 에포크 동안 처음부터 학습되었습니다.
- **`AlterNet` 설계**: 사전 활성화 `ResNet-50`을 기반으로 `AlterNet`을 구축했습니다. 각 단계의 끝에 있는 Conv 블록을 MSA 블록으로 **교체**하고, 후기 단계의 MSA 블록에는 **더 많은 헤드**와 **더 높은 임베딩 차원**을 사용하도록 설계했습니다. `AlterNet`은 Swin과 같은 **지역 MSA**(**local MSA**)와 상대적인 위치 인코딩을 사용했습니다.

## Results

- **손실 지형**:
  - `ViT`의 손실 지형은 `ResNet`보다 **더 평탄**하며, Hessian 고유값의 크기가 더 작습니다.
  - 하지만 소규모 데이터셋에서는 `ViT`의 손실 함수가 `ResNet`보다 더 많은 **음의 Hessian 고유값**을 가져 **비볼록성**이 높게 나타나 최적화를 방해합니다.
  - **대규모 데이터셋**은 이러한 음의 고유값을 억제하여 손실을 볼록하게 만들고 `ViT`의 학습을 돕습니다.
  - **`GAP` 분류기**나 **지역 MSA**(`Swin`) 및 **다단계 아키텍처**(`PiT`)는 손실 지형을 평탄화하고 음의 고유값을 억제하여 성능을 향상시킵니다.
  - **다중 헤드**의 수가 많거나 **헤드당 임베딩 차원**이 높을수록 손실 지형이 더욱 볼록해지고 평탄해집니다.
- **과적합**: `ViT`는 소규모 데이터셋(**CIFAR**)에서도 과적합되지 않습니다. 훈련 `NLL`과 테스트 에러가 함께 증가하는 양상을 보여, 소규모 데이터셋에서의 저조한 성능이 과적합 때문이 아닌 **비볼록 손실 함수** 때문임을 시사합니다.
- **MSA와 Conv의 동작 차이**:
  - `MSAs`는 **고주파 신호를 감소**시켜 저주파 필터처럼 작동하며, `ViT`가 고주파 노이즈에 강인함을 보였습니다. 반면 `Convs`와 `MLPs`는 고주파 신호를 증폭시키며, `ResNet`은 고주파 노이즈에 취약합니다.
  - `MSAs`는 특징 맵의 **분산을 감소**시켜 특징 맵을 집계/앙상블하는 경향이 있으며, `Convs`와 `MLPs`는 분산을 증가시켜 특징 맵을 다양화합니다.
  - 이러한 결과는 MSA가 이미지의 **형태**(**shape**)에 편향되고 Conv는 **질감**(**texture**)에 편향됨을 시사합니다.
- **`AlterNet` 성능**:
  - **각 단계의 끝**에 MSA를 적용하는 것이 정확도를 크게 향상시킵니다.
  - `AlterNet`은 **CIFAR**와 같은 **소규모 데이터셋**은 물론 **ImageNet**과 같은 **대규모 데이터셋**에서도 **CNN**을 능가하는 **정확도**와 **강인성**(**robustness**)을 보였습니다. 이는 `AlterNet`이 MSA와 Conv의 상호 보완적인 특성을 효과적으로 활용했음을 보여줍니다.
  - `AlterNet`의 손실 지형은 대규모 고유값을 억제하면서도 음의 고유값을 소수만 허용하여 `ResNet`보다 평탄합니다.
  - `Alter-ResNet-50`은 `ResNet-50`보다 학습 처리량이 약 20% 더 빠릅니다.
