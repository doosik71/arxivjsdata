# Learning Latent Plans from Play

Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet

## 🧩 Problem to Solve

로봇에게 다양한 범용 기술을 가르치는 것은 오랜 난제로 남아 있습니다. 기존의 모방 학습(Learning from Demonstration, LfD)이나 강화 학습(Reinforcement Learning, RL)은 각 작업별로 대량의 레이블링된 전문가 데모를 수집하거나 보상 함수를 수동으로 설계해야 하므로 비용이 많이 들고 확장성이 낮습니다. 또한, 로봇은 제한된 이산적(discrete) 작업 세트가 아닌 "연속적인 행동 범위(continuum of behaviors)"를 수행해야 하지만, 현재 방법들은 미세한 작업 변화에도 새로운 데모나 보상 함수를 필요로 하는 문제에 직면합니다. 이 논문은 이러한 한계를 극복하고, 어떠한 초기 상태($s_c$)에서든 어떠한 도달 가능한 목표 상태($s_g$)로 이동할 수 있는 "작업 불가지론적(task-agnostic) 제어" 능력을 학습하는 것을 목표로 합니다.

## ✨ Key Contributions

- **"플레이(Play)" 데이터의 도입 및 활용:** 작업별 분할이나 라벨링 없이 저렴하게 대량 수집할 수 있으며, 기존 작업 데모보다 약 4배 더 넓은 상호작용 공간을 커버하는 인간 텔레오퍼레이션 플레이 데이터를 제안하고 활용했습니다.
- **Play-LMP(Play-supervised Latent Motor Plans) 방법론 제안:** 플레이 데이터에서 자기 지도 방식으로 행동 레퍼토리(behavior repertoire)를 잠재 공간에서 조직화하고, 이를 테스트 시 특정 목표 달성에 재사용하는 Play-LMP를 개발했습니다.
- **전문가 훈련 정책 대비 우월한 성능:** 레이블링되지 않은 플레이 데이터로 자기 지도 학습한 Play-LMP가 시뮬레이션 로봇 환경에서 18가지 어려운 시각 조작 작업에 대해 각 작업 전문가가 훈련한 정책보다 훨씬 뛰어난 성능을 보였습니다.
- **강건성 및 재시도 행동 발현:** 플레이 학습 모델은 섭동(perturbations)에 더 강건하며, 초기 실패 후 목표를 달성하기 위해 여러 번 재시도하는 행동을 자연스럽게 발현하는 질적 증거를 발견했습니다. 이는 전문가 훈련 모델에서는 나타나지 않는 특징입니다.
- **잠재 계획 공간의 기능적 조직화:** 작업 라벨 없이 훈련되었음에도 불구하고, Play-LMP 에이전트가 잠재 계획 공간을 기능적 작업(예: 서랍 조작, 버튼 조작) 중심으로 조직화함을 보여주었습니다.

## 📎 Related Works

- **모방 학습(Imitation Learning) 및 강화 학습(Reinforcement Learning):** 로봇 기술 습득의 전통적인 방법이지만, 각각 대량의 작업별 데모 데이터 또는 수동으로 설계된 보상 함수가 필요하다는 한계가 있습니다.
- **목표 조건부 정책(Goal-conditioned Policies):** 현재 상태와 목표 상태에 기반하여 행동을 생성하는 정책으로, 강화 학습 및 역 모델(inverse models) 분야에서 연구되었으나, 샘플 효율성 문제와 다중 모드성(multimodality) 문제로 인해 주로 짧고 단순한 작업에 제한되었습니다.
- **계층적 잠재 변수 모델(Hierarchical Latent Variable Models):** 행동의 다중 모드성을 명시적으로 모델링하여 장기적이고 복잡한 작업을 다룰 수 있는 접근법입니다.
- **변분 자동 인코더(Variational Autoencoder, VAE):** 관측된 데이터의 잠재 표현을 학습하는 데 널리 사용되는 프레임워크로, 본 논문에서는 이를 조건부 버전(CVAE)으로 확장하여 계획 학습에 적용했습니다.
- **이전 연구와의 차별점:** 이 논문은 메타 학습 단계, 값비싼 작업별 데모, 혹은 보상 함수나 값비싼 RL 단계 없이 비작업 특정 경험으로부터 일반 목적 정책을 학습합니다.

## 🛠️ Methodology

이 논문은 레이블 없는 텔레오퍼레이션 플레이 데이터로부터 작업 불가지론적 제어를 학습하기 위해 **Play-GCBC** (Play-supervised Goal-Conditioned Behavioral Cloning)와 **Play-LMP** (Play-supervised Latent Motor Plans) 두 가지 자기 지도 방법을 제안합니다.

### Play-GCBC (Play-Supervised Goal-Conditioned Behavioral Cloning)

가장 간단한 형태로, 플레이 데이터($D$)에서 무작위로 $\kappa$ 길이의 (관측, 행동) 시퀀스($\tau$)를 추출하여 자기 지도 방식으로 목표 조건부 정책을 학습합니다.

1. **데이터 샘플링:** 플레이 데이터에서 무작위로 $(O_{t:t+\kappa}, a_{t:t+\kappa})$ 시퀀스를 샘플링합니다.
2. **목표 상태 정의:** 샘플링된 시퀀스의 마지막 관측($O_{t+\kappa}$)을 합성 목표 상태($s_g$)로 인코딩합니다.
3. **정책 훈련:** 현재 인코딩된 상태 $s_t = \Phi(O_t)$와 목표 상태 $s_g$에 조건화된 RNN 정책 $\pi_{GCBC}(a_t|s_t, s_g)$를 훈련하여 샘플링된 시퀀스 내의 각 행동 $a_t$의 로그 우도(log likelihood)를 최대화합니다.
   $$L_{GCBC} = -\frac{1}{\kappa} \sum_{t=k}^{k+\kappa} \log(\pi_{GCBC}(a_t|s_t, s_g))$$
   하지만 동일한 $(s_c, s_g)$ 쌍을 연결하는 여러 유효한 고수준 행동이 존재할 수 있어, 이는 정책 학습에 다중 모드성 문제를 야기할 수 있습니다.

### Play-LMP (Play-Supervised Latent Motor Plans)

Play-LMP는 다중 모드 문제를 해결하기 위해 잠재 계획(latent plan)을 명시적으로 학습하는 접근 방식을 사용합니다. 이는 조건부 시퀀스-투-시퀀스 VAE (Conditional Sequence-to-Sequence VAE, CVAE) 구조를 가집니다.

1. **데이터 샘플링 및 상태 인코딩:** Play-GCBC와 동일하게 $\kappa$ 길이의 플레이 시퀀스 $\tau$를 샘플링하고, 원시 관측값을 인코딩된 상태 $\tau^* = \Phi(\tau)$로 변환합니다.
2. **계획 인식 (Plan Recognition):**
   - `V_enc`라는 양방향 시퀀스 인코더는 $\tau^*$를 입력으로 받아 잠재 계획 공간에서 변분 사후 분포 $q_{\phi}(z|\tau)$의 매개변수($\mu_{\phi}, \sigma_{\phi}$)를 출력합니다.
     $$z \sim N(\mu_{\phi}, \text{diag}(\sigma^2_{\phi}))$$
3. **계획 제안 (Plan Proposal):**
   - 동일한 시퀀스 $\tau$에서 초기 상태 $s_c = \Phi(O_t)$와 최종 상태 $s_g = \Phi(O_{t+\kappa})$를 추출합니다.
   - `CG_enc`라는 피드포워드 네트워크는 $s_c$와 $s_g$를 입력으로 받아 잠재 계획 공간에서 조건부 사전 분포 $p_{\theta}(z|s_c, s_g)$의 매개변수($\mu_{\psi}, \sigma_{\psi}$)를 출력합니다.
   - `V_enc`와 `CG_enc`는 이 두 분포 간의 KL 발산(KL divergence)을 최소화하도록 함께 훈련됩니다.
     $$L_{KL} = KL(N(z|\mu_{\phi}, \text{diag}(\sigma^2_{\phi}))||N(z|\mu_{\psi}, \text{diag}(\sigma^2_{\psi})))$$
4. **계획 및 목표 조건부 정책 (Plan and Goal-Conditioned Policy):**
   - $\pi_{LMP}$라는 RNN 정책은 현재 상태 $s_t$, 목표 상태 $s_g$, 그리고 계획 인식 네트워크에서 샘플링된 잠재 계획 $z$를 입력으로 받아 다음 행동 $a_t$의 분포 매개변수를 출력합니다.
   - 정책은 플레이 중 취해진 행동을 재구성하도록 훈련되며, 행동 재구성 손실을 최소화합니다.
     $$L_{\pi} = -\frac{1}{\kappa} \sum_{t=k}^{k+\kappa} \log(\pi_{LMP}(a_t|s_t, s_g, z))$$
5. **총 훈련 목표:** 전체 Play-LMP 모델은 다음과 같은 손실 함수를 최소화하도록 종단간(end-to-end) 훈련됩니다.
   $$L_{LMP} = L_{\pi} + \beta L_{KL}$$
   - 여기서 $\beta$는 KL 손실의 기여도를 조절하는 가중치로, "후방 붕괴(posterior collapse)" 문제를 방지하는 데 사용됩니다.

### 테스트 시 작업 불가지론적 제어 (Task-Agnostic Control at Test Time)

테스트 시 에이전트는 사용자가 제공한 현재 상태($O_c$)와 목표 상태($O_g$)를 받습니다.

1. **잠재 계획 추론:** 인코딩된 $s_c$와 $s_g$는 `CG_enc`에 입력되어 목표 달성을 위한 잠재 행동 계획 $z$의 분포를 출력합니다.
2. **계획 샘플링:** 에이전트는 이 분포에서 단일 계획 $z$를 샘플링합니다.
3. **폐쇄 루프 실행:** 에이전트는 (현재 상태 $s_t$, 목표 상태 $s_g$, 샘플링된 계획 $z$)를 $\pi_{LMP}$에 지속적으로 입력하여 낮은 수준의 행동 $a_t$를 샘플링하고 환경에서 실행합니다.
4. **재계획 (Replanning):** 에이전트는 약 $\kappa$ 타임스텝마다 (약 1Hz) 새로운 잠재 계획을 추론하고 샘플링하여 재계획을 수행합니다.

## 📊 Results

- **18가지 시각 조작 작업에서의 탁월한 성능:**
  - Play-LMP는 작업 라벨 없이 훈련되었음에도 불구하고, 픽셀 기반 실험에서 69.4%, 지상 진실 상태(ground truth state) 기반 실험에서 85.5%의 성공률을 달성했습니다. 이는 18가지 개별 전문가 훈련 BC(Behavioral Cloning) 정책(픽셀 66.5%, 상태 70.3%) 및 다중 작업 BC 정책(상태 66.2%)을 크게 능가하는 결과입니다.
- **데이터 효율성 및 확장성:** 단 30분 분량의 플레이 데이터로 훈련된 Play-LMP 정책(71.8%)이 90분 분량의 전문가 작업별 데모로 훈련된 18개 BC 정책(70.3%)보다 뛰어난 성능을 보였습니다. 이는 플레이 데이터 수집의 효율성과 방법론의 확장성을 강조합니다.
- **강건성 향상:** 플레이 데이터로 훈련된 모델(Play-LMP 및 Play-GCBC)은 초기 로봇 엔드 이펙터 위치 섭동(perturbations)에 대해 전문가 데모로만 훈련된 BC 모델보다 훨씬 더 강건했습니다. 이는 플레이 데이터의 광범위한 상호작용 공간 커버리지 덕분으로 분석됩니다.
- **비지도 작업 발견:** Play-LMP가 학습한 잠재 계획 공간(latent plan space)을 시각화(t-SNE)한 결과, 작업 라벨 없이 훈련되었음에도 불구하고 서랍 조작, 버튼 조작 등과 같은 기능적 작업 중심으로 공간이 명확하게 조직화되는 것을 발견했습니다.
- **자연스러운 재시도 행동 발현:** 플레이 학습 모델은 초기 실패 후 목표를 달성하기 위해 여러 번 재시도하는 행동을 자연스럽게 보여주었으며, 이는 전문가 데모 모델에서는 관찰되지 않는 특성이었습니다.

## 🧠 Insights & Discussion

- **플레이 데이터의 패러다임 변화:** 이 연구는 로봇 기술 학습에서 값비싸고 제한적인 전문가 데모 대신, 저렴하고 대량 수집 가능한 레이블 없는 인간 '플레이' 데이터가 얼마나 강력한 대안이 될 수 있는지 보여줍니다. 플레이 데이터의 풍부함과 넓은 커버리지는 로봇이 다양한 작업을 일반화하고 예측 불가능한 상황에 강건하게 대응하는 데 필수적인 요소로 작용합니다.
- **잠재 계획 학습의 효과:** Play-LMP의 핵심 아이디어인 잠재 계획 학습은 다중 모드 행동 문제를 효과적으로 해결합니다. 정책이 복잡한 계획 추론의 부담에서 벗어나 단일 모드 행동 실행에 집중할 수 있게 함으로써, 학습의 효율성과 일반화 성능을 크게 향상시킵니다.
- **자기 지도 학습의 잠재력 입증:** 작업 라벨이나 보상 함수 없이 원시 플레이 데이터만으로도 로봇이 작업의 의미론을 학습하고, 시각적 관측부터 행동 생성까지의 전체 심층 신경망 스택을 처음부터 훈련할 수 있음을 입증했습니다. 이는 로봇 학습의 새로운 방향을 제시합니다.
- **한계점:**
  - **목표 상태의 한계:** 사용자가 목표 상태만으로 작업을 지정한다는 가정은 "서랍을 천천히 열어라"와 같이 작업 수행 방식까지 지정해야 하는 경우에는 제한적일 수 있습니다.
  - **환경 일반화 범위:** 현재 연구는 훈련 및 테스트 작업이 동일한 "플레이룸" 환경에서 이루어지는 "단일 방 일반화"에 초점을 맞춥니다. 거실에서 학습한 기술을 부엌에 일반화하는 등의 외부 분포(out-of-distribution) 일반화는 향후 연구 과제입니다.
  - **플레이 데이터 분포 불균형:** 플레이 데이터가 특정 객체 상호작용에 지나치게 편향되지 않는다는 가정이 장기 학습 상황에서는 문제가 될 수 있습니다.
  - **잠재 공간의 표현력:** 잠재 계획 공간을 간단한 단일 모드 가우시안 분포로 모델링한 것은 잠재적으로 표현력을 제한할 수 있으며, 향후 더 유연한 잠재 변수 모델을 탐색할 수 있습니다.

## 📌 TL;DR

이 논문은 로봇이 일반 목적 기술을 광범위하게 습득하는 문제를 해결하기 위해, **레이블 없는 인간 텔레오퍼레이션 '플레이(Play)' 데이터**로부터 자기 지도 방식으로 제어를 학습하는 **Play-LMP** 방법을 제안합니다. 플레이 데이터는 수집이 저렴하고 풍부하며, 기존 전문가 데모보다 넓은 상호작용 공간을 커버합니다. Play-LMP는 조건부 변분 자동 인코더(CVAE)를 사용하여 **잠재 행동 계획(latent plans)**을 학습하고, 이를 통해 현재 상태와 목표 상태 사이의 행동 다중 모드성 문제를 해결합니다. 결과적으로 Play-LMP는 18가지 시뮬레이션 로봇 조작 작업에서 **전문가 훈련 정책보다 훨씬 뛰어난 성능**을 보였고, 적은 데이터로도 **확장성**을 입증했습니다. 또한, 플레이 학습 모델은 **섭동에 강건**하며, 작업 라벨 없이도 **잠재 계획 공간을 기능적으로 조직화**하고, 초기 실패 후 **자연스러운 재시도 행동**을 발현하는 등 강력한 일반화 능력을 보여주며, 로봇 기술 학습의 효율성과 강건성을 위한 새로운 길을 제시합니다.
