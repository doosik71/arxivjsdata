# Improving Vision-Language-Action Model with Online Reinforcement Learning

Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, Jianyu Chen

## 🧩 Problem to Solve

최근 연구들은 대규모 비전-언어 모델(VLM)을 로봇 전문가 데이터셋을 이용한 지도 미세 조정(SFT)을 통해 저수준 로봇 제어에 성공적으로 통합하여 비전-언어-액션(VLA) 모델을 개발했습니다. 이러한 VLA 모델은 강력하지만, 환경과의 상호작용을 통해 대규모 모델을 개선하는 방법은 여전히 미해결 과제입니다. 특히, 온라인 강화 학습(RL)을 대규모 VLA 모델에 직접 적용할 경우, 다음과 같은 심각한 문제가 발생합니다:

1. **훈련 불안정성**: 대규모 모델의 성능에 치명적인 영향을 미치는 훈련 불안정성.
2. **계산 부담**: 대부분의 로컬 머신으로는 감당하기 어려운 막대한 컴퓨팅 자원 요구.
   또한, SFT는 고품질의 전문가 데이터셋에 의존하지만, 로봇 분야에서 이러한 데이터셋을 확보하기는 어렵고 비용이 많이 듭니다. 지도 학습만으로는 분포 변화(distribution shift) 문제로 인해 VLA 모델이 물리적 환경과 완전히 정렬되지 않을 수도 있습니다.

## ✨ Key Contributions

- **iRe-VLA 프레임워크 제안**: 온라인 강화 학습(RL) 단계와 지도 학습(SL) 단계를 반복하여 VLA 모델을 효과적으로 개선하는 프레임워크를 제안합니다.
- **훈련 안정화**: RL 단계에서 VLM 파라미터를 고정하여 대규모 VLA 모델 훈련 시 발생하는 불안정성을 해결합니다.
- **계산 효율성 증대**: RL 단계에서 경량 액션 헤드만 훈련하고 VLM을 고정함으로써 계산 부담을 크게 줄여 로컬 머신에서도 실행 가능하게 만듭니다.
- **모델 성능 향상**: 시뮬레이션(MetaWorld, Franka Kitchen) 및 실제 로봇 조작 벤치마크에서 VLA 모델의 성능을 일관되게 향상시킵니다.
- **일반화 능력 개선**: 온라인 상호작용을 통해 VLA 모델의 일반화 능력을 향상시키고, 훈련되지 않은 미지 작업에서도 성공률을 높입니다.
- **파국적 망각(catastrophic forgetting) 완화**: 성공적인 궤적을 재수집하여 지도 학습에 활용함으로써 이전에 학습한 작업에 대한 파국적 망각을 방지합니다.

## 📎 Related Works

- **Embodied Control을 위한 파운데이션 모델**: 대규모 언어 모델(LLM) 및 비전-언어 모델(VLM)을 로봇 제어에 활용하는 연구로, 고수준 계획(텍스트 시퀀스, 코드 생성)과 저수준 제어(액션 헤드 추가, 언어 토큰을 액션으로 대체하여 직접 제어 신호 출력) 두 가지 접근 방식이 있습니다.
- **RL을 이용한 대규모 모델 미세 조정**: LLM을 인간 선호도에 맞게 정렬하는 데 성공적으로 사용된 인간 피드백 기반 강화 학습(RLHF) 연구들이 있습니다. 그러나 RLHF는 주로 오프라인, 인간 라벨링 데이터셋을 사용하는 밴딧 환경에 적용되는 반면, VLA 모델 미세 조정은 알려지지 않은 동역학과 온라인 탐색을 요구합니다. 기존 연구들은 주로 고수준 계획을 접지하는 데 중점을 두었으며, 희소한 보상을 가진 긴 지평의 저수준 제어 신호 개선에는 제한적이었습니다.

## 🛠️ Methodology

저자들은 온라인 상호작용을 통해 VLA 모델을 효과적으로 개선하고 로봇 시스템에 적합한 계산 비용을 유지하는 학습 방법을 개발하는 것을 목표로 합니다.

- **모델 아키텍처**:

  - VLA 모델은 시각 입력 $o \in O$와 자유 형식 언어 지침 $i \in L$을 저수준 로봇 액션 $a \in A$로 변환합니다.
  - 이 모델은 사전 훈련된 대규모 VLM 백본(BLIP-2 3B)과 경량 액션 헤드로 구성됩니다. 액션 헤드는 VLM의 마지막 hidden representation $h \in \mathbb{R}^{m \times d}$를 Token Learner를 통해 $h' \in \mathbb{R}^d$로 변환한 다음, MLP(Multi-Layer Perceptron)를 통해 액션 $a \in \mathbb{R}^{d_a}$으로 매핑합니다.
  - **LoRA(Low-Rank Adaptation)**: 수십억 개의 파라미터를 가진 전체 VLA 모델을 미세 조정하는 데 필요한 계산 자원을 줄이기 위해 VLM 부분에 LoRA를 적용합니다. 학습 가능한 총 파라미터는 LoRA 파라미터 $\theta$와 액션 헤드 파라미터 $\phi$로 구성됩니다.

- **학습 파이프라인 (iRe-VLA)**:
  1. **Stage 0: 전문가 데이터셋을 이용한 지도 학습 (SFT)**:
     - 먼저 VLA 모델 $\pi_{\theta, \phi}$를 전문가 로봇 데이터셋 $D_e = \{(o_1, l_1, a_1), \dots, (o_i, l_i, a_i)\}$에 대해 표준 지도 미세 조정(MSE 손실)을 수행합니다.
     - $J_0(\theta, \phi) = \mathbb{E}_{(o,l,a) \sim D_e} [||\pi_{\theta,\phi}(o,l)-a||_2^2]$
     - 이 단계를 통해 초기 VLA 모델 $\pi^0_{\theta,\phi}$를 얻습니다.
  2. **Stage 1: VLM 고정 상태의 온라인 RL**:
     - SFT된 모델 $\pi^0_{\theta,\phi}$를 시작점으로 활용하여 새로운 작업에 대한 성능 향상을 위해 온라인 RL을 수행합니다.
     - 모델 붕괴를 방지하고 학습을 가속화하기 위해 **VLM 파라미터 $\theta$를 고정**하고 **액션 헤드 파라미터 $\phi$만 최적화**합니다. 이때 크리틱 헤드도 추가합니다.
     - $J_1(\phi) = \mathbb{E}_{((s_0,o_0,a_0), \dots) \sim p_{\phi}} \left[ \sum_{t} \gamma^t R(o_t, a_t) \right]$
     - RL 과정에서 로봇은 새로운 작업 해결을 위한 새로운 궤적 $x_i$를 발견하며, 이 성공적인 궤적들을 온라인 데이터셋 $D_{RL}$에 수집합니다.
  3. **Stage 2: 전문가 및 온라인 수집 데이터에 대한 지도 학습**:
     - Stage 1에서 새로운 작업을 학습하는 동안 이전에 학습한 작업에 대한 파국적 망각을 완화하기 위해, 새로 수집된 온라인 데이터 $D_{RL}$와 원본 전문가 데이터셋 $D_e$를 모두 사용하여 **전체 모델($\theta, \phi$)을 다시 지도 학습**합니다.
     - $J_2(\theta, \phi) = \mathbb{E}_{(o,l,a) \sim D_e \cup D_{RL}} [||\pi_{\theta,\phi}(o,l)-a||_2^2]$
  4. **Stage 1과 Stage 2 반복**: Stage 1에서 새로운 솔루션을 탐색하고, Stage 2에서 사용 가능한 모든 성공적인 궤적을 모방하는 과정을 반복함으로써, 대규모 VLA 모델은 점진적으로 더 넓은 범위의 작업을 처리하고 이전에 학습한 작업에 대한 파국적 망각을 방지하며 일반화 능력을 향상시킵니다.

## 📊 Results

저자들은 시뮬레이션 환경(Metaworld, FrankaKitchen) 및 실제 환경(Panda manipulator)에서 iRe-VLA 프레임워크의 효과를 검증했습니다.

- **훈련 안정성 및 계산 부담 관리**:

  - 표준 RL을 대규모 VLA 모델에 직접 적용할 경우 Metaworld 벤치마크의 5개 작업 중 4개에서 성능 저하를 보이는 등 불안정했습니다. 반면, iRe-VLA는 RL 단계에서 VLM을 고정하여 모델 붕괴를 방지하고 훈련 과정을 안정화했습니다.
  - 수십억 개의 파라미터를 가진 VLA 모델의 전체 미세 조정은 대부분의 로컬 머신 능력을 초과하지만, iRe-VLA는 RL 단계에서 VLM을 고정하고 경량 액션 헤드만 학습하여 로컬 머신에서 계산 부담을 감당할 수 있게 했습니다. SL 단계의 대규모 계산은 원격 서버로 위임하여 처리했습니다.

- **시뮬레이션 조작 실험**:

  - iRe-VLA는 전문가 데이터셋에 있는 기존 작업, RL로 훈련된 새로운 작업, 그리고 한 번도 보지 못한 미지 작업 모두에서 SFT 정책보다 훨씬 높은 성공률을 달성했으며, 표준 PPO-Replay 알고리즘보다 뛰어난 성능을 보였습니다.
  - **기존 작업 성능 향상**: Franka Kitchen 벤치마크의 'left-door-open' 작업에서 SFT 정책의 성공률이 0.43이었으나 iRe-VLA를 통해 0.83으로 향상되었습니다.
  - **RL 작업 성능 향상**: Metaworld의 'Button-Press-new'에서 SFT 0.56 -> iRe-VLA 1.00, 'Drawer-Open-new'에서 SFT 0.48 -> iRe-VLA 0.84 등으로 새로운 RL 작업에서 높은 성공률을 달성했습니다.
  - **미지 작업 일반화 능력 향상**: Metaworld의 미지 작업 10개에서 SFT 0.51 -> iRe-VLA 0.80으로 향상되었으며, Franka Kitchen에서도 미지 객체 색상 변화('Slide-door-open-red', 'Slide-door-open-yellow')에 대한 일반화 성능이 크게 개선되었습니다.

- **Ablation Study**: VLM을 항상 고정하는 'iRe-VLA-freeze' 방법은 성능 저하를 초래했습니다. 이는 액션 헤드의 표현력 한계와 VLM의 상위 계층 표현을 개선하지 못하기 때문으로 분석됩니다.

- **실제 환경 조작 실험**:
  - 초기 SFT 모델은 미지 객체(가지, 당근) 집기에 대해 0.35의 성공률을 보였습니다. iRe-VLA 파이프라인을 통해 이 성공률은 0.80으로 증가했습니다.
  - 기존 작업의 성공률은 안정적으로 유지되었으며, 미지 객체 집기 성공률 또한 0.37에서 0.61로 향상되었습니다.
  - 각 새로운 작업에 대한 실제 RL 훈련은 약 1시간이 소요되어 계산 효율성도 입증되었습니다.

## 🧠 Insights & Discussion

- **의의**: iRe-VLA는 온라인 RL의 탐색적 이점과 지도 학습의 안정성을 결합하여 대규모 VLA 모델을 효과적으로 개선하는 실용적인 방법론을 제시합니다. 이는 대규모 VLA 모델의 훈련 불안정성과 높은 계산 비용이라는 핵심 과제를 해결하고, 모델이 환경과의 상호작용을 통해 지속적으로 성능을 향상시키고 새로운 작업에 대한 일반화 능력을 강화할 수 있음을 보여줍니다.
- **한계**: 제안된 방법은 기존에 학습된 기술 유형 내에서 성능을 향상시키는 데는 효과적이지만, 희소 보상(sparse-reward) 온라인 RL 조건에서 완전히 새로운 기술을 학습하는 데는 제한적일 수 있습니다.

## 📌 TL;DR

**문제**: 지도 학습(SFT)으로 훈련된 VLA 모델은 환경 상호작용을 통한 성능 개선이 어려우며, 온라인 강화 학습(RL) 적용 시 훈련 불안정성과 막대한 계산 비용 문제가 발생합니다.
**제안 방법**: iRe-VLA는 온라인 RL 단계(VLM 고정, 액션 헤드만 훈련)와 지도 학습 단계(전체 모델을 전문가 및 온라인 수집 데이터로 재훈련)를 반복하여 이 문제를 해결합니다.
**주요 결과**: iRe-VLA는 훈련 안정성을 크게 높이고 계산 부담을 줄이며, 시뮬레이션 및 실제 환경 모두에서 VLA 모델의 성능과 미지 작업에 대한 일반화 능력을 성공적으로 향상시켰습니다.
