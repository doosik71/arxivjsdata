{
  "url": "http://arxiv.org/abs/2505.18719v1",
  "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable\n  Reinforcement Learning",
  "authors": "Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, Ziwei Wang",
  "year": 2025,
  "abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated\nimpressive performance on a range of robotic manipulation tasks by imitating\nhuman demonstrations. However, exploiting offline data with limited visited\nstates will cause execution failure in out-of-distribution scenarios.\nIntuitively, an exploration-based method that improves on online collected data\nat test time could address this limitation. We present VLA-RL, an algorithmic\nand systematic framework that leverages online reinforcement learning (RL) to\nimprove pretrained auto-regressive VLAs in downstream tasks. Within a unified\nperspective, we first introduce a trajectory-level RL formulation for\nauto-regressive VLA training, which models general robotic manipulation\ntrajectory as multi-modal multi-turn conversation. To address the challenge of\nsparse rewards, we fine-tune a pretrained vision-language model as a robotic\nprocess reward model, which is trained on pseudo reward labels annotated on\nautomatically extracted task segments. To scale up, we identify several\nimplementation findings that improve the stability and efficiency including\ncurriculum selection strategy, GPU-balanced vectorized environments, batch\ndecoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest\nfinetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in\nLIBERO, and even matches the performance of advanced commercial models such as\n$\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time\noptimization, indicating an early spark of inference scaling laws in robotics."
}