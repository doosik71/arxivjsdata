# VLA-Cache: 로봇 조작을 위한 적응형 토큰 캐싱을 통한 효율적인 Vision-Language-Action 모델

Siyu Xu, Yunke Wang, Chenghao Xia, Dihao Zhu, Tao Huang, Chang Xu

## 🧩 Problem to Solve

Vision-Language-Action (VLA) 모델은 강력한 멀티모달 추론 능력으로 시각 및 언어 지침을 처리하여 로봇 동작을 종단 간 방식으로 생성할 수 있습니다. 그러나 VLA 모델은 상당한 계산 비용을 요구하며, 이는 환경 변화에 신속하게 반응해야 하는 로봇 작업의 실시간 의사 결정에 큰 어려움을 야기합니다. 특히, 로봇 제어는 순차적인 의사 결정을 포함하며, 연속적인 단계 간에 시각 입력의 변화가 미미한 경우가 많습니다. 이 논문은 이러한 중복 계산 문제를 해결하여 VLA 모델의 효율성을 개선하는 것을 목표로 합니다.

## ✨ Key Contributions

- **VLA-Cache 제안:** 로봇 조작을 위한 효율적인 Vision-Language-Action 모델인 VLA-Cache를 제안합니다.
- **적응형 토큰 선택 메커니즘:** 연속적인 시각 입력 간에 변화가 거의 없는 시각 토큰을 적응적으로 식별하는 토큰 선택 메커니즘을 통합합니다.
- **KV-캐시를 통한 계산 결과 재사용:** 변하지 않은 토큰에 대한 계산 결과를 후속 단계에서 KV-캐시($K, V$ 캐시)를 통해 재사용하여 효율성을 크게 향상시킵니다.
- **작업 관련 토큰 필터링:** 모델의 정확도를 유지하기 위해 작업 관련 토큰을 정적 토큰 집합에서 필터링하는 정밀한 선택 방식을 도입합니다.
- **계층 적응형 토큰 재사용 전략:** VLA 디코더 내의 어텐션 분포 변화를 고려하여 각 계층의 어텐션 집중도에 따라 재사용되는 토큰의 비율을 조절하는 계층 적응형 전략을 제안합니다.
- **실험적 검증:** 시뮬레이션(LIBERO 벤치마크, SIMPLER) 및 실제 로봇 환경에서 VLA-Cache가 성공률을 최소한으로 희생하면서 실질적인 가속을 달성함을 입증합니다.

## 📎 Related Works

- **Vision-Language-Action 모델 (VLA Models):** 대규모 비전-언어 모델(VLM)의 능력을 확장하여 행동 양식을 통합하고 종단 간 시각-운동 제어를 가능하게 합니다 (예: Zitkovich et al., Li et al., Kim et al.). 높은 계산 비용이 실시간 배포의 도전 과제입니다.
- **VLM 가속화 기법:** 양자화(quantization), 가지치기(pruning)와 같은 일반적인 가속화 기법은 VLM 작업에서 성공을 거두었지만, 짧고 전문화된 행동 시퀀스를 요구하는 VLA 작업의 실시간 요구 사항을 간과하는 경향이 있습니다.
  - **토큰 수준 방법:** FastV (Chen et al.), SparseVLM (Zhang et al.) 등은 중복 토큰을 가지치기하거나 병합하여 추론을 최적화하지만, VLA 작업에서의 효과는 불분명합니다.
  - **VLA 아키텍처 수정:** DeeR-VLA (Yue et al.)는 추론 깊이를 동적으로 조정하고, QAIL (Park et al.)은 양자화 인식 학습을 통합합니다. RoboMamba (Liu et al.) 및 TinyVLA (Wen et al.)는 어텐션 메커니즘을 대체하거나 소형 모델을 처음부터 학습시키는 데 중점을 둡니다.
- **VLA-Cache의 차별점:** 기존 방법들과 달리 VLA-Cache는 훈련 없이 정적 토큰을 선택적으로 캐싱하고 동적 또는 작업 관련 토큰만 재계산함으로써, 중요한 시간적 단서를 보존하면서 추론 지연 시간을 줄이고 로봇 제어 효율성을 향상시킵니다.

## 🛠️ Methodology

VLA-Cache는 연속된 이미지 프레임에서 동적인 부분과 정적인 부분을 구분하고, 정적인 부분의 계산 결과를 재사용하여 효율성을 높입니다.

1. **Dynamic Token Selection (동적 토큰 선택)**
   - **Static Token Selection (정적 토큰 선택):**
     - 입력 이미지 $I$를 $p \times p$ 크기의 $N^2$개 패치 $P = \{P_{i,j}\}$로 나눕니다.
     - 연속적인 두 프레임 $I_t$와 $I_{t-1}$에 대해 해당 패치들 간의 코사인 유사도를 계산합니다:
       $$ \text{Sim}(P*{i,j}^t, P*{i,j}^{t-1}) = \frac{P*{i,j}^t \cdot P*{i,j}^{t-1}}{\|P*{i,j}^t\|\_2 \|P*{i,j}^{t-1}\|\_2} $$
     - 유사도 점수가 사전 정의된 임계값 $\tau$를 초과하는 패치를 정적 패치로 간주하고, 추가로 Top-k 필터를 적용하여 재사용할 최상위 패치 $P_{\text{top-k}}$를 선택합니다.
   - **Evicting Task-Relevant Tokens (작업 관련 토큰 제외):**
     - VLA 모델의 교차-어텐션 모듈에서 텍스트 토큰에서 비전 토큰으로의 어텐션 점수 $S_{\text{task-relevance}}$를 계산하여 각 비전 토큰의 작업 관련성을 결정합니다.
     - 특정 계층 $l$에서 텍스트 토큰과 시각 토큰 간의 어텐션 매핑 $A_{\text{vis-text}}^l$을 추출하고, 헤드별 평균을 계산합니다.
     - 여러 계층 $L$에 걸쳐 평균을 내어 최종 작업 관련성 점수를 얻습니다.
     - 이 점수를 기반으로 임계값 $\tau_{\text{task}}$ 이상인 토큰 $P_{\text{task-relevant}}$을 작업 관련 토큰으로 식별합니다.
     - 최종적으로 재사용 가능한 토큰 집합 $P_{\text{final}}$은 정적 토큰 집합 $P_{\text{static}}$에서 작업 관련 토큰 $P_{\text{task-relevant}}$을 제외하여 얻습니다:
       $$ P*{\text{final}} = P*{\text{static}} \setminus P\_{\text{task-relevant}} $$
2. **Layer Adaptive Token Reusing (계층 적응형 토큰 재사용)**
   - 각 계층의 어텐션 집중도가 다르다는 점에 착안하여, 엔트로피 측정치 $H_l$을 통해 어텐션 분포를 정량화합니다.
   - 계층별 엔트로피 비율 $R_l = \frac{H_{l-1} - H_l}{H_{l-1}}$을 정의하여 이전 계층 대비 현재 계층의 어텐션 집중도를 포착합니다.
   - 이 비율을 누적하여 누적 점수 $R_l^{\text{cum}}$을 얻고, 이를 통해 계층 $l$에서 재사용될 정적 토큰의 비율 $\alpha_l$을 결정합니다:
     $$ \alpha*l = \min \left(k \sum*{j=1}^l R_j, 1 \right) $$
        여기서 $k$는 하이퍼파라미터입니다. 어텐션 집중도가 높아질수록 더 많은 토큰을 재사용하도록 합니다.
3. **Inference with Cached Representation (캐시된 표현을 통한 추론)**
   - 추론 시, VLA-Cache는 $P_{\text{final}}$과 $\{\alpha_l\}$을 기반으로 재사용할 토큰 $P_{\text{reuse}}$를 결정합니다.
   - 쿼리 $Q_t^l(i) = W_Q^l H_t^l(i)$는 항상 재계산됩니다.
   - 키 $K_t^l(i)$와 값 $V_t^l(i)$는 $P_{\text{reuse}}$에 속하는 토큰에 대해서는 이전 시점의 캐시된 값 $K_{t-1}^l(i)$, $V_{t-1}^l(i)$를 재사용하고, 그렇지 않은 토큰에 대해서는 $W_K^l H_t^l(i)$, $W_V^l H_t^l(i)$와 같이 새로 계산합니다.
     $$ K*t^l(i) = \begin{cases} K*{t-1}^l(i), & \text{if } i \in P*{\text{reuse}} \\ W_K^l H_t^l(i), & \text{otherwise} \end{cases} $$
        $$ V_t^l(i) = \begin{cases} V*{t-1}^l(i), & \text{if } i \in P\_{\text{reuse}} \\ W_V^l H_t^l(i), & \text{otherwise} \end{cases} $$
   - 이는 각 계층에서 선택적으로 이루어지며, 동적 또는 작업 관련 토큰만 재계산하여 계산 비용을 줄이고 추론 시간을 단축합니다.
4. **Theoretical Analysis of Computational Complexity (계산 복잡도 이론적 분석)**
   - 표준 VLA 추론에서 각 트랜스포머 계층의 FLOPs 비용은 약 $4LD^2 + 2L^2D + 2LDM$ 입니다.
   - VLA-Cache는 계층당 유효 토큰 수 $L_r = \alpha \times P_{\text{final}}$로 줄여, 이론적인 FLOPs 감소를 가져옵니다.
   - 토큰 선택 과정의 오버헤드(패치 유사도 검사 $O(H^2)$, 교차 모달 어텐션 집계 $O(L_t L_v D)$, 엔트로피 계산 $O(L^2 D)$)는 전체 FLOPs 감소량보다 훨씬 작습니다.

## 📊 Results

- **LIBERO 벤치마크 (시뮬레이션):**
  - OpenVLA 모델을 기반으로 VLA-Cache는 FLOPs를 $27.31\%$ 감소시키고, 추론 속도를 $1.63$배 향상시키면서, 전체 성공률은 $0.3\%$만 감소했습니다 (평균 성공률 $75.0\% \to 74.7\%$).
  - SparseVLM 및 FastV와 같은 토큰 가지치기 방법들은 VLA 시나리오에서 오히려 추론 시간이 증가하거나 성공률이 더 크게 감소하는 경향을 보였습니다.
  - 계층 적응형 전략은 작업 관련 토큰을 필터링하는 것만으로 $82.6\%$에서 $83.8\%$로 성공률을 향상시켰습니다.
- **SIMPLER 환경 (시뮬레이션):**
  - CogACT 모델에 적용했을 때, VLA-Cache는 기준 모델과 유사한 성공률(Visual Matching $74.8\% \to 74.4\%$, Variant Aggregation $61.3\% \to 62.3\%$)을 유지하면서 FLOPs를 약 $20\%$ 감소시키고 추론 지연 시간을 $1.37$배 줄였습니다.
- **실제 로봇 (Kinova Jaco2):**
  - 네 가지 실제 로봇 조작 작업에서 VLA-Cache는 평균 성공률을 $82.1\%$에서 $84.5\%$로 $2.4\%$ 향상시키는 예상치 못한 결과를 보였습니다. 동시에 FLOPs 및 추론 시간도 크게 감소했습니다.

## 🧠 Insights & Discussion

- VLA-Cache는 로봇 조작과 같이 시각 입력의 변화가 미미한 순차적 의사 결정 작업에서 VLA 모델의 고질적인 계산 비용 문제를 효과적으로 해결합니다.
- 정적 토큰 선택과 작업 관련 토큰 제외는 중복 계산을 크게 줄이면서도 모델의 핵심 정보 손실을 방지하여 정확도를 유지합니다. 특히, 작업 관련 토큰을 재계산하도록 강제하는 것은 로봇 조작의 정밀도를 유지하는 데 필수적입니다.
- 계층 적응형 토큰 재사용 전략은 VLA 디코더의 계층별 어텐션 분포 특성을 활용하여, 초기 계층에서는 더 많은 토큰을 재계산하고 깊은 계층에서는 더 많이 재사용함으로써 모델 성능을 미세 조정하고 향상시킬 수 있음을 보여줍니다.
- 다른 가속화 방법들이 VLM과 VLA 사이의 시퀀스 길이 차이로 인해 VLA 작업에서 효율성 향상에 어려움을 겪는 반면, VLA-Cache는 프레임 간 토큰 재사용을 통해 로봇 행동 생성에 효과적인 접근 방식을 제공합니다.
- 실제 로봇 환경에서의 성공률 향상은 VLA-Cache가 단순히 효율성만 높이는 것이 아니라, 기본 모델의 견고성을 바탕으로 불필요하거나 방해가 되는 토큰을 제거함으로써 모델의 강건성을 더 높일 수 있음을 시사합니다.

## 📌 TL;DR

VLA-Cache는 Vision-Language-Action (VLA) 모델의 높은 계산 비용을 해결하기 위해 연속된 로봇 조작 시퀀스에서 시각 토큰의 변화가 적다는 점에 착안합니다. 이 방법은 정적 토큰을 식별하고 KV-캐시를 통해 계산 결과를 재사용하며, 작업 관련 토큰은 정확도를 위해 재계산하도록 필터링합니다. 또한, 계층별 어텐션 집중도에 따라 재사용 토큰 비율을 동적으로 조절하는 계층 적응형 전략을 도입합니다. 실험 결과, VLA-Cache는 시뮬레이션 및 실제 로봇 환경에서 성공률을 거의 희생하지 않으면서 최대 $1.7$배의 추론 속도 향상을 달성하며, VLA 모델의 효율적인 배포 가능성을 열어줍니다.
