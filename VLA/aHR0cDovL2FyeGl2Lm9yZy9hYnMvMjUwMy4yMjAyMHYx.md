# CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models

Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, Tsung-Yi Lin

## 🧩 Problem to Solve

기존 Vision-Language-Action (VLA) 모델은 주로 직접적인 입력-출력 매핑에 초점을 맞추어, 복잡한 로봇 조작 작업에 필수적인 중간 추론 단계를 결여하고 있습니다. 이로 인해 현재 VLA는 시간적 계획(temporal planning)이나 추론 능력이 부족하여 일반화 성능에 한계가 있습니다. 본 논문은 이러한 중간 추론 단계의 부재를 해결하고자 합니다.

## ✨ Key Contributions

- **시각적 사고 연쇄 (Visual Chain-of-Thought) 추론 도입**: 로봇 제어를 위한 중간 추론 단계로 서브골 이미지(subgoal image) 생성을 활용하는 시각적 사고 연쇄 추론 방식을 제안합니다. 이는 추상적인 표현(바운딩 박스, 키포인트 등) 대신 영상에서 샘플링된 서브골 이미지를 사용합니다.
- **CoT-VLA 시스템 개발**: 시각적 사고 연쇄 추론을 통합한 7B 파라미터의 VLA 모델인 CoT-VLA를 제안합니다. 이 모델은 픽셀 및 텍스트 생성을 위한 인과적 어텐션(causal attention)과 액션 예측을 위한 완전 어텐션(full attention)을 결합한 하이브리드 어텐션 메커니즘을 사용합니다.
- **최고 수준의 성능 달성**: 시뮬레이션 및 실제 환경에서의 포괄적인 평가를 통해 시각적 사고 연쇄 추론이 VLA 성능을 향상시키며, 제안된 시스템이 다양한 로봇 플랫폼과 작업에서 최첨단 성능을 달성함을 입증했습니다.

## 📎 Related Works

- **사고 연쇄(Chain-of-Thought, CoT) 추론**: 대규모 언어 모델(LLM)에서 단계별 추론을 통해 복잡한 문제 해결 능력을 향상시키는 기법으로, 초기 연구는 텍스트 기반 추론에 집중했습니다.
- **멀티모달 CoT**: 시각 도메인으로 확장되어 경계 상자 생성, 중간 이미지 채우기, CLIP 임베딩 생성 등 다양한 방식으로 시각 정보 처리의 반복적 단계를 탐구했습니다.
- **구현된(Embodied) CoT**: 로봇 공학 분야에서 텍스트 계획, 키포인트/바운딩 박스 라벨링, 미래 이미지 궤적 생성 등으로 확장되어 왔습니다.
- **Vision-Language-Action (VLA) 모델**: 사전 학습된 Vision-Language 모델(VLM)을 활용하여 자연어 지시와 시각적 관찰을 로봇 액션에 매핑하는 모델입니다. 대부분의 기존 VLA는 직접적인 액션 예측에 중점을 둡니다.
- **서브골(Subgoal) 생성 및 목표 조건부 모방 학습**: 이전 연구에서 서브골 생성 및 목표 조건부 모방 학습이 탐구되었지만, CoT-VLA는 이를 VLA의 중간 사고 연쇄 추론 단계로 통합한 첫 시도입니다.

## 🛠️ Methodology

CoT-VLA는 두 가지 순차적인 단계로 작동합니다:

1. **서브골 이미지 예측 (Visual Reasoning)**:
   - 현재 관찰 $s_t$ 및 언어 지시 $l$로부터 $n$프레임 앞선 서브골 이미지 $\hat{s}_{t+n}$을 예측합니다.
   - $$ \hat{s}_{t+n} \sim P_\theta(s\_{t+n} | s_t, l) $$
   - 이 단계는 로봇 시연 데이터 $D_r$와 액션 없는 비디오 데이터 $D_v$ 모두를 사용하여 훈련됩니다.
2. **액션 시퀀스 생성 (Action Generation)**:
   - 현재 관찰 $s_t$, 언어 지시 $l$, 그리고 예측된 서브골 이미지 $\hat{s}_{t+n}$에 기반하여 $m$개의 액션으로 구성된 짧은 액션 시퀀스 $\{\hat{a}_t, ..., \hat{a}_{t+m}\}$을 생성합니다.
   - $$ \{\hat{a}_t, ..., \hat{a}_{t+m}\} \sim P*\theta(\{a_t, ..., a*{t+m}\} | s*t, l, \hat{s}*{t+n}) $$
   - 이 단계는 로봇 시연 데이터 $D_r$만 사용하여 훈련됩니다.

**기반 모델**: VILA-U [67] (통합 멀티모달 파운데이션 모델)을 기반으로 합니다. VILA-U는 이미지와 텍스트 토큰을 이해하고 생성할 수 있으며, 잔차 양자화(residual quantization)를 사용하여 시각적 특징의 표현 능력을 향상시킵니다.

**훈련 절차**:

- **시각 토큰 예측**: 서브골 이미지 생성을 위해 각 훈련 시퀀스 $(l, s_t, s_{t+n})$에서 깊이 변환기(depth transformer)가 LLM 생성 코드 임베딩 $h_j$를 기반으로 잔차 토큰을 예측합니다.
  - $$ L*{\text{visual}} = - \sum_j \sum*{d=1}^D \log P*\delta(k*{jd} | k\_{j,\lt d}) $$
- **액션 토큰 예측**: 액션 예측을 위해 각 훈련 시퀀스 $(l, s_t, s_{t+n}, a_t, ..., a_{t+m})$에서 액션 $a_i$는 7개의 토큰으로 표현되며, 각 액션 차원은 독립적으로 이산화됩니다. 액션 토큰 예측에는 완전 어텐션(full attention)을 사용합니다.
  - $$ L*{\text{action}} = - \sum*{i=1}^m \log P*\theta(a_t ... a*{t+m} | l, s*t, s*{t+n}) $$
- 전체 손실은 $L = L_{\text{action}} + L_{\text{visual}}$입니다.
- **액션 청킹 (Action Chunking)**: 단일 액션 대신 $m$개의 액션 시퀀스를 예측하여 성능을 향상시킵니다.
- **하이브리드 어텐션 (Hybrid Attention)**: 이미지 및 텍스트 생성에는 인과적 어텐션을, 액션 예측에는 완전 어텐션을 사용합니다.

**훈련 단계**:

- **사전 훈련 (Pretraining)**: Open X-Embodiment 데이터셋의 일부 $D_r$와 EPIC-KITCHENS [27], Something-Something V2 [20] 같은 액션 없는 비디오 데이터셋 $D_v$를 혼합하여 VILA-U 모델을 사전 훈련합니다.
- **적응 단계 (Adaptation)**: 다운스트림 작업에 맞게 특정 로봇 시연 데이터 $D_r$로 사전 훈련된 모델을 미세 조정합니다.

## 📊 Results

- **LIBERO 시뮬레이션 벤치마크**: CoT-VLA는 LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long의 모든 작업 스위트에서 기존 VLA 및 Diffusion Policy, Octo, OpenVLA 같은 최첨단 모델보다 뛰어나거나 경쟁력 있는 성능을 달성했습니다. 특히, 시각적 추론을 통한 언어 지시 이해 능력이 향상되었습니다.
- **Bridge-V2 실제 로봇 실험**: CoT-VLA는 시각, 모션, 의미, 언어 일반화 범주에서 OpenVLA와 유사하거나 더 나은 성능을 보였습니다. SUSIE [2]에 비해 서브골 이미지 품질은 낮을 수 있으나, 전반적인 작업 성공률은 더 높았습니다.
- **Franka-Tabletop 실제 로봇 실험**: 소량의 시연 데이터로 미세 조정된 CoT-VLA는 단일 지시 및 다중 지시 작업 모두에서 최고 평균 성능을 달성했습니다. 특히 언어 이해가 중요한 다중 지시 작업에서 OpenX 데이터셋으로 사전 훈련된 모델들(Octo, OpenVLA)보다 우수했습니다.
- **제거 연구 (Ablation Study)**:
  - **액션 청킹 (Action Chunking)**: 단일 액션 예측보다 액션 시퀀스 예측이 일관되게 성능을 향상시켰습니다.
  - **하이브리드 어텐션 (Hybrid Attention)**: 액션 시퀀스 예측에 완전 어텐션을 추가하면 성능이 더욱 향상되었습니다.
  - **시각적 사고 연쇄 (Visual CoT)**: CoT-VLA의 완전한 접근 방식이 최상의 결과를 보이며, 시각적 CoT 추론의 효과를 입증했습니다.
  - **사전 훈련 (Pretraining)**: OpenX 및 액션 없는 비디오 데이터로 사전 훈련하는 것이 Franka-Tabletop 설정에서 46.7%의 상대적 성능 향상으로 이어져, 다운스트림 작업 적응에 중요함을 보여주었습니다.
- **향상된 시각적 추론의 이점**: 액션 없는 비디오 데이터를 활용한 시각적 추론 능력 향상이 로봇 작업 성능으로 직접 이어짐을 보여주었습니다. OOD(out-of-distribution) 작업에서 생성된 목표 이미지 대신 ground-truth 목표 이미지를 사용했을 때 성공률이 40% 증가했습니다.

## 🧠 Insights & Discussion

- **시각적 사고 연쇄의 중요성**: CoT-VLA는 로봇이 액션을 취하기 전에 미래의 원하는 상태를 시각적으로 "생각"하게 함으로써 복잡한 조작 작업에서 더 나은 지시 이해 및 일반화 성능을 보여줍니다.
- **액션 없는 데이터 활용**: 액션 주석이 없는 풍부한 비디오 데이터를 서브골 이미지 생성 훈련에 활용할 수 있어, 시각적 추론 능력 강화에 큰 잠재력을 가집니다. 이는 로봇 시연 데이터의 희소성 문제를 완화할 수 있습니다.
- **제한 사항**:

  - **계산 오버헤드**: 추론 시 중간 이미지 토큰 생성은 직접적인 액션 생성 방식보다 상당한 계산 오버헤드를 발생시킵니다 (액션 청킹 시 평균 7배 느림).
  - **낮은 시각적 품질**: 자율회귀적 이미지 생성 방식은 최신 Diffusion 기반 모델에 비해 시각적 품질이 낮을 수 있습니다.
  - **불연속적 액션**: 액션 청킹은 청크 간 불연속적인 액션을 유발할 수 있으며, 실행 중 고주파 피드백이 부족합니다.
  - **OOD 일반화 한계**: 현재 CoT-VLA는 완전히 새로운 작업에 대한 시각적 추론 일반화 능력에 한계가 있습니다.

- **향후 연구 방향**:
  - 빠른 이미지 생성 또는 LLM 추론 기술을 통합하여 계산 오버헤드를 줄일 수 있습니다.
  - 향상된 통합 멀티모달 모델을 통해 이미지 생성 품질을 높일 수 있습니다.
  - 시간적 평활화(temporal smoothing) 기술이나 단계별 예측 방식을 통해 액션의 불연속성을 해결할 수 있습니다.
  - 대규모 비디오/이미지 생성 모델 및 월드 모델의 발전이 시각적 추론 및 예측 모델링을 통해 일반화 능력 향상에 기여할 것으로 기대됩니다.

## 📌 TL;DR

CoT-VLA는 로봇이 시각적 서브골을 먼저 예측하여 "시각적으로 생각"한 후 액션 시퀀스를 생성하는 시각적 사고 연쇄 추론(Visual CoT)을 VLA 모델에 통합합니다. 이를 통해 기존 VLA의 중간 추론 능력 부족 문제를 해결하며, 액션 없는 비디오 데이터를 활용하여 시각적 추론 능력을 강화합니다. CoT-VLA는 하이브리드 어텐션과 액션 청킹을 사용하여 LIBERO 시뮬레이션 및 실제 로봇 작업에서 최첨단 성능을 달성했으며, 특히 복잡한 언어 지시 이해와 일반화 능력을 향상시켰습니다. 계산 오버헤드 및 이미지 품질의 한계는 있지만, 향상된 시각적 추론이 로봇 성능에 직접적인 영향을 미침을 보여주며, 미래 대규모 멀티모달 모델의 발전 가능성을 시사합니다.
