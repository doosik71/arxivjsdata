# OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction

Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel

## 🧩 Problem to Solve

로봇 행동 예측을 목표로 하는 기존 Vision-Language-Action (VLA) 모델들은 시각 및 언어 특성 추출을 위해 사전 훈련된 Vision-Language Model (VLM)을 미세 조정합니다. 하지만 로봇 데이터셋은 대규모 시각-언어 데이터셋에 비해 의미론적 다양성이 부족하여, 이러한 미세 조정 과정이 VLM의 사전 훈련된 강력한 시각-언어 정렬(semantic alignment)을 손상시키고 미확인(unseen) 객체나 환경에서 일반화 성능을 저하시키는 문제를 겪습니다. 이 논문은 미세 조정을 통해 약화될 위험 없이, 사전 훈련된 VLM의 일반화 능력을 효과적으로 활용하는 방법을 모색합니다.

## ✨ Key Contributions

- **OTTER 아키텍처 제안:** 사전 훈련된 VLM의 의미론적 정렬 능력을 활용하여 더 나은 일반화를 달성하는 새로운 VLA 모델인 OTTER를 제안합니다. OTTER는 텍스트를 인지하는 시각 특성(text-aware visual features)을 추출하여 사전 훈련된 VLM을 고정(frozen) 상태로 유지하며, 풍부한 시각-언어 이해를 로봇 작업 실행에 활용합니다.
- **우수한 제로샷 일반화 성능:** 사전 훈련된 VLM을 미세 조정하지 않고 고정함으로써, 대규모 사전 훈련을 통해 학습된 의미론적 이해를 효과적으로 활용하여 미확인 객체 및 환경을 처리합니다. 이를 통해 OTTER는 미확인 로봇 조작 작업에서 최첨단 VLA 모델보다 훨씬 뛰어난 제로샷 일반화 능력을 보여줍니다.
- **다축 스케일링 가능성:** OTTER의 성능이 더 큰 사전 훈련 VLM 인코더, 증가된 정책 네트워크 용량, 그리고 더 큰 로봇 데이터셋에서의 사전 훈련을 통해 여러 측면에서 확장될 수 있음을 실증적으로 보여줍니다.

## 📎 Related Works

- **Vision-Language Pre-training (VLP):** CLIP (Radford et al., 2021) 및 SigLIP (Zhai et al., 2023)과 같은 모델들은 대규모 이미지-텍스트 쌍 데이터셋에 대한 대조 학습(contrastive learning)을 통해 강력한 시각-언어 정렬과 제로샷 능력을 보여주었습니다. 하지만 CLIP 위에 미세 조정을 하거나 추가 레이어를 적용하면 (Kerr et al., 2023; Lan et al., 2024) 바닐라 CLIP에 비해 추론 능력과 일반화가 약화될 수 있다는 연구가 OTTER의 고정 인코더 접근 방식의 동기가 되었습니다.
- **Vision-Language-Action (VLA) Models:** LLM과 VLM의 성공에 영감을 받아 로봇 파운데이션 모델 개발이 활발합니다. Octo (Octo Model Team et al., 2024), OpenVLA (Kim et al., 2024), RT-2 (Brohan et al., 2023)와 같은 대부분의 기존 VLA 모델들은 시각, 언어, 로봇 고유수용성(proprioception) 데이터를 개별적으로 인코딩하여 하나의 트랜스포머 정책(transformer policy)에 직접 전달합니다 (Direct Feature Passing). OTTER는 이러한 방식과 달리, 로봇 정책에 전달하기 전에 시각 및 언어 입력을 텍스트 인지 시각 특성 추출을 통해 먼저 결합합니다.

## 🛠️ Methodology

OTTER는 사전 훈련된 VLM의 시각-언어 정렬을 활용하여 로봇 조작 정책을 학습합니다. 핵심은 사전 훈련된 CLIP 모델의 인코더를 고정하고, 언어 지침에 따라 작업 관련 시각 특성을 선택적으로 추출하는 것입니다.

1. **텍스트 인지 시각 특성 추출 (Text-Aware Visual Feature Extraction):**
   - **입력:** 사전 훈련된 CLIP 모델에서 언어 인코더를 통해 텍스트 토큰 특성 $f_l$ (m 토큰)을 추출하고, 비전 인코더의 마지막 어텐션 레이어 출력 $X_{attn}$을 시각 특성 $f_v$ (n 패치 토큰)로 사용합니다. $X_{attn}$이 $X_{out}$보다 더 깨끗한 의미론적 정보를 담고 있기 때문입니다.
   - **정규화:** 텍스트 특성($f_l$)과 시각 특성($f_v$)을 CLIP의 투영 행렬 $w_l, w_v$와 L2 정규화를 사용하여 정규화합니다: $\hat{f}_l = \text{LN}_{\text{final}}(f_l)w_l$, $\hat{f}_v = \text{LN}_{\text{post}}(f_v)w_v$.
   - **온도 가중 어텐션 (Temperature-Weighted Attention):** 정규화된 특성을 결합하여 텍스트 인지 시각 토큰 $f_{vl} \in \mathbb{R}^{m \times d}$를 생성합니다.
     $$f_{vl} = \text{softmax}(\hat{f}_l \hat{f}_v^\top / \tau)(\hat{f}_v + \text{PE})$$
     여기서 $\tau$는 학습 가능한 온도 파라미터이며, $\text{PE}$는 2D 사인-코사인 위치 임베딩입니다. $\text{softmax}$는 특정 언어 토큰과 관련된 패치 특성을 선택하고 가중 평균을 계산하는 선택 함수 역할을 합니다. 중요한 점은 **전체 CLIP 모델은 훈련 내내 고정**되며, 오직 $\tau$만 학습 가능하다는 것입니다.
2. **모델 아키텍처:**
   - **정책 네트워크 입력:**
     - 추출된 텍스트 인지 시각 특성 $f_{vl}$은 각 카메라에 대해 학습 가능한 교차 어텐션 풀링(cross-attention pooling)을 통해 단일 토큰 $f'_{vl}$로 압축됩니다.
     - 텍스트 특성 $f_l$ 또한 학습 가능한 교차 어텐션 풀링을 통해 단일 텍스트 토큰 $f'_l \in \mathbb{R}^{d_l}$로 압축됩니다.
     - 로봇의 고유수용성(proprioceptive) 상태는 MLP를 통해 인코딩되어 신체(embodiment) 특성 $f_e$를 생성합니다.
     - 시점 $t$에서 $f'_l$, $f'_{vl}$, $f_e$가 채널 차원을 따라 연결되어 단일 토큰 $f_t$를 형성합니다.
   - **정책 네트워크 및 액션 헤드:** $f_t$는 4개의 레이어와 8개의 헤드, 512의 은닉 차원을 가진 인과적 트랜스포머(causal transformer) 정책 네트워크의 입력으로 사용됩니다. $T=12$ 스텝의 컨텍스트 윈도우로 모델은 미래 행동($a_t$)을 자기회귀적으로 예측합니다.

## 📊 Results

- **실제 로봇 실험 (단일 조작: Pick and Place):**
  - OTTER는 훈련 작업에서 68%, 미확인 작업에서 62%의 성공률을 달성했습니다. OXE로 사전 훈련된 OTTER-OXE는 훈련 72%, 미확인 73%로 더 높은 성능을 보였습니다.
  - 기존 최첨단 모델인 Octo, OpenVLA, DFP-OTTER는 미확인 작업에서 4%~12%의 저조한 성공률을 보이며, OTTER의 일반화 능력이 훨씬 우수함을 입증했습니다.
- **실제 로봇 실험 (다중 조작: Pouring, Drawer, Poking, Pick and Place):**
  - OTTER-OXE-L(더 큰 모델, OXE 사전 훈련)은 미확인 작업에서 평균 77%의 높은 성공률을 기록했습니다 (Pouring 77%, Drawer 75%, Poking 93%, Pick and Place 75%).
  - Octo와 OpenVLA는 Pouring 및 Drawer 작업에서 0%의 성공률을 기록하며 다중 조작 제로샷 일반화의 어려움을 드러냈습니다.
- **시뮬레이션 실험 (LIBERO 벤치마크):**
  - OTTER는 훈련 작업에서 평균 85%, 미확인 작업에서 59%의 성공률을 달성했습니다.
  - 다른 모델들은 훈련 작업에서는 유사한 성능을 보였으나, 미확인 작업에서는 26%~29%로 크게 낮은 성공률을 보였습니다.
- **어블레이션 연구 (Ablation Studies):**
  - 신체(embodiment) 특성($f_e$)이 없으면 성능이 훈련 작업에서 28%, 미확인 작업에서 33% 하락하여, $f_e$가 작업 완수와 일반화에 필수적임을 보여주었습니다.
  - CLIP 비전 인코더 없이 (ViT를 처음부터 훈련) OTTER를 사용하면 성능이 50% 이상 급락하며, 사전 훈련된 VLM 사용의 중요성을 강조했습니다.
  - CLIP을 미세 조정한 OTTER는 고정된 CLIP을 사용한 OTTER보다 성능이 현저히 낮아 (훈련 26% vs 68%, 미확인 15% vs 62%), 미세 조정이 VLM의 시각-언어 이해 능력을 저하시켜 일반화 능력을 손상시킨다는 것을 시사합니다.
- **VLM 스케일업:** CLIP 모델 크기가 커질수록 (ViT-B/32 $\to$ ViT-B/16 $\to$ ViT-L/14), OTTER의 작업 성공률도 크게 향상되었습니다 (미확인 Pick and Place 작업에서 ViT-B/32 대비 ViT-L/14가 39.3% 향상).

## 🧠 Insights & Discussion

- **핵심 통찰:** 사전 훈련된 VLM의 인코더를 고정하고 텍스트 인지 시각 특성을 선택적으로 추출하는 OTTER의 접근 방식은, VLM의 강력한 의미론적 이해를 보존하여 로봇이 미확인 시나리오에 효과적으로 일반화하도록 돕는다는 것을 입증했습니다. 기존 VLA 모델처럼 VLM을 미세 조정하는 것은 이러한 이점을 상쇄합니다.
- **시사점:** OTTER는 작업 계획 (관련 시각 특성 선택)과 로봇 행동 계획 (적절한 행동 예측)을 효과적으로 분리합니다. 이는 VLM의 사전 훈련된 강력한 표현력을 로봇 제어에 직접적으로 활용할 수 있게 합니다.
- **한계:**
  - SE(3) 변환으로 쉽게 매개변수화할 수 없는 다양한 로봇 형태(예: 다지 핸드 로봇)에 대한 확장성.
  - 장기적인 작업(long-horizon tasks)이나 더 복잡한 장면에서의 확장 가능성에 대한 탐색이 부족합니다.
- **의의:** 이 연구는 로봇 데이터에서 시각-언어 정렬을 직접 학습하거나 미세 조정하는 대신, 사전 훈련된 VLM의 기존 정렬을 보존하는 것이 더 유능하고 일반화 가능한 로봇 학습 시스템 개발에 유익함을 시사합니다.

## 📌 TL;DR

**문제:** 기존 VLA 모델은 사전 훈련된 VLM을 미세 조정하여 로봇 데이터의 낮은 다양성으로 인해 VLM의 강력한 시각-언어 정렬과 제로샷 일반화 능력을 잃습니다.
**방법:** OTTER는 사전 훈련된 CLIP 모델의 비전 및 언어 인코더를 **고정**하고, 언어 지침과 시각 패치 특성 간의 의미론적 유사도를 계산하여 **텍스트 인지 시각 특성**을 선택적으로 추출합니다. 이 특성과 언어 토큰, 고유수용성 데이터를 인과적 트랜스포머 정책에 입력하여 로봇 행동을 예측합니다.
**발견:** OTTER는 시뮬레이션 및 실제 환경 모두에서 최첨단 VLA 모델보다 훨씬 뛰어난 성능과 **강력한 제로샷 일반화 능력**을 보여줍니다. VLM을 미세 조정하는 대신 고정하는 것이 일반화에 훨씬 효과적이며, 더 큰 VLM과 정책 용량, 데이터셋을 통해 성능이 확장됨을 입증했습니다.
