# LLaDA-VLA: Vision Language Diffusion Action Models

Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, Xiaoyan Sun

## 🧩 Problem to Solve

- 기존 자동회귀(auto-regressive) 기반 시각-언어 모델(VLM)은 로봇 조작을 위한 시각-언어-행동(VLA) 모델에서 강점을 보였지만, 순차적인 토큰 생성 방식은 효율성과 유연성을 제한합니다.
- 최근 마스크 확산 모델(MDM)은 텍스트 생성 및 멀티모달 애플리케이션에서 경쟁력 있는 성능을 보여주며 확산 기반 VLM(d-VLM)으로 발전했습니다. 그러나 이러한 d-VLM을 로봇 정책 학습에 활용하는 연구는 거의 이루어지지 않았습니다.
- d-VLM을 로봇 도메인에 적용하는 데에는 두 가지 주요 도전 과제가 있습니다:
  1. **도메인 간극 (Domain Gap):** d-VLM은 고수준의 의미론적 정보를 담은 대규모 일반 데이터셋으로 훈련되는 반면, VLA는 정확한 행동 생성을 위해 저수준 시각 단서를 해석해야 합니다.
  2. **구조화된 행동 시퀀스 생성의 어려움:** 마스크 확산 패러다임은 본질적으로 구조화된 행동 시퀀스 생성에 적합하지 않습니다. 기존 디코딩 전략은 로봇 행동의 강력한 계층적 의존성을 모델링하기 어렵고 합리적인 행동 궤적을 생성하는 데 실패합니다.

## ✨ Key Contributions

- 사전 학습된 확산 기반 시각-언어 모델(d-VLM)을 기반으로 구축된 최초의 Vision-Language-Diffusion-Action 모델(LLaDA-VLA)을 제안하여 로봇 정책 학습을 위한 새로운 패러다임을 확립했습니다.
- 마스크 확산 모델이 행동 생성에 잘 맞도록 두 가지 핵심 기술을 개발했습니다:
  - **지역화된 특수 토큰 분류(Localized Special-token Classification) 전략:** 도메인 적응 난이도를 줄이기 위해 분류 공간을 특수 행동 토큰으로 제한합니다.
  - **계층적 행동 구조 디코딩(Hierarchical Action-structured Decoding) 전략:** 행동 내(intra-action) 및 행동 간(inter-action)의 의존성을 고려하여 행동 생성에 원활하게 통합됩니다.
- SimplerEnv 및 CALVIN 시뮬레이션 벤치마크와 WidowX 실제 로봇에 대한 광범위한 실험을 통해 LLaDA-VLA의 뛰어난 성능을 입증했으며, 로봇 조작에서 d-VLM의 잠재력을 강조했습니다.

## 📎 Related Works

- **대규모 언어 확산 모델 (Large Language Diffusion Models):**
  - 이미지 도메인에서 큰 발전을 이룬 확산 모델을 텍스트 생성으로 확장하려는 시도.
  - LLaDA [39], Dream7B [53]와 같은 마스크 확산 모델은 자동회귀 LLM과 유사한 성능과 확장성을 보여줌.
  - LLaDA-V [55], MMaDA [52], LaViDA [26] 등 멀티모달(시각-언어) 영역으로 확장.
- **시각-언어-행동 모델 (Vision-Language-Action Models, VLAs):**
  - VLM [1, 21, 24, 31, 32]의 강력한 멀티모달 이해 및 일반화 능력을 활용하여 로봇 정책을 개발하려는 노력.
  - RT-2 [6], OpenVLA [23], LLARVA [40], CogACT [25], $\pi^0$ [4]와 같은 다양한 VLA 모델이 개발됨.
  - 기존 VLA 모델은 거의 대부분 자동회귀 VLM 기반으로 구축되었으며, 확산 기반 VLM의 잠재력은 미개척 상태.

## 🛠️ Methodology

LLaDA-VLA는 사전 학습된 d-VLM을 로봇 조작에 효과적으로 적용하기 위한 새로운 프레임워크를 제안합니다.

1. **마스크 확산 모델 (Mask Diffusion Models, MDMs) 개요:**
   - MDM은 이산 토큰에 대한 순방향-역방향 확산 과정을 기반으로 하는 생성 패러다임으로, 자동회귀 모델과 근본적으로 다릅니다.
   - **순방향 과정:** 길이 $N$ 및 어휘 크기 $V$인 입력 시퀀스 $x_0 = \{x_{i}^{0}\}_{i=1}^{N}$ 에서 각 토큰 $x_{i}^{0}$는 확률 $t$로 특수 마스크 토큰 $[M]$으로 독립적으로 대체됩니다:
     $$q_{t|0}(x_t|x_0) = \prod_{i=0}^{N-1} q_{t|0}(x_{i}^{t}|x_{i}^{0})$$
     $$q_{t|0}(x_{i}^{t}|x_{i}^{0}) = \begin{cases} 1-t, & x_{i}^{t} = x_{i}^{0} \\ t, & x_{i}^{t} = [M] \end{cases}$$
   - **역방향 과정:** MDM은 완전히 마스크된 시퀀스에서 시작하여 마스크된 토큰을 점진적으로 의미 있는 내용으로 변환합니다. 각 샘플링 단계에서 마스크 예측기 $p_\theta(x_{i}^{0}|x_t)$를 통해 토큰의 일부가 예측되고, 신뢰도가 낮은 토큰은 다시 마스크되어 다음 샘플링 단계에서 재예측됩니다.
   - **훈련 목표:** 마스크 예측기 $p_\theta$는 마스크된 토큰에 대해서만 교차 엔트로피 손실로 훈련됩니다:
     $$L(\theta) \triangleq -\mathbb{E}_{t,x_0,x_t} \left[ \frac{1}{t} \sum_{i=1}^{L} \mathbb{1}_{\{x_{i}^{t} = M\}} \log p_\theta(x_{i}^{0}|x_t) \right]$$
2. **LLaDA-VLA 모델 아키텍처:**
   - **시각-언어 모듈:** LLaDA-V [55]를 따라 LLaDA [39]를 언어 백본으로 사용하고, SigLIP-2 [49]를 시각 인코더로, MLP를 프로젝터로 사용합니다.
   - **입력:** 로봇의 작업을 지정하는 언어 지침과 전면 RGB 이미지.
   - **행동 토큰화 및 청킹 (Action Tokenization and Chunking):**
     - 연속적인 행동 값을 $V_a$ 크기의 이산적인 구간으로 이산화합니다.
     - 기존 어휘에 $V_a$개의 특수 토큰 $S = \{s_0, s_1, \dots, s_{V_a-1}\}$을 추가합니다.
     - 각 타임스텝의 행동은 7개의 특수 행동 토큰(위치 변위 3개, 회전 변화 3개, 그리퍼 상태 1개)으로 표현됩니다.
     - 모델은 $K$개의 연속적인 타임스텝을 포함하는 행동 청크를 예측하여 다단계 궤적을 생성합니다.
3. **지역화된 특수 토큰 분류 (Localized Special-token Classification, LSC):**
   - 사전 학습된 d-VLM은 전체 어휘에 대한 분류를 수행하지만, 로봇 행동 생성에서는 특수 행동 토큰만 예측하면 됩니다.
   - 학습 난이도를 줄이기 위해, 전체 어휘 크기 $V_{total}$ 대신 특수 행동 토큰 집합 $S$에 대해서만 분류를 수행합니다.
   - 훈련 중, 원래 토큰 라벨 $y_i \in V$는 로컬 클래스 $l_i \in \{0, \dots, V_a-1\}$로 매핑됩니다:
     $$l_i = \begin{cases} \text{map}(y_i), & \text{if } y_i \in S \\ -100, & \text{otherwise (loss에서 무시)} \end{cases}$$
   - 손실은 특수 행동 토큰에 대한 로짓 $z_i = \text{logits}[i, S] \in \mathbb{R}^{V_a}$을 사용하여 마스크된 위치에서만 계산됩니다:
     $$L_{\text{token}} = \frac{1}{|M|} \sum_{i \in M} \text{CE}(z_i, l_i)$$
   - 이 전략은 학습을 행동 관련 토큰에 집중시켜 정확도를 향상시키고 훈련을 용이하게 합니다.
4. **계층적 행동 구조 디코딩 (Hierarchical Action-structured Decoding, HAD):**
   - 기존 MDM 디코딩은 모든 토큰을 동일하게 처리하여 행동 청크 내의 구조화된 의존성을 무시합니다.
   - LLaDA-VLA는 행동 내 및 행동 간 상관관계를 명시적으로 포착하기 위해 계층적 디코딩을 도입합니다.
   - **행동 수준 신뢰도:** 각 행동에 포함된 토큰들의 신뢰도를 합산하여 행동 수준 신뢰도 $C_a^{(i)}$를 계산합니다:
     $$C_a^{(i)} = \sum_{j=1}^{D} c_{i,j}$$
     여기서 $c_{i,j}$는 $i$번째 행동의 $j$번째 토큰의 신뢰도이며, $D$는 행동당 토큰 수입니다.
   - **계층적 디코딩 단계:**
     1. 예측된 행동 청크 내의 모든 행동을 행동 수준 신뢰도 $C_a^{(i)}$에 따라 순위를 매깁니다.
     2. 가장 높은 신뢰도를 가진 행동을 부분적으로 보존하고, 나머지 행동은 다시 마스크합니다 (행동 수준 리마스크).
     3. 선택된 행동 내에서 토큰 수준 신뢰도에 따라 토큰들의 순위를 매깁니다.
     4. 일부 고신뢰도 토큰만 보존하고, 나머지는 다시 마스크합니다 (토큰 수준 리마스크).
     5. 다시 마스크된 토큰들은 후속 확산 단계에서 재생성됩니다.
   - 이 계층적 절차는 궤적이 행동 단위로 생성되도록 보장하고, 구조적 무결성을 유지하며, 각 개별 행동 내에서의 추가적인 개선을 가능하게 합니다.

## 📊 Results

- **SimplerEnv 벤치마크:**
  - LLaDA-VLA는 OpenVLA보다 평균 50.9%p, CogACT보다 4.2%p 높은 55.1%의 평균 성공률을 달성하여 최첨단 자동회귀 VLA 모델보다 우수한 성능을 보였습니다.
- **CALVIN 벤치마크:**
  - LLaDA-VLA는 OpenVLA보다 0.74 더 높은 평균 에피소드 길이(Avg. Len.)를 달성했으며, 다른 저명한 방법론보다도 뛰어난 성능을 보였습니다.
- **실제 WidowX 로봇:**
  - 4가지 실제 로봇 조작 작업에서 평균 58%의 성공률을 달성했으며, CogACT 및 $\pi^0$ (각각 평균 30%, 35%)보다 일관되게 우수한 성능을 보였습니다.
- **일반화 능력 (Generalization Capability):**
  - 보지 못한 물체, 용기, 방해물을 포함하는 4가지 OOD(Out-of-Distribution) 실제 로봇 작업에서 강력한 일반화 성능을 보여주었습니다. $\pi^0$ 대비 평균 성공률 25%p 향상을 달성했습니다.
- **어블레이션 스터디 (CALVIN):**
  - **지역화된 특수 토큰 분류 (LSC):** LSC를 추가하자 baseline 대비 0.79의 성능 향상(Avg. Len.)을 보였으며, 이는 모델이 행동 관련 토큰에 집중하여 적응 난이도를 효과적으로 줄였음을 입증합니다.
  - **계층적 행동 구조 디코딩 (HAD):** HAD를 통합하자 vanilla 디코딩 전략 대비 0.58점의 상당한 성능 향상(Avg. Len.)을 가져왔으며, 이는 행동 내 및 행동 간 의존성을 모델링하는 것이 일관된 행동 궤적 생성에 기여함을 보여줍니다.
  - **행동 청크 크기 (Action Chunk Size):** 청크 크기를 적당히 늘리면 성능이 향상될 수 있지만, 너무 커지면 예측 난이도가 증가하여 정확도가 감소할 수 있습니다. 본 연구에서는 5가 최적의 청크 크기였습니다.
- **정성적 결과:**
  - CALVIN 및 SimplerEnv 시뮬레이션에서 다단계 및 정밀 조작 작업을 성공적으로 완료했습니다.
  - 실제 로봇 실험에서 인-도메인 작업의 신뢰성뿐만 아니라, 보지 못한 물체나 용기, 방해물이 있는 OOD 작업에서도 강력한 일반화 능력을 보여주었습니다.

## 🧠 Insights & Discussion

- LLaDA-VLA는 로봇 조작 영역에서 확산 기반 VLM(d-VLM)의 미개척 잠재력을 성공적으로 탐구하고 입증했으며, 이는 자동회귀 모델이 지배적이던 VLA 분야에 새로운 패러다임을 제시합니다.
- 제안된 지역화된 특수 토큰 분류 전략은 d-VLM과 로봇 도메인 간의 도메인 간극을 효과적으로 해소하여 학습을 로봇 행동에 필요한 토큰에 집중시켰습니다.
- 계층적 행동 구조 디코딩 전략은 복잡한 로봇 행동의 구조적 의존성(행동 내 및 행동 간)을 명시적으로 모델링하여, 더욱 일관성 있고 합리적인 행동 궤적 생성을 가능하게 했습니다.
- 시뮬레이션 및 실제 로봇 모두에서 최첨단 성능을 달성함으로써 LLaDA-VLA는 d-VLM이 복잡한 로봇 조작 작업에서 강력하고 일반화 가능한 정책을 학습할 수 있음을 강력하게 보여주었습니다.
- 행동 청크 크기 설정의 중요성은 확산 모델의 특성상 생성해야 할 토큰 수가 늘어날수록 예측 난이도가 증가할 수 있음을 시사하며, 이는 향후 동적 청크 크기 조정 메커니즘에 대한 연구로 이어질 수 있습니다.

## 📌 TL;DR

LLaDA-VLA는 사전 학습된 확산 기반 시각-언어 모델(d-VLM)을 로봇 조작에 적용한 최초의 모델입니다. 이 모델은 d-VLM과 로봇 도메인 간의 도메인 간극을 줄이고 구조화된 행동 생성을 가능하게 하기 위해 **지역화된 특수 토큰 분류**와 **계층적 행동 구조 디코딩**이라는 두 가지 핵심 전략을 제안합니다. 지역화된 분류는 학습을 행동 관련 토큰에 집중시키고, 계층적 디코딩은 행동의 계층적 의존성을 고려하여 일관된 궤적을 생성합니다. 실험 결과, LLaDA-VLA는 시뮬레이션(SimplerEnv, CALVIN) 및 실제 로봇 환경에서 기존 자동회귀 기반 VLA 모델 대비 최첨단 성능과 뛰어난 일반화 능력을 입증했으며, 로봇 조작에서 d-VLM의 잠재력을 강력하게 시사합니다.
