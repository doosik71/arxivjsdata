# IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model for End-to-End Autonomous Driving

Anqing Jiang, Yu Gao, Yiru Wang, Zhigang Sun, Shuo Wang, Yuwen Heng, Hao Sun, Shichen Tang, Lijuan Zhu, Jinhao Chai, Jijun Wang, Zichong Gu, Hao Jiang, Li Sun

## 🧩 Problem to Solve

자율주행 분야에서 Vision-Language-Action (VLA) 모델은 잠재력을 보여주었지만, 다음과 같은 두 가지 핵심 문제에 직면해 있습니다:

1. **모방 학습의 한계**: 기존 VLA 아키텍처는 주로 개방 루프(open-loop) 모방 학습(Imitation Learning, IL)에 기반하여 데이터셋에 기록된 행동을 모방하는 경향이 있으며, 이는 최적화되지 않고 제한적인 성능으로 이어집니다. 운전은 본질적으로 다양한 최적의 솔루션과 여러 평가 기준(예: 충돌 회피, 운전 가능 영역 준수, 교통 규칙 준수)을 만족해야 하는 다중 목표 및 다중 양식(multi-modality) 작업이기 때문입니다.
2. **시뮬레이터 의존성**: 폐쇄 루프(close-loop) 훈련은 높은 충실도의 센서 시뮬레이션에 크게 의존하는데, 이는 시뮬레이션-실제(Sim2Real) 도메인 간극(domain gap)과 높은 계산 비용이라는 상당한 장벽을 만듭니다.

이 논문은 이러한 한계를 극복하고, 무거운 시뮬레이터 없이 확장 가능한 강화 학습을 통해 고용량 VLA 모델의 잠재력을 개선하는 것을 목표로 합니다.

## ✨ Key Contributions

- **IRL-VLA 프레임워크 제안**: Vision-Language-Action (VLA) 모델을 위한 새로운 폐쇄 루프 강화 학습 프레임워크인 IRL-VLA를 제안합니다. 이는 시뮬레이터 피드백 기반 강화 학습의 선구적인 접근 방식입니다.
- **효율적인 보상 월드 모델(RWM) 개발**: 계산 비용이 많이 드는 시뮬레이터 기반 보상 계산을 대체하기 위해 역 강화 학습(Inverse Reinforcement Learning, IRL) 기반의 효율적인 보상 월드 모델(RWM)을 도입하여 확장 가능하고 효과적인 보상 예측을 가능하게 합니다. 이 학습된 보상 모델은 훈련 중 시뮬레이터에 의존하지 않고 VLA 에이전트를 강화 학습으로 훈련하는 데 사용됩니다.
- **새로운 VLA 모델 아키텍처 제안**: 모방 학습 및 강화 학습 설정 모두에서 우수한 성능을 달성하는 새로운 VLA 모델을 제안하여 다양한 훈련 패러다임에서 최적의 성능을 가능하게 합니다.
- **최첨단 성능 달성**: NAVSIM v2 엔드-투-엔드 주행 벤치마크에서 최첨단 성능을 달성하며, CVPR 2025 Autonomous Grand Challenge에서 1위를 차지했습니다. 이는 제안된 접근 방식의 효과와 일반화 가능성을 입증합니다.

## 📎 Related Works

- **엔드-투-엔드 자율주행**:
  - **UniAD [10], VAD [15], SparseDrive [26]**: 지각, 예측, 계획과 같은 모듈식 작업을 단일 모델에 통합하는 종합 프레임워크를 제안합니다.
  - **DiffusionDrive [21], Diffusion Planer [12], DiffSemanticFusion [27]**: 로봇 공학의 확산 정책을 활용하여 다양한 주행 동작을 생성합니다.
- **자율주행의 Vision-Language-Action (VLA) 모델**:
  - **[11, 14, 31]**: VLM과 엔드-투-엔드 프레임워크 간의 연결을 구축하여 궤적 계획 정확도를 향상시킵니다.
  - **Recogdrive [17]**: VLM, 확산 기반 플래너, 시뮬레이터 지원 강화 학습을 결합하여 안전하고 사람과 유사한 궤적을 생성합니다.
  - **SimLingo [23], ORION [7]**: 지시문 기반 행동 평가 및 비전-언어 추론과 궤적 계획의 융합을 제안합니다. 그러나 이러한 접근 방식은 주로 모방 학습에 의존하여 실세계 다중 양식 및 다중 목표 시나리오로의 일반화가 제한됩니다.
- **자율주행의 강화 학습 (RL)**:
  - **RAD [8]**: 3D Gaussian Splatting (3DGS) 시뮬레이션 프레임워크 내에서 엔드-투-엔드 자율주행 에이전트를 훈련하기 위해 RL을 사용하지만, 높은 계산 비용과 Sim2Real 간극 문제가 있습니다.
  - **[28, 30], CarPlanner [29]**: 액션이 자율주행 차량 중심의 계획된 궤적으로 직접 표현되는 학습 기반 궤적 계획 프레임워크를 제안합니다.
  - **DiffVLA [13]**: VLM 내비게이션 안내를 통한 계층적 미세-조확(coarse-to-fine) 확산 기반 궤적 생성을 가진 효율적인 VLA 모델을 제안하지만, 모방 학습 설정에 한계가 있습니다. IRL-VLA는 RL을 전체 VLA 모델 아키텍처로 확장하여 모델 성능의 상한선을 더욱 개선합니다.

## 🛠️ Methodology

IRL-VLA는 세 단계로 구성된 프레임워크입니다:

1. **모방 정책 학습 (Imitation Policy Learning)**

   - **VLA 모델 구성**:
     - **의미론적 추론 (Semantic Reasoning)**: Senna-VLM 프레임워크 기반의 VLM 명령어 안내 모듈을 통해 멀티모달 정보를 처리하고 융합하여 깊이 있는 장면 이해를 가능하게 합니다.
     - **3D 추론 (3D Reasoning)**: BEV(Bird's Eye View) 비전 인코더와 어댑터를 사용하여 다중 뷰 이미지를 BEV 공간의 특징 맵으로 인코딩합니다. 이후 감지 토큰(detection tokens)과 맵 토큰(map tokens)을 사용하여 벡터화된 맵 요소와 에이전트 모션 정보를 학습합니다.
     - **통합 확산 플래너 (Unified Diffusion Planner)**: 확산 기반 접근 방식을 사용하여 가우시안 노이즈가 있는 앵커 제안 궤적을 처리하여 다양하고 정보가 풍부한 미래 궤적 분포를 생성합니다. BEV 토큰, 맵 토큰, 감지 토큰과 같은 풍부한 장면 의미론을 계층적으로 통합하여 환경 제약과 일치하는 궤적을 합성합니다.
   - **모방 정책 학습 손실**: 확산 기반 모방 학습 훈련 정책과 유사하게, 궤적 재구성 손실과 분류 손실을 결합한 손실 함수 $L_{IL}$을 사용합니다:
     $$L_{IL} = \sum_{k=1}^{N_{anchor}} [y_k L_{rec}(\hat{\tau}_k, \tau_{gt}) + \lambda BCE(\hat{s}_k, y_k)]$$
     여기서 $L_{rec}$은 L1 재구성 손실, $BCE$는 이진 교차 엔트로피 분류 손실입니다.

2. **역 환경 학습 (Inverse Environment Learning)**

   - **보상 데이터 수집**: 효과적인 보상 월드 모델(RWM) 개발을 위해 Ego-Pseudo Driving Metric System (EPDMS) [3, 6]의 9가지 하위 점수(NC, DAC, DDC, TLC, EP, TTC, LK, HC, EC)를 활용합니다. 훈련 데이터의 다양성 확보를 위해 다음 전략을 사용합니다:
     - 확산 프로세스의 각 시간 단계에서 궤적과 EPDMS 점수를 기록합니다.
     - K-평균 클러스터링을 사용하여 인간 시연 데이터에서 여러 궤적 패턴을 샘플링합니다.
     - NAVSIM 데이터셋의 각 장면에 대해 여러 차량 자세를 적용하여 다양한 샘플을 생성합니다.
   - **보상 월드 모델 (RWM)**: 전통적인 시뮬레이터에 대한 경량의 데이터 기반 대안으로 RWM을 제안합니다. RWM은 다중 시점 카메라 정보와 에이전트의 예측된 미래 궤적을 입력으로 받아 시뮬레이션된 환경 내에서 에이전트의 미래 보상을 예측합니다. 각 메트릭에 대해 $m \in \{NC, DAC, DDC, TLC, EP, TTC, LK, HC\}$에 대한 보상 $\hat{r}_m$을 MLP를 통해 예측하며, 최종 보상은 각 구성 요소의 가중 합으로 계산됩니다:
     $$\hat{r}_{epdms} = \sum_m w_m \cdot \hat{r}_m$$
   - **RWM 최적화**: 예측된 점수와 실제 점수 간의 오류를 최소화하도록 RWM을 훈련합니다. 손실 함수는 다음과 같습니다:
     $$L_{R_{epdms}} = \sum_{i,m} w_m (\hat{r}_{i_m} - r_{i_m})$$

3. **RWM을 이용한 강화 학습 (Reinforcement Learning with RWM)**
   - **정책 최적화**: 모방 학습의 한계를 극복하기 위해 RWM을 이용한 폐쇄 루프 강화 학습으로 VLA 정책을 미세 조정합니다. 안정성과 샘플 효율성 때문에 PPO (Proximal Policy Optimization) 알고리즘을 채택합니다.
   - **PPO 알고리즘**: RWM이 제공하는 실시간 보상 피드백을 사용하여 VLA 모델이 다양한 주행 시나리오를 탐색하고 다중 목표(안전, 효율성, 교통 규칙 준수)를 최적화할 수 있도록 합니다.
     - 정책 $\pi_\theta$에서 행동 $a_t$를 샘플링합니다.
     - RWM으로부터 다음 상태 $\hat{s}_{t+1}$와 보상 $\hat{r}_{t+1}$을 계산합니다.
     - GAE (Generalized Advantage Estimation)를 사용하여 이점 $A_t$를 계산합니다.
     - PPO 정책 손실 $L_{CLIP}(\theta)$와 가치 손실 $L_{VF}(\phi)$를 사용하여 정책 $\theta$와 가치 함수 $\phi$를 업데이트합니다.
   - **확산 정책**: 확산 정책 $\pi_\theta$는 가우시안 노이즈에서 시작하여 점진적으로 노이즈를 제거하여 일련의 행동을 생성하는 고유한 마르코프 결정 프로세스(MDP)로 간주됩니다. 전체 궤적에 대한 결합 로그-가능도(joint log-likelihood)는 다음과 같습니다:
     $$\log \pi_\theta (x_{0:\tau}) = \sum_{t=1}^\tau \log \pi_\theta (x_{t-1}|x_t,c)$$
   - **강화 학습 손실**: 정책 최적화를 위해 PPO 알고리즘을 사용하며, 강화 학습 손실은 다음과 같습니다:
     $$L_{RL} = -\frac{1}{T_{trajs}} \sum_{i=1}^{T_{trajs}} \frac{1}{\tau} \sum_{t=1}^\tau \gamma^{t-1} \log \pi_\theta (x^{(i)}_{t-1}|x^{(i)}_t,c) \hat{A}_i - D_{KL}(\pi_\theta || \pi_{ref})$$
     여기서 $\gamma$는 할인 계수, $D_{KL}$은 KL 발산입니다.
   - **최종 손실**: 안정성을 유지하고 사전 훈련된 정책의 치명적인 망각을 방지하기 위해 강화 학습 목표와 행동 복제(Behavior Cloning, BC) 항을 결합한 최종 손실을 사용합니다:
     $$L = L_{RL} + w_{IL} L_{IL}$$
     여기서 $w_{IL}$은 행동 복제 손실 가중치입니다.

## 📊 Results

- **NAVSIM v2 Navhard 벤치마크 성능**:
  - 사전 훈련된 모델(IRL-VLA-PT)은 EPDMS 74.4를 달성하여 DiffusionDrive (63.2), WOTE (66.7), GTRS-Aug (74.3)와 같은 여러 최첨단 방법을 능가합니다. 특히 안전(NC) 성능을 유지하면서 EP (83.9 vs. 76.1) 및 EC (76.0 vs. 54.2)와 같은 안락함 관련 점수에서 상당한 개선을 보였습니다.
  - RWM으로 강화 학습을 통해 미세 조정된 최종 모델(IRL-VLA-RL)은 EPDMS 74.9로 더욱 향상된 성능을 달성하며 최첨단(SOTA)을 달성했습니다.
- **CVPR 2025 Autonomous Grand Challenge**: NAVSIM v2 벤치마크에서 45.0 EDPM5 점수를 기록하며 1st runner-up을 차지했습니다.
- **계층적 추론 확산 VLA 에이전트의 Ablation Study**:
  - 3D 추론만 사용 시 EPDMS 70.0.
  - 의미론적 추론 추가 시 EPDMS 71.4 (1.4 개선).
  - 확산 플래너 추가 시 EPDMS 74.4 (3.0 개선). 이는 제안된 계층적 추론 확산 VLA 스키마가 안전하고 편안한 주행 행동을 생성하는 데 효과적임을 보여줍니다.
- **모방 손실 가중치 ($w_{IL}$)의 Ablation Study**:
  - $w_{IL} = 1.0$ (IL과 RL이 동등하게 기여)일 때 EPDMS 73.9.
  - $w_{IL} = 0.1$ (IL 항 약화)일 때 EPDMS 73.4 (훈련 붕괴로 이어질 수 있음).
  - $w_{IL} = 0.5$일 때 EPDMS 74.9로 모방 학습과 강화 학습 간의 최상의 균형을 달성했습니다.

## 🧠 Insights & Discussion

- **Sim2Real 간극 및 계산 비용 문제 해결**: IRL-VLA는 모방 학습의 개방 루프 한계와 고충실도 시뮬레이션 기반 훈련의 Sim2Real 간극 및 높은 계산 비용 문제를 효과적으로 해결합니다. RWM은 실시간 보상 피드백을 제공하여 값비싼 센서 렌더링 및 물리 기반 시뮬레이션 없이 확장 가능하고 효율적인 훈련을 가능하게 합니다.
- **다중 목표 최적화**: VLA 아키텍처는 명시적인 점수 메커니즘에 의존하지 않고도 안전과 안락함 사이의 균형을 효과적으로 최적화합니다. 이는 RWM이 안전 사고, 편안한 주행, 교통 효율성을 효과적으로 균형 잡도록 안내하는 강화 학습을 통해 달성됩니다.
- **최초의 폐쇄 루프 VLA 접근 방식**: IRL-VLA는 센서 입력을 통합하면서 시뮬레이터에 의존하지 않는 최초의 폐쇄 루프 VLA 접근 방식입니다. 이는 자율주행 연구에서 VLA 모델을 위한 확장 가능하고 효율적인 강화 학습의 가능성을 열었습니다.
- **미래 연구 방향**: 본 프레임워크는 폐쇄 루프 자율주행 분야에서 VLA 연구를 가속화할 것으로 기대됩니다.

## 📌 TL;DR

IRL-VLA는 자율주행 VLA 모델의 두 가지 핵심 문제, 즉 개방 루프 모방 학습의 한계와 고충실도 시뮬레이션 기반 폐쇄 루프 훈련의 비효율성 및 Sim2Real 간극을 해결하기 위한 혁신적인 프레임워크입니다. 이 논문은 모방 학습으로 VLA 모델을 사전 훈련하고, 역 강화 학습을 통해 경량 보상 월드 모델(RWM)을 구축하며, 이 RWM을 이용한 PPO 강화 학습으로 정책을 미세 조정하는 3단계 접근 방식을 제안합니다. 이 방법은 훈련 중 시뮬레이터에 의존하지 않고 확장 가능하며 효율적인 폐쇄 루프 훈련을 가능하게 하여, NAVSIM v2 벤치마크에서 74.9 EPDMS로 최첨단 성능을 달성하며 안전성과 편안함을 균형 있게 최적화했습니다. IRL-VLA는 센서 입력을 통합하면서 시뮬레이터 없이 작동하는 최초의 폐쇄 루프 VLA 접근 방식이라는 점에서 중요한 의미를 가집니다.
