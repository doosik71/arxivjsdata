# Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust

Asher J. Hancock, Allen Z. Ren, and Anirudha Majumdar

## 🧩 Problem to Solve

거대 언어-시각-행동(VLA) 모델은 대규모 인터넷 데이터 및 로봇 시연을 통해 일반화된 로봇 정책으로서의 잠재력을 보여주지만, 작업과 무관한 시각적 세부 사항(예: 방해물, 배경색 변화)에 취약합니다. 이러한 모델은 훈련 데이터에 없는 미묘한 환경 변화에 쉽게 깨집니다. 이 연구는 모델의 재훈련이나 가중치 접근 없이 VLA 모델의 시각적 견고성을 런타임에 개선하는 가볍고 모델 불가지론적인 도구를 개발하는 것을 목표로 합니다.

## ✨ Key Contributions

- **런타임 개입 시스템 BYOVLA 제안:** VLA 모델의 시각적 일반화 능력을 향상시키기 위한 런타임 개입(run-time intervention) 스키마인 BYOVLA를 제안합니다.
- **민감 영역 동적 식별:** 시각적 입력에서 모델이 민감하게 반응하는 영역을 "시각적 민감도 프로브(visual sensitivity probe)"를 사용하여 동적으로 식별합니다.
- **작업 무관 영역의 최소한의 변경:** 비전-언어 모델(VLM)을 사용하여 작업과 무관한 영역을 식별하고, 해당 영역이 VLA 모델에 민감한 경우에만 자동화된 이미지 편집 도구(예: 인페인팅)를 사용하여 최소한으로 변경합니다.
- **모델 불가지론적(Model-Agnostic) 접근:** VLA 모델의 미세 조정이나 가중치 접근 없이 어떠한 상용(off-the-shelf) VLA 모델에도 적용 가능합니다.
- **성능 개선 입증:** 방해물 및 배경 변화 환경에서 BYOVLA가 최신 VLA 모델의 작업 성공률을 최대 40%까지 저하시킬 수 있는 시나리오에서 원래의 명목 성능을 거의 유지하게 합니다.
- **기존 런타임 개입 및 귀인(Attribution) 방법 대비 우위:** 모델의 시각적 민감도를 고려하지 않거나 GradCAM과 같은 기존 귀인 방법을 사용하는 기준선보다 훨씬 향상된 성능을 보여줍니다.

## 📎 Related Works

- **VLA 모델:** RT-1, RT-2, Octo, OpenVLA와 같은 최신 VLA 모델들이 넓은 작업 일반화(task generalization)를 보이지만, 시각적 환경 변화(환경 일반화)에는 취약하다는 점이 지적됩니다.
- **시각적 방해물에 대한 정책 견고성 개선:**
  - **훈련 중 데이터 증강:** 대규모 도메인 무작위화(domain randomization) [10, 11] 또는 자동 이미지 편집 도구(예: 인페인팅)를 사용한 데이터 증강 [12, 13, 14, 15]이 훈련 중에 적용됩니다. BYOVLA는 런타임에만 작동하며 모델 가중치를 변경하지 않습니다.
  - **마스킹(Masking):** 배경이나 관련 없는 객체를 마스킹하는 방법 [16, 17, 18, 19]이 있지만, 비현실적인 관측을 생성할 수 있습니다. 일부 작업 [20]은 VLM을 사용하여 관련 객체를 결정하지만, 이 또한 훈련이 필요합니다. BYOVLA는 재훈련 없이 모델 민감도에 기반한 선택적 마스킹을 사용합니다.
  - **다른 훈련 전략:** 어텐션 메커니즘의 병목 현상 생성 [22, 23, 24, 25]이나 정보 병목 현상 [26, 27], 비시뮬레이션 기반 상태 추상화 [28, 29]를 사용하여 작업 관련 정보만 인코딩하도록 훈련하는 방법들은 모두 훈련 파이프라인 변경을 요구합니다.
- **장면에서 작업 관련 요소 결정:** VLM [20]이나 엔드투엔드 모듈 [16]을 사용하여 작업 무관 요소를 결정하는 연구가 있습니다. BYOVLA도 VLM을 활용하지만, 모델이 민감한 경우에만 시각적 조작을 가합니다.
- **모델 민감도 결정 및 귀인 방법:** SHAP [30], LIME [31], GradCAM [6], SmoothGrad [32]와 같은 귀인 방법들은 이미지 분류 설정에서 입력의 어떤 부분이 모델 출력에 가장 큰 영향을 미치는지 파악하는 데 사용됩니다. 그러나 이러한 방법들은 구현 세부 사항에 민감하고 신뢰할 수 없는 경우가 많습니다 [33, 34]. BYOVLA의 시각적 민감도 프로브는 시각적 입력의 다른 부분을 교란하여 행동 출력의 변화를 직접 측정하며, GradCAM보다 효과적임이 실험적으로 입증됩니다.

## 🛠️ Methodology

BYOVLA는 런타임에 원본 관측 $o_t$를 새로운 관측 $\rho_f(o_t)$로 처리하여 VLA 모델 $f(\rho_f(o_t), l)$에 입력함으로써, 시각적 방해물이 있는 환경에서 미리 훈련된 VLA의 성능을 향상시키는 것을 목표로 합니다. 이는 모델의 미세 조정이나 가중치 접근이 필요 없습니다.

1. **작업 무관 객체 위치 파악 (Step 1: Localize task-irrelevant objects):**
   - 초기 관측 $o_0$에서 OpenAI의 GPT-4o VLM을 사용하여 작업과 무관한 시각적 영역을 식별합니다. (몇 가지 예시와 함께 프롬프트 제공).
   - VLM의 출력은 텍스트 형태의 영역 제안(region proposal)입니다.
   - 이 제안을 Grounded-SAM2 [37, 38, 39, 40]와 같은 세분화(segmentation) 모델에 제공하여 픽셀 수준에서 해당 영역의 마스크를 얻습니다. (정적 환경에서는 초기화 시 한 번만 호출).
2. **시각적 민감도 프로브 적용 (Step 2: Apply visual sensitivity probe):**
   - VLM 및 세분화 모델로부터 얻은 작업 무관 영역 $R_t$ 중 VLA 모델 $f$의 출력에 영향을 미치는 영역을 결정합니다.
   - 영역 $r \in R_t$에 대한 $f$의 민감도 $\Delta_f(o_t, r)$는 해당 영역의 이미지를 교란하여 $\tilde{o}_t$를 얻고, 원래 관측 $o_t$에 대한 예측된 행동 덩어리 $(a_t,...,a_{t+T_a})$와 교란된 관측 $\tilde{o}_t$에 대한 행동 덩어리 $(\tilde{a}_t,...,\tilde{a}_{t+T_a})$ 간의 가중 $L_2$-norm 차이를 측정하여 정량화합니다:
     $$\Delta_f(o_t, r) := \frac{1}{KT_a} \sum_{k=1}^K \sum_{t'=0}^{T_a} \sqrt{\langle w\Delta a^k_{t+t'}, \Delta a^k_{t+t'} \rangle}$$
     여기서 $K$는 샘플링된 행동 덩어리 수, $T_a$는 행동 예측 범위, $w \in \mathbb{R}^7$는 사용자 정의 가중치 벡터입니다.
   - 객체 방해물에는 가우시안 블러링(Gaussian blurring)을, 배경 방해물에는 가우시안 노이즈(Gaussian noise)를 적용하여 이미지를 교란합니다.
   - **민감도 임계값 $\tau$ 결정:** $\Delta_f(o_t, r)$가 임계값 $\tau$보다 크면 해당 영역에 개입합니다. BridgeV2 데이터셋 [42]의 환경에서 오프라인으로 실험하여 객체 방해물에는 $\tau = 0.002m$, 배경 방해물에는 $\tau = 0.001m$으로 설정합니다.
3. **이미지 변환 (Step 3: Transform the image):**
   - 영역 $r$이 임계값 $\tau$ 이상으로 민감하다고 판단되면, 이미지 변환을 적용합니다.
   - VLM이 해당 영역을 객체 방해물로 분류하면, Inpaint Anything [41]과 같은 인페인팅 모델을 사용하여 이미지에서 객체를 제거합니다.
   - 배경 방해물로 분류하면, 해당 영역의 RGB 픽셀을 $\Delta_f(o_t, r) < \tau$가 될 때까지 무작위의 중립적인 색상으로 변경합니다. 이는 변환된 관측이 훈련 데이터와 더 잘 일치하도록 합니다.

## 📊 Results

BYOVLA는 Octo-Base (93M 파라미터) 및 OpenVLA (7B 파라미터) 두 가지 최첨단 VLA 모델로 평가되었습니다.

- **Octo-Base 평가 (작업: "put the carrot on yellow plate"):**
  - **객체 방해물:** 작업 무관 객체(오렌지, 파란색 수건, 칼, 녹색 컵, 도넛)는 Octo-Base의 성공률을 40% 감소시켰습니다. BYOVLA는 이러한 방해물이 있을 때 모델의 명목 성공률(67%)을 거의 유지할 수 있었으며, BYOVLA\Sens (시각적 민감도 프로브 없음) 및 GradCAM 기반 기준선보다 우수한 성능을 보였습니다.
  - **배경 방해물:** 노란색 타일링 배경 변화 시에도 유사한 성능 추세를 보였으며, BYOVLA는 Octo-Base의 작업 성공률을 약 25% 향상시켰습니다.
  - **런타임 민감도 프로브 적용:** 매 스텝마다 민감도 프로브를 적용하는 것은 초기화 시 한 번만 적용하는 것에 비해 정적 환경 및 단순 작업에서는 추가적인 이점을 제공하지 않았습니다.
  - **실패 모드:** 대부분의 실패는 작업 객체의 너무 이른 파지(early grasping) 또는 접근 시 객체 놓침(missing)이었습니다.
- **OpenVLA 평가 (작업: "put the eggplant in the pot"):**
  - **객체 및 배경 방해물:** 세 개의 은색 뚜껑, 올리브 오일, 후추, 포도, 분홍색 접시 등의 객체 방해물과 갈색 벽돌 배경 방해물이 사용되었습니다. OpenVLA는 명목상 100%의 성공률을 보였으나, 방해물 존재 시 성능이 40% 감소했습니다.
  - **BYOVLA의 개선:** BYOVLA 및 BYOVLA\Sens 모두 방해물 존재 시 기본 성능을 20-25% 향상시켰습니다.
  - **명목 성능 미달성:** Octo-Base와 달리 OpenVLA의 명목 성공률을 완전히 회복하지는 못했습니다. 이는 OpenVLA 실험 환경이 훨씬 더 복잡하여 GPT-4o가 모든 작업 무관 객체를 정확히 파악하지 못했기 때문으로 추정됩니다.
  - **실패 모드:** 작업 객체를 성공적으로 파지한 후에도 위치 변화 명령을 내리지 않는 경향(예: 가지를 스토브에서 들어 올리지 않음)이 주요 실패 모드였습니다.
- **런타임 오버헤드:** VLM(GPT-4o) 쿼리는 평균 3초 미만 소요되며, 초기화 시 한 번만 실행됩니다. 세분화 및 민감도 프로브 단계를 포함한 BYOVLA의 전체 처리 시간은 약 2초였습니다. Octo-Base와 OpenVLA는 각각 13Hz, 6Hz로 작동하여 전반적인 오버헤드는 수용 가능한 수준입니다.

## 🧠 Insights & Discussion

BYOVLA는 VLA 모델이 훈련 없이도 작업 무관 방해물에 대한 견고성을 대폭 향상시킬 수 있는 효과적인 런타임 개입 방법임을 보여줍니다. 특히, 모델의 시각적 민감도를 측정하고, _민감하면서도 작업에 무관한_ 영역만 선택적으로 조작하는 접근 방식은, 단순히 모든 관련 없는 영역을 제거하는 것(BYOVLA\Sens)보다 우수했습니다. 이는 훈련 데이터에 일부 방해물이 포함될 수 있고, 무조건적인 제거는 또 다른 분포 변화를 야기할 수 있기 때문입니다. GradCAM과 같은 기존 귀인 방법이 모델의 실제 민감도를 충분히 포착하지 못하는 반면, BYOVLA의 직접적인 행동 변화 측정 방식이 더 신뢰성 있음을 입증했습니다.

**제한 사항 및 향후 연구:**

- **통합의 어려움:** 여러 기초 모델을 파이프라인으로 통합하는 데 어려움이 있습니다.
- **VLM의 한계:** VLM이 객체와 배경 방해물을 정확하게 구별하는 능력에 의존하며, 현재는 주로 분리된 객체/배경 시나리오에 초점을 맞추었습니다.
- **임계값 $\tau$의 선택:** $\tau$는 하이퍼파라미터이며 최적의 결과를 위해 실제 환경에서 미세 조정이 필요합니다. 향후에는 컨포멀 예측(conformal prediction) [47, 48] 등을 활용하여 동적으로 임계값을 결정하는 방법을 모색할 계획입니다.
- **정적 환경에 대한 초점:** 현재 연구는 정적 환경에 초점을 맞추고 있습니다. 모바일 로봇이나 인간의 행동이 포함된 동적 환경에서는 매 스텝마다 BYOVLA 파이프라인 전체를 실행하는 것이 이점을 제공할 것으로 예상됩니다.
- **향상된 인페인팅 스키마:** VLA 훈련 데이터의 배경을 활용하여 배경 영역을 교체하는 더 정교한 인페인팅 기법을 탐색할 예정입니다.

결론적으로, 런타임 개입은 추가적인 훈련 없이 VLA의 기본 역량을 크게 향상시킬 수 있는 미개척 분야이며, 이 연구가 해당 분야의 추가 연구를 촉진하기를 기대합니다.

## 📌 TL;DR

VLA 모델은 방해물에 시각적으로 취약하지만, BYOVLA는 **훈련 없이 런타임에** 이 문제를 해결합니다. VLM으로 **작업 무관 영역**을 찾고, "시각적 민감도 프로브"로 VLA가 **민감하게 반응하는 영역**만 식별한 후, 이미지 편집 도구로 해당 영역을 **최소한으로 변경**합니다. 이는 VLA의 작업 성공률을 20-40% 향상시키며, 기존 방법보다 견고성이 뛰어남을 보여주었습니다.
