# CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation

Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo

## 🧩 Problem to Solve

기존의 대규모 시각-언어-행동(VLA) 모델들은 사전 학습된 대규모 시각-언어 모델(VLM)을 활용하여 로봇 조작 작업에서 언어 기반의 실행 및 미학습 시나리오에 대한 일반화 능력을 크게 향상시켰습니다. 그러나 단순한 액션 양자화나 회귀 기반 학습 방식을 사용한 기존 VLA 모델들은 실제 작업 성공률이 낮아 만족스럽지 못한 성능을 보였습니다. 이는 로봇 액션이 본질적으로 연속적이고, 다중 모드(multimodal)이며, 시간적 상관관계가 있고, 높은 정밀도를 요구하는 특성을 충분히 반영하지 못하기 때문입니다.

## ✨ Key Contributions

- **액션 확산(Diffusion) 프로세스의 대규모 VLA 모델 통합:** 로봇 액션의 연속적이고 다중 모드 특성을 효과적으로 모델링하기 위해 액션 확산 프로세스를 대규모 VLA 모델에 통합했습니다.
- **컴포넌트화된 VLA 모델 아키텍처 및 대규모 액션 모듈 연구:** "인지(cognition)"와 "액션(action)" 기능을 분리하여, VLM의 인지 정보는 조건으로 사용하고 전문화된 확산 트랜스포머(DiT) 기반의 액션 모듈을 통해 액션을 예측하는 컴포넌트화된 아키텍처를 제안했습니다. 또한 이러한 액션 모듈의 설계 및 스케일링 특성을 체계적으로 연구했습니다.
- **적응형 액션 앙상블(Adaptive Action Ensemble, AAE) 알고리즘 제안:** 추론 시 과거 액션 예측과 현재 예측을 적응적으로 융합하여 궤적의 부드러움과 작업 성공률을 높이는 효과적인 시간적 융합 알고리즘을 개발했습니다.
- **우수한 성능 및 일반화 능력 입증:** 제안된 모델이 기존 VLA 모델들을 능가하는 상당한 성능 향상을 보였으며, 새로운 로봇 및 미학습 객체, 배경에 대한 뛰어난 적응 및 일반화 능력을 입증했습니다.

## 📎 Related Works

- **시각-언어-행동(VLA) 모델:** LLM 및 VLM의 성공에 힘입어 액션 생성을 통합한 VLA 모델이 등장했습니다. RoboFlamingo, RT-2, OpenVLA 등은 VLM을 액션 예측에 단순하게 활용(예: 액션 토큰화)하여 유망한 일반화 성능을 보였으나, 액션의 연속적이고 시간적 특성을 간과하는 한계가 있었습니다.
- **대규모 액션 모델:** Diffusion Transformer (DiT) 기반의 대규모 액션 모델 연구들이 동시에 진행되었으나, 이들은 사전 학습된 VLM의 강력한 일반화 및 명령어 추론 능력을 활용하지 못하는 차이가 있습니다.
- **확산 기반 로봇 정책:** Diffusion Policy, Octo 등은 확산 모델을 로봇 액션 모델링에 도입하여 액션 분포의 다중 모드 특성을 포착했습니다. 특히 Octo는 작은 확산 헤드를 사용했지만, 강력한 시각-언어 모델의 이점을 충분히 활용하지 못했습니다. 본 연구는 대규모 전용 액션 모듈(헤드가 아닌)을 확산 트랜스포머 아키텍처와 함께 연구하여 이와 차별점을 둡니다.

## 🛠️ Methodology

CogACT는 다음 세 가지 컴포넌트로 구성된 VLA 모델 아키텍처를 제안합니다.

1. **Vision Module (시각 모듈):**
   - DINOv2와 SigLIP 같은 강력한 시각 트랜스포머를 활용하여 원본 이미지($o_t$)를 일련의 시각 지각 토큰($V$)으로 인코딩합니다.
   - 이 모듈은 인터넷 규모의 이미지 데이터로 사전 학습되어 풍부한 시각적 특징과 포괄적인 의미론적 이해를 제공합니다.
2. **Language Module (언어 모듈):**
   - LLAMA-2 모델을 백본으로 사용하여 언어 지시($l$)를 언어 토큰($T$)으로 변환합니다.
   - 이 언어 토큰은 시각 토큰($V$) 및 학습 가능한 인지 토큰($c$)과 결합되어 인과적 어텐션 메커니즘을 통해 처리됩니다.
   - 인지 토큰에 해당하는 결과 출력 특징($f^c_t$)은 현재 작업을 실행하는 데 필요한 통합 정보를 인코딩하며, 이는 다음 액션 모듈의 조건으로 사용됩니다.
3. **Diffusion Action Module (확산 액션 모듈):**
   - 실제 물리적 액션의 연속적이고 다중 모드 특성을 처리하기 위해 확산 모델링 프로세스 [47]를 사용합니다.
   - 복잡하고 시간적 상관관계가 있는 액션을 모델링하기 위해 확산 트랜스포머(DiT) [51]를 강력한 백본으로 사용합니다.
   - 이 모듈은 인지 특징($f^c_t$)과 일련의 노이즈가 추가된 액션($a^i_t, ..., a^i_{t+N}$)을 입력으로 받아 다단계 노이즈 제거 과정을 통해 최종 액션 시퀀스($a_t, ..., a_{t+N}$)를 예측합니다. 여기서 $N$은 예측할 미래 액션 단계의 수입니다 (기본값 $N=15$).
   - 훈련 목적 함수는 예측된 노이즈($\hat{\epsilon}_i$)와 실제 노이즈($\epsilon$) 간의 평균 제곱 오차(MSE)를 최소화하는 것입니다: $L_{MSE} = E_{\epsilon \sim N(0,1),i} ||\hat{\epsilon}_i - \epsilon||^2$.
4. **Adaptive Action Ensemble (적응형 액션 앙상블, AAE):**
   - 추론 시 액션 궤적의 부드러움을 높이고 성능을 개선하기 위해 제안된 전략입니다.
   - 현재 관측 $o_t$에 대한 액션 예측 $a_t|o_t$와 과거 관측 $o_{t-K}, ..., o_{t-1}$에 기반한 해당 액션 예측 $a_t|o_{t-K}, ..., a_t|o_{t-1}$을 융합합니다.
   - 최종 실행 액션 $\hat{a}_t$는 다음과 같이 계산됩니다: $$\hat{a}_t = \sum_{k=0}^{K} w^{ada}_k \cdot a_t|o_{t-k}$$
   - 여기서 적응형 가중치 스칼라 $w^{ada}_k = \text{exp}(\alpha \cdot \lt a_t|o_t, a_t|o_{t-k} \gt)$는 현재 예측과 유사한 과거 예측에 더 큰 중요성을 부여하며, $\lt \cdot, \cdot \gt$는 코사인 유사도를 나타냅니다.

## 📊 Results

- **시뮬레이션 평가 (SIMPLER):**
  - Google Robot에서 Visual Matching 및 Variant Aggregation 설정 모두에서 가장 높은 평균 성공률을 달성했습니다 (각각 74.8%, 61.3%).
  - RT-1 (Google Robot 전용 데이터셋 훈련)보다 Visual Matching에서 22.4%, Variant Aggregation에서 17.6% 높은 성공률을 보였습니다.
  - 55B 파라미터의 RT-2-X보다 훨씬 작은 모델 크기(7.6B)에도 불구하고 시뮬레이션에서 18% 더 높은 절대 성공률을 기록했습니다.
  - WidowX Robot에서도 51.3%의 가장 높은 평균 성공률을 달성했습니다.
- **실제 로봇 평가 (Realman Robot, Franka Robot):**
  - Realman Robot에서 Pick, Stack, Place 태스크 전반에 걸쳐 OpenVLA 대비 59.1%p 높은 성공률(71.2% vs 12.1%)을 보였습니다.
  - 미학습 테이블과 미학습 방해물, 미학습 색상, 형태, 카테고리에 대한 일반화 평가에서도 OpenVLA를 크게 능가하는 강력한 성능을 입증했습니다.
  - Franka Robot에서도 Octo-Base 및 OpenVLA를 크게 능가하는 평균 61.4%의 성공률을 달성하며 다른 로봇 플랫폼에 대한 일반화 능력을 확인했습니다.
- **어블레이션 스터디:**
  - **액션 모델 아키텍처:** DiT (Diffusion Transformer) 기반 액션 모듈이 MLP보다 우수한 성능을 보였으며, 모델 크기가 증가함에 따라 성공률이 거의 선형적으로 향상되는 유리한 스케일링 특성을 확인했습니다. DiT-Large (308M)가 64.8%로 최고 성능을 보였습니다.
  - **다단계 액션 예측:** 15단계 미래 액션 예측 시 가장 높은 성능을 달성했습니다 (0단계 대비 성능 향상).
  - **적응형 액션 앙상블:** 제안된 적응형 앙상블 전략이 기존 Action Chunking 및 Temporal Ensemble 전략보다 우수한 성능을 보였습니다.

## 🧠 Insights & Discussion

CogACT는 VLA 모델의 성능 향상에 있어 "인지"와 "액션" 기능의 분리가 핵심적인 통찰력임을 보여줍니다. VLM이 제공하는 광범위한 시각 및 의미론적 지식(인지)은 액션 예측의 강력한 조건 역할을 하지만, 실제 로봇 액션의 복잡한 특성(연속성, 다중 모드, 시간적 상관관계, 정밀도)을 효과적으로 다루기 위해서는 전문화된 액션 모듈이 필수적이라는 것을 확인했습니다.

특히 확산 트랜스포머(DiT)를 액션 모듈로 사용하는 것은 이러한 액션 신호 모델링에 매우 효과적임을 입증했습니다. 또한, 액션 모듈의 파라미터 수가 VLM 백본에 비해 상대적으로 적음에도 불구하고, 수억 개의 파라미터 증가가 상당한 성능 향상을 가져오는 유리한 스케일링 행동은 효율적인 VLA 모델 확장의 새로운 방향을 제시합니다. 이는 VLM을 직접 액션 예측에 사용하는 것보다, VLM은 인지 기반을 제공하고 전용 액션 모듈이 미세한 액션 제어를 담당하는 컴포넌트화된 접근 방식이 더욱 효과적임을 시사합니다.

## 📌 TL;DR

CogACT는 로봇 조작의 낮은 작업 성공률 문제를 해결하기 위해 인지(VLM)와 액션(DiT 기반 액션 모듈) 기능을 분리한 새로운 시각-언어-행동(VLA) 모델 아키텍처를 제안합니다. 이 모델은 VLM에서 추출한 인지 정보를 조건으로 사용하여 확산 트랜스포머 기반의 전문 액션 모듈이 연속적이고 다중 모드인 다단계 액션 시퀀스를 예측합니다. 적응형 액션 앙상블 전략을 통해 궤적의 부드러움을 향상시키고, 시뮬레이션 및 실제 로봇 환경 모두에서 기존 VLA 모델들을 크게 능가하는 성능과 새로운 로봇 및 미학습 객체/배경에 대한 뛰어난 일반화 능력을 입증했습니다.
