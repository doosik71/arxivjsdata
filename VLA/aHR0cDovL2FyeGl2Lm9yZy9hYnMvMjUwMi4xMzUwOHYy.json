{
  "url": "http://arxiv.org/abs/2502.13508v2",
  "title": "VLAS: Vision-Language-Action Model With Speech Instructions For\n  Customized Robot Manipulation",
  "authors": "Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang",
  "year": 2025,
  "abstract": "Vision-language-action models (VLAs) have become increasingly popular in\nrobot manipulation for their end-to-end design and remarkable performance.\nHowever, existing VLAs rely heavily on vision-language models (VLMs) that only\nsupport text-based instructions, neglecting the more natural speech modality\nfor human-robot interaction. Traditional speech integration methods usually\ninvolves a separate speech recognition system, which complicates the model and\nintroduces error propagation. Moreover, the transcription procedure would lose\nnon-semantic information in the raw speech, such as voiceprint, which may be\ncrucial for robots to successfully complete customized tasks. To overcome above\nchallenges, we propose VLAS, a novel end-to-end VLA that integrates speech\nrecognition directly into the robot policy model. VLAS allows the robot to\nunderstand spoken commands through inner speech-text alignment and produces\ncorresponding actions to fulfill the task. We also present two new datasets,\nSQA and CSI, to support a three-stage tuning process for speech instructions,\nwhich empowers VLAS with the ability of multimodal interaction across text,\nimage, speech, and robot actions. Taking a step further, a voice\nretrieval-augmented generation (RAG) paradigm is designed to enable our model\nto effectively handle tasks that require individual-specific knowledge. Our\nextensive experiments show that VLAS can effectively accomplish robot\nmanipulation tasks with diverse speech commands, offering a seamless and\ncustomized interaction experience."
}