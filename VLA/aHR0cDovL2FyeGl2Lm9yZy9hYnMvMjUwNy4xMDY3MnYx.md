# Vision Language Action Models in Robotic Manipulation: A Systematic Review

Muhayy Ud Din, Waseem Akram, Lyes Saad Saoud, Jan Rosell, Irfan Hussain

## 🧩 Problem to Solve

기존 로봇 시스템은 특정 작업에 맞춰 프로그래밍되어 동적이고 비정형적인 환경에서 유연성이 부족합니다. 이 논문은 시각적 인지, 자연어 이해, 그리고 구체화된 제어를 단일 학습 프레임워크 내에서 통합하여 로봇이 자연어 명령을 이해하고 복잡한 작업을 자율적으로 수행할 수 있도록 하는 VLA(Vision Language Action) 모델의 핵심 연구 문제를 다룹니다. 급변하는 VLA 분야에서 아키텍처, 데이터셋, 시뮬레이션 플랫폼, 평가 프로토콜을 체계적으로 정리하고 종합하는 문헌의 부족을 지적합니다.

## ✨ Key Contributions

- **VLA 아키텍처의 구조화된 분류:** VLA 모델 아키텍처를 인지, 자연어 이해, 로봇 제어 통합 방식에 따라 핵심 범주로 세분화하여 제시합니다.
- **VLA 데이터셋을 위한 새로운 정량적 벤치마킹:** 작업 복잡성($\mathcal{C}_{\text{task}}$) 및 모달리티 풍부성($\mathcal{C}_{\text{mod}}$)을 기준으로 VLA 데이터셋을 체계적으로 벤치마킹하는 새로운 정량적 프레임워크를 제안합니다. 이를 통해 매우 높은 작업 복잡성과 포괄적인 다중 모달리티를 결합한 데이터셋의 부족과 같은 현재 데이터 환경의 중요한 격차를 밝혀냅니다.
- **시뮬레이션 플랫폼에 대한 심층 검토:** VLA 연구에 사용되는 가장 중요한 시뮬레이션 플랫폼의 특징, 적용 분야, 그리고 대규모 데이터 생성 및 재현 가능한 실험을 촉진하는 역할을 검토합니다.
- **과제 식별 및 미래 로드맵 제시:** VLA 모델 개발이 직면한 지속적인 과제를 식별하고, 아키텍처 모듈성, 확장 가능한 데이터 생성 전략, 미분 가능한 접촉 모델링을 통한 동적 시뮬레이션 발전, 그리고 강력하고 확장 가능한 실세계 배포를 위한 통합된 언어 접지 API와 같은 핵심 영역을 강조하는 미래 연구 로드맵을 제시합니다.

## 📎 Related Works

이 논문은 VLA 모델의 기반을 형성하는 다양한 선행 연구를 참조합니다:

- **트랜스포머 아키텍처:** Vaswani et al. (2017)의 트랜스포머는 자연어 처리와 컴퓨터 비전에 혁명을 가져왔습니다.
- **시각 트랜스포머 (ViT):** Dosovitskiy et al. (2020)은 이미지 패치를 입력 토큰으로 처리하여 트랜스포머를 시각 도메인으로 확장했습니다.
- **대규모 언어 모델 (LLMs):** GPT (Brown et al., 2020), BERT (Devlin et al., 2018), PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023)와 같은 모델은 대규모 텍스트 데이터셋으로 훈련되어 추론 및 명령 수행 능력을 보여주었습니다.
- **시각 언어 모델 (VLMs):** CLIP (Radford et al., 2021), BLIP (Li et al., 2023), Flamingo, PaLI-X/PaLM-E 등은 시각 및 텍스트 데이터를 융합하여 이미지 캡셔닝, 시각 질문 답변, 명령 접지 등의 작업을 수행합니다.
- **초기 VLA 모델:** RT-1 (Brohan et al., 2022), SayCan (Ahn et al., 2022), VIMA (Jiang et al., 2022), Octo (Team et al., 2024) 등이 로봇 제어를 위한 비전-언어 융합 아키텍처를 제시했습니다.
- **주요 데이터셋:** Open X-Embodiment (Collaboration et al., 2025), DROID (Khazatsky, 2024) 등은 VLA 훈련 및 벤치마킹을 위한 대규모 다중 모달 데이터를 제공합니다.
- **시뮬레이션 플랫폼:** Habitat (Savva et al., 2019), Isaac Gym (Makoviychuk et al., 2021), RoboSuite (Zhu et al., 2020), iGibson (Xia et al., 2020), AI2-THOR (Kolve et al., 2017) 등은 대규모 데이터 생성 및 실세계 전이를 위한 환경을 제공합니다.

## 🛠️ Methodology

이 논문은 VLA 모델의 일반적인 아키텍처, 훈련 데이터셋 형식, 그리고 데이터셋 벤치마킹 방법을 체계적으로 설명합니다.

- **VLA 아키텍처 (그림 7):**

  - **입력:** 장면 이미지(RGB, 깊이, 의미), 자연어 지시(고수준 목표 또는 단계별 지시), 로봇의 내부 상태(관절 각도, 엔드 이펙터 자세, 그리퍼 상태).
  - **인코더 스트림:**
    - **시각 인코더 (Visual Encoder):** Transformer 기반 (예: ViT, DINOv2)으로 원본 이미지를 고정 길이 특징 토큰 시퀀스로 처리합니다.
    - **언어 인코더 (Language Encoder):** 사전 훈련된 모델 (예: PaLM, LLaMA)로 자연어 지시를 토큰화하고 임베딩합니다.
    - **상태 인코더 (State Encoder):** MLP 또는 작은 트랜스포머를 통해 로봇의 고유 수용 감각 및 운동 상태를 임베딩합니다.
  - **정보 융합:** 모든 토큰은 연결되어 트랜스포머 기반 모델로 전달됩니다.
  - **행동 디코더 (Action Decoder):**
    - **확산 트랜스포머 (Diffusion Transformer):** 확산 정책(Diffusion Policy, VLAFlow 등)을 사용하여 노이즈가 있는 잠재 궤적을 반복적으로 디노이징하여 미세 조정된, 시간에 걸쳐 부드러운 제어 신호를 생성합니다.
    - **직접 정책 (Direct Policy):** 확산 과정 없이 단일 패스로 임베딩을 직접 예측합니다.
  - **출력:** 엔드 이펙터 속도 또는 관절 토크와 같은 연속 제어 신호로 변환되거나, 일부 구현에서는 다음 프레임 이미지를 생성하여 "상상 및 검증" 루프를 가능하게 합니다.

- **VLA 훈련 데이터셋 (그림 9):**

  - **통합 데이터 형식:**
    - **시각 스트림:** 원본 RGB 프레임, 비디오 스니펫, 선택적으로 깊이 맵 및 분할 마스크. JPEG/PNG, MP4 형식.
    - **언어 스트림:** 자연어 지시 또는 대화, 토큰화 메타데이터. JSON 또는 일반 텍스트 파일.
    - **행동/제어 레이블:** 이산 행동 토큰 또는 연속 제어 벡터 (관절 위치, 엔드 이펙터 궤적). NumPy 배열 또는 구조화된 데이터 컨테이너.
  - **저장 형식:** JSON (경량 메타데이터), TFRecord/TF-Example (고처리량 훈련), HDF5 (동기화된 프레임, 행동, 상태 배열에 효율적인 무작위 접근).

- **VLA 데이터셋 벤치마킹:**
  - **작업 복잡성 ($\mathcal{C}_{\text{task}}$):** 다음 요소를 통합하여 정량화합니다: 에피소드당 평균 저수준 행동 수($T$), 구별되는 고수준 기술 수($S$), 순차적 작업 종속성($D$), 언어적 추상화 수준($L$).
    $$ \mathcal{C}_{\text{task}}(\mathcal{D}) = \alpha_{1}\log(1 + T) + \alpha*{2}S + \alpha*{3}D + \alpha*{4}L $$
    여기서 $\alpha*{i}$는 가중치이며, 벤치마크에서는 모두 1로 설정됩니다.
  - **모달리티 풍부성 ($\mathcal{C}_{\text{mod}}$):** 다음 요소를 통합하여 정량화합니다: 구별되는 모달리티 수($M$), 평균 품질($Q$), 모달리티 간 시간 정렬 정확도($A$), 추론에 중요한 모달리티 존재 여부($R$).
    $$ \mathcal{C}_{\text{mod}} = \beta_{1}M + \beta*{2}Q + \beta*{3}A + \beta*{4}R $$
    여기서 $\beta*{i}$는 가중치이며, 벤치마크에서는 모두 1로 설정됩니다.
  - **정규화:** $\mathcal{C}_{\text{task}}$는 $[1,5]$ 스케일로, $\mathcal{C}_{\text{mod}}$는 $[2,5]$ 스케일로 정규화됩니다. 데이터셋 규모는 버블 크기로 표현됩니다.

## 📊 Results

이 논문은 VLA 모델의 아키텍처 동향, 데이터셋 벤치마킹 분석, 그리고 주요 VLA 모델의 평가 결과를 제시합니다.

- **VLA 아키텍처 동향 (그림 8, 표 1):**

  - **시각 인코더:** CLIP 및 SigLIP 기반 ViT 백본이 풍부하고 의미론적으로 정렬된 시각 특징 추출에 가장 많이 사용됩니다. DINOv2 및 Qwen2 ViT는 장거리 공간 종속성 모델링에 활용됩니다. ResNet, EfficientNet과 같은 CNN 기반 인코더도 사용됩니다.
  - **언어 인코더:** LLaMA 및 Vicuna 계열이 지시 이해 및 제로샷 추론에 널리 사용됩니다. T5 스타일 모델은 시퀀스 생성에 유연하며, GPT 및 Qwen 기반 인코더는 일반화와 경량 배포의 균형을 이룹니다.
  - **행동 디코더:** 확산 기반 트랜스포머는 반복적인 노이즈 제거를 통해 미세하고 시간적으로 부드러운 제어를 제공하여 가장 많이 선택됩니다. Gato와 같은 자기회귀(autoregressive) 트랜스포머 헤드는 단계별 행동 시퀀스를 생성하며, MLP 또는 토큰 예측기 헤드는 효율적인 저수준 제어에 사용됩니다.
  - **데이터셋 사용:** 많은 팀이 자체 수집한 조작 데모에 의존하지만, 공개된 Open X-Embodiment 데이터셋이 가장 일반적으로 사용됩니다.

- **VLA 데이터셋 벤치마킹 (그림 10):**

  - 대부분의 VLA 벤치마크는 낮은-중간 작업 복잡성과 최소-풍부한 모달리티 범위에 집중되어 있습니다.
  - **낮은 복잡성/최소 모달리티:** EmbodiedQA, R2R, RoboSpatial은 단순하고 이산적인 의사결정 작업에 중점을 둡니다.
  - **중간 복잡성/풍부한 모달리티:** ALFRED, DialFRED, CoVLA는 깊이, 언어, 고유 수용 감각과 같은 추가 센서 스트림을 통합하여 더 정교한 정책 학습을 가능하게 합니다.
  - **높은 복잡성/풍부한 모달리티:** Iref-VLA, Robo360, TLA, CALVIN, Open X-Embodiment와 같은 데이터셋은 다중 로봇, 다중 기술 데모 등 높은 복잡성과 풍부한 모달리티를 동시에 달성합니다.
  - **최고의 복잡성/포괄적 모달리티:** Kaiwu 데이터셋은 비전, 깊이, 언어, 고유 수용 감각, 햅틱 등 가장 포괄적인 모달리티와 매우 높은 작업 복잡성을 결합하여 독보적입니다.
  - **주요 격차:** 현재 VLA 벤치마크는 장기적인(long-horizon), 다중 기술 제어 과제를 포괄적인 다중 모달 입력(비전, 깊이, 언어, 고유 수용 감각, 햅틱, 오디오, 장면 그래프)과 완전히 통합하지 못하고 있습니다.

- **VLA 모델 평가 (표 5):**
  - **광범위한 일반화:** RT-2, Octo, Gato, OpenVLA와 같은 대규모 일반주의(generalist) 아키텍처는 방대한 트랜스포머 기반 백본과 확산 디코더를 사용하여 광범위한 제로샷 일반화 능력을 보여줍니다.
  - **모듈형/작업 특화 시스템:** DexVLA, CLIPort, TLA, RoboAgent와 같은 모델은 특정 조작 기술에 대한 견고성과 데이터 효율성을 높이기 위해 객체 중심 ViT, 햅틱 인코더, LoRA 어댑터, 의미론적 증강(semantic augmentation)과 같은 특정 모듈을 사용합니다.
  - **실세계 검증:** RT-2, Octo, OpenVLA, RoboAgent 등 다수의 모델이 실제 로봇에서 성공적으로 검증되었습니다. TLA는 햅틱 언어 모델로 접촉이 많은 작업에서 85% 이상의 성공률을 달성했습니다.

## 🧠 Insights & Discussion

VLA 모델은 로봇 공학의 새로운 지평을 열고 있지만, 실세계 배포를 위한 견고성과 일반화 능력을 완전히 달성하기 위해서는 여전히 여러 중요한 과제에 직면해 있습니다.

**1. 아키텍처 과제:**

- **토큰화 및 어휘 정렬:** 이종 입력(자연어, 이미지 패치, 연속 로봇 상태)을 처리할 때 정보 손실 없이 효율적으로 인코딩하고, 노이즈나 새로운 구성에 동적으로 적응하며, 저지연(low-latency) 토큰 생성을 달성해야 합니다.
- **모달리티 융합:** 픽셀 수준과 단어 수준 표현의 통계적 특성 불일치로 인해 약한 시각 접지(visual grounding)가 발생합니다. 비동기식 센서 스트림(햅틱, 오디오)을 효과적으로 융합하고, 도메인 전환(lighting changes) 시 모달리티 중요도를 동적으로 재조정하며, 교차-어텐션 계층의 해석 가능성을 높여야 합니다.
- **다양한 로봇 기구(Embodiments) 간 일반화:** 고정된 행동 어휘와 경직된 운동학적 바인딩은 다른 로봇 모델로의 전이를 심각하게 제한합니다. 완전히 새로운 로봇 모델에 대한 제로샷 일반화와 시뮬레이션에서 실세계로의 불안정한 전이는 해결해야 할 과제입니다.
- **조작 동작의 부드러움:** 이산 행동 토큰 예측에 중점을 두어 연속 동작 궤적의 품질을 간과하는 경향이 있습니다. 저지연 확산 모델의 실시간 추론, 센서 노이즈와 동적 불확실성 하의 충돌 회피, 궤적의 부드러움과 목표 변경에 대한 빠른 반응성 사이의 균형 유지가 필요합니다.

**2. 데이터셋 과제:**

- **작업 다양성:** 현재 데이터셋은 좁고 단기적인 작업에 특화되어 있으며, 공간 추론, 내비게이션, 미세 객체 조작을 결합한 장기적인 작업 계획을 통합하는 데이터셋은 부족합니다.
- **모달리티 불균형:** 대부분의 데이터셋은 RGB 이미지와 텍스트 주석에 주로 의존하며, 깊이 맵, 힘/토크 신호, 햅틱 피드백 등 중요한 센서 모달리티가 종종 누락되거나 일관되지 않은 샘플링 속도로 캡처됩니다.
- **주석 품질 및 비용:** 6-DoF 객체 자세, 프레임 정렬 다중 센서 데이터, 상세한 자연어 설명을 얻는 것은 비용과 시간이 많이 소요됩니다. 시뮬레이션은 완벽한 주석을 제공하지만, 시뮬레이션-실세계 격차가 발생합니다.
- **현실성 및 규모:** 실세계 데이터셋은 높은 충실도를 제공하지만 수집 비용으로 인해 규모가 제한됩니다. 시뮬레이션은 수백만 개의 궤적을 효율적으로 생성하지만, 복잡한 실세계 역학을 재현하는 데 어려움이 있습니다.

**3. 시뮬레이션 과제:**

- **물리 정확도 및 접촉 모델링:** MuJoCo, PyBullet, NVIDIA Isaac Sim과 같은 인기 있는 물리 엔진은 물리적 상호작용을 단순화하여 소프트 바디 변형, 가변 표면 마찰, 관절 유연성과 같은 필수 역학을 정확히 포착하지 못합니다.
- **시각적 현실성과 처리량 상충:** AI2-THOR, Habitat과 같은 고충실도 시뮬레이션 플랫폼은 사실적인 렌더링을 제공하지만 낮은 프레임 속도와 높은 GPU 요구 사항으로 인해 대규모 강화 학습에는 적합하지 않습니다.
- **내장된 언어 접지(Language Grounding) API 부족:** 대부분의 시뮬레이터는 자연어 명령을 에이전트 행동으로 접지하기 위한 기본 지원을 제공하지 않아 사용자 정의 주석 파이프라인이 필요합니다.
- **다중 로봇 및 에이전트 지원 기능:** 다중 로봇 지원은 시뮬레이터마다 크게 다르며, 플랫폼 간의 일관성이 부족하여 교차 플랫폼 사전 훈련 및 재현성을 저해합니다.

**미래 방향:**

- **아키텍처:** 학습 가능한 모달리티 인식 토크나이저, 동적 융합 블록, 계층적 아키텍처, 그리고 안전 필터가 있는 확산 기반 궤적 생성기를 통합해야 합니다.
- **데이터셋:** 절차적 작업 문법(procedural task grammars)을 통한 장기적인 시나리오 자동 생성, 표준화된 다중 모달 캡처 파이프라인, 자기 지도 학습/약한 지도 학습을 통한 주석 비용 절감, 하이브리드 합성-실세계 파이프라인을 통한 현실성-규모 격차 해소가 필요합니다.
- **시뮬레이션:** 미분 가능한 다중 스케일 접촉 모델을 통한 물리적 정확도 향상, 고충실도와 처리량을 결합한 하이브리드 렌더링 파이프라인, 시뮬레이터 불가지론적인(agnostic) 언어 접지 API, 다중 로봇 및 다중 에이전트 시나리오 지원이 중요합니다.

이러한 과제들을 해결하는 것은 VLA 연구를 발전시키고 로봇이 복잡하고 개방적인 환경에서 추론하고 행동하며 적응할 수 있도록 하는 데 필수적입니다.

## 📌 TL;DR

**문제:** 로봇이 복잡하고 비정형적인 환경에서 자연어 명령을 이해하고 자율적으로 작업을 수행하기 어려워, 시각, 언어, 행동을 통합하는 통일된 프레임워크가 필요합니다.
**방법:** VLA(Vision Language Action) 모델은 Transformer 기반 아키텍처를 사용하여 시각 인코더(예: ViT), 언어 인코더(예: LLaMA), 그리고 확산(Diffusion) 기반 행동 디코더를 통해 시각적 인지, 자연어 이해, 그리고 구체화된 제어를 통합합니다.
**결과 및 시사점:** VLA 모델은 광범위한 작업 일반화 및 새로운 환경 적응 능력을 보여주지만, 매우 높은 작업 복잡성과 포괄적인 다중 모달리티를 결합한 데이터셋의 부족, 시뮬레이션의 물리적 정확도 및 언어 기반 접지(grounding) API 부족과 같은 여러 과제가 남아있습니다. 미래 연구는 이러한 격차를 해소하고 실세계 배포를 위한 로봇의 견고성과 적응성을 향상하는 데 중점을 두어야 합니다.
