# RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models

Yuxuan Chen and Xiao Li

## 🧩 Problem to Solve

VLA(Vision-Language-Action) 모델은 복잡한 로봇 조작 작업에서 뛰어난 능력을 보이지만, 방대한 매개변수 크기와 높은 추론 지연 시간 때문에 자원 제약이 있는 로봇 플랫폼에 실제 배포하기 어렵습니다. 기존의 모델 압축 기법들은 VLA의 메모리 효율성과 추론 속도 문제를 완전히 해결하지 못하며, 로봇 제어의 본질적인 적합성을 충분히 고려하지 못하는 한계가 있습니다.

## ✨ Key Contributions

- VLA에 양자화(quantization), 가지치기(pruning), 지식 증류(knowledge distillation)와 같은 일반적인 모델 압축 기법을 적용하여 그 효과와 상충 관계에 대한 실증적 통찰을 제공했습니다.
- 구조화된 가지치기(structured pruning), SFT(Supervised Fine-Tuning) 및 RL(Reinforcement Learning) 기반 성능 복구, 그리고 후처리 양자화(post-training quantization)를 결합한 새로운 압축 프레임워크인 RLRC를 제안했습니다. 이를 통해 경쟁력 있는 압축 성능을 달성하면서 작업 정확도를 유지하거나 향상시켰습니다.
- 광범위한 실험을 통해 제안된 접근 방식의 실용적 가치를 입증하여, 자원 제약이 있는 환경에서 VLA를 효율적으로 배포할 수 있는 강력한 잠재력을 보여주었습니다.

## 📎 Related Works

- **VLA 가속화 방법:** VLA-Cache [8]의 토큰 선택 메커니즘, FlashVLA [9]의 토큰 인식 액션 재사용, Park et al. [7]의 모방 학습 기반 정책 모델을 위한 양자화 프레임워크 등이 있었으나, 메모리 소비 절감 및 가속화 측면에서 여전히 부족한 부분이 있습니다.
- **LLM 모델 압축:** 가중치 및 활성화 정밀도를 낮추는 양자화 (GPTQ [11], LLM-QAT [12]), 중요도가 낮은 가중치나 뉴런을 제거하는 가지치기 (Magnitude, Wanda [26]와 같은 비구조화 방법, LLM-Pruner [13], FLAP [14]와 같은 구조화 방법), 작은 모델을 훈련하여 큰 모델을 모방하는 지식 증류 (MiniLLM [16]) 기법들이 활용되었습니다.
- **VLA를 위한 강화 학습 미세 조정 (RLFT):** SFT의 오프라인 데이터 한계를 극복하기 위해 RL이 도입되었습니다. iRe-VLA [18]는 RL과 지도 학습 단계를 번갈아 사용하고, VLA-RL [19]은 학습된 보상 모델을 도입하며, RIPT-VLA [20]는 critic-free 이진 보상을 제안했습니다. Liu et al. [21]은 RL이 의미 이해 및 실행 일반화 능력을 크게 향상시킬 수 있음을 보였습니다.

## 🛠️ Methodology

RLRC는 VLA 모델을 압축하고 성능을 복구하기 위한 세 단계 파이프라인으로 구성됩니다.

1. **구조화된 가지치기 (Structured Pruning):**
   - VLA 모델 내 LLM(Large Language Model) 컴포넌트에 집중하여 가지치기를 적용합니다. 이는 매개변수의 대부분을 차지하고 하드웨어 친화적인 가속화를 가능하게 하기 때문입니다.
   - **프레임워크:** LLM-Pruner [13]를 활용합니다.
   - **가지치기 기준:** 블록 단위(block-wise) 가지치기를 적용하고, Taylor 중요도 기준을 사용하여 각 구조 그룹의 중요도를 평가합니다.
   - **가지치기 비율:** 모델의 표현 능력과 안정성을 위해 첫 번째 및 마지막 디코더 레이어는 유지하고, 중간 레이어에 90%의 공격적인 가지치기 비율을 적용하여 모델 크기를 크게 줄입니다.
   - **효과:** 자가 주의(self-attention) 및 피드포워드(feedforward) 하위 모듈 내 선형 레이어의 중간 차원을 크게 줄여 매개변수 수와 계산 비용을 줄입니다.
2. **SFT 및 RL 기반 성능 복구 (Performance Recovery Based on SFT and RL):**
   - **SFT (Supervised Fine-Tuning):** 가지치기로 인한 성능 저하를 완화하고 모델의 작업 실행 능력을 복구하기 위해, 가지치기된 VLA 모델을 작업별 데이터셋으로 SFT합니다. 이를 통해 모델이 축소된 아키텍처에 적응하고 초기 기능을 회복합니다.
   - **RL (Reinforcement Learning):** SFT만으로는 완전한 성능 복구가 어렵거나, 특히 공격적인 4비트 양자화 후 발생하는 정확도 저하를 해결하기 위해 RL을 도입합니다. RL은 장기 보상 최적화를 통해 미묘한 의사 결정 능력을 복구하는 데 효과적입니다.
     - **알고리즘:** PPO (Proximal Policy Optimization) [24]를 사용하여 정책을 최적화합니다.
     - **아키텍처:** 가지치기된 VLA를 액터(actor)로 사용하며, 액터와 크리틱(critic)은 전체 트랜스포머 백본을 공유합니다. 최종 트랜스포머 블록의 첫 번째 액션 토큰 위치에서 추출된 히든 표현 $h_0$는 경량 MLP(Multi-Layer Perceptron)에 입력되어 스칼라 값(상태 값)으로 회귀됩니다. 이 설계는 PPO 훈련의 메모리 사용량을 줄이고 효율성을 높입니다.
     - **보상 함수:** 희소 보상(sparse reward)을 사용합니다:
       $$ r_t = \begin{cases} 1.0, & \text{if the object is successfully placed at time } t \\ 0.1, & \text{if the object is being grasped at time } t \\ 0, & \text{otherwise} \end{cases} $$
     - **효과:** SFT 이후 RL은 OOD(Out-Of-Distribution) 작업에 대한 일반화 능력을 향상시키고, 원래 VLA 모델의 성능을 능가할 수 있도록 합니다.
3. **추가 양자화 (Further Quantization of VLAs):**
   - 가지치기 및 성능 복구 단계를 거친 VLA 모델에 4비트 양자화를 추가로 적용하여 극단적인 메모리 압축을 달성합니다.
   - 이 단계는 모델의 메모리 사용량을 크게 줄여 자원 제약이 있는 환경에 원활하게 배포할 수 있도록 합니다.
   - 양자화는 메모리 소비를 더욱 줄이지만, 성능에 미미한 영향을 미치고 추론 지연을 증가시킬 수 있으므로 "선택 사항(optional)"으로 간주됩니다.

## 📊 Results

- **성능 복구 단계의 효과:**
  - SFT는 가지치기된 VLA의 성능 대부분을 약 1만 스텝의 짧은 훈련으로 복구합니다.
  - RL은 추가적인 성능 향상을 제공하며, 특히 OOD(Out-Of-Distribution) 작업에서 0.6M 스텝 이상의 훈련 시 지속적으로 성능을 개선합니다. 2M 스텝 RL 훈련 후 OOD 성공률이 87.5%에 달해 원본 VLA 대비 약 30% 향상됩니다.
  - RL은 가지치기 및 4비트 양자화된 모델의 성능을 원래 VLA 모델 이상으로 끌어올릴 수 있음을 확인했습니다.
- **RLRC의 압축 및 성능 결과:**
  - RLRC는 원본 VLA 모델의 작업 성공률을 유지하거나 심지어 능가하면서, 메모리 사용량을 최대 **8배 감소**시키고 추론 처리량을 최대 **2.3배 향상**시켰습니다.
  - 예를 들어, RLRC는 IND 성공률 90.62%, OOD 성공률 62.50%, 메모리 3.856GB, 처리량 13.5 samples/s를 달성했습니다. 4비트 양자화를 적용한 RLRC-4bit는 메모리를 1.772GB까지 줄였고, IND 85.93%, OOD 54.68%, 처리량 9.9 samples/s를 보였습니다.
- **기존 방법과의 비교:** VLA-Cache [8]를 비롯한 다른 가속 방법들과 비교했을 때, RLRC는 작업 성공률, 메모리 효율성, 추론 속도 등 모든 핵심 지표에서 지속적으로 우수한 성능을 보였습니다. 4비트 양자화나 LLM-Pruner만 단독으로 적용하는 경우 상당한 성능 저하가 발생했던 것과 대조적으로, RLRC는 높은 압축률에도 불구하고 작업 실행 능력을 유지하거나 향상시켰습니다.
- **Ablation Study:** SFT를 먼저 수행하는 것이 RL 훈련 효율성에 필수적임을 확인했습니다. SFT 없이 RL을 가지치기된 VLA에 직접 적용했을 때는 200만 스텝 이후에도 유의미한 성능 향상이 없었습니다. 이는 SFT가 RL을 위한 효과적인 초기 정책(warm start)을 제공하기 때문입니다.

## 🧠 Insights & Discussion

RLRC는 자원 제약이 있는 로봇 플랫폼에 VLA를 효율적으로 배포할 수 있는 실용적인 해결책을 제시합니다. 특히, 공격적인 모델 압축 후 RL을 통해 OOD 일반화 능력을 크게 향상시킬 수 있다는 점은 고무적입니다. 이 연구는 VLA 기술의 실제 적용 가능성을 확장하는 데 중요한 발걸음이 될 것입니다.

**제한 사항:**

- **추가 훈련 오버헤드:** RLRC의 SFT 및 RLFT 단계는 모델 성능을 복구하고 향상시키는 데 효과적이지만, 추가적인 훈련 시간과 비용을 발생시킵니다.
- **RL의 시뮬레이션 환경 의존성:** RL 컴포넌트는 효율적인 학습을 위해 병렬화된 시뮬레이션 환경에 크게 의존합니다. 이는 실제 로봇 플랫폼에 직접 배포하기 어려울 수 있으며, Sim-to-Real 갭(gap)을 메우기 위한 추가적인 연구가 필요합니다.
- **LLM 중심 가지치기 전략:** 현재의 가지치기 전략은 로봇 작업에 특화된 고려 사항(예: 로봇 데이터셋 기반 보정)을 통합하지 않고 LLM 중심의 방법을 채택했습니다. 로봇 환경의 고유한 요구 사항에 맞춘 가지치기 방식을 도입한다면 더 나은 압축-성능 절충점을 찾을 수 있을 것입니다.

## 📌 TL;DR

자원 제약이 있는 로봇에 VLA 모델을 효율적으로 배포하기 위해, 이 논문은 가지치기된 VLA의 성능을 효과적으로 복구하고 향상시키는 3단계 압축 프레임워크 **RLRC**를 제안합니다. RLRC는 LLM 컴포넌트에 90%의 구조화된 가지치기를 적용한 후, SFT와 PPO 기반 RL을 결합하여 성능을 복구 및 강화하며, 마지막으로 4비트 양자화를 통해 메모리 사용량을 더욱 줄입니다. 실험 결과, RLRC는 원래 VLA 모델의 작업 성공률을 유지하거나 능가하면서 **메모리 사용량을 최대 8배, 추론 처리량을 2.3배 향상**시켜, 자원 제약이 있는 환경에서 VLA를 효율적으로 배포할 수 있는 실용적이고 효과적인 솔루션을 제공합니다.
