# Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey

Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie

## 🧩 Problem to Solve

전통적인 로봇 조작 방식은 사전 정의된 작업과 엄격한 제어 정책에 기반하여, 비정형적이고 새로운 시나리오에서 확장성 및 일반화 능력이 부족합니다. 이는 동적인 환경에서 시각 및 의미론적 단서를 통합적으로 이해하고 정밀한 모터 제어를 수행하는 데 한계가 있습니다. 최근 대규모 시각-언어 모델(VLM)을 기반으로 한 시각-언어-행동(VLA) 모델이 부상하고 있지만, 이 분야의 용어와 모델링 가정이 일관되지 않고 연구가 파편화되어 있으며, VLM과 로봇 조작의 교차점에서 발생하는 고유한 도전과 발전에 대한 체계적인 통합 분석이 부족합니다.

## ✨ Key Contributions

- **대규모 VLM 기반 VLA 모델 개발의 종단적 종합:** VLM의 발전, 조작 학습의 기술적 진보, 그리고 대규모 VLM 기반 VLA 패러다임의 등장을 체계적으로 검토합니다. 특히 모놀리식(Monolithic) 모델과 계층적(Hierarchical) 모델의 발전을 분석하고, 주요 도전 과제 및 미래 방향을 제시합니다.
- **대규모 VLM 기반 VLA 모델링 실제의 교차적 종합:** 모놀리식 모델과 계층적 모델에 대한 구조적 및 기능적 관점의 세분화된 비교 분류를 제공하고, 대규모 VLM 기반 VLA 모델의 고급 연구 분야, 특징 및 개발을 뒷받침하는 데이터셋을 심층적으로 탐구합니다.
- **연구 통합 및 격차 해소:** 기존 분류의 불일치를 해소하고, 연구 파편화를 완화하며, 대규모 VLM과 로봇 조작 연구의 체계적인 통합을 통해 중요한 연구 공백을 메웁니다.

## 📎 Related Works

본 논문은 대규모 VLM 기반 VLA 모델에 초점을 맞추어 기존 연구의 한계를 지적합니다.

- **광범위한 VLA 아키텍처:** Ma et al. [2]와 같은 연구는 모듈형, 종단 간(end-to-end), 하이브리드 VLA 아키텍처를 광범위하게 다루지만, 사전 학습된 VLM을 기반 구성 요소로 활용하는 최근 동향에 대한 집중적인 분석이 부족합니다.
- **다양한 VLA 응용 분야:** Sapkota et al. [8]은 자율 주행, 증강 현실 내비게이션 등 다양한 VLA 응용 분야를 탐구하지만, 실시간 구동 제약, 센서 노이즈 강건성, 장기 의사 결정과 같은 로봇 공학 특정 문제에 대한 심층 분석이 부족합니다.
- **LLM 기반 로봇 계획:** Wang et al. [88]은 텍스트 전용 대규모 언어 모델(LLM)을 로봇 작업 계획에 통합하는 초기 연구를 제시하지만, VLM을 로봇 시각 인식 및 행동 결정에 기반(grounding)하는 과제는 다루지 않습니다.

## 🛠️ Methodology

이 논문은 대규모 VLM 기반 VLA 모델을 체계적으로 정의하고 분류하여 로봇 조작 분야의 발전을 종합합니다.

**1. 대규모 VLM 기반 VLA 모델의 정의:**

- (1) 시각적 관찰과 자연어 지시를 이해하기 위해 대규모 VLM을 활용하고,
- (2) 로봇 행동 생성에 직접 또는 간접적으로 기여하는 추론 과정을 수행합니다.

**2. 주요 아키텍처 패러다임:**

- **모놀리식 모델 (Monolithic Models):** 환경 이해(시각, 언어, 로봇 상태)와 행동 생성을 단일 아키텍처 내에서 통합합니다.

  - **단일 시스템 (Single-system):** 모든 양식을 처리하고 실행 가능한 행동을 디코딩하는 통합 아키텍처입니다.
    - **고전적 패러다임: 자기회귀 디코딩 (Autoregressive Decoding)**: 연속적인 로봇 행동 공간을 토큰 시퀀스로 이산화하여 순차적으로 행동 토큰을 예측합니다 (예: RT-2 [27], OpenVLA [26]).
    - **모델 성능 향상:**
      - **인지 양식 강화 (Enhancing Perception Modalities)**: 3D (예: Leo Agent [94], SpatialVLA [100]), 4D (예: TraceVLA [97], 4D-VLA [108]), 촉각 및 청각 (예: VTLA [103], VLAS [102]) 정보를 통합하여 실제 정보 획득을 개선합니다.
      - **추론 능력 강화 (Enhancing Reasoning Capabilities)**: LLM이 사고 과정("Reasoning")을 생성하고 이를 최종 "Action" 생성의 컨텍스트 정보로 활용하여 고급 숙고적 의사 결정을 가능하게 합니다 (예: ECoT [95], CoT-VLA [33]).
      - **일반화 능력 강화 (Enhancing Generalization Capabilities)**: 다양한 플랫폼과 시나리오에서 다양한 작업을 수행하는 모델의 능력을 개선합니다 (예: UniAct [99]의 범용 액션 코드북, ReVLA [96]의 가역 학습 전략, WorldVLA [38]의 물리 역학 이해).
    - **추론 효율성 최적화 (Inference Efficiency Optimization)**: 로봇 조작의 높은 제어 주파수 요구 사항에 맞춰 느린 추론 속도를 개선합니다.
      - **아키텍처 최적화 (Architectural Optimization)**: 레이어 건너뛰기, 조기 종료(early-exit) 등을 통해 중복 계산을 피합니다 (예: MoLe-VLA [114], DeeR-VLA [36]). Mamba [122] 아키텍처를 도입하여 선형 복잡도를 달성하기도 합니다 (예: RoboMamba [112]).
      - **매개변수 최적화 (Parameter Optimization)**: 모델 압축을 통해 매개변수 수를 직접 줄여 배포 난이도를 낮춥니다 (예: BitVLA [116]).
      - **추론 가속 (Inference Acceleration)**: 자기회귀 디코딩을 병렬 디코딩으로 대체하여 단일 전달에서 완전한 행동 시퀀스를 생성합니다 (예: RoboFlamingo [111], PD-VLA [113]).
  - **이중 시스템 (Dual-system):** VLM 백본(System 2, 느리지만 일반화된 추론)과 행동 전문가(System 1, 빠르고 반응적인 행동)를 사용하여 기능을 분할합니다.
    - **계단식 기반 방법 (Cascade-based Methods):** System 2가 고수준 의미론적 추론을 수행하고, System 1이 이를 실행 가능한 로봇 행동으로 디코딩합니다 (예: CogACT [128], GR00T N1 [32], Fast-in-Slow [40]).
    - **병렬 기반 방법 (Parallel-based Methods):** VLM 백본과 행동 전문가가 병렬로 작동하며 추론 중 정보를 교환합니다 (예: $\pi_0$ [29], $\pi_{0.5}$ [30], SmolVLA [31]).

- **계층적 모델 (Hierarchical Models):** 계획과 정책 실행을 해석 가능한 중간 표현을 통해 명시적으로 분리합니다 (예: 하위 작업, 키포인트, 프로그램).
  - **플래너 전용 (Planner-Only):** 플래너가 중간 프로그램을 생성하고, 정책은 기존의 기성품(off-the-shelf)을 활용합니다.
    - **프로그램 기반 (Program-based Methods):** 로봇 실행 가능한 프로그램 (예: Chain-of-Modality [151], Instruct2Act [159]) 또는 보조 프로그램 (예: ROVI [152], ReLEP [153])을 생성합니다.
    - **키포인트 기반 (Keypoint-based Methods):** 관찰에서 중요한 지점(예: 그리퍼가 도달해야 할 부분)을 예측합니다 (예: MoManipVLA [148], RoboPoint [149], ManipLVM-R1 [46]).
    - **하위 작업 기반 (Subtask-based Methods):** 고수준 지시를 단계별 텍스트 명령으로 분해합니다 (예: PaLM-E [85], Embodied-Reasoner [47]).
  - **플래너+정책 (Planner+Policy):** 플래너가 중간 표현을 생성하고, 특정 정책이 이를 실행합니다.
    - **키포인트 기반 (Keypoint-based Methods):** 플래너가 공간 원시 요소로 하위 목표를 설정하고, 저수준 정책이 연속적인 궤적을 예측합니다 (예: HAMSTER [48], ReKep [50]).
    - **하위 작업 기반 (Subtask-based Methods):** 플래너가 지시를 분해하고, 정책이 행동 시퀀스를 생성합니다 (예: HiRobot [156], DexVLA [158], RoboMatrix [160]).

**3. 기타 고급 분야 (Other Advanced Field):**

- **강화 학습 기반 방법 (RL-based Methods):** 로봇 정책을 최적화하여 일반화 능력과 작업 완료율을 향상시킵니다 (예: VLA-RL [181], ReWiND [39]).
- **학습 불필요 방법 (Training-Free Methods):** 재학습이나 아키텍처 변경 없이 기존 VLA 아키텍처를 개선하여 효율성을 높입니다 (예: FlashVLA [115], EfficientVLA [190]).
- **인간 비디오 학습 (Learning from Human Videos):** 인간 비디오 데이터를 활용하여 로봇 정책 학습을 안내하고 도메인 간 전이를 가능하게 합니다 (예: Human-Robot Semantic Alignment [42], UniVLA [194]).
- **월드 모델 기반 VLA (World Model-based VLA):** 예측적 월드 모델을 VLA 시스템에 통합하여 환경 역학을 모델링하고 행동 계획을 강화합니다 (예: WorldVLA [38], World4Omni [43]).

**4. 데이터셋 및 벤치마크 (Datasets and Benchmarks):**

- **실제 로봇 데이터셋:** 환경 복잡성을 포착하고 언어 접지 및 의미론적 일반화에 필수적인 실제 데이터를 제공합니다 (예: OXE [90], RT-1 [83], OpenVLA [26]).
- **시뮬레이션 데이터셋 및 벤치마크:** 확장 가능하고 안전하며 재현 가능한 상호작용 데이터를 제공하여 복잡한 지시 따르기 및 다단계 계획을 지원합니다 (예: BEHAVIOR [209], ALFRED [210], CALVIN [216]).
- **인간 행동 데이터셋:** VLA 모델 사전 학습에 적합한 의미론적으로 풍부하고 상황에 따라 다양한 시연을 제공합니다 (예: Ego4D [225], EPIC-Kitchens [229], EgoDex [233]).
- **구현된 데이터셋 및 벤치마크:** 구현된 에이전트의 계획 및 추론 능력을 평가하는 데 중점을 둡니다 (예: EmbodiedQA [238], IQUAD [239], LoTa-Bench [244]).

## 📊 Results

대규모 VLM 기반 VLA 모델은 로봇 조작 분야에 혁신적인 발전을 가져왔으며, 다음과 같은 핵심 특징에서 뛰어난 성능을 보입니다.

- **다중 모드 융합 (Multimodal Fusion):**

  - 시각적 관찰 및 언어적 지시를 의미론적으로 정렬된 공유 잠재 공간에 임베딩하여 지각과 명령 간의 긴밀한 의미적 접지를 촉진합니다.
  - 비전, 언어, 고유 수용성(proprioception) 및 행동과 같은 연속적인 양식을 토큰 시퀀스로 이산화하여 토큰 수준의 다중 모드 통합과 미세 조정된 조정을 가능하게 합니다.
  - 3D (예: 포인트 클라우드), 촉각 입력, 주변 오디오와 같은 다양한 감각 양식을 원활하게 수용하여 실제 배포를 위한 확장성과 견고성을 높입니다.

- **지시 따르기 (Instruction Following):**

  - 사전 학습된 VLM의 풍부한 세계 지식을 활용하여 자연어 지시를 동적으로 해석하고 실행 가능한 행동으로 접지시킵니다.
  - 특히 장기 작업을 위해 고수준 명령을 하위 목표로 분해하고, 저수준 제어기가 이를 실행하는 계층적 작업 분해를 효과적으로 수행합니다.
  - 사고 과정(Chain-of-Thought) 추론을 의사 결정 루프에 통합하여 잠재적인 시각적 목표를 예측함으로써 복잡한 다단계 작업의 실행 신뢰도를 높입니다.

- **다차원 일반화 (Multi-Dimensional Generalization):**
  - **교차 작업 일반화:** DexVLA [158]와 같은 모델은 작업별 튜닝 없이도 다양한 조작에서 일관되게 높은 성공률을 달성하며, 새로운 객체와 장면에 일반화하는 탁월한 제로샷 및 소수샷(few-shot) 일반화 능력을 보여줍니다.
  - **교차 도메인 데이터 일반화:** $\pi_{0.5}$ [30]와 같은 "기반 VLA" 모델은 이기종 데이터(웹 텍스트, 시뮬레이션 비디오, 교차 로봇 구현체)를 공동으로 학습하여 광범위한 문맥적 및 의미적 이해를 습득합니다.
  - **교차 구현체 및 시뮬레이션-실제 일반화:** 계층적 VLA 아키텍처는 고수준 플래너를 VLM 공간에서 학습하고 저수준 제어를 도메인별 디코더에 위임함으로써, 다양한 역학 및 로봇 형태에 걸쳐 일반화할 수 있도록 합니다 (예: HAMSTER [48]).

이러한 결과는 VLA 모델이 기존 로봇 조작 모델의 한계를 극복하고, 제한된 로봇 데이터만으로도 견고성과 다재다능성을 제공함을 입증합니다.

## 🧠 Insights & Discussion

- **모놀리식 모델 대 계층적 모델 비교:**

  - **모놀리식 모델:** 단일 통합 파이프라인으로 지각, 추론, 제어를 함께 최적화하여 고수준 다중 모드 의미론을 저수준 행동으로 직접 변환합니다. 이는 로봇 조작에서 전체론적이고 긴밀하게 결합된 학습을 가능하게 하며, 인간이 명시적으로 설계하지 않은 효율적인 작업 분해를 발견할 수 있습니다. 하지만 내부 추론 과정이 불투명하다는 한계가 있습니다.
  - **계층적 모델:** 고수준 계획과 저수준 정책 실행을 명시적으로 분리하는 다단계 설계를 채택하여 모듈성, 해석 가능성 및 유연성을 촉진합니다. 구성 요소를 독립적으로 설계, 학습 또는 교체할 수 있어 도메인 지식 통합이나 새로운 로봇 조작 작업 적응이 용이합니다. 명확하고 인간이 이해할 수 있는 중간 출력을 생성하므로 설명 가능성이 중요한 시나리오에 유리합니다.
  - 두 접근 방식 모두 VLA 분야에 고유한 강점을 제공하며, 모놀리식은 일반화에, 계층적은 투명성과 복잡한 다단계 작업에 기여합니다.

- **향후 연구 방향:**
  - **데이터셋 및 벤치마킹:** 시뮬레이션-실제 간 격차 해소, 대규모 실제 데이터 수집, 장기 계획, 모바일 조작, 다중 에이전트 협업을 포괄하는 현실적인 벤치마크 및 하위 작업 성공률, 시간 효율성, 교란에 대한 견고성 등 풍부한 평가 지표가 필요합니다.
  - **기억 메커니즘 및 장기 계획:** 현재 VLA는 단기적인 행동에 초점을 맞추므로, 에피소드 인식을 갖춘 미래 예측 계획 및 기억 메커니즘을 통합하여 일관된 목표 지향적 행동 시퀀스를 가능하게 해야 합니다.
  - **3D 및 4D 인지:** 깊이, 포인트 클라우드 관찰을 통합하고 다중 모드 입력을 단일 표현으로 융합하여 3D 장면의 시간적 변화를 이해하는 4D 인지 능력 개발이 필수적입니다.
  - **모바일 조작:** 보행(locomotion)과 조작을 동시에 실행하는 통합 정책을 학습하여 로봇이 환경과 상호작용하는 능력을 향상시키는 것이 중요합니다.
  - **다중 에이전트 협력:** 의도 협상, 팀원의 행동에 대한 적응, 다단계 목표에 대한 공동 추론을 요구하는 다중 에이전트 협업 작업을 위해 상호작용 인지 표현과 공유 월드 모델 통합이 필요합니다.
  - **개방형 세계에서의 평생 학습:** 치명적인 망각 없이 지속적으로 기술을 습득하고 새로운 경험을 통합하며, 경험을 재사용 가능한 추상화로 자기 조직화하는 메커니즘 개발이 필요합니다.
  - **모델 효율성:** 높은 계산 및 메모리 비용 문제를 해결하기 위해 작업 인지 동적 토큰 가지치기, 비동기 추론, 하드웨어 친화적인 양자화(quantization) 방식과 같은 효율성 최적화가 필요합니다.

## 📌 TL;DR

- **문제:** 기존 로봇 조작은 복잡하고 비정형적인 환경에서 일반화 및 확장 능력이 부족합니다.
- **방법:** 본 논문은 대규모 VLM(시각-언어 모델)을 활용하여 시각 관찰 및 자연어 지시를 이해하고 로봇 행동을 생성하는 VLA(시각-언어-행동) 모델을 체계적으로 분류 및 분석합니다. VLA 모델은 크게 두 가지 패러다임으로 나뉩니다:
  1. **모놀리식 모델:** 시각, 언어, 로봇 상태 이해 및 행동 생성을 단일 시스템으로 통합하거나 (단일 시스템), VLM 백본과 행동 전문가를 분리하여 정보(잠재 표현)를 교환하는 방식(이중 시스템)을 사용합니다. 효율성을 위해 성능 향상 및 추론 최적화 기법을 적용합니다.
  2. **계층적 모델:** 계획(플래너)과 실행(정책)을 명시적으로 분리하며, 플래너는 하위 작업, 키포인트, 프로그램 등 해석 가능한 중간 표현을 생성하고, 정책은 이를 바탕으로 행동을 실행합니다 (플래너 전용 또는 플래너+정책).
- **주요 결과:** 대규모 VLM 기반 VLA 모델은 강력한 다중 모드 융합, 복잡한 지시 이해 및 다차원 일반화(작업, 도메인, 구현체 간) 능력을 보여줍니다. 강화 학습, 학습 불필요 최적화, 인간 비디오 학습, 월드 모델 통합 등 다양한 고급 기법들이 활용됩니다.
- **향후 방향:** 시뮬레이션-실제 격차 해소, 기억 메커니즘을 통한 장기 계획, 3D/4D 인지, 모바일 및 다중 에이전트 조작, 평생 학습, 그리고 모델 효율성 개선이 중요한 연구 과제로 제시됩니다.
