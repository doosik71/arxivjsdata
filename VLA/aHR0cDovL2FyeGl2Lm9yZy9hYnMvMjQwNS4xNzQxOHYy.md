# A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation

Chenxuan Li, Jiaming Liu, Guanqun Wang, Xiaoqi Li, Sixiang Chen, Liang Heng, Chuyan Xiong, Jiaxin Ge, Renrui Zhang, Kaichen Zhou, Shanghang Zhang

## 🧩 Problem to Solve

기존 Vision-Language-Action (VLA) 모델은 새로운 또는 복잡한 로봇 조작 작업에서 실패하는 경향이 있으며, 제어 과정에서 실패를 감지하고 수정하는 메커니즘이 부족합니다. 특히, 로봇 엔드-이펙터의 저수준 SE(3) 포즈를 직접 수정하는 데 어려움을 겪고, 성공적으로 수정된 경험으로부터 학습하여 정책을 개선하는 능력이 없습니다. 이는 불확실성과 예상치 못한 상황이 흔한 실제 환경에서의 실용성을 제한합니다.

## ✨ Key Contributions

- **자기 수정 (SC-)VLA 프레임워크 제안:** 인간의 "빠른 시스템"과 "느린 시스템" 사고방식에서 영감을 받아, 직접 행동을 예측하는 "빠른 시스템"과 실패한 행동을 숙고하고 수정하는 "느린 시스템"을 단일 VLA 정책에 통합했습니다.
- **느린 시스템을 위한 CoT(Chain-of-Thought) 훈련 전략 개발:** 조작 실패 후 인간의 반성을 모방하도록 설계되었으며, 행동 실패의 원인을 식별하고, 전문가 피드백을 적응적으로 요청하며, 현재 실패 시나리오를 숙고하고, 단계별로 수정 행동을 반복적으로 생성합니다.
- **연속 정책 학습(Continuous Policy Learning) 방법 도입:** 성공적으로 수정된 샘플을 기반으로 연속 정책 학습 방법을 설계하여 빠른 시스템의 조작 정확도를 향상시키고 전문가 개입 빈도를 줄입니다.

## 📎 Related Works

- **로봇 조작 (Robotic Manipulation):**
  - **MLLM 기반 접근:** Palm-E [8]는 조작 계획에 멀티모달 인코딩을 사용하고, VoxPoser [21]는 MLLM을 활용하여 zero-shot 궤적을 생성하며, RT-2 [73]는 새로운 작업에 대한 빠른 적응을 가능하게 합니다. Robotflamingo [30]는 장기 조작을 위한 모방 학습에 MLLM을 미세 조정합니다.
  - **VLA 모델:** 최근 연구 [25, 30, 31, 39, 73]는 엔드-이펙터 포즈 예측을 위해 MLLM을 강화하고 있습니다.
- **로봇 실패 수정 (Robotic Failure Correction):**
  - REFLECT [40]: LLM을 사용하여 과거 경험을 바탕으로 추론하고 실패 설명을 통해 계획을 개선합니다.
  - MULTIREACT [67]: 비전-언어 모델 [45]을 보상 모델로 사용하여 중간 실패를 인식하고 복구합니다.
  - DoReMi [17]: LLM을 사용하여 계획과 실행 간의 불일치를 실시간으로 감지하고 복구합니다.
  - CLAIRify [50]: 반복적인 프롬프팅을 통해 계획의 유효성을 보장합니다.
  - **제한점:** 위 방법들은 저수준 SE(3) 포즈를 직접 수정하는 능력이 부족하고, 수정 피드백으로부터 학습하지 못합니다.

## 🛠️ Methodology

SC-VLA는 LLaMA-Adapter V2 [12]의 사전 훈련된 파라미터를 기반으로 하며, CLIP [45] 시각 인코더와 LLaMA [53] 언어 모델을 사용합니다. 모델은 어댑터만 미세 조정하여 사전 훈련된 MLLM의 강력한 능력을 보존합니다.

1. **빠른 시스템: 포즈 예측 (Fast system: pose prediction)**
   - **입력:** 초기 상태 이미지 ($I_{i}$)와 작업 언어 프롬프트 ($L_{i}$) (예: "Predict the contact point and orientation for [task name]").
   - **출력:** 6-DoF 엔드-이펙터 포즈 ($a_{i} = (a_{pos,i}, a_{rot,i})$)를 언어 형식으로 예측합니다. 연속적인 6-DoF 값을 100개의 정수 bin으로 이산화하여 분류 문제로 변환합니다.
   - **방식:** $I_{i}$와 $L_{i}$를 기반으로 $\pi(I_{i}, L_{i}) \rightarrow a_{i}$를 통해 엔드-이펙터 포즈를 직접 생성합니다.
2. **느린 시스템: 숙고 및 수정 (Slow system: reflection and correction)**
   - 조작 실패 발생 시 활성화되며, CoT(Chain-of-Thought) 전략을 통해 6-DoF 제어 행동을 수정합니다.
   - **실패 감지:** 최종 상태 이미지 ($I_{e}$)와 로봇 상태 ($L_{a_{i}}$)를 활용하여 작업 완료 여부를 판단하고, 실패 시 원인(위치, 회전, 또는 복합 오류)을 식별합니다 ($\pi(I_{e}, L_{a_{i}}) \rightarrow c_{i}$).
   - **전문가 피드백 요청:** 식별된 오류 유형 ($c_{i}$)에 따라 전문가에게 수정 피드백 ($f_{i}$)을 동적으로 요청합니다.
     - **위치 수정:** Where2Act [44]로 어포던스 맵을 생성하고, GPT-4V [1]와 같은 추론 전문가가 잠재적인 접촉 지점을 정제합니다.
     - **회전 수정:** Anygrasp [10]가 조작 박스(예측된 접촉 지점에서 5픽셀 확장) 내에서 잠재적인 회전을 예측합니다.
     - 복합 오류의 경우, 위치 수정을 먼저 수행한 다음 회전 수정을 적용합니다.
   - **수정 행동 생성:** 잘못된 행동의 6-DoF 포즈, 오류 원인, 전문가의 수정 피드백을 입력 프롬프트로 결합하여 모델이 수정된 포즈를 다시 예측하도록 합니다 ($\pi(I_{i}, f_{i}) \rightarrow a_{c}$).
3. **연속 정책 학습 (Continuous policy learning)**
   - 성공적으로 수정된 샘플을 활용하여 빠른 시스템의 포즈 예측 정확도를 향상시킵니다.
   - **학습 방식:** 50개의 수정된 샘플마다 모델을 업데이트하며, 치명적 망각(catastrophic forgetting)을 완화하기 위해 주입된 어댑터와 함께 EMA (Exponential Moving Average) [52] 기법을 사용합니다 ($\pi_{t} = \alpha\pi_{t-1} + (1-\alpha)\pi_{t}$, 여기서 $\alpha=0.999$).

## 📊 Results

- **시뮬레이션 결과 (SAPIEN):**
  - **오픈-루프 (Open-loop) 작업:**
    - SC-VLA (Fast+Slow)는 학습된 작업(seen tasks)에서 87%, 처음 보는 작업(unseen tasks)에서 68%의 성공률을 달성했습니다.
    - 이전 SOTA인 ManipLLM 대비 학습된 작업에서 30%, 처음 보는 작업에서 21% 향상된 정확도를 보였습니다.
    - 연속 정책 학습(CPL+Fast)은 빠른 시스템(Fast) 대비 학습된 작업에서 13%, 처음 보는 작업에서 39% 정확도 향상을 보여 모델의 일반화 능력을 강화했습니다.
  - **클로즈드-루프 (Closed-loop) 작업 (RLBench):**
    - Fast 시스템은 평균 63%의 성공률을 달성했으며, Fast+Slow 시스템은 91%의 성공률을 달성하여 순차적 행동 예측 및 수정에서 SC-VLA의 강력함을 입증했습니다.
- **실제 환경 결과:**
  - 4가지 조작 작업(pick and place, unplug charger, place bottle at rack, stack blocks)에서 테스트되었습니다.
  - SC-VLA (Fast+Slow)는 평균 58%의 성공률을 달성했습니다.
  - ManipLLM 대비 "stack block" 작업에서 40% (10% $\rightarrow$ 50%) 향상되었고, 3D Diffusion Policy (DP3) 대비 평균 성능이 15% 향상되었습니다.
- **어블레이션 연구 (Ablation Study):**
  - **전문가 유형의 영향:** 적응형 전문가 피드백 방식(SC-VLA)은 결합된 피드백 방식과 유사한 성능을 보였으나, 전문가 개입 비용이 더 낮았습니다.
  - **수정 횟수의 영향:** 한 번의 수정만으로도 빠른 시스템 대비 학습된 작업에서 15.4%, 처음 보는 작업에서 17.8%의 정확도 향상을 가져왔으며, 3회 이상의 수정은 안정적인 조작 정확도를 제공했습니다.
  - **연속 정책 학습 횟수의 영향:** 여러 차례 미세 조정 후 학습된 및 처음 보는 작업 모두에서 빠른 시스템의 예측 정확도가 크게 향상되어, 느린 시스템의 지식이 빠른 시스템으로 효과적으로 전달됨을 확인했습니다.

## 🧠 Insights & Discussion

SC-VLA는 다니엘 카네만(Daniel Kahneman)의 "빠른 시스템"과 "느린 시스템" 개념을 활용하여 인간과 유사한 사고방식을 로봇 조작에 구현함으로써 안정적인 조작 성능을 달성합니다. 빠른 시스템은 신속한 직접 포즈 예측을 가능하게 하고, 느린 시스템은 실패 시 CoT 추론을 통해 오류를 분석하고 수정합니다. 또한, 성공적으로 수정된 행동으로부터 지속적으로 학습하여 빠른 시스템의 정확도를 개선하는 연속 정책 학습 메커니즘을 도입하여 모델의 적응성과 일반화 능력을 크게 향상시켰습니다.

주요 제한점으로는 느린 시스템의 CoT 추론이 추가적인 계산 비용을 유발한다는 점입니다. 이 문제는 더 효율적인 LLM을 통합함으로써 극복될 수 있을 것으로 예상됩니다.

## 📌 TL;DR

기존 VLA 모델의 실패 처리 및 학습 능력 부족 문제를 해결하기 위해, SC-VLA는 인간의 인지 과정을 모방한 **빠른 시스템 (직접 행동 예측)**과 **느린 시스템 (실패 숙고 및 수정)**을 통합합니다. 느린 시스템은 실패 유형을 감지하고, 전문가 피드백을 적응적으로 활용하여 CoT 추론을 통해 정확한 6-DoF 포즈를 생성합니다. 성공적으로 수정된 샘플은 EMA 기반의 **연속 정책 학습**을 통해 빠른 시스템의 정확도와 일반화 능력을 지속적으로 향상시킵니다. 결과적으로 SC-VLA는 시뮬레이션 및 실제 환경에서 SOTA 대비 상당한 조작 성능 향상을 보이며, 로봇 조작의 견고성과 적응성을 크게 개선했습니다.
