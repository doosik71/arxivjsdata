{
  "title": "CLIPort: What and Where Pathways for Robotic Manipulation",
  "authors": "Mohit Shridhar, Lucas Manuelli, Dieter Fox",
  "year": 2021,
  "url": "http://arxiv.org/abs/2109.12098v1",
  "abstract": "How can we imbue robots with the ability to manipulate objects precisely but\nalso to reason about them in terms of abstract concepts? Recent works in\nmanipulation have shown that end-to-end networks can learn dexterous skills\nthat require precise spatial reasoning, but these methods often fail to\ngeneralize to new goals or quickly learn transferable concepts across tasks. In\nparallel, there has been great progress in learning generalizable semantic\nrepresentations for vision and language by training on large-scale internet\ndata, however these representations lack the spatial understanding necessary\nfor fine-grained manipulation. To this end, we propose a framework that\ncombines the best of both worlds: a two-stream architecture with semantic and\nspatial pathways for vision-based manipulation. Specifically, we present\nCLIPort, a language-conditioned imitation-learning agent that combines the\nbroad semantic understanding (what) of CLIP [1] with the spatial precision\n(where) of Transporter [2]. Our end-to-end framework is capable of solving a\nvariety of language-specified tabletop tasks from packing unseen objects to\nfolding cloths, all without any explicit representations of object poses,\ninstance segmentations, memory, symbolic states, or syntactic structures.\nExperiments in simulated and real-world settings show that our approach is data\nefficient in few-shot settings and generalizes effectively to seen and unseen\nsemantic concepts. We even learn one multi-task policy for 10 simulated and 9\nreal-world tasks that is better or comparable to single-task policies."
}