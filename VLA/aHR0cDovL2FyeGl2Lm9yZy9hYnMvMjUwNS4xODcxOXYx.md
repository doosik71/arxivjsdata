# VLA-RL: Masterful하고 일반적인 로봇 조작을 위한 확장 가능한 강화 학습

Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, Ziwei Wang

## 🧩 Problem to Solve

최근 높은 역량을 가진 Vision-Language-Action (VLA) 모델들은 인간 시범을 모방하여 다양한 로봇 조작 작업에서 인상적인 성능을 보여주었습니다. 그러나 오프라인 데이터를 활용하는 방식은 방문한 상태가 제한적이기 때문에 **분포 외(Out-of-Distribution, OOD) 시나리오에서 실행 실패**를 야기합니다. 이러한 한계를 극복하기 위해, 테스트 시 온라인으로 수집된 데이터를 통해 개선되는 탐색 기반 방법(exploration-based method)이 필요합니다.

## ✨ Key Contributions

- **VLA-RL 프레임워크 제안:** 사전 학습된 자기 회귀(auto-regressive) VLA 모델의 성능을 온라인 강화 학습(RL)을 통해 향상시키는 알고리즘 및 체계적인 프레임워크를 제시합니다.
- **다중 모달 다중 턴 대화로서의 RL 공식화:** 일반적인 로봇 조작 궤적을 다중 모달(multi-modal) 및 다중 턴(multi-turn) 대화로 모델링하여 자기 회귀 VLA 학습을 위한 궤적 수준(trajectory-level) RL을 공식화합니다.
- **로봇 프로세스 보상 모델(RPRM) 도입:** 희소 보상(sparse rewards) 문제를 해결하기 위해, 자동으로 추출된 작업 세그먼트의 의사 보상(pseudo-reward) 레이블로 훈련된 사전 학습된 Vision-Language 모델을 로봇 프로세스 보상 모델로 미세 조정하여 사용합니다.
- **확장성 및 효율성 향상 기법:** 커리큘럼 선택 전략(curriculum selection strategy), GPU 균형 벡터화 환경(GPU-balanced vectorized environments), 배치 디코딩(batch decoding), 비평가 웜업(critic warmup) 등 안정성과 효율성을 개선하는 여러 구현 방안을 제시합니다.
- **최고 성능 달성:** LIBERO 벤치마크의 40가지 도전적인 로봇 조작 작업에서 OpenVLA-7B 모델의 성능을 강력한 미세 조정 기준선보다 4.5% 향상시키고, π$_{0}$-FAST와 같은 상업용 모델과 동등한 성능을 달성했습니다.
- **추론 스케일링 법칙의 가능성 제시:** VLA-RL의 성능이 테스트 시간 최적화 증가에 비례하여 지속적으로 개선됨을 관찰하며, 이는 로봇 공학에서 추론 스케일링 법칙(inference scaling laws)의 초기 신호일 수 있음을 시사합니다.

## 📎 Related Works

- **로봇 파운데이션 모델(Robotic Foundation Models):** OpenVLA-7B와 같은 대규모 Vision-Language-Action (VLA) 모델들은 다중 작업 훈련을 통해 일반화된 로봇 행동을 학습하는 데 큰 잠재력을 보여주었지만, 오프라인 시범 데이터에 의존하는 모방 학습(Imitation Learning)은 OOD 시나리오에서 한계를 가집니다.
- **로봇 공학을 위한 강화 학습(Reinforcement Learning for Robotics Models):** 기존의 로봇 RL은 데이터 비효율성 및 보상 설계의 어려움을 겪었으며, 주로 단순한 도메인이나 단일 작업 학습에 집중했습니다. 본 연구는 대규모 로봇 파운데이션 모델에서 미세 조정하여 복잡한 움직임 패턴을 학습하는 궤적 수준 RL을 탐구합니다.
- **대규모 모델을 위한 강화 학습(Reinforcement Learning for Large Models):** LLM의 추론 능력을 강화하는 데 사용된 PPO, STaR, Process Reward Models, 대화 기반 훈련 방법 등 RL 기술의 발전이 본 연구에 영감을 주었습니다. VLA-RL은 이러한 LLM의 RL 프레임워크를 로봇 공학에 적용하여 다중 모달, 다중 턴 대화로 궤적 수준 최적화를 공식화합니다.

## 🛠️ Methodology

VLA-RL은 온라인 강화 학습(RL)을 통해 자기 회귀 VLA를 훈련하는 체계적인 프레임워크입니다.

1. **VLA 모델(정책 및 가치):**
   - 기반 모델로 OpenVLA-7B를 사용합니다. 이는 Llama-2-7B LLM과 SigLIP 및 DinoV2 시각 인코더를 결합한 자기 회귀 모델입니다.
   - 로봇 조작 궤적을 다중 모달 다중 턴 대화로 공식화합니다. 상태 공간은 이미지 ($O$)와 입력 텍스트($V_{m}$)의 곱 $S = O \times V_{m}$이며, 행동 공간은 VLA가 생성하는 출력 텍스트 시퀀스($V_{n}$)입니다.
   - 정책 $\pi_{\theta}: O \times V_{m} \to V_{n}$는 입력 이미지 $o_{t}$와 지시 $v_{in_{t}}$가 주어졌을 때 출력 시퀀스 $v_{out_{t}}$를 내보낼 확률을 할당합니다.
   - **PPO(Proximal Policy Optimization)**를 사용하여 정책을 최적화하며, 일반화된 이점 추정(Generalized Advantage Estimation, GAE)을 통해 이점 값($A_{t}$)을 계산합니다.
   - 행동 시퀀스의 로그 확률은 토큰 수준 로그 확률의 합으로 분해됩니다: $\text{log}\pi_{\theta}(a_{t}|o_{t}, v_{in_{t}}) = \sum_{i=1}^{|A|} \text{log}\pi_{\theta}(v_{out_{t,i}}|o_{t}, v_{in_{t}})$.
2. **로봇 프로세스 보상 모델(Robotic Process Reward Model, RPRM):**
   - 환경에서 제공되는 희소 보상을 보완하기 위해 Dense Reward를 제공합니다.
   - 사전 훈련된 Vision-Language 모델을 미세 조정하여 RPRM을 만듭니다.
   - 보상 모델링을 다음 토큰 예측 문제로 재구성합니다. 훈련 목표는 작업 완료 진행 상황을 나타내는 의사 보상 신호로 가중치를 부여한 유망한 행동 토큰의 로그 가능성을 최대화하는 것입니다: $L_{rprm}(\phi) = -E_{t}[\sum_{j=1}^{K} \text{log} p_{\phi}(v_{rprm_{t,j}}|v_{out_{t,\lt j}}, o_{t}, v_{in_{t}})]$.
   - **자율 의사 보상 레이블 생성 파이프라인:**
     - **마일스톤 분할(Milestone Segmentation):** 성공적인 궤적을 그리퍼 개방도의 중요한 변화에 기반하여 하위 작업으로 분할합니다.
     - **진행 레이블링(Progress Labeling):** 로봇의 말단 효과기(end-effector) 속도가 0에 가까워지는 키프레임에 도달하는 VLA 행동 시퀀스에 양의 의사 보상을 할당합니다.
   - 최종 보상은 환경의 희소 보상과 RPRM 예측 보상의 합입니다.
3. **VLA-RL 시스템 구현 개선:**
   - **커리큘럼 선택 전략:** 에이전트의 현재 능력에 따라 작업을 선택하는 적응형 커리큘럼을 구현합니다. 성공률이 ~50%인 작업에 우선순위를 부여하여 탐색을 최적화합니다: $P(\text{task}_{j}) \propto \text{exp}((0.5 - s_{j})/\tau)$.
   - **비평가 웜업(Critic Warmup):** 정책-가치 공동 최적화 전에 초기 시범 학습된 정책을 사용하여 가치 네트워크를 단독으로 여러 번 반복하여 훈련합니다.
   - **GPU 균형 벡터화 환경:** 병렬 롤아웃을 위해 여러 벡터화 환경을 구현하며, 각 GPU 워커는 자체 환경을 가지고 상호작용합니다. 전체 환경 상태는 `all_reduce` 연산을 통해 추론 엔진으로 집계됩니다.
   - **인프라:** bfloat16을 사용하여 모델을 메모리에 맞춥니다. vLLM 가속을 위해 전용 GPU 1개를 할당하고, 나머지 GPU는 Ray 및 PyTorch FSDP를 사용하여 학습에 사용합니다.

## 📊 Results

- **LIBERO 벤치마크 성능:**
  - LIBERO (Spatial, Object, Goal, Long) 벤치마크의 40개 로봇 작업에서 VLA-RL은 OpenVLA-7B SFT 기준선보다 평균 4.5% 높은 성공률을 달성했습니다.
  - GRAPE (DPO) 기준선보다도 1.8% 높은 성공률을 보였습니다.
  - 48 GPU 시간의 RL 훈련 후, VLA-RL은 OpenVLA-7B를 미세 조정하여 π$_{0}$-FAST와 같은 고급 상업용 모델의 성능에 필적했습니다.
- **테스트 시간 스케일링:**
  - RL 훈련 과정에서 4가지 LIBERO 작업 스위트 모두에서 평가 성공률이 테스트 시간 최적화와 함께 지속적으로 향상되어, 로봇 공학에서도 추론 스케일링 법칙의 초기 신호를 보였습니다.
- **훈련 동역학 분석:**
  - **생성된 에피소드 길이:** 훈련이 진행됨에 따라 에피소드 길이가 점진적으로 감소하여 모델이 더 효율적인 행동 시퀀스를 학습함을 나타냅니다.
  - **훈련 중 보상 동역학:** 훈련 전반에 걸쳐 보상이 일관되게 개선되었으며, 이는 물리적 작업 성공률과 강하게 연관되어 RPRM이 의미 있는 진행 상황을 효과적으로 포착함을 시사합니다.
  - **정책의 롤아웃 엔트로피:** 적절한 수준의 엔트로피를 유지하며, 훈련이 진행됨에 따라 점진적으로 감소하여 초기 탐색을 허용하면서 안정적인 개선을 이끌었습니다.
  - **시간 비용 분포:** GPU 균형 벡터화 환경과 vLLM 가속 덕분에 환경 시뮬레이션 및 모델 롤아웃에 드는 시간이 크게 줄었으며, 이제 학습 단계가 주요 병목이 되었습니다.
- **어블레이션 연구(Ablation Study):**
  - RPRM 제거 시 성공률이 85.8%로 (90.2% 대비) 하락했습니다.
  - 커리큘럼 제거 시 성공률이 88.0%로 하락했습니다.
  - 샘플링 온도(1.5에서 1.0으로)를 낮추면 성공률이 85.8%로 하락했습니다.
  - 비평가 웜업 제거 시 성공률이 80.0%로 크게 하락했습니다.
  - 높은 학습률(2e$^{-4}$)은 불안정성을 야기하여 성공률이 0.2%로 급락했습니다.
  - 이는 제안된 각 구성 요소가 VLA-RL의 전체적인 효과에 시너지 효과를 내며 필수적임을 입증합니다.
- **RL 대 SFT:**
  - **행동 커버리지 분석:** RL 에이전트가 생성한 행동은 SFT 모델(오프라인 데이터 기반)에 비해 행동 공간 전체에 걸쳐 더 균일하게 분포되어, RL 정책의 더 강력한 견고성을 보여줍니다.
  - **사례 연구:** VLA-RL은 SFT 기준선이 정렬 문제로 실패하는 접촉이 많은 작업에서 물체를 성공적으로 잡는 등 미세한 상호작용이 필요한 작업에서 뛰어난 효과를 보였습니다.

## 🧠 Insights & Discussion

- **의미:** VLA-RL은 온라인 RL을 통해 기존 모방 학습 기반 VLA의 OOD 시나리오 한계를 극복할 수 있음을 보여주었습니다. 이는 로봇 공학에서도 언어 모델의 추론 스케일링 법칙과 유사하게, 테스트 시간 최적화(즉, 더 많은 RL 훈련)가 성능 향상으로 이어진다는 초기 증거를 제시합니다. VLA-RL의 성능은 지속적으로 개선되어 고용량 VLA 모델의 잠재력을 더욱 확장할 수 있음을 시사합니다.
- **한계:**
  - 의사 보상 레이블을 추출하기 위한 휴리스틱(heuristics)이 보다 정교한 조작 작업의 미묘한 차이를 완전히 포착하지 못하여 비효율적인 정책 최적화로 이어질 수 있습니다.
  - 향후 연구에서는 자기 회귀 VLA를 넘어 확산 기반 정책(diffusion-based policies)에 RL을 적용하고, 대규모 실제 경험을 통한 온라인 자가 개선(online self-improvement)을 탐색할 계획입니다.

## 📌 TL;DR

VLA-RL은 기존 VLA 모델의 오프라인 데이터 의존성으로 인한 OOD 시나리오 실패 문제를 해결하기 위해 **온라인 강화 학습(RL)을 도입한 체계적인 프레임워크**입니다. 이 프레임워크는 로봇 조작 궤적을 **다중 모달 다중 턴 대화로 모델링**하고, 사전 학습된 VLM을 활용한 **로봇 프로세스 보상 모델(RPRM)로 희소 보상 문제를 해결**합니다. 커리큘럼 선택, 비평가 웜업 등의 구현 개선을 통해 **LIBERO 벤치마크에서 OpenVLA-7B 성능을 4.5% 향상**시켰으며, **테스트 시간 최적화를 통한 성능 향상은 로봇 공학에서도 추론 스케일링 법칙이 적용**될 수 있음을 시사합니다.
