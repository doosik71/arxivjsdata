# BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization

Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, Lichao Sun

## 🧩 Problem to Solve

VLA(Vision-Language-Action) 모델은 멀티모달 입력에서 로봇 제어를 위한 엔드투엔드 의사결정을 가능하게 하여 로봇 공학을 발전시켰습니다. 그러나 이들의 긴밀하게 결합된 아키텍처는 새로운 보안 취약점을 노출합니다. 특히, 훈련 서비스(Training-as-a-Service, TaaS) 패러다임 하에서 백도어 공격은 은밀하고 지속적인 위협이지만, VLA 모델의 맥락에서는 거의 탐구되지 않았습니다. 기존의 단일 모달 백도어 공격은 VLA의 (1) 장기 시퀀스 역학, (2) 교차 모달 얽힘, (3) 데이터 희소성 및 큐레이션 문제로 인해 비효과적이거나 적용할 수 없습니다. 이 논문은 VLA 모델의 백도어 취약점을 처음으로 밝히는 것을 목표로 합니다.

## ✨ Key Contributions

- **새로운 위협 발견:** VLA 시스템의 엔드투엔드 구조와 TaaS 훈련 파이프라인이 백도어 공격에 취약하다는 새로운 공격 표면을 식별하고 공식화했습니다. 이는 이전에 탐구되지 않았던 영역입니다.
- **표적 공격 설계:** VLA 모델을 위한 최초의 백도어 프레임워크인 BadVLA를 제안합니다. 이는 목표 분리형 2단계 공격 전략을 기반으로 클린 작업 정확도를 유지하면서 정밀한 제어 주입을 가능하게 합니다.
- **종합적인 실증 평가:** 여러 VLA 아키텍처 및 표준 로봇 벤치마크에 걸쳐 광범위한 실험을 수행했습니다. BadVLA가 클린 작업 성능 저하를 최소화하면서 약 96.7%의 공격 성공률(ASR)을 달성함을 보여줍니다. 또한, 기존 방어 메커니즘(예: 압축, 가우시안 노이즈)은 BadVLA를 감지하거나 완화하는 데 실패하여 VLA 모델에 대한 강력한 보안 연구의 시급성을 강조합니다.

## 📎 Related Works

- **Vision-Language-Action 모델:** RT-2, Octo, OpenVLA와 같이 시각, 언어, 행동 양식을 통합하여 로봇 제어를 위한 엔드투엔드 정책 학습을 가능하게 하는 멀티모달 파운데이션 모델들이 발전하고 있습니다. 본 논문은 이러한 VLA 모델의 강력한 기능보다는 견고성과 보안에 초점을 맞춥니다.
- **로봇 보안 위협:** 모듈형 로봇 시스템에 대한 다양한 위협이 밝혀져 왔습니다.
  - 물리적 패치를 통한 백도어 트리거 ([37, 5]).
  - 적대적 공격 ([45, 41–43]).
  - 명령 수준의 언어 교란 ([18, 40, 12]).
  - 교차 모달 트리거 ([21, 47, 38]).
    최근 연구([37])는 VLA 모델의 적대적 공격에 대한 취약성을 밝혀냈지만, VLA 모델에 대한 백도어 위협은 거의 탐구되지 않았습니다. 본 연구는 이 공백을 메우기 위해 VLA 모델에 대한 비표적 백도어 공격을 조사합니다.

## 🛠️ Methodology

BadVLA는 VLA 모델에 은밀한 백도어를 삽입하면서 클린 입력에 대한 성능을 유지하기 위한 두 단계 훈련 프레임워크를 제안합니다. VLA 모델 $f_\theta$는 인식 모듈 $f_p$, 백본 모듈 $f_b$, 행동 모듈 $f_a$의 세 가지 주요 구성 요소로 분해됩니다. 훈련 가능 파라미터는 $\theta=\{\theta_p, \theta_b, \theta_a\}$입니다.

1. **1단계: 참조 정렬 최적화를 통한 트리거 주입 (Stage I: Trigger Injection via Reference-Aligned Optimization)**
   - **목표:** 원래 작업 동작을 엄격히 유지하면서 VLA 모델에 잠재적 백도어를 삽입하고, 트리거가 있는 경우 출력 특징이 클린 입력과 크게 이탈하도록 합니다.
   - **고정 참조 모델 ($f_\text{ref}$):** 원본 모델을 고정된 참조 모델로 보존합니다.
   - **손실 함수 ($L_\text{trig}$):** 학습 가능한 모델 $f_\theta$는 두 가지 목적을 동시에 만족하도록 최적화됩니다.
     - **제한 (Restrict):** 클린 입력 $x_i$에 대해 $f_\theta$의 출력 $f_\theta(x_i)$가 $f_\text{ref}$의 출력 $f_\text{ref}(x_i)$와 일관성을 유지하도록 합니다.
     - **트리거 분리 (Trigger Separation):** 트리거 주입 함수 $T(\cdot, \delta)$를 통해 생성된 트리거 입력 $x'_i = T(x_i, \delta)$에 대해 $f_\theta(x'_i)$가 $f_\theta(x_i)$와 크게 분리되도록 합니다.
   - 수학적 표현:
     $$L_\text{trig} = \frac{1}{N}\sum_{i=1}^{N} \Vert f_\theta(x_i) - f_\text{ref}(x_i) \Vert_2^2 - \alpha \cdot \frac{1}{N}\sum_{i=1}^{N} \Vert f_\theta(T(x_i, \delta)) - f_\theta(x_i) \Vert_2^2$$
     여기서 $\alpha > 0$는 트레이드오프를 제어하는 하이퍼파라미터입니다. 이 단계에서는 $\theta_b$, $\theta_a$는 고정되고 $\theta_p$만 최적화됩니다.
2. **2단계: 고정된 인식 모듈을 통한 클린 작업 향상 (Stage II: Clean Task Enhancement with Frozen Perception Module)**
   - **목표:** 1단계에서 확립된 특징 공간 분리를 보존하면서 클린 데이터에 대한 작업 성능을 향상시킵니다.
   - **인식 모듈 고정:** $\theta_p$는 고정됩니다.
   - **백본 및 행동 모듈 미세 조정:** 백본 $\theta_b$ 및 행동 정책 $\theta_a$ 모듈만 클린 데이터셋 $D_\text{clean}$에 대해 미세 조정됩니다.
   - **훈련 목표:** 클린 데이터 분포 $D_\text{clean}$에 대한 음의 로그 가능도(Negative Log-Likelihood)를 최소화합니다.
     $$L_{\theta/\theta_p} = -E_{(v_i,l_i,a_i)\sim D_\text{clean}}[\log f_\theta(a_i|v_i,l_i)]$$
   - **결과:** 인식 모듈이 고정되어 있기 때문에 행동 및 백본 모듈은 클린 정렬된 특징 임베딩에만 노출됩니다. 추론 시 트리거가 발생하면 인식 모듈은 입력값을 훈련 중 관찰된 분포를 벗어나는 표현으로 변환하여, 디코더가 의미론적으로 불일치하거나 적대적인 행동을 생성하게 합니다.

## 📊 Results

- **주요 결과:**
  - BadVLA는 OpenVLA 모델 (LIBERO 벤치마크) 및 SpatialVLA 모델 (SimplerEnv)에 걸쳐 평가되었습니다.
  - **높은 공격 성공률(ASR)과 클린 성능 유지:** BadVLA는 픽셀 블록, 빨간색 머그, 빨간색 막대 등 다양한 시각적 트리거 유형에 대해 일관되게 95.0% 이상의 ASR을 달성하면서, 트리거가 없을 때 클린 작업 성공률(SR w/o)은 95.0% 이상을 유지했습니다.
  - **기존 방법 대비 우수성:** "Data-Poisoned" 및 "Model-Poisoned"와 같은 기존 백도어 주입 방법은 성능을 저하시키거나(SR=0.0) 트리거에 모델을 둔감하게 만드는(ASR=0.0) 등 완전히 실패했습니다.
  - **일반화 능력:** SpatialVLA 모델의 SimplerEnv 환경에서도 BadVLA는 클린 작업 성공률을 손상시키지 않으면서 최대 100.0%의 ASR로 백도어 동작을 안정적으로 활성화했습니다.
- **트리거 분석:**
  - **트리거 크기 및 위치:** 트리거 크기(이미지 영역의 1%, 5%, 10%)나 위치(중앙, 왼쪽 상단, 오른쪽 하단)에 관계없이 ASR은 지속적으로 높게 유지되었습니다. 이는 BadVLA가 공간적 지역성에 과적합되지 않고 표현 수준에서 트리거 의미를 인코딩함을 시사합니다.
  - **교차 모달 트리거:** 빨간색 머그와 같은 실제 물리적 또는 의미론적 객체도 백도어 동작을 성공적으로 활성화했습니다. 이는 BadVLA가 특정 픽셀이나 패턴을 암기하는 대신 잠재적 트리거 개념을 학습하여, 일반적인 환경 객체가 무의식적인 트리거로 사용될 수 있음을 시사합니다.
- **체계적 분석:**
  - **궤적 분석:** 트리거가 활성화된 경우 로봇의 궤적은 의도된 경로에서 점진적으로 벗어나 오류를 축적하고 작업 실패로 이어졌습니다. 이는 BadVLA가 고정된 적대적 행동을 주입하는 대신, 시간이 지남에 따라 복합되는 잠재적 불안정성을 도입함을 보여줍니다.
  - **특징 공간 분석:** 1단계 훈련 후 클린 입력과 트리거 입력 간의 코사인 유사도가 0.98에서 0.21로 크게 떨어져 잠재 공간에서 명확한 분리를 보였습니다. 이는 트리거가 하류 모듈이 변경된 방식으로 반응할 수 있도록 하는 뚜렷한 표현 시그니처를 유도함을 확인합니다.
  - **구성 요소 분석:** 참조 정렬 손실($L_1$)과 트리거 분리 손실($L_2$) 및 2단계 훈련(Sec)이 모두 필수적임을 입증하는 절제 연구를 통해, BadVLA의 단계적 분리 전략이 은밀성과 효과성을 모두 달성하는 데 중요함을 강조했습니다.
- **방어 분석:**
  - **입력 교란에 대한 강건성:** JPEG 압축(q=20%)이나 가우시안 노이즈($\epsilon$=0.08)와 같은 일반적인 입력 교란에도 불구하고, BadVLA는 높은 ASR을 유지하면서 클린 작업 성능을 90% 이상으로 보존했습니다. 이는 백도어가 저수준 시각적 충실도에 의존하지 않으며, 기존 이미지 전처리 방어책이 비효과적임을 시사합니다.
  - **재미세 조정에 대한 강건성:** 백도어가 주입된 모델을 새로운 작업에 대해 미세 조정한 후에도, 클린 작업 성능은 회복되었지만 ASR은 여전히 높게 유지되었습니다. 이는 백도어가 단순히 표면 수준 파라미터에 인코딩되는 것이 아니라 더 깊은 특징 표현에 내장되어 있어, 새로운 배포 환경에서도 위협이 지속될 수 있음을 나타냅니다.

## 🧠 Insights & Discussion

- **핵심 통찰:** 본 연구는 VLA 모델의 심각한 보안 사각지대, 즉 잠재적 조작에 대한 고유한 취약성을 드러냅니다. 엔드투엔드 구조와 TaaS 훈련 패러다임은 백도어 공격에 대한 새로운 위협을 야기합니다. BadVLA의 목표 분리형 최적화 전략은 클린 작업 성능에 영향을 주지 않으면서 VLA 모델의 인식 모듈에 은밀하고 지속적인 백도어를 효과적으로 삽입할 수 있음을 입증합니다. 이러한 백도어는 입력 교란이나 후속 미세 조정에도 강력하게 유지됩니다.
- **시사점:** 이는 미래의 멀티모달 로봇 정책에 대한 안전하고 신뢰할 수 있는 설계를 위한 강력한 훈련, 검증 및 방어 메커니즘의 필요성을 시급히 강조합니다. 특히 일반적인 환경 객체가 무심코 학습된 특징 경로와 정렬되어 트리거 역할을 할 수 있다는 점은 위험한 함의를 가집니다.
- **한계:** 본 연구는 TaaS 패러다임 하에서 VLA 모델의 취약성을 노출하는 데 중점을 두었으며, 주입된 백도어의 잠재적 심각성이나 하류 악용에 대해서는 탐구하지 않았습니다. 특히, 표적 백도어 공격이 VLA 모델에 대해 여전히 효과적인지 여부는 이 연구의 범위를 벗어납니다. 향후 연구에서는 표적 백도어 공격의 타당성과 영향에 대해 조사할 예정입니다.

## 📌 TL;DR

**문제:** VLA 모델은 로봇 제어에서 발전했지만, 엔드투엔드 구조와 TaaS 훈련으로 인해 백도어 공격에 취약하며, 기존 백도어 기법은 VLA의 복잡성으로 인해 비효과적입니다.
**방법:** BadVLA는 (1) 참조 정렬 최적화를 통해 인식 모듈에 은밀하게 트리거를 주입하고, (2) 인식 모듈을 고정한 채 클린 데이터로 다른 모듈을 미세 조정하여 클린 작업 성능을 유지하는 두 단계 목표 분리형 최적화 전략을 제안합니다.
**발견:** BadVLA는 클린 작업 성능 저하를 최소화하면서 (ASR 96.7% 이상) 효과적인 백도어 공격을 달성합니다. 이 공격은 다양한 트리거 유형, 크기, 위치 및 입력 교란(압축, 노이즈)에 강력하며, 모델 재미세 조정 후에도 지속됩니다. 이는 VLA 모델의 심각한 보안 취약점을 드러내며, 안전한 로봇 모델 설계의 필요성을 강조합니다.
