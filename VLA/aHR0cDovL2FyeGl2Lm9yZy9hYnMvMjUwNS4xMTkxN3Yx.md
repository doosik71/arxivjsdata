# OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning

Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, Yang Gao

## 🧩 Problem to Solve

로봇이 다양한 작업을 수행하기 위해 필요한 추론(reasoning) 및 행동(acting) 능력은 기존의 이중 시스템 접근 방식(고수준 추론과 저수준 행동 분리)에서 다음과 같은 문제점을 겪습니다. 시스템 간의 상호 이해 부족으로 인해 추론 시스템이 행동 시스템이 실행할 수 없는 명령을 생성하거나, 지연(latency) 문제로 인해 실시간 환경에서 오래되거나 관련 없는 지침을 제공하는 경우가 발생합니다. 이로 인해 로봇의 일반화 능력과 장기적인 작업 계획 능력이 제한됩니다. 이 연구는 추론과 행동 간의 시너지 효과를 높이기 위해 단일 통합 모델을 개발하는 것을 목표로 합니다.

## ✨ Key Contributions

- **단일 통합 비전-언어-행동(VLA) 모델 제안:** 기존의 이중 시스템(System One: 행동, System Two: 추론) 대신, 추론과 행동을 모두 수행할 수 있는 OneTwoVLA라는 단일 모델을 제안합니다.
- **적응형 추론 및 행동 전환:** OneTwoVLA는 작업 실행 중 중요한 순간(예: 하위 작업 완료, 오류 감지, 사람 개입 필요)에 명시적인 자연어 추론 모드로 전환하고, 그 외 시간에는 최신 추론 내용을 기반으로 행동을 생성합니다.
- **확장 가능한 데이터 합성 파이프라인:** 로봇 데이터와 함께 공동 학습(co-training)하기 위한 신체화된 추론 중심(embodied reasoning-centric) 시각-언어 데이터(vision-language data)를 합성하는 확장 가능한 파이프라인을 설계했습니다.
- **다양한 능력에서 우수한 성능 입증:** 장기적인 작업 계획, 오류 감지 및 복구, 자연스러운 인간-로봇 상호작용, 일반화 가능한 시각적 그라운딩(visual grounding) 등 네 가지 핵심 능력에서 OneTwoVLA의 우수성을 광범위한 실험을 통해 입증했습니다.

## 📎 Related Works

- **Vision-Language-Action Models (VLAs):** 사전 학습된 VLM(Vision-Language Models)에서 시작하여 다양한 현실 조작 작업을 처리할 수 있지만, 장기적인 작업이나 복잡한 환경에서 제한된 추론 능력을 보입니다. OneTwoVLA는 통합 모델 아키텍처와 공동 학습 프레임워크를 통해 추론 및 일반화 능력을 향상시킵니다.
- **Reasoning for Robot Control (Dual-System Frameworks):** GPT-4V [4]와 같은 파운데이션 모델을 고수준 추론(System Two)에 사용하고, 저수준 정책을 행동(System One)에 사용하는 이중 시스템 프레임워크는 장기적인 조작 작업에 효과적입니다. 그러나 시스템 간 상호 이해 부족 및 지연 문제가 있습니다. OneTwoVLA는 이러한 한계를 극복하기 위해 추론 시점을 자율적으로 결정하는 통합 모델을 제안합니다.
- **Co-training for Robot Learning:** 로봇 정책을 로봇 데이터와 함께 액션 없는(action-free) 시각-언어 데이터로 공동 학습하는 방식은 일반화에 도움이 됩니다. 하지만 기존 방법들은 도메인 간극이 크거나 확장하기 어려운 데이터셋에 의존합니다. OneTwoVLA는 신체화된 추론이 풍부한 고품질 시각-언어 데이터를 합성하는 확장 가능한 파이프라인을 제안하여 이를 해결합니다.

## 🛠️ Methodology

### 3.1 Framework of OneTwoVLA

- **문제 정의:** 로봇 제어 정책 $ \pi\_\theta $ 가 매 타임스텝 $ t $마다 추론할지 행동할지 자율적으로 결정하는 능력을 개발합니다.
- **추론 모드:**
  - 입력: 현재 이미지 $ I*{1:n}^t $, 최신 추론 타임스텝의 참조 이미지 $ I*{1:n}^{ref} $, 언어 지침 $ \ell $, 최신 추론 내용 $ R $.
  - 출력: 업데이트된 추론 내용 $ \hat{R} \sim \pi*\theta(\cdot|I*{1:n}^t, I\_{1:n}^{ref}, \ell, R) $.
- **행동 모드:**
  - 입력: 위 추론 모드 입력에 로봇의 고유 상태 $ s_t $ 추가.
  - 출력: 최신 추론 내용에 기반한 행동 청크 $ A*t \sim \pi*\theta(\cdot|I*{1:n}^t, I*{1:n}^{ref}, \ell, R, s_t) $.
- **적응형 추론 (Adaptive Inference, Algorithm 1):**
  - 모델은 먼저 두 가지 특수 결정 토큰(decision token, DT) 중 하나를 예측합니다: `[BOR]` (Beginning of Reasoning) 또는 `[BOA]` (Beginning of Action).
  - `[BOR]`이 예측되면, 모델은 추론 모드에 진입하여 `[EOS]` (End of Sentence) 토큰을 생성할 때까지 텍스트 추론 내용 $ \hat{R} $을 생성합니다. 이 때 $ R $은 $ \hat{R} $로 업데이트되고, $ I\_{ref} $는 $ I_t $로 설정됩니다.
  - `[BOA]`가 예측되면, 모델은 행동 모드에 진입하여 즉시 행동 청크 $ A_t $를 생성합니다.
  - 이 프레임워크는 오류 복구 및 인간-로봇 상호작용을 지원하며, 추론은 특정 중요한 시점에만 발생하여 전체 실행 효율성에 미치는 영향이 적습니다.
- **모델 구현:**
  - 기존 VLA 모델인 $ \pi_0 $ [12]를 기본 모델로 사용합니다.
  - 텍스트 추론은 교차 엔트로피 손실(cross-entropy loss)을 통해 지도 학습됩니다.
  - 복잡한 연속 행동 분포 모델링을 위해 $ \pi_0 $의 행동 전문가(action expert) 아키텍처를 계승하고 흐름 매칭 손실(flow matching loss) [62, 63]을 사용하여 학습합니다.

### 3.2 Curating Robot Data with Embodied Reasoning

- **새로운 로봇 데이터 형식:** 인간 전문가의 시연 궤적(demonstration trajectories)을 수집한 후, 추론 간격(reasoning intervals)과 행동 간격(acting intervals)으로 나눕니다.
  - **추론 간격:** 모델 추론이 필요한 주요 단계(예: 하위 작업 완료, 오류 감지, 사람 개입)를 포착하며, 텍스트 추론 내용으로 주석을 추가합니다.
  - **행동 간격:** 모델이 관찰 및 최신 추론 내용을 기반으로 행동을 예측하도록 학습합니다.
- **신체화된 추론 내용:** 다음 네 가지 구성 요소로 이루어집니다.
  - **장면 설명 (scene description):** 주로 작업 관련 객체의 위치에 초점.
  - **고수준 계획 (high-level plan):** 작업을 완료하기 위한 순차적 단계 개요.
  - **역사적 요약 (historical summary):** 작업 진행 상황을 모델에 알립니다.
  - **다음 단계 (immediate next step):** 로봇이 실행해야 할 즉각적인 명령.
- **오류 감지 및 복구, 인간-로봇 상호작용 데이터:** 실패 상태에서 복구하는 데 초점을 맞춘 로봇 데이터를 수집 및 라벨링하고, 특정 시연 간격에 상호작용 텍스트(예: 로봇의 질문, 인간의 답변)를 주석으로 추가합니다.

### 3.3 Scalable Synthesis of Vision-Language Data with Embodied Reasoning

- **확장 가능한 합성 파이프라인:**
  1. **텍스트 설명 생성:** Gemini 2.5 Pro [64]를 프롬프트하여 일반 가정 용품이 특징인 다양한 탁상 레이아웃의 텍스트 설명을 생성합니다.
  2. **이미지 합성:** 텍스트 설명을 기반으로 텍스트-이미지 생성 모델 FLUX.1-dev [65]를 사용하여 고품질 이미지를 합성합니다. 어안 왜곡(fisheye distortion)이나 로봇 그리퍼 합성 등의 증강 기법을 적용하여 실제 로봇 관찰과 유사하게 만듭니다.
  3. **지침 및 추론 내용 생성:** 다시 Gemini 2.5 Pro를 활용하여 합성된 각 이미지에 대한 작업 지침과 해당 추론 내용을 생성합니다.
- **생성된 데이터 유형:**
  - **시각적 그라운딩 작업 (Visual grounding tasks):** 공간적 관계, 객체 속성 또는 의미적 특징을 통해 이미지 내 객체를 암시적으로 참조하는 지침. 추론은 객체의 명시적 이름과 위치를 명시해야 합니다.
  - **장기적인 작업 (Long-horizon tasks):** 확장된 다단계 목표를 설명하는 지침. 추론은 작업을 완료하기 위한 고수준의 단계별 계획을 제공해야 합니다.
- 이 파이프라인을 통해 16,000개의 데이터 샘플을 자동 생성했습니다.

## 📊 Results

- **장기적인 작업 계획 (Long-horizon Task Planning):**

  - **성능:** "Tomato-Egg", "Hotpot", "Cocktail" 세 가지 도전적인 장기 작업을 평균 87%의 성공률로 수행하여, 기준선 $ \pi_0 $ (57%)보다 30%, 이중 시스템 접근 방식 (63%)보다 24% 우수했습니다.
  - **원인:** OneTwoVLA는 정확한 계획을 일관되게 생성하고, 작업 진행 상황을 정확히 추적하며, 정밀한 행동을 출력합니다. 반면, $ \pi_0 $는 추론과 과거 맥락 부족으로 현재 단계를 놓치거나 정밀한 조작에 어려움을 겪었습니다. 이중 시스템은 시스템 간 상호 이해 부족으로 실행 불가능한 명령을 내리거나 지연 문제로 인해 상황에 뒤처지는 안내를 제공했습니다.
  - **일반화 가능한 계획 (Generalizable Planning):** 합성된 시각-언어 데이터와 공동 학습하여 로봇 데이터에는 없던 새로운 작업(예: "Help me stay awake"를 위한 커피 준비)에서도 강력한 일반화 능력을 보였습니다.

- **오류 감지 및 복구 (Error Detection and Recovery):**

  - **능력:** OneTwoVLA는 실시간으로 실행 오류를 감지하고, 복구 전략을 빠르게 추론하며, 교정 행동을 수행할 수 있습니다. 예를 들어, 그리퍼가 스트레이너를 잡지 못했을 때 후퇴, 재정렬 후 다시 잡는 행동을 수행했습니다.
  - **대비:** $ \pi_0 $는 오류를 무시하고 진행하는 경향이 있었고, 이중 시스템은 지연 문제로 인해 복구 시점이 너무 늦는 경우가 많았습니다.

- **자연스러운 인간-로봇 상호작용 (Natural Human-Robot Interaction):**

  - **능력:** 적응형 특성과 명시적 추론 과정 덕분에 OneTwoVLA는 인간의 개입을 원활하게 처리하고 모호한 상황에서 적극적으로 설명을 요청하는 등 자연스러운 방식으로 인간과 상호작용합니다. (예: 추가 채소 요청 시 어떤 채소를 원하는지 확인 질문, 보드카 변경 요청 시 즉시 행동 조정).
  - **대비:** 이중 시스템은 상호작용 중 맥락을 잃고 일관된 추론을 유지하는 데 어려움을 겪었으며, $ \pi_0 $는 텍스트 추론 출력이 불가능하여 언어 기반 상호작용에 참여할 수 없었습니다.

- **향상된 시각적 그라운딩 (Enhanced Visual Grounding):**
  - **Single-Env 설정:** 78%의 성공률을 달성하여 $ \pi_0 $ (5%)를 크게 능가했습니다. 이는 명시적인 추론 학습이 모델이 시각적 세계를 진정으로 이해하고, 공간 관계, 객체 속성 및 의미적 특징을 정확하게 해석하도록 돕는다는 것을 보여줍니다.
  - **Open-World 설정:** OneTwoVLA-VL(로봇 데이터 및 합성 VL 데이터로 훈련)은 73%의 성공률을 기록하여 OneTwoVLA(로봇 데이터로만 훈련, 8%)와 $ \pi_0 $ (3%)를 크게 능가했습니다.
  - **일반화:** 로봇 데이터에는 없었지만 VL 데이터에 있던 객체는 물론, 두 데이터셋 모두에 없던 새로운 객체(예: Sprite, GoPro)에 대해서도 일반화 능력을 보였습니다. 이는 VL 데이터 공동 학습이 사전 훈련된 VLM에 인코딩된 웹 지식을 활성화하는 데 기여했기 때문입니다.

## 🧠 Insights & Discussion

OneTwoVLA는 단일 통합 모델로 추론과 행동을 유연하게 전환함으로써 로봇 제어의 새로운 패러다임을 제시했습니다. 특히, 로봇 데이터와 함께 합성된 추론 중심의 시각-언어 데이터를 공동 학습하는 전략은 모델의 일반화 능력과 다양한 작업 시나리오에 대한 이해도를 크게 향상시켰습니다. 이러한 접근 방식은 로봇이 단순히 행동을 모방하는 것을 넘어, 상황을 이해하고 계획하며 오류를 복구하고 인간과 상호작용하는 복잡한 지능을 갖추도록 돕습니다.

**제한 사항:**

- **수동 추론 주석:** OneTwoVLA의 추론 내용은 세심한 수동 주석에 기반하고 있어, 대규모 언어 모델 분야에서 사용되는 강화 학습 기반 추론 능력 향상 기법 [80–82]을 탐구할 필요가 있습니다.
- **동기식 추론 지연:** 적응형 프레임워크는 몇몇 중요한 단계에서만 추론하지만, 여전히 로봇이 추론 동안 2~3초간 멈춰야 합니다. 비동기 아키텍처를 통해 동시 추론 및 행동 생성을 가능하게 하는 연구가 필요합니다.
- **제한적인 시각-언어 데이터 소스:** 고품질 합성 시각-언어 데이터의 효과만 조사했으므로, 다양한 소스의 시각-언어 데이터가 VLA 추론 능력에 미치는 영향을 탐구할 수 있습니다.

## 📌 TL;DR

OneTwoVLA는 추론과 행동을 유연하게 전환하는 단일 통합 모델로, 기존 이중 시스템의 상호 이해 부족 및 지연 문제를 해결합니다. 자체 개발한 확장 가능한 파이프라인으로 신체화된 추론 중심의 시각-언어 데이터를 합성하고 로봇 데이터와 공동 학습하여, 장기적인 작업 계획, 오류 감지 및 복구, 자연스러운 인간-로봇 상호작용, 일반화 가능한 시각적 그라운딩에서 뛰어난 성능을 보였습니다.
