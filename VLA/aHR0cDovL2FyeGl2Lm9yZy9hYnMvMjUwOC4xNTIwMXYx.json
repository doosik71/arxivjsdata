{
  "url": "http://arxiv.org/abs/2508.15201v1",
  "title": "Survey of Vision-Language-Action Models for Embodied Manipulation",
  "authors": "Haoran Li, Yuhui Chen, Wenbo Cui, Weiheng Liu, Kai Liu, Mingcai Zhou, Zhengtao Zhang, Dongbin Zhao",
  "year": 2025,
  "abstract": "Embodied intelligence systems, which enhance agent capabilities through\ncontinuous environment interactions, have garnered significant attention from\nboth academia and industry. Vision-Language-Action models, inspired by\nadvancements in large foundation models, serve as universal robotic control\nframeworks that substantially improve agent-environment interaction\ncapabilities in embodied intelligence systems. This expansion has broadened\napplication scenarios for embodied AI robots. This survey comprehensively\nreviews VLA models for embodied manipulation. Firstly, it chronicles the\ndevelopmental trajectory of VLA architectures. Subsequently, we conduct a\ndetailed analysis of current research across 5 critical dimensions: VLA model\nstructures, training datasets, pre-training methods, post-training methods, and\nmodel evaluation. Finally, we synthesize key challenges in VLA development and\nreal-world deployment, while outlining promising future research directions.",
  "citation": 0
}