{
  "url": "http://arxiv.org/abs/2505.11214v1",
  "title": "Unveiling the Potential of Vision-Language-Action Models with Open-Ended\n  Multimodal Instructions",
  "authors": "Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang",
  "year": 2025,
  "abstract": "Vision-Language-Action (VLA) models have recently become highly prominent in\nthe field of robotics. Leveraging vision-language foundation models trained on\nlarge-scale internet data, the VLA model can generate robotic actions directly\nfrom visual observations and human instructions through a single end-to-end\nneural network. Despite their effectiveness, current VLA models usually accept\nonly one form of human prompting, language instructions, which may constrain\ntheir applicability in open-ended human-robot interactions. For example, a user\nmight expect the robot to retrieve an object shown in an image, follow an\ninstruction written on the whiteboard, or imitate a behavior demonstrated in a\nvideo, rather than relying solely on language-based descriptions. To address\nthis gap, we introduce OE-VLA, which explores the potential of VLA models for\nopen-ended multimodal instructions. Extensive results demonstrate that our\nOE-VLA not only achieves comparable performance to traditional VLA models with\nlinguistic input but also delivers impressive results across four additional\ncategories of open-ended tasks. The proposed methodology could significantly\nexpand the applications of VLA models across various everyday scenarios and\nfacilitate human-robot interaction.",
  "citation": 4
}