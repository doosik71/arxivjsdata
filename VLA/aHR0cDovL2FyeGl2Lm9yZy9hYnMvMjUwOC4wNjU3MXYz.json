{
  "url": "http://arxiv.org/abs/2508.06571v3",
  "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World\n  Model",
  "authors": "Anqing Jiang, Yu Gao, Yiru Wang, Zhigang Sun, Shuo Wang, Yuwen Heng, Hao Sun, Shichen Tang, Lijuan Zhu, Jinhao Chai, Jijun Wang, Zichong Gu, Hao Jiang, Li Sun",
  "year": 2025,
  "abstract": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous\ndriving. However, two critical challenges hinder their development: (1)\nExisting VLA architectures are typically based on imitation learning in\nopen-loop setup which tends to capture the recorded behaviors in the dataset,\nleading to suboptimal and constrained performance, (2) Close-loop training\nrelies heavily on high-fidelity sensor simulation, where domain gaps and\ncomputational inefficiencies pose significant barriers. In this paper, we\nintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\n\\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model\nwith a self-built VLA approach. Our framework proceeds in a three-stage\nparadigm: In the first stage, we propose a VLA architecture and pretrain the\nVLA policy via imitation learning. In the second stage, we construct a\nlightweight reward world model via inverse reinforcement learning to enable\nefficient close-loop reward computation. To further enhance planning\nperformance, finally, we design specialized reward world model guidence\nreinforcement learning via PPO(Proximal Policy Optimization) to effectively\nbalance the safety incidents, comfortable driving, and traffic efficiency. Our\napproach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving\nbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that\nour framework will accelerate VLA research in close-loop autonomous driving.",
  "citation": 0
}