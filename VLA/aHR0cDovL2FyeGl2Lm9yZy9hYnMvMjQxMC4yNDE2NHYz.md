# $\pi_0$: A Vision-Language-Action Flow Model for General Robot Control

Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky

## 🧩 Problem to Solve

로봇 학습은 유연하고 범용적이며 정교한 로봇 시스템의 잠재력을 발휘하고 인공지능의 깊은 질문을 해결할 큰 가능성을 가지고 있습니다. 그러나 로봇 학습이 실제 세계 시스템에 효과적으로 적용될 수 있는 수준의 일반성을 달성하기 위해서는 데이터 부족, 일반화 능력 및 강건성 측면에서 주요 장애물에 직면해 있습니다. 이 논문은 복잡하고 고도로 정교한 작업을 위한 효과적인 범용 로봇 정책(즉, 로봇 파운데이션 모델)을 설계하고 이러한 문제를 해결하는 방법을 제시하고자 합니다.

## ✨ Key Contributions

- 인터넷 규모의 의미론적 지식을 활용하기 위해 사전 훈련된 비전-언어 모델(VLM) 백본 위에 구축된 새로운 흐름 매칭(flow matching) 아키텍처를 제안했습니다.
- 단일 팔, 이중 팔, 모바일 매니퓰레이터 등 다양한 로봇 플랫폼에서 수집된 대규모의 다양한 데이터셋에 모델을 훈련시키는 방법을 제시했습니다.
- 직접 프롬프트, 언어 지시 따르기, 미세 조정을 통한 새로운 기술 습득 능력 측면에서 모델을 평가했습니다.
- 세탁물 접기, 테이블 정리, 상자 조립 등 다양한 작업을 통해 이전에 시연된 로봇 파운데이션 모델보다 훨씬 높은 수준의 정교함과 일반화를 입증했습니다.
- 대규모 사전 훈련 접근 방식과 미세 조정 전략을 포함하는 효과적인 훈련 레시피를 개발하고 그 효과를 실증적으로 조사했습니다.

## 📎 Related Works

- **Vision-Language-Action (VLA) 모델:** 사전 훈련된 VLM을 로봇 제어에 미세 조정하는 RT-2 [7], OpenVLA [24]와 같은 VLA 모델과 밀접하게 관련되어 있습니다. 그러나 기존 VLA 모델들이 주로 자동 회귀 이산화(autoregressive discretization)를 사용하여 액션을 표현하는 반면, 본 연구는 흐름 매칭 [32, 28]을 통해 고주파 액션 청크(high-frequency action chunks)를 처리합니다.
- **액션 생성을 위한 확산 모델 (Diffusion Models for Action Generation):** 액션 생성을 위해 확산 모델을 사용한 최근 연구들 [9, 60]과 유사하나, $\pi_0$는 사전 훈련된 VLM 백본 [5]을 사용한다는 차이점이 있습니다.
- **언어 모델과 확산 모델의 결합 (Combining Language Models with Diffusion):** 이미지 생성 분야에서 언어 모델과 확산 모델을 결합한 연구 [40, 41, 14] 및 확산과 자동 회귀 LLM을 하이브리드화한 연구 [19, 29, 59]의 개념을 활용하여, 연속적인 출력과 이산적인 출력을 하나의 트랜스포머로 훈련하는 방식을 따랐습니다.
- **대규모 로봇 학습 (Large-Scale Robot Learning):** 자가 지도 학습 [26, 22, 8] 및 다양한 고품질 로봇 제어 데이터셋 [23, 10, 52, 33, 34, 43, 13, 6]에 대한 이전 연구를 기반으로 합니다. 본 연구는 약 10,000시간의 로봇 데이터를 활용하여 이전 연구보다 훨씬 큰 규모의 데이터셋으로 복잡하고 정교한 행동을 연구합니다.

## 🛠️ Methodology

$\pi_0$ 모델은 사전 훈련된 **PaliGemma 비전-언어 모델 (VLM)**을 백본으로 사용하며, 로봇 제어에 특화된 요소를 추가하여 **비전-언어-행동 (VLA) 모델**로 확장됩니다.

1. **$\pi_0$ 모델 아키텍처:**

   - **VLM 백본 활용:** PaliGemma [5]의 언어 모델 트랜스포머 백본을 활용하여 이미지 관측값과 언어 토큰을 동일한 임베딩 공간에 임베딩합니다.
   - **로봇 고유 입력/출력:** 로봇의 고유 상태(proprioceptive state) $q_t$와 로봇 액션 $A_t = [a_t, a_{t+1}, \dots, a_{t+H-1}]$를 추가합니다. 여기서 $H=50$은 액션 청크의 길이를 나타냅니다.
   - **흐름 매칭 (Flow Matching) 기반 액션 생성:** 연속적인 액션 분포를 모델링하기 위해 조건부 흐름 매칭 [28, 32]을 사용합니다. 이는 모델이 높은 정밀도와 다중 모달 모델링 기능을 갖추게 하여 고주파수 정교한 작업에 적합합니다.
   - **액션 전문가 (Action Expert):** 이미지와 텍스트 입력에는 VLM 백본 가중치를, $q_t$와 노이즈가 추가된 액션 $A_t^\tau$와 같은 로봇 고유 입력/출력에는 별도의 가중치 세트(약 300M 파라미터)를 사용합니다. 이는 두 가지 요소가 결합된 "mixture of experts"와 유사한 디자인입니다.
   - **어텐션 마스크:** 입력 토큰을 3개의 블록(VLM 입력, 로봇 상태, 액션 토큰)으로 나누어, 각 블록 내에서는 완전한 양방향 어텐션을 허용하지만, 미래 블록의 토큰에는 어텐션할 수 없도록 블록별 인과적(causal) 어텐션 마스크를 사용합니다.
   - **흐름 매칭 타임스텝 샘플링:** 흐름 매칭 타임스텝 $\tau \in [0,1]$ 값은 낮은 타임스텝(더 높은 노이즈 수준)을 강조하는 이동된 베타 분포 $p(\tau) = \text{Beta}(\frac{s-\tau}{s}; 1.5, 1)$에서 샘플링됩니다.

2. **훈련 레시피 (Pre-training 및 Post-training):**

   - **사전 훈련 (Pre-training):**
     - **목표:** 광범위한 기능을 습득하고 일반화 능력을 구축하는 기본 모델을 훈련합니다.
     - **데이터:** 약 10,000시간 이상의 로봇 데이터(10억 개 이상의 스텝)를 포함하는 대규모의 다양한 데이터셋을 사용합니다. 여기에는 Physical Intelligence 자체의 정교한 조작 데이터셋(7가지 로봇 구성, 68가지 작업)과 공개 소스 OXE [10], Bridge v2 [52], DROID [23] 데이터셋이 포함됩니다.
     - **언어 라벨:** 작업 이름과 세그먼트 주석(미세한 하위 궤적 라벨)을 모두 활용하여 다양한 언어 라벨을 사용합니다.
   - **사후 훈련 (Post-training) / 미세 조정 (Fine-tuning):**
     - **목표:** 특정 하위 작업에 모델을 전문화시켜 능숙하고 유창하게 작업을 실행하도록 만듭니다.
     - **데이터:** 더 작고 고품질이며 신중하게 선별된 작업별 데이터셋을 사용합니다.
   - **접근 방식의 직관:** 다양하지만 품질이 낮은 사전 훈련 데이터는 모델이 실수로부터 복구하고 다양한 상황을 처리하는 방법을 가르치고, 고품질의 사후 훈련 데이터는 모델이 작업을 효율적이고 강건하게 수행하도록 가르칩니다.

3. **언어 및 고수준 정책 활용:**
   - "테이블 정리"와 같은 복잡한 작업의 경우, 고수준 작업을 "냅킨 집어 쓰레기통에 버리기"와 같은 세부 하위 작업으로 분해하는 고수준 VLM 정책을 활용하여 로봇의 전략적 추론을 돕습니다.

## 📊 Results

1. **즉시 사용 (Out-of-box) 평가:**

   - $\pi_0$는 사전 훈련 후 미세 조정 없이도 셔츠 접기, 쉬운/어려운 테이블 정리, 식료품 포장, 토스트 꺼내기 등 다양한 작업에서 OpenVLA, Octo, $\pi_0$-small과 같은 모든 기준 모델을 크게 능가했습니다.
   - 특히, $\pi_0$의 "컴퓨팅 패리티(compute parity)" 버전(더 적은 훈련 스텝)조차 모든 기준 모델을 앞섰으며, 이는 대규모 VLM 사전 훈련과 흐름 매칭 아키텍처의 중요성을 입증합니다.

2. **언어 명령 따르기:**

   - 미세 조정된 $\pi_0$ 모델은 $\pi_0$-small(VLM 사전 훈련이 없는 소형 모델)보다 언어 명령을 따르는 능력에서 현저히 우수했습니다.
   - 인간 전문가의 중간 명령($\pi_0$-human) 또는 고수준 VLM 정책($\pi_0$-HL)의 안내를 받았을 때 성능이 크게 향상되었으며, 이는 VLM 사전 훈련이 언어 이해 능력을 크게 향상시킨다는 것을 시사합니다.

3. **새로운 정교한 작업 학습 (미세 조정):**

   - 그릇 쌓기, 수건 접기, 전자레인지에 밀폐용기 넣기, 휴지 교체, 서랍에 물건 넣기 등 사전 훈련 데이터와는 다른 새로운 작업에 미세 조정을 적용했을 때, $\pi_0$는 일반적으로 ACT, Diffusion Policy, OpenVLA, Octo를 능가했습니다.
   - 사전 훈련된 모델은 처음부터 훈련된 모델보다 더 나은 성능을 보였으며, 특히 사전 훈련 데이터와 유사성이 높은 작업에서 큰 개선을 보였습니다. 이는 사전 훈련이 새로운 작업 학습에 강력한 시작점을 제공함을 보여줍니다.

4. **복잡한 다단계 작업 마스터하기:**
   - 세탁물 접기(정지형/모바일 로봇), 건조기 비우기, 테이블 정리, 상자 조립, 즉석 식품 상자 포장, 달걀 포장 등 수십 개의 개별 행동과 복잡한 물리적 속성, 다양한 객체 구성에 대한 일반화가 필요한 매우 어렵고 시간적으로 긴 다단계 작업에서 $\pi_0$는 높은 성공률을 달성했습니다.
   - 전체 사전 훈련 및 미세 조정 레시피를 적용한 $\pi_0$가 다른 변형(사전 훈련만, 처음부터 훈련)보다 일관되게 가장 좋은 성능을 보였습니다. 이는 로봇 조작 학습 정책에서 이러한 도전적인 작업에 대한 새로운 최첨단 성능을 나타냅니다.

## 🧠 Insights & Discussion

- $\pi_0$는 대규모 사전 훈련과 복잡한 작업에 대한 미세 조정을 통해 정교함, 일반화 및 시간적으로 확장된 다단계 행동을 결합한 작업에서 뛰어난 성능을 보여주는 로봇 파운데이션 모델을 위한 프레임워크를 제시합니다.
- 인터넷 규모의 VLM 사전 훈련과 흐름 매칭을 통한 고주파 액션 청크 표현의 결합이 핵심 성공 요인입니다. 이는 대규모 언어 모델(LLM)이 사전 훈련에서 "지식"을 습득하고, 사후 훈련에서 사용자의 명령에 "정렬"되는 것과 유사한 현상이 로봇 파운데이션 모델에서도 발생할 수 있음을 시사합니다.
- **한계점:** 사전 훈련 데이터셋 구성(어떤 데이터가 더 유용한지, 가중치는 어떻게 부여해야 하는지)에 대한 포괄적인 이해가 부족합니다. 또한, 특정 작업에서 거의 완벽한 성능을 달성하는 데 필요한 데이터 양과 종류를 예측하는 것이 어렵습니다. 마지막으로, 다양한 작업과 로봇 간의 긍정적인 전이(positive transfer)가 자율 주행, 내비게이션, 보행 등 완전히 다른 영역으로 얼마나 확장될 수 있는지는 미래 연구 과제입니다.

## 📌 TL;DR

$\pi_0$는 사전 훈련된 비전-언어 모델(VLM) 백본과 흐름 매칭(flow matching) 기반 액션 전문가(action expert)를 결합한 새로운 범용 로봇 제어 정책입니다. 약 10,000시간의 방대한 크로스-엔바디먼트(cross-embodiment) 데이터로 사전 훈련되고 고품질 데이터로 미세 조정되어, 셔츠 접기, 테이블 정리, 상자 조립, 세탁물 접기 등 광범위하고 정교하며 다단계적인 로봇 작업을 탁월하게 수행하며, 기존 로봇 학습 모델 대비 현저히 개선된 성능을 보였습니다. 이는 대규모 VLM 사전 훈련과 흐름 매칭을 통한 액션 모델링이 범용 로봇 파운데이션 모델 구현에 핵심적인 역할을 할 수 있음을 시사합니다.
