{
  "title": "Accelerating Vision-Language-Action Model Integrated with Action\n  Chunking via Parallel Decoding",
  "authors": "Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, Haoang Li",
  "year": 2025,
  "url": "http://arxiv.org/abs/2503.02310v1",
  "abstract": "Vision-Language-Action (VLA) models demonstrate remarkable potential for\ngeneralizable robotic manipulation. The performance of VLA models can be\nimproved by integrating with action chunking, a critical technique for\neffective control. However, action chunking linearly scales up action\ndimensions in VLA models with increased chunking sizes. This reduces the\ninference efficiency. To tackle this problem, we propose PD-VLA, the first\nparallel decoding framework for VLA models integrated with action chunking. Our\nframework reformulates autoregressive decoding as a nonlinear system solved by\nparallel fixed-point iterations. This approach preserves model performance with\nmathematical guarantees while significantly improving decoding speed. In\naddition, it enables training-free acceleration without architectural changes,\nas well as seamless synergy with existing acceleration techniques. Extensive\nsimulations validate that our PD-VLA maintains competitive success rates while\nachieving 2.52 times execution frequency on manipulators (with 7 degrees of\nfreedom) compared with the fundamental VLA model. Furthermore, we\nexperimentally identify the most effective settings for acceleration. Finally,\nreal-world experiments validate its high applicability across different tasks."
}