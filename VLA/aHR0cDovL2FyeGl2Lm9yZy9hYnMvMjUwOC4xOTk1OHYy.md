# Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation

Yiguo Fan, Pengxiang Ding, Shuanghao Bai, Xinyang Tong, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, Zhaoxin Fan, Badong Chen, Donglin Wang

## 🧩 Problem to Solve

기존 Vision-Language-Action (VLA) 모델은 주로 단기(short-horizon) 로봇 조작 작업에 초점을 맞춰왔습니다. 하지만 장기(long-horizon), 다단계 로봇 조작 작업에서는 "스킬 체이닝(skill chaining)" 문제와 하위 작업 간의 의존성으로 인해 그 효과가 제한적입니다. 스킬 체이닝은 하위 작업 경계에서 동적 결합 및 오류 전파를 야기하여 전체 작업 성능을 크게 저하시킵니다. 온라인 적응형 최적화나 모듈형 아키텍처와 같은 기존의 해결책은 VLA 모델의 확장 가능하고 종단 간(end-to-end) 훈련 패러다임과 호환되지 않는다는 근본적인 문제가 있습니다.

## ✨ Key Contributions

- **Long-VLA 제안:** 장기 로봇 작업을 위해 특별히 설계된 최초의 종단 간 VLA 모델인 Long-VLA를 소개합니다.
- **위상 인식 입력 마스킹 전략:** 각 하위 작업을 이동(moving) 및 상호작용(interaction) 단계로 적응적으로 분할하고, 각 단계에 맞는 관련 시각적 단서에 모델이 집중하도록 하는 새로운 위상 인식(phase-aware) 입력 마스킹 전략을 제안합니다. 이는 하위 작업 호환성을 향상시킵니다.
- **VLA 훈련 패러다임 유지:** 이 통합 전략은 VLA 훈련의 확장성과 데이터 효율성을 유지합니다.
- **아키텍처 불가지론적 모듈:** 기존 VLA 모델에 원활하게 통합될 수 있는 아키텍처 불가지론적 모듈을 제공합니다.
- **L-CALVIN 벤치마크 개발:** 장기 조작 작업을 체계적으로 평가하기 위한 L-CALVIN 벤치마크를 제안합니다.
- **SOTA 성능 달성:** 시뮬레이션 및 실제 작업에서 기존 최첨단(SOTA) 방법보다 훨씬 뛰어난 성능을 달성하여 장기 제어를 위한 새로운 기준을 제시합니다.

## 📎 Related Works

- **Vision-Language-Action (VLA) 모델:** 시각, 언어, 로봇 행동 생성을 통합하여 자율 제어를 가능하게 하는 유망한 패러다임으로, 주로 대규모 로봇 학습 데이터셋의 사전 훈련으로 발전했습니다. 그러나 대부분 단기 작업에 최적화되어 있습니다 ([25, 26, 27, ..., 36]).
- **장기 로봇 조작(Long-horizon Robot Manipulation):** 일반적으로 복잡한 작업을 하위 작업으로 분해하고 각 하위 목표에 대해 별도의 지역 정책을 최적화하는 분해(decomposition) 방식을 사용합니다 ([41, 42, 43, ..., 49]). 하지만 하위 작업 간의 전환 및 의존성을 명시적으로 모델링하지 않아 스킬 체이닝 문제에 직면합니다 ([18, 50]).
- **스킬 체이닝 문제 해결 방안:**
  - **온라인 적응형 최적화(Online adaptive optimization):** 실행 중 실시간 교정 또는 보상 조정을 통해 오류 전파를 해결합니다 ([20, 21, 22, 19]).
  - **훈련-테스트 간극 최소화(Minimizing the train-test gap):** 훈련과 배포 간의 불일치를 줄여 견고성을 향상시킵니다 (예: Plan-Seq-Learn [3]은 다른 입력 양식을 사용하여 모션 계획과 실행을 분리).
- **VLA 모델에서의 장기 로봇 조작:** Dex-VLA [17] 및 Gato [16]와 같은 최근 접근 방식은 LLM을 활용하여 VLA 프레임워크에 작업 분해를 도입했지만, 스킬 체이닝 문제를 충분히 고려하지 못했습니다. 보상 기반 온라인 방법은 VLA의 오프라인 훈련 패러다임과 비호환적이며, 모듈형 아키텍처는 종단 간 훈련을 방해합니다.

## 🛠️ Methodology

### 3.1 Revisiting Decomposition Strategy

- **위상 수준 분해(Phase-level decomposition):** 각 하위 작업을 `이동(moving)` 단계와 `상호작용(interaction)` 단계로 세분화합니다. 기존 연구 [3]는 정확한 3D 목표가 필요한 역운동학(IK)을 사용했지만, 본 연구에서는 전용 이동 정책을 훈련하여 IK를 대체합니다.
- **분해된 데이터 수집(Decomposed Data Collection):** CALVIN 데이터셋 [40]에서 L-CALVIN 데이터셋을 구성하여 각 작업을 이동 및 상호작용 단계로 분할합니다. 이동 단계에는 제3자 시점, 상호작용 단계에는 그리퍼 시점을 활용합니다.
- **분해 전략의 성능(Performance with Decomposition Strategy):** MDT [52]에 별도의 이동 정책을 결합한 결과 성능이 향상되었지만, 두 개의 별도 모델 훈련은 확장 가능한 장기 학습에 최적화되지 않습니다. 이를 해결하기 위해 Long-VLA는 단계별 데이터를 더 효과적으로 활용하는 통일된(unified) 종단 간 VLA 모델을 제안합니다.

### 3.2 Long-VLA

#### 3.2.1 Training Paradigm

- **데이터 및 위상 분해(Data and Phase Decomposition):** 각 언어 주석이 달린 궤적은 이동 단계 ($M$)와 상호작용 단계 ($I$)로 분해됩니다. 액션 표현은 현재 단계를 나타내는 1차원 위상 식별자 $s_p$ (이동 시 $-1$, 상호작용 시 $1$)를 추가하여 확장됩니다. 최종 액션 토큰은 $[x, y, z, eu_x, eu_y, eu_z, s_g, s_p]$로 표현됩니다.
- **마스킹을 통한 입력 수준 적응 전략(Input-level Adaptation Strategy via Masking):**
  - 이동 단계에서는 제3자 카메라 시점을 사용하여 정확한 객체 탐색에 집중하고, 상호작용 단계에서는 시각적 분포 변화를 완화하고 정확한 조작을 위해 그리퍼 카메라에 집중하도록 합니다.
  - 완전한 양식을 제거하는 대신 마스킹 전략을 사용하여 시각적 입력을 동적으로 조정합니다.
  - 각 토큰에 이진 마스크 $m \in \{0, 1\}$을 할당하여 해당 토큰이 어텐션 계산에 참여할지 여부를 결정합니다.
  - 마스킹된 어텐션 가중치 $A_{ij}$는 다음과 같이 계산됩니다:
    $$A_{ij} = \frac{\exp(P_{ij})M_{ij}}{\sum_k \exp(P_{ik})M_{ik}}$$
    여기서 $P$는 쿼리-키 유사도 행렬이고, $M$은 마스크 행렬입니다.
- **훈련 손실(Training Loss):** 액션 생성을 위해 조건부 확산 모델(conditional diffusion model)을 사용합니다. 분해된 데이터셋으로 이동 및 상호작용 단계를 공동으로 감독하는 단일 스코어 매칭 손실 $\mathcal{L}_{policy}$로 훈련됩니다. 시각적 목표와 언어 지침의 의미적 일관성을 위해 InfoNCE 손실 $\mathcal{L}_{goal}$을 사용합니다. 총 훈련 손실은 $\mathcal{L} = \mathcal{L}_{policy} + \alpha \mathcal{L}_{goal}$입니다.

#### 3.2.2 Model Architecture

- **Observation Encoder:** 그리퍼 카메라($s_g^t$) 및 정적 카메라($s_s^t$) 시점은 학습 가능한 ResNet-18 [53] 인코더를 사용하여 각각 $e_g$ 및 $e_s$로 임베딩됩니다.
- **Goal Encoder:** 미래 관측 $s^{t'}$ 또는 언어 주석이 목표로 사용되며, 고정된 CLIP 모델 [54]의 텍스트 및 이미지 인코더로 인코딩됩니다.
- **Detection Integration:** Grounding DINO [55]를 LoRA [56]로 미세 조정하여 언어 쿼리에 기반한 제3자 이미지에서 픽셀 수준 바운딩 박스를 예측합니다. 이러한 바운딩 박스는 학습 가능한 위치 인코더를 통해 잠재 공간으로 투영되어 탐지 특징 $e_d$를 얻습니다. 이 $e_d$를 FiLM 전략 [57]을 통해 정적 카메라 특징 $e_s$와 결합하여 탐지 강화 표현 $e_s'$를 만듭니다.
- **Multimodal Encoder:** GPT-2 스타일의 Transformer 아키텍처를 기반으로 합니다. 입력 $e_{pre}$는 모든 양식 특징 $[e_s', e_g, e_{goal}, e_d]$를 연결합니다.
- **Action Decoder:** 조건부 확산 모델을 사용하여 가우시안 노이즈에서 액션 $a_t$를 점진적으로 디노이징합니다. DDIM 샘플링으로 역과정을 구현하며, 디코딩된 출력은 2계층 MLP를 통해 액션 벡터로 매핑됩니다.

## 📊 Results

- **L-CALVIN 벤치마크 평가:** CALVIN의 작업 시퀀스를 5단계에서 10단계로 확장한 새로운 L-CALVIN 벤치마크를 제안합니다.
- **시뮬레이션 및 실제 환경에서의 성능 (Long-VLA vs. Base Policy):**
  - **L-CALVIN 시뮬레이션 (Figure 4):** Long-VLA는 MDT [52] 기반 정책 대비 상당한 성능 향상을 보이며, 특히 작업 기간이 길어질수록, 데이터가 제한될수록 개선 효과가 더 큽니다.
  - **실제 Sorting 작업 (Figure 5):** 기본 정책은 7번째 작업 이후 성공률이 0%로 떨어지는 반면, Long-VLA는 8개 모든 작업에서 약 25%의 성공률을 달성하며, 작업 시퀀스 길이가 길어질수록 상대적 성능 향상이 두드러집니다.
  - **실제 Cleaning 작업 (Figure 6):** 더 다양한 행동과 시각적 복잡성을 가진 청소 작업에서 Long-VLA는 모든 시간 범위에서 기본 정책보다 훨씬 더 큰 개선을 보였습니다. 이는 Long-VLA의 장기 적응 패러다임이 시각적 방해 요소와 복잡한 환경에 효과적임을 시사합니다.
- **최첨단(SOTA) 방법과의 비교 (Table 2, Figure 7):** Long-VLA는 GR-1 [58], RoboVLMs [60], VLAS [31]와 같은 비디오 생성 기반 및 VLM 기반 VLA 모델뿐만 아니라 Gato [16]를 포함한 기존 SOTA 방법들을 시뮬레이션 및 실제 실험에서 일관되게 능가합니다.
- **어블레이션 분석 (Table 3):**
  - **분해 전략(Decomposition Strategy):** 기본 정책에 분해 전략을 도입하면 상당한 성능 향상이 있습니다.
  - **입력 수준 적응(Input-level Adaptation):** 입력 수준 적응은 일반화를 향상시키지만, 통합 훈련을 방해할 수 있습니다.
  - **통합 모델(Unified Model) + 마스킹:** 마스킹 메커니즘을 도입한 Long-VLA는 최상의 성능을 달성합니다. 이는 입력 수준 적응의 시각적 일반화 능력과 VLA 모델 내의 공동 훈련을 결합하여 데이터 기반 이점을 활용하기 때문입니다.
- **Long-VLA 패러다임의 확장성 (Table 4):** Long-VLA는 MDT [52] 및 HULC [62]와 같은 다양한 백본 아키텍처에서 일관되게 강력한 성능을 보이며, 그 효과성과 아키텍처 불가지론적 설계가 입증되었습니다.

## 🧠 Insights & Discussion

Long-VLA는 위상 인식 입력 적응 전략을 통해 스킬 체이닝 문제를 효과적으로 해결하여 장기 조작을 위한 통일되고 종단 간 접근 방식을 제시함으로써 일반 로봇 공학 분야를 발전시켰습니다. 이동 및 상호작용 단계별로 맞춤형 마스킹을 적용하여 분포 변화를 완화하고 하위 작업 호환성을 향상시켜 복잡한 작업에서 견고한 성능을 가능하게 합니다. L-CALVIN 벤치마크 및 실제 시나리오에 대한 광범위한 평가는 Long-VLA가 기존 SOTA 방법을 능가할 뿐만 아니라 현재 VLA 모델에 쉽게 통합될 수 있는 확장 가능하고 아키텍처 불가지론적인 솔루션임을 입증했습니다.

**제한 사항:**

- 단계별 훈련 데이터셋 분해가 여전히 수동 작업에 의존하며, VLM의 도움으로 자동화 가능성이 있습니다.
- 고려된 장기 작업의 범위가 여전히 제한적입니다. 초기 상태 간극을 줄일 수는 있지만, 더 긴 시퀀스에서 실패 사례를 처리하기 위한 추가 개선이 필요합니다.
- 테스트된 시퀀스 길이가 제약되어 있습니다.
- 미래에는 모델 프레임워크의 이주 호환성(migratory compatibility)을 다양화하는 것을 목표로 합니다.

## 📌 TL;DR

Long-VLA는 장기 로봇 조작 작업을 위해 특별히 고안된 최초의 종단 간 Vision-Language-Action (VLA) 모델입니다. 이 모델은 각 하위 작업을 이동 및 상호작용 단계로 분할하고, 각 단계에 최적화된 시각적 입력(예: 이동 시 제3자 시점, 상호작용 시 그리퍼 시점)에 집중하도록 하는 새로운 '위상 인식 입력 마스킹 전략'을 도입하여 스킬 체이닝 문제를 효과적으로 해결합니다. L-CALVIN 벤치마크 및 실제 환경 실험에서 Long-VLA는 기존 최첨단(SOTA) 방법보다 뛰어난 성능을 보이며, VLA 모델의 확장성과 데이터 효율성을 유지하면서 복잡하고 장기적인 로봇 작업을 처리하는 데 있어 강력하고 아키텍처 불가지론적인 솔루션을 제공합니다.
