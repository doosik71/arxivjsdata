# Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions

Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding

## 🧩 Problem to Solve

기존의 Vision-Language-Action (VLA) 모델은 로봇 관측 및 텍스트 기반 명령에만 국한되어 있습니다. 이들은 디지털 세계의 파운데이션 모델에서 가능한 이미지-텍스트 혼합(interleaved) 명령의 유연성이 부족합니다. 특정 형태나 색상의 물체를 "이것처럼 생긴 물체를 집어라"와 같이 구두로만 지시하는 것은 모호하거나 번거로울 수 있습니다. 이러한 혼합 모달리티 명령은 디지털 파운데이션 모델에서 효과가 입증되었음에도 불구하고, 로봇의 저수준 동작(low-level actions)을 위한 실제 데이터셋과 정책의 부족으로 인해 그 실용적 가치가 충분히 탐구되지 못했습니다.

## ✨ Key Contributions

- **Interleave-VLA 프레임워크 제안:** 물리적 세계에서 이미지-텍스트 혼합 명령을 이해하고 연속적인 동작 시퀀스를 직접 생성할 수 있는 최초의 프레임워크입니다. 이는 기존 VLA 모델을 최소한의 변경으로 확장하며, 강력한 제로샷(zero-shot) 일반화 능력을 제공하는 유연하고 모델에 구애받지 않는(model-agnostic) 패러다임입니다.
- **Open Interleaved X-Embodiment 데이터셋 구축:** Open X-Embodiment의 기존 텍스트 전용 데이터셋을 이미지-텍스트 혼합 명령으로 자동 변환하는 파이프라인을 개발하여 21만 에피소드, 1,300만 프레임 규모의 최초 대규모 실제 혼합 로봇 조작 데이터셋을 생성했습니다.
- **향상된 일반화 및 제로샷 능력 입증:** 시뮬레이션 및 실제 로봇 실험을 통해 Interleave-VLA가 미지의 객체에 대한 도메인 외(out-of-domain) 일반화 성능을 2~3배 향상시키며, 손으로 그린 스케치와 같은 다양한 사용자 제공 이미지 명령을 제로샷 방식으로 처리하는 능력을 보여주었습니다.
- **제로샷 성능 요인 분석:** 이질적인 데이터셋과 인터넷 이미지를 포함한 다양한 명령 이미지를 활용하는 혼합 패러다임이 강력한 제로샷 성능을 가능하게 하며, 확장 가능성이 크다는 것을 분석했습니다.

## 📎 Related Works

- **Interleaved Vision-Language Models (디지털 도메인):** 이미지-텍스트 쌍에서 임의로 혼합된 이미지 및 텍스트 시퀀스를 처리하는 모델로 발전했습니다(예: Flamingo, Qwen2.5-VL). 이는 대규모 멀티모달 웹 코퍼스를 활용하여 유연성과 일반화 능력을 향상시켰습니다.
- **Vision Language Action Models (로봇 공학):** 대부분의 기존 VLA 모델(예: RT-1, OpenVLA, $\pi^0$)은 텍스트 전용 명령을 사용합니다. VIMA는 시뮬레이션에서 2D 객체 포즈 추정을 위한 혼합 명령 사용을 개척했지만, 일반화 또는 실제 적용 가능성과 같은 광범위한 이점은 탐구하지 못했습니다. 본 연구는 이 간극을 메우기 위해 Interleave-VLA를 제안합니다.

## 🛠️ Methodology

- **문제 정의:** 모델은 관측 $o_t = (I_t, I, q)$를 입력으로 받아 로봇의 행동 공간에서 행동 $A_t$를 출력합니다. 여기서 $I_t$는 관측 이미지, $q$는 로봇의 고유 상태(proprioceptive state), $I$는 텍스트 세그먼트 $l_i$와 이미지 $I_i$가 혼합된 시퀀스 $I = (l_1, I_1, l_2, I_2, ...)$ 형태의 혼합 명령입니다.
- **Interleave-VLA 아키텍처:**
  - 기존 VLA 모델의 핵심 아키텍처를 변경하지 않고 입력 형식을 혼합 이미지 및 텍스트 토큰을 받아들이도록 수정하는 모델 불가지론적(model-agnostic) 접근 방식입니다.
  - **OpenVLA [11]**의 경우, Prismatic [31] VLM 백본을 혼합 입력 처리를 기본적으로 지원하는 InternVL2.5 [24]로 교체했습니다.
  - **$\pi^0$ [13]**의 경우, 원래 아키텍처를 유지하고 입력 파이프라인만 혼합 토큰을 처리하도록 조정했습니다. 기본 Paligemma [32] VLM이 혼합 데이터로 훈련되지 않았음에도 효과적인 처리가 가능함을 보여줍니다.
- **Open Interleaved X-Embodiment 데이터셋 구축 파이프라인:**
  1. **명령 구문 분석 (Instruction Parsing):** Qwen2.5 [33]를 사용하여 언어 명령에서 핵심 객체를 추출합니다.
  2. **개방형 어휘 객체 감지 (Open-vocabulary Detection):** OWLv2 [36]를 사용하여 구문 분석된 키워드를 기반으로 궤적 프레임에서 대상 객체를 찾아 자릅니다. 대부분의 경우 99% 이상의 정확도를 달성합니다.
  3. **데이터 품질 검증 (Data Quality Verification):** OWLv2가 실패하는 경우, Qwen2.5-VL [5]이 감지된 객체를 검증하고, 필요한 경우 Segment Anything [37]을 사용하여 보다 정확한 분할을 위한 키포인트를 제공하여 어려운 객체(예: 가지)에 대한 자르기 정확도를 50% 미만에서 95%로 높입니다.
  - 이 파이프라인을 Open X-Embodiment의 11개 데이터셋에 적용하여 21만 에피소드, 1,300만 프레임의 대규모 실제 혼합 데이터셋을 생성했습니다.

## 📊 Results

- **시뮬레이션 성능 (SIMPLER, VIMA-Bench):**
  - **SIMPLER:** Interleave-VLA는 텍스트 전용 baseline 대비 도메인 내(in-domain) 작업에서 성능을 향상시키고, 의미적으로 도메인 외(out-of-domain) 작업에서 2~3배 더 강력한 일반화 성능을 보였습니다. Open Interleaved X-Embodiment 데이터셋으로 공동 훈련(co-training)하면 의미론적 일반화가 더욱 향상됩니다.
  - **VIMA-Bench:** Interleave-VLA는 원래 OpenVLA보다 모든 일반화 수준에서 평균 2배 이상 높은 성능을 달성했습니다. 또한, 스케치 기반 명령을 처리하는 유연성을 입증했습니다.
- **실제 로봇 성능:**
  - Interleave-VLA는 텍스트 전용 $\pi^0$ 대비 도메인 외(out-of-domain) 성능을 2~3배 향상시켰습니다.
  - 소규모 데이터 환경에서 Open Interleaved X-Embodiment 데이터셋으로 사전 훈련하는 것이 FANUC 로봇 암과 같은 다른 로봇으로의 강력한 교차-장치(cross-embodiment) 전이를 가능하게 했습니다.

## 🧠 Insights & Discussion

- **작업 유연성 및 새로운 일반화 능력:** Interleave-VLA는 VIMA-Bench의 목표 이미지 매칭, 다단계 명령 수행과 같은 다양한 조작 작업을 위한 통합된 시퀀스 기반 인터페이스를 제공합니다. SIMPLER 및 실제 로봇 실험에서 특히 미지의 객체 및 방해물과 같은 어려운 도메인 외 시나리오에서 텍스트 전용 baseline보다 훨씬 강력한 일반화를 제공합니다.
- **제로샷 일반화 능력의 출현:** Interleave-VLA는 훈련 데이터셋에서 한 번도 보지 못한 손으로 그린 스케치, 사용자 크롭 이미지, 인터넷 사진 등 다양한 유형의 사용자 명령을 추가적인 미세 조정 없이 제로샷 방식으로 유연하게 처리하는 놀라운 능력을 보여주었습니다. 이는 인간-로봇 상호작용의 직관성과 실용성을 크게 향상시킵니다.
- **혼합 다양성의 중요성:** Interleave-VLA의 강력한 제로샷 일반화는 훈련 데이터셋의 규모와 다양성, 그리고 프롬프트 이미지의 다양성이라는 두 가지 핵심 요인에 의해 주도됩니다.
  - 대규모 사전 훈련은 특히 데이터가 부족한 환경에서 필수적이며, 교차-장치 공동 훈련은 의미론적 일반화와 도메인 외 강건성을 더욱 향상시킵니다.
  - 프롬프트 이미지의 다양성 측면에서는 인터넷 이미지와 작업별 크롭 이미지를 혼합하여 사용하는 것이 전반적으로 가장 우수한 성능을 보였습니다. 인터넷 이미지는 다양성을 제공하고, 크롭 이미지는 작업 관련성을 유지합니다.
- **제한 사항:**
  - 이미지 토큰의 증가로 인해 훈련에 더 많은 계산 자원과 시간이 필요합니다. 이미지 토큰 압축은 향후 연구 방향입니다.
  - 진정한 로봇 파운데이션 모델은 입력뿐만 아니라 출력을 혼합 모달리티(예: 텍스트 또는 미래 이미지와 함께 동작 생성)로 지원할 필요가 있습니다.

## 📌 TL;DR

본 논문은 로봇 조작을 위한 이미지-텍스트 혼합 명령을 처리하는 최초의 프레임워크인 **Interleave-VLA**를 제안합니다. 기존 VLA 모델을 최소한의 변경으로 확장하며, Open X-Embodiment 데이터셋에서 21만 에피소드의 대규모 혼합 데이터셋을 자동으로 구축하는 파이프라인을 개발했습니다. Interleave-VLA는 시뮬레이션 및 실제 로봇 실험에서 미지의 객체에 대한 일반화 성능을 2~3배 향상시키고, 손으로 그린 스케치, 크롭 이미지, 인터넷 이미지 등 다양한 사용자 명령을 **제로샷** 방식으로 처리하는 탁월한 능력을 보여주며, 실용적이고 유연한 인간-로봇 상호작용의 가능성을 제시합니다.
