# What Can RL Bring to VLA Generalization? An Empirical Study

Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang

## 🧩 Problem to Solve

로봇 공학 분야에서 유망한 VLA(Vision-Language-Action) 모델은 주로 SFT(Supervised Fine-tuning)를 통해 훈련됩니다. 하지만 이러한 훈련 방식은 분포 변화(distribution shift) 상황에서 누적 오류(compounding errors)에 취약하여 일반화 능력을 제한합니다. RL(Reinforcement Learning)은 시행착오를 통해 태스크 목표를 최적화함으로써 이러한 한계를 극복할 수 있는 잠재력을 가지지만, SFT와 비교했을 때 VLA의 특정 일반화 이점에 대한 체계적인 이해는 부족합니다. 본 연구는 RL 미세 조정이 VLA의 일반화에 어떤 독특한 이점을 제공하는지 종합적으로 규명하는 것을 목표로 합니다.

## ✨ Key Contributions

- VLA 미세 조정 방법이 다양한 시각, 의미론, 실행 차원에서 일반화에 미치는 영향을 평가하기 위한 엄격하고 도전적인 벤치마크를 구축했습니다.
- VLA 미세 조정을 위한 선호되는 RL 알고리즘으로 PPO(Proximal Policy Optimization)를 식별했으며, LLM(Large Language Model)/VLM(Vision-Language Model) 패러다임의 RL 알고리즘(GRPO, DPO)을 VLA에 적용할 때의 주요 과제를 논의했습니다.
- 공유 액터-크리틱 백본, VLA 모델 웜업, 최소 PPO 훈련 에폭을 특징으로 하는 효율적인 PPO 기반 VLA 미세 조정 레시피를 개발하고 그 실용적 유용성을 입증했습니다.
- RL이 VLA의 의미론적 이해(Semantic Understanding)와 구현 견고성(Embodied Execution Robustness)에서 SFT보다 우월한 일반화 능력을 보이며, 시각적 견고성(Visual Robustness)은 SFT와 비슷함을 입증했습니다.

## 📎 Related Works

- **Vision-Language-Action (VLA) 모델**: 사전 훈련된 LLM 또는 VLM의 지식을 활용하여 정책 일반화를 개선하는 VLA 모델(예: OpenVLA, RT-1, RT-2)의 발전.
- **대규모 (시각-)언어 모델을 위한 RL 미세 조정**: LLM/VLM 분야에서 SFT가 훈련 데이터를 암기하는 경향이 있는 반면, RL 미세 조정이 OOD(Out-of-Distribution) 성능 및 추론 능력을 향상시키는 데 기여한다는 연구들(예: RLHF, DPO, GRPO).
- **로봇 태스크를 위한 정책 미세 조정**: 모방 학습의 한계를 넘어 보상 기반 학습을 가능하게 하는 RL 미세 조정 노력(예: 오프라인 RL, PPO 활용 다단계 프로세스). 기존 연구에도 불구하고 VLA 모델에서 RL 미세 조정의 일반화 능력에 대한 체계적인 연구는 부족함을 지적합니다.

## 🛠️ Methodology

### 문제 정의

- 각 언어 조건부 로봇 태스크 $T \in \mathcal{T}$를 POMDP(Partially Observable Markov Decision Process) $M = (S, A, P, R, O, L, P(s_0), \gamma)$로 모델링합니다. 여기서 $S$는 상태 공간, $A$는 액션 공간, $O$는 관측 공간, $L$은 자연어 지시, $R$은 보상 함수, $\gamma$는 할인 인자입니다.
- **SFT(Supervised Fine-tuning)**: 전문가가 수집한 데모 데이터셋 $D_T = \{(\tau^{(i)}, l^{(i)})\}_{i=1}^N$로부터 VLA 정책 $\pi_\theta$를 훈련하여 다음 토큰 예측, $L_1$ 회귀 또는 확산 손실 $\ell_{\text{SFT}}$를 최소화합니다.
  $$L_{\text{SFT}}(\theta) = \sum_{(\tau^{(i)}, l^{(i)}) \in D_T} \sum_{t=0}^{K_i-1} \ell_{\text{SFT}} \left( \hat{a}^{(i)}_t, a^{(i)}_t \right), \quad \hat{a}^{(i)}_t = \pi_\theta \left( o^{(i)}_{\{t-H+1:t\}}, l^{(i)} \right)$$
- **RL(Reinforcement Learning) fine-tuning**: 환경과의 직접적인 상호작용을 통해 누적 태스크 보상을 최대화합니다. 에피소드 길이 $M$에 대한 음의 할인된 리턴을 최소화하거나 정책 그래디언트 대리자(surrogate)를 사용합니다.
  $$L_{\text{RL}}(\theta) = -\mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{M-1} \gamma^t R(s_t, l) \right]$$
  이는 일반적으로 정책 그래디언트 $L_{\text{PG}}(\theta) = -\mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{M-1} A^\pi_t \log \pi_\theta \left( a_t | o_{\{t-H+1:t\}}, l \right) \right]$를 통해 최적화됩니다.

### VLA 모델

- **OpenVLA (Kim et al., 2024)**: SigLIP (Zhai et al., 2023)과 DINOv2 (Oquab et al., 2023)의 융합 시각 인코더와 Llama-2 7B (Touvron et al., 2023) 언어 백본을 결합한 오픈 소스 모델을 기반으로 합니다.
- RT-2 (Brohan et al., 2023a) 이산화 방식을 사용하여, 연속 명령 $a_t \in \mathbb{R}^{d_a}$의 각 스칼라를 256개의 bin으로 매핑하여 액션 토큰 $u_t \in \{0, ..., 255\}^{d_a}$을 생성합니다.

### RL 알고리즘 평가 및 선택

- **PPO (Proximal Policy Optimization)**: 클립된 중요도 비율(clipped importance ratios)과 GAE(Generalized Advantage Estimation)를 사용하여 온-폴리시 업데이트를 수행합니다. GAE를 비활성화한 PPO-ORZ 변형도 평가했습니다.
- **GRPO (Group Relative Policy Optimization)**: 샘플 그룹을 사용하여 기준선(baseline)을 추정하고, 명시적 가치 추정 없이 이점(advantage)을 계산합니다. 로봇 태스크의 비정상적인 POMDP 환경에서는 이점 추정의 불안정성으로 인해 성능이 저하되는 것을 발견했습니다.
- **DPO (Direct Preference Optimization)**: 짝을 이루는 선호도 주석이 달린 오프라인 데이터셋을 활용합니다. 로봇 태스크의 희소한 보상 구조와 오프라인-온라인 분포 변화로 인해 성능이 저하되는 것을 발견했습니다.
- **결과**: PPO가 GRPO 및 DPO에 비해 일관되게 우수한 성능을 보여 VLA 미세 조정을 위한 주력 알고리즘으로 채택되었습니다.

### PPO의 효율적인 미세 조정 설계

- **공유 액터-크리틱 백본**: 사전 훈련된 VLA 정책을 액터로 사용하고, 경량 크리틱을 부착합니다. 크리틱은 전체 트랜스포머 백본을 공유하며, 최종 트랜스포머 블록에서 생성된 첫 번째 액션-토큰 위치의 은닉 벡터 $h_0$를 입력으로 받는 3계층 MLP 가치 헤드를 사용합니다. 이 설계가 가장 효율적이고 안정적인 성능을 보였습니다.
- **VLA 웜업**: OXE 데이터셋으로 사전 훈련된 OpenVLA 모델을 Octo-Small 데모 140개로 웜업하여, 수렴에 필요한 환경 스텝 수를 약 50% 감소시켰습니다.
- **최소 PPO 에폭**: `epoch=1`로 설정하여 가장 빠른 훈련 시간을 달성하면서도 성능 저하가 없음을 확인했습니다.

### 일반화 평가 환경 및 데이터셋

- **태스크**: 대표적인 '픽-앤-플레이스(pick-and-place)' 태스크에 초점을 맞춥니다.
- **3가지 일반화 차원**:
  1. **시각 (Vision)**: 새로운 배경(보지 못한 테이블), 전경/전체 이미지에 보지 못한 텍스처 오버레이(약/강), 동적 노이즈(약/강).
  2. **의미론 (Semantics)**: 보지 못한 객체, 새로운 용기, 다양한 지시문 구문, 다중 객체(본/보지 못한 객체), 방해 용기, 다중 용기 시나리오.
  3. **실행 (Execution)**: 객체 및 용기 위치 변화, 로봇 초기 포즈 변화, 에피소드 중 객체 동적 재배치.
- **훈련/테스트 데이터**: 훈련 시 시각(16개 테이블), 의미론(16개 객체), 실행(객체/용기 포즈 섭동) 요소를 무작위화합니다. 테스트 시에는 9개 새로운 객체, 16개 보지 못한 용기, 5개 새로운 테이블, 16개 방해 텍스처를 사용하여 OOD(Out-of-Distribution) 일반화 능력을 평가합니다.
- **로봇 및 관측**: ManiSkill 시뮬레이션 환경, 8-DoF WidowX-250S 로봇 팔. 640x480 RGB 프레임 및 자연어 지시를 관측합니다.
- **보상**: 희소 보상(Sparse reward) 체계: 올바른 객체 잡기 및 지속적으로 잡고 있기 0.1, 성공적으로 놓기 1.0.
- **SFT 데이터**: MPLib 모션 플래너를 사용하여 데모 궤적을 수집하고 LoRA (Hu et al., 2022)를 사용하여 미세 조정했습니다.

## 📊 Results

- **RL 알고리즘 비교**: 로봇 태스크의 비정상적인 역학과 희소 보상으로 인해 GRPO와 DPO는 학습에 어려움을 겪었으나, PPO는 일관되게 우수한 성능을 보였습니다.
- **PPO 효율성 설계**:
  - **공유 액터-크리틱 백본**: 마지막 트랜스포머 블록의 첫 번째 액션-토큰 은닉 벡터 $h_0$를 사용하는 3계층 MLP 가치 헤드가 가장 효율적이고 안정적인 성능을 제공했습니다. 이는 별도의 액터-크리틱 백본보다 VRAM을 83% 적게 사용하고 35% 빠르게 훈련되었습니다.
  - **VLA 웜업**: 140개의 데모 궤적으로 웜업하면 수렴에 필요한 환경 스텝 수가 약 50% 감소했습니다.
  - **최소 PPO 에폭**: `epoch=1`로 설정했을 때 가장 빠른 훈련 시간을 달성했으며 성능 저하가 없었습니다.
- **SFT 데이터 스케일 영향**: SFT는 16k 궤적(약 1.26M 전환)에서 성능이 정체되는 경향을 보였으며, 이 지점을 RL과의 비교를 위한 기준선(SFT-16k)으로 사용했습니다.
- **RL vs. SFT 일반화 성능 비교**:
  - **훈련 분포 내 성능**: RL은 훈련 분포 내에서 SFT-16k와 비슷한 성능을 보였으나, 약 0.4M 환경 스텝 이후 OOD 태스크에서 SFT-16k를 추월했습니다.
  - **OOD 일반화**:
    - **시각 (Vision)**: RL은 SFT와 비슷한 수준의 시각적 견고성을 유지했습니다. 이는 두 방법 모두 주어진 시각적 무작위성 이상으로 시각적 견고성을 유도하지 못했기 때문으로 추정됩니다.
    - **의미론 (Semantics)**: RL은 OOD 객체(단일 및 다중 객체 시나리오 모두)에서 SFT를 크게 능가했습니다. RL이 시행착오를 통해 객체 유형에 덜 의존적인 '잡기' 기술을 학습했기 때문으로 분석됩니다.
    - **실행 (Execution)**: RL은 OOD 객체 및 용기 위치, OOD 로봇 초기 포즈, 에피소드 중 객체 재배치 등 모든 평가 시나리오에서 SFT를 크게 능가했습니다. RL 궤적은 SFT 데모 데이터에서는 볼 수 없는 넓은 작업 공간과 풍부한 말단 이펙터 방향을 포함했습니다.

## 🧠 Insights & Discussion

- **RL의 장점**: RL은 시행착오를 통해 학습하면서 VLA 모델이 좁은 전문가 궤적을 넘어 탐색하고 수정 행동을 학습할 수 있도록 하여, 특히 의미론적 이해와 구현 견고성에서 뛰어난 일반화 능력을 발휘합니다. 이는 SFT가 데모 데이터에 없는 상황에서 흔히 겪는 누적 오류 문제에 대한 효과적인 해결책을 제시합니다.
- **SFT의 한계**: 모방 학습에 기반한 SFT는 훈련 데이터와 다른 환경 조건(예: 동적 노이즈, 예상치 못한 객체 이동)에서 정책이 낯선 상태로 벗어나거나 고착되는 경향이 있습니다.
- **PPO의 실용성**: 본 연구에서 개발된 효율적인 PPO 미세 조정 레시피는 계산 효율성과 VLA 모델 성능을 동시에 확보하며, 대규모 VLA 모델에 RL을 적용하는 실용적인 길을 제시합니다.
- **시사점**: RL 미세 조정은 시뮬레이션 환경에서 VLA 모델의 OOD 일반화 능력을 크게 향상시킬 수 있음을 보여주며, 이는 미래의 로봇 에이전트가 더욱 견고하고 적응적으로 작동할 수 있는 중요한 단계를 의미합니다.

- **한계**:
  - 모션 플래너로 생성된 데모에만 의존하여 인간이 수집한 데이터의 가변성을 완전히 포착하지 못할 수 있습니다.
  - 평가가 '픽-앤-플레이스' 태스크로 제한되어 있어, 더 복잡한 다중 태스크 설정으로의 확장이 필요합니다.
  - 모든 실험은 시뮬레이션에서 수행되었으므로, 물리적 로봇에서의 VLA 일반화 검증을 위해 심-투-리얼(sim-to-real) 전이가 중요합니다.

## 📌 TL;DR

- **문제**: 기존 VLA 모델은 SFT 훈련으로 인해 분포 변화 환경에서 일반화 능력이 제한적이며, RL이 VLA의 일반화에 미치는 구체적인 이점이 명확하지 않습니다.
- **방법**: OpenVLA 모델을 사용하여 PPO, GRPO, DPO와 같은 RL 알고리즘의 일반화 성능을 시각, 의미론, 실행의 세 가지 차원에서 평가했습니다. 공유 액터-크리틱 백본, VLA 웜업, 최소 PPO 에폭을 포함하는 효율적인 PPO 미세 조정 레시피를 개발했습니다.
- **결과**: PPO는 GRPO와 DPO보다 월등한 성능을 보였으며, SFT와 비교했을 때 VLA의 의미론적 이해와 구현 견고성에서 현저한 일반화 개선을 달성했습니다. 시각적 견고성은 SFT와 비슷한 수준을 유지했습니다. 이러한 결과는 RL이 시행착오를 통해 다양한 OOD 시나리오에 대한 적응형 정책을 학습하여 SFT의 한계를 극복할 수 있음을 보여줍니다.
