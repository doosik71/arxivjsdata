{
  "title": "OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive\n  Reasoning",
  "authors": "Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, Yang Gao",
  "year": 2025,
  "url": "http://arxiv.org/abs/2505.11917v1",
  "abstract": "General-purpose robots capable of performing diverse tasks require\nsynergistic reasoning and acting capabilities. However, recent dual-system\napproaches, which separate high-level reasoning from low-level acting, often\nsuffer from challenges such as limited mutual understanding of capabilities\nbetween systems and latency issues. This paper introduces OneTwoVLA, a single\nunified vision-language-action model that can perform both acting (System One)\nand reasoning (System Two). Crucially, OneTwoVLA adaptively switches between\ntwo modes: explicitly reasoning at critical moments during task execution, and\ngenerating actions based on the most recent reasoning at other times. To\nfurther unlock OneTwoVLA's reasoning and generalization capabilities, we design\na scalable pipeline for synthesizing embodied reasoning-centric vision-language\ndata, used for co-training with robot data. We validate OneTwoVLA's\neffectiveness through extensive experiments, highlighting its superior\nperformance across four key capabilities: long-horizon task planning, error\ndetection and recovery, natural human-robot interaction, and generalizable\nvisual grounding, enabling the model to perform long-horizon, highly dexterous\nmanipulation tasks such as making hotpot or mixing cocktails."
}