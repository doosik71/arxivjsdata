# CLIPORT: What and Where Pathways for Robotic Manipulation

Mohit Shridhar, Lucas Manuelli, Dieter Fox

## 🧩 Problem to Solve

로봇은 추상적인 개념(예: "커피콩 한 스쿱 가져와", "천을 반으로 접어")으로 사물을 추론하는 동시에 정밀하게 조작할 수 있는 능력이 필요합니다. 기존의 종단 간(end-to-end) 조작 방법은 정밀한 공간 추론을 학습할 수 있지만, 새로운 목표에 일반화하거나 작업 전반에 걸쳐 개념을 빠르게 학습하는 데 실패하는 경우가 많습니다. 반면, 대규모 인터넷 데이터를 학습하여 시각 및 언어에 대한 일반화 가능한 의미론적 표현을 학습하는 데 큰 발전이 있었지만, 이러한 표현은 미세한 조작에 필요한 공간 이해가 부족합니다. 이 연구는 로봇이 추상적인 의미론적 개념을 정밀한 공간 추론에 효율적으로 접목하는 방법을 찾는 것을 목표로 합니다.

## ✨ Key Contributions

- **CLIPORT 프레임워크 제안:** CLIP(시각-언어 모델)의 폭넓은 의미 이해(`what` 경로)와 Transporter(로봇 조작 모델)의 공간적 정밀성(`where` 경로)을 결합한 언어 조건부 모방 학습 에이전트인 CLIPORT를 제안합니다.
- **두 가지 스트림 아키텍처:** 시각 기반 조작을 위한 의미론적(semantic) 및 공간적(spatial) 경로를 가진 투 스트림(two-stream) 아키텍처를 개발했습니다.
- **광범위한 조작 작업 해결:** 명시적인 객체 자세, 인스턴스 분할, 메모리, 상징적 상태 또는 구문 구조 없이도 보지 못한 객체 포장부터 천 접기까지 다양한 언어 지정 탁상 작업(tabletop tasks)을 해결합니다.
- **데이터 효율성 및 일반화:** 소수 학습(few-shot) 설정에서 데이터 효율성을 보이고, 보았거나 보지 못한 의미론적 개념에 효과적으로 일반화됨을 시뮬레이션 및 실제 환경에서 입증했습니다.
- **다중 작업 정책 학습:** 10가지 시뮬레이션 작업과 9가지 실제 작업에 대해 단일 다중 작업 정책을 학습하여 단일 작업 정책보다 우수하거나 필적하는 성능을 달성했습니다.
- **새로운 벤치마크 및 리소스 공개:** Ravens 벤치마크에 언어-접지(language-grounding) 조작 작업을 확장하여 추가하고, 코드와 사전 학습된 모델을 공개했습니다 (cliport.github.io).

## 📎 Related Works

- **Vision-based Manipulation:** 전통적인 방식은 객체 감지, 분할, 자세 추정에 의존했지만, 변형 가능한 객체나 새로운 객체에 대한 일반화가 어려웠습니다. Transporter [2]와 같은 종단 간 접근 방식은 정밀한 순차적 정책을 학습하지만, 의미론적 개념 이해가 제한적이고 목표 이미지에 의존합니다.
- **Semantic Models:** 대규모 모델(예: ViLBERT [13], UNITER [14], LXMERT [15])은 시각 및 언어 표현을 공동으로 학습하지만, 바운딩 박스나 인스턴스 분할에 제약이 있습니다. CLIP [1]은 수백만 개의 이미지-캡션 쌍으로 학습하여 시각과 언어 표현을 정렬하는 데 큰 진전을 보였습니다.
- **Language Grounding for Robotics:** 자연어 명령을 로봇에게 지시하는 연구는 있었지만 (예: Shridhar and Hsu [7], Matuszek et al. [8]), 대부분 지각과 행동을 분리된 파이프라인으로 처리하여 정밀한 공간 조작에 필요한 정확성이 부족했습니다.
- **Two-Stream Architectures:** 비디오 액션 인식 [49, 50, 51] 및 오디오 인식 시스템 [52, 53]에서 널리 사용됩니다. 로봇 공학에서는 Zeng et al. [54]과 Jang et al. [55]이 새로운 객체에 대한 어포던스 예측을 위해 투 스트림 파이프라인을 제안했지만, 목표 이미지나 단일 단계 작업으로 제한되었습니다. CLIPORT는 순차적 작업을 위한 직관적인 언어 명령 인터페이스를 제공합니다.

## 🛠️ Methodology

CLIPORT는 다음 네 가지 핵심 원칙을 기반으로 하는 모방 학습(imitation-learning) 에이전트입니다.

1. **두 단계 기본 동작:** 각 행동은 시작 및 최종 말단 효과기(end-effector) 포즈를 포함하는 픽-앤-플레이스(pick-and-place) 원시 동작을 통해 조작이 이루어집니다.
2. **시각적 표현의 등변성:** 행동의 시각적 표현은 변환 및 회전에 대해 등변(equivariant)입니다.
3. **의미론적 및 공간적 경로:** 의미론적 정보와 공간적 정보를 위한 두 개의 별도 경로를 가집니다.
4. **언어 조건부 정책:** 목표를 지정하고 작업 간 개념을 전이하는 언어 조건부 정책을 사용합니다.

### 언어 조건부 조작 문제 정의

- 정책 $\pi$는 시각적 관측 $o_t$와 영어 명령 $l_t$로 구성된 입력 $\gamma_t = (o_t, l_t)$가 주어졌을 때, 행동 $a_t = (T_{\text{pick}}, T_{\text{place}})$를 출력합니다.
  $$ \pi(\gamma*t) = \pi(o_t, l_t) \to a_t = (T*{\text{pick}}, T\_{\text{place}}) \in \mathcal{A} $$
- 행동 $a=(T_{\text{pick}}, T_{\text{place}})$는 각각 집기(picking) 및 놓기(placing)를 위한 말단 효과기 포즈를 지정합니다. 여기서 $T_{\text{pick}}, T_{\text{place}} \in \text{SE}(2)$입니다.

### Transporter 기반 Pick-and-Place

- 정책 $\pi$는 Transporter [2]를 사용하여 공간 조작을 수행하도록 학습됩니다.
  - **집기(Pick) 모듈 ($Q_{\text{pick}}$):** $f_{\text{pick}}$ FCN(Fully-Convolutional-Network)은 $\gamma_t$를 입력받아 픽셀별 행동 값 $Q_{\text{pick}} \in \mathbb{R}^{H \times W}$를 출력합니다.
    $$ T*{\text{pick}} = \text{argmax}*{(u,v)} Q\_{\text{pick}}((u,v)|\gamma_t) $$
  - **놓기(Place) 모듈 ($Q_{\text{place}}$):** $\Phi_{\text{query}}$ FCN은 $T_{\text{pick}}$을 중심으로 $o_t$를 크롭한 $\gamma_t[T_{\text{pick}}]$과 $l_t$를 입력받아 쿼리 특징 임베딩을 출력합니다. $\Phi_{\text{key}}$ FCN은 전체 $\gamma_t$를 입력받아 키 특징 임베딩을 출력합니다. $Q_{\text{place}}$는 이 쿼리 및 키 특징의 교차 상관(cross-correlation)으로 계산됩니다.
    $$ Q*{\text{place}}(\Delta\tau|\gamma_t,T*{\text{pick}}) = \left( \Phi*{\text{query}}(\gamma_t[T*{\text{pick}}]) \ast \Phi\_{\text{key}}(\gamma_t) \right)[\Delta\tau] $$
      여기서 $\Delta\tau \in \text{SE}(2)$는 잠재적 놓기 포즈를 나타내며, 회전은 $k$개의 이산 각도 회전으로 처리됩니다($k=36$ 사용).

### 투 스트림(Two-Stream) 아키텍처

- 세 가지 FCN ($f_{\text{pick}}$, $\Phi_{\text{query}}$, $\Phi_{\text{key}}$)의 네트워크 아키텍처를 언어 입력 및 고수준 의미론적 개념 추론을 허용하도록 확장합니다.
- **공간 스트림 (Spatial Stream):** Transporter의 ResNet 아키텍처와 동일하게 RGB-D 입력을 받아 인코더-디코더 모델을 통해 밀집 특징(dense features)을 출력합니다.
- **의미론적 스트림 (Semantic Stream):**
  - 사전 학습된 `CLIP ResNet50` (고정)을 사용하여 RGB 입력을 인코딩합니다.
  - `CLIP의 Transformer 기반 문장 인코더`를 사용하여 언어 명령 $l_t$를 목표 인코딩 $g_t$로 생성합니다.
  - $g_t$는 채널 차원과 시각적 특징의 공간 차원에 맞게 다운샘플링되고 타일링됩니다.
  - 디코더 특징은 타일링된 목표 특징과 요소별 곱(element-wise product) $v_t^{(l)} \odot g_t^{(l)}$을 통해 조건화됩니다.
  - CLIP ResNet50 인코더에서 디코더 레이어로의 스킵 연결(skip connection)을 추가하여 다양한 수준의 의미 정보(모양, 부분, 객체 수준 개념)를 활용합니다.
  - 공간 스트림에서 의미론적 스트림으로의 측면 연결(lateral connection)을 추가하여 두 특징 텐서를 연결하고 $1 \times 1$ 컨볼루션을 적용하여 채널 차원을 줄입니다.
  - 최종 밀집 특징 융합은 $f_{\text{pick}}$의 경우 덧셈(addition)을, $\Phi_{\text{query}}$ 및 $\Phi_{\text{key}}$의 경우 $1 \times 1$ 컨볼루션 융합을 사용합니다.

### 구현 세부 사항

- **데모 학습:** 전문가 데모 데이터셋 $D$를 사용하여 모방 학습으로 CLIPORT를 학습합니다.
- **손실 함수:** 전문가 데모 행동의 원-핫(one-hot) 픽셀 인코딩 $Y_{\text{pick}}, Y_{\text{place}}$를 사용하여 모델을 종단 간 지도 학습합니다. 교차 엔트로피 손실(cross-entropy loss)을 사용합니다.
  $$ L = -E*{Y*{\text{pick}}}[\text{log}V_{\text{pick}}] - E*{Y*{\text{place}}}[\text{log}V_{\text{place}}] $$
    여기서 $V_{\text{pick}}=\text{softmax}(Q_{\text{pick}}((u,v)|\gamma_t))$이고 $V_{\text{place}}=\text{softmax}(Q_{\text{place}}((u',v',\omega')|\gamma_t,T_{\text{pick}}))$입니다.
- **학습 반복:** 단일 작업 모델은 200K 반복, 다중 작업 모델은 600K 반복 학습합니다. 데이터 증강(random $\text{SE}(2)$ 변환)을 사용하여 과적합을 방지하고 공간 등변 표현 학습에 도움을 줍니다.

## 📊 Results

- **시뮬레이션 결과:**
  - **투 스트림 성능:** CLIPORT(단일 작업)는 Transporter-only (언어 명령 사용 안 함, ~50%에서 포화) 및 CLIP-only (공간 정밀도 부족, ~76%에서 포화)보다 훨씬 우수한 90% 이상의 평균 점수를 달성했습니다. 이는 의미론적 및 공간적 스트림이 모두 미세한 조작에 필수적임을 보여줍니다. 100개의 데모만으로도 대부분의 작업에서 86%의 효율적인 성능을 보였습니다.
  - **다중 작업 성능:** CLIPORT(다중 작업) 모델은 10개 모든 작업에 대해 학습되었음에도 불구하고, 평가의 57%에서 단일 작업 CLIPORT(단일 작업) 모델보다 뛰어난 성능을 보였습니다. 특히 100개 이하의 데모에서 더욱 두드러졌습니다. 이는 언어가 작업 간 개념을 재사용하는 강력한 조건화 메커니즘임을 시사합니다.
  - **보지 못한 속성으로의 일반화:** CLIPORT(단일 작업)는 보지 못한 색상, 모양, 객체에 대해서도 우수한 성능을 보였지만, 여전히 보지 못한 속성을 가진 작업에서는 성능이 낮았습니다. CLIPORT(multi-attr)는 다른 작업에서 보지 못한 속성을 명시적으로 학습하여(예: '핑크색' 블록을 본 경험이 다른 작업에서 '핑크색' 상자를 포장하는 데 도움을 줌) 성능을 크게 향상시켰습니다.
- **실제 로봇 실험:**
  - Franka Panda 로봇으로 9가지 실제 작업에서 다중 작업 모델을 평가했습니다.
  - 단 179개의 이미지-액션 쌍으로 학습된 모델이 간단한 블록 조작 작업에서 약 70%의 성공률을 보이며 소수 학습(few-shot learning)의 효율성을 입증했습니다.
  - 시뮬레이션 결과와 유사한 성능을 보였으며, 편향되지 않은 데이터셋이 실제 환경에서 중요함을 강조했습니다.
- **데모 조건부 작업(부록 G):** 언어 조건화가 없는 CLIPORT 버전이 원래 Transporter의 데모 조건부 작업에서 더 나은 성능을 보여, CLIP-ResNet이 일반화 가능한 정책 학습을 위한 강력한 시각적 사전 지식(prior)을 제공함을 시사합니다.

## 🧠 Insights & Discussion

- **의미:** 로봇 공학에서 데이터 중심 접근 방식의 일반화 능력은 아직 완전히 활용되지 않고 있습니다. 적절한 행동 추상화와 공간-의미론적 사전 지식(prior)이 결합된 종단 간(end-to-end) 방법은 작업별 엔지니어링 없이도 새로운 기술을 빠르게 학습할 수 있습니다.
- **한계:**
  - **조작 복잡성:** 투 스텝 기본 동작을 넘어선 6-자유도(DOF) 또는 N-자유도(DOF)의 능숙한 조작으로 확장하는 것은 여전히 도전 과제입니다. 복잡한 부분 관측 장면이나 다지(multi-fingered) 핸드를 위한 연속 제어, 작업 완료 예측은 불가능합니다.
  - **데이터셋 균형:** 모델은 균형 잡힌 훈련 데이터셋에 크게 의존합니다. 데이터에 편향이 있을 경우 모델이 이를 악용할 수 있습니다 (예: 항상 "노란색 블록"을 "파란색 그릇"에 놓는 경우).
  - **캘리브레이션 및 제어:** 핸드-아이 캘리브레이션 정확도에 민감하며, 픽-앤-플레이스 기본 동작 자체는 개방 루프(open-loop)로 작동하여 준정적(quasi-static) 작업에 사용이 제한됩니다.
  - **복잡한 객체 관계 추론:** 여러 객체에 대한 추론이 필요한 복잡한 객체 관계(예: 객체 개수 세기, "중간"과 같은 공간 관계)를 이해하는 데 어려움을 겪습니다. 모델은 시공간 전반에 걸쳐 이력을 유지하지 않아 '모든' 또는 '어떤'과 같은 한정사에 제한됩니다.
  - **언어 접지의 범위:** 모델의 동사-명사 구문 이해는 훈련 중 관찰된 데모 및 작업에 밀접하게 연결되어 있습니다. 더 일반적인 의미의 언어 이해는 어렵습니다.
  - **작업 완료 지시:** CLIPORT는 작업 완료를 전문가에게 의존합니다. 실제 환경에서는 사용자가 실행을 중단할 때까지 계속 동작합니다.
  - **사전 학습 모델의 위험:** CLIP은 인터넷의 대규모 이미지-캡션 쌍으로 학습되어 편향 및 오염된 연관성에 취약하며, 이는 유해할 수 있습니다. 실제 로봇에 CLIP의 표현을 사용하여 행동을 취할 때 이러한 문제가 증폭될 수 있으므로, 안전한 배치를 위해서는 인간 개입이 필수적입니다.
- **의의:** 데이터와 구조적 사전 지식의 융합은 확장 가능하고 일반화 가능한 로봇 시스템을 구축하는 데 유망합니다.

## 📌 TL;DR

로봇이 정밀한 조작 능력과 추상적인 언어 개념 이해를 동시에 갖추고 일반화하는 것은 어려운 문제입니다. 본 논문은 **CLIPORT**라는 새로운 프레임워크를 제안합니다. 이는 **CLIP**의 폭넓은 의미 이해(무엇을 할지)와 **Transporter**의 정밀한 공간 파악(어디를 집고 놓을지)을 결합한 **투 스트림(two-stream) 신경망 아키텍처**를 사용한 언어 조건부 모방 학습 에이전트입니다. CLIPORT는 데이터 효율적이며, 시뮬레이션 및 실제 환경 모두에서 보았거나 보지 못한 개념과 다중 작업 설정에서 우수한 일반화 성능을 보여줍니다. 특히, 단일 다중 작업 정책이 종종 단일 작업 정책보다 뛰어난 성능을 발휘하여, **언어가 개념 전이(concept transfer)를 위한 강력한 메커니즘**임을 입증했습니다.
