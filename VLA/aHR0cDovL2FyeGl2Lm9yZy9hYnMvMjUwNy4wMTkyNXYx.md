# A Survey on Vision-Language-Action Models: An Action Tokenization Perspective

Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen and Yaodong Yang

## 🧩 Problem to Solve

최근 비전 및 언어 기반 모델의 발전으로 비전-언어-액션(Vision-Language-Action, VLA) 모델 연구가 활발해졌습니다. 그러나 현재 VLA 모델들이 다양한 접근 방식을 보임에도 불구하고, 액션 토큰(action token)에 대한 포괄적인 이해와 체계적인 분류가 부족하여 VLA 모델 개발 및 향후 연구 방향 설정에 어려움이 있습니다. 이 논문은 이러한 격차를 해소하고 VLA 연구의 효과적인 발전을 저해하는 액션 토큰의 부족한 이해를 해결하는 것을 목표로 합니다.

## ✨ Key Contributions

- **VLA 통합 프레임워크 및 액션 토큰 분류**: 기존 VLA 모델들을 비전 및 언어 입력이 VLA 모듈을 통해 액션 토큰 체인을 생성하고 궁극적으로 실행 가능한 액션을 생성하는 통합 프레임워크로 제시합니다. 또한, 액션 토큰을 8가지 유형(언어 설명, 코드, 어포던스, 궤적, 목표 상태, 잠재 표현, 원시 액션, 추론)으로 분류하고, 액션 토큰이 LLM의 언어 토큰에 대한 일반화된 대응물임을 정의합니다.
- **액션 토큰 트렌드 분석**: 미래 VLA 모델은 단일 액션 토큰이 아닌 전략적 조합에 있음을 강조합니다. 언어 계획은 장기 계획에 필수적이며, 코드는 포괄적인 함수 라이브러리를 통해 잠재력이 커지고, 어포던스와 궤적 간의 시너지가 중요하며, 잠재 표현과 원시 액션의 과제 및 추론의 메타 토큰 역할을 논의합니다.
- **새로운 액션 토큰 유형 및 VLA 아키텍처 트렌드 제시**: 더 강력한 모델과 새로운 양상(예: 오디오, 촉각)이 새로운 액션 토큰 유형을 만들어낼 것을 예측하고, 장기 계획 및 논리 제어를 위한 언어 설명과 코드를 사용하는 계층적 VLA 아키텍처를 제안합니다.
- **모방 학습에서 강화 학습으로의 전환 제안**: VLA 모델이 모방 학습의 한계를 극복하고 시행착오 및 자기 주도적 탐색을 통해 인간과 유사한 학습을 달성하기 위해 강화 학습 통합의 중요성을 강조합니다.
- **VLA 모델에서 VLA 에이전트로의 진화 촉구**: 기억, 탐색, 계획, 반성 등의 인지 아키텍처를 갖춘 능동적인 VLA 에이전트 시스템으로 발전해야 함을 제안하며, 현재의 선형 처리 아키텍처에서 더 복잡한 양방향 및 그래프 구조 토폴로지로의 전환을 주장합니다.
- **모델, 데이터, 하드웨어의 삼중 진전 강조**: 물리적 세계의 비정형적이고 개방적인 특성을 다루기 위해 모델, 데이터, 하드웨어 간의 시너지가 필수적임을 지적하며, 현재의 데이터 부족 및 제한된 로봇 플랫폼의 한계를 극복해야 함을 역설합니다.
- **안전 및 정렬의 중요성 강조**: 현재 VLA 연구가 주로 모델 능력에 초점을 맞추고 있지만, 미래 연구에서는 안전 및 인간 정렬에 더 큰 중요성을 두어야 한다고 제언합니다.

## 📎 Related Works

이 논문은 VLA 모델의 발전을 논하기 위해 다양한 기초 모델 분야를 참조합니다.

- **언어 기반 모델 (Language Foundation Models, LFMs)**: Transformer 아키텍처 [42]를 기반으로 BERT [43], T5 [46], GPT 모델 [47,48,49] 등이 언어 이해, 추론, 생성 능력에서 발전했습니다. InstructGPT [51]의 인간 의도 정렬 및 Llama [55], Gemma [58], Mistral [61]과 같은 오픈 소스 LLM, Mixture-of-Experts (MoE) [69,70], Mamba [71] 등의 효율성 개선 연구가 VLA의 기반을 제공합니다.
- **비전 기반 모델 (Vision Foundation Models, VFMs)**: Vision Transformer (ViT) [81]의 도입 이후, CLIP [5], DINO [6,7], SAM [8,9] 등이 이미지 표현 학습, 분할, 물체 감지 등에서 뛰어난 일반화 능력을 보여주었습니다. Depth Anything [86], Grounding DINO [92], Diffusion Models (Stable Diffusion [101], Sora [107]) 등은 깊이 추정, 개방형 어휘 감지, 이미지 및 비디오 생성에서 진전을 이루었습니다.
- **비전-언어 모델 (Vision-Language Models, VLMs)**: BLIP [114], Flamingo [116], LLaVA [83], Qwen-VL [119] 시리즈 등은 시각 및 텍스트 양상을 통합하여 멀티모달 이해, 추론, 생성을 가능하게 했습니다. GPT-4o [10] 및 Gemini 2.5 Pro [11]는 현재 VLM의 선두 주자입니다.

이러한 기반 모델들은 VLA 모델이 물리적 세계로 지능을 확장하는 데 필요한 강력한 인식 및 인지 능력을 제공합니다. 본 서베이는 이러한 VLM을 기반으로 하는 VLA 모델들을 **액션 토큰화** 관점에서 체계적으로 분류하고 분석합니다.

## 🛠️ Methodology

이 서베이 논문의 방법론은 기존 VLA 연구를 체계적으로 검토하고 분석하는 데 중점을 둡니다. 구체적으로 다음과 같은 단계를 따릅니다.

1. **VLA 통합 프레임워크 제안**: 비전 및 언어 입력이 VLA 모듈을 거쳐 점진적으로 지상화되고 실행 가능한 정보를 인코딩하는 액션 토큰 체인을 생성하며, 궁극적으로 실행 가능한 액션을 만들어내는 통합 프레임워크를 제시합니다.
2. **액션 토큰 분류**: VLA 모델에서 액션 토큰이 어떻게 구성되는지에 따라 기존 연구를 8가지 주요 유형(언어 설명, 코드, 어포던스, 궤적, 목표 상태, 잠재 표현, 원시 액션, 추론)으로 분류합니다. 이는 VLA 모델의 설계 선택을 구분하는 핵심 요소로 작용합니다.
3. **각 액션 토큰 유형 심층 분석**: 각 액션 토큰 유형에 대해 동기, 대표적인 접근 방식, 특징, 장점, 한계 및 향후 연구 방향을 분석합니다. 이를 통해 각 토큰 유형의 강점과 약점을 파악하고 개선 영역을 식별합니다.
4. **확장 가능한 데이터 소스 요약**: VLA 모델의 발전을 지원하는 웹 데이터, 인간 비디오, 합성 및 시뮬레이션 데이터, 실제 로봇 데이터 등 다양한 데이터 소스에 대한 노력을 요약합니다.
5. **미래 연구 방향 제시**: 서베이된 연구 동향과 새로운 통찰력을 바탕으로 VLA 분야의 발전을 위한 일반적인 논의 및 향후 연구 방향을 제시합니다. 여기에는 액션 토큰 및 VLA 모델의 트렌드, VLA 모델에서 에이전트로의 전환, 모방 학습에서 강화 학습으로의 전환, 하드웨어 제약 극복, 안전 인식 강화, 데이터 부족 문제 해결 등이 포함됩니다.

이러한 체계적인 검토 및 분석을 통해 VLA 모델의 광범위한 진화에 대한 통합된 전망을 제시하고, 탐구되지 않았지만 유망한 방향을 제시하며, 미래 연구를 위한 지침을 제공합니다.

## 📊 Results

이 서베이의 주요 결과는 다음과 같습니다.

- **VLA 통합 프레임워크 제시**: 현재 VLA 모델들이 비전 및 언어 입력을 처리하여 점진적으로 실질적인(grounded) 실행 정보를 인코딩하는 액션 토큰 체인을 생성하고 궁극적으로 실행 가능한 액션을 만들어내는 단일 프레임워크로 통합될 수 있음을 보여줍니다. ($VLA_{inputs} \xrightarrow{VLA_{module}} Action Token_1 \xrightarrow{VLA_{module}} \dots \xrightarrow{VLA_{module}} Action Token_N \xrightarrow{Action_{Expert}} Executable Actions$).
- **8가지 액션 토큰 유형 분류 및 특성 분석**:
  1. **언어 설명**: 언어 계획 및 언어 동작으로 나뉘며 LLM/VLM 통합이 용이하고 장기 계획에 필수적이지만, 표현의 모호성과 높은 지연 시간이 한계입니다.
  2. **코드**: LLM 지원이 좋고 명확한 논리 제어가 가능하지만, 사전 정의된 API에 과도하게 의존하고 실행에 취약합니다.
  3. **어포던스**: 키포인트, 바운딩 박스, 분할 마스크, 어포던스 맵 등 공간적으로 접지된 표현으로 정밀한 상호작용이 가능하지만, 3D 공간 이해 부족 및 시간에 따른 변화 모델링이 어렵습니다.
  4. **궤적**: 비액션(action-free) 인간 비디오에서 학습 가능하며 작업 간 일반화에 강점을 보이지만, 3D 표현력이 제한적이고 VLM 지원이 부족합니다.
  5. **목표 상태**: 파운데이션 모델의 지원이 좋고 후향 리라벨링(hindsight relabeling)을 통한 데이터 확장성이 뛰어나며 작업 특이성이 높지만, 고품질/일관된 목표 상태 생성 및 높은 지연 시간이 과제입니다.
  6. **잠재 표현**: 데이터 확장성이 높고 강력한 표현 잠재력을 가지지만, 해석 불가능하며 세분성, 포괄성, 작업 중심 정렬 개선이 필요합니다.
  7. **원시 액션**: 최소한의 인간 지식과 액션 토큰 주석이 필요하며 VLM과 유사한 훈련 및 스케일링 잠재력을 가지지만, 데이터 희소성, 수집 어려움, 교차 구현 일반화 부족이 한계입니다.
  8. **추론**: 다른 액션 토큰 생성을 강화하고 복잡한 문제 해결에 기여하지만, 높은 지연 시간과 유연한 추론 패러다임 개발의 필요성이 있습니다.
- **데이터 소스 계층화**: 웹 데이터/인간 비디오 (하위 계층), 합성/시뮬레이션 데이터 (중간 계층), 실제 로봇 데이터 (상위 계층)로 구성된 "데이터 피라미드" 패러다임을 강조하며, 각 계층이 VLA 모델 학습에 어떻게 기여하는지 분석합니다.
- **VLA 모델 발전의 일반적인 트렌드 도출**: 모델, 데이터, 하드웨어의 공동 발전을 촉진하는 방향으로 VLA 연구의 향후 진로를 제시합니다.

## 🧠 Insights & Discussion

이 서베이는 VLA 모델의 현재 상태에 대한 심층적인 통찰력을 제공하고, 미래 연구를 위한 몇 가지 핵심 논의 사항과 방향을 제시합니다.

- **액션 토큰 및 VLA 모델의 계층적 트렌드**:

  - 미래 VLA는 단일 액션 토큰 유형이 아닌, **전략적인 조합**을 통해 **계층적 아키텍처**를 채택할 것입니다.
  - 최상위 계층에서는 장기 계획 및 논리 제어를 위해 **언어 계획**과 **코드**가 핵심적인 역할을 할 것입니다.
  - 중간 계층에서는 3D 어포던스, 궤적 모델링, 목표 비디오 예측을 통해 정확하고 해석 가능한 동작 표현을 제공할 것입니다.
  - 낮은 계층에서는 이러한 비전 기반 표현을 원시 액션으로 매핑하는 정책 모듈이 필요합니다.
  - 잠재 표현은 현재의 훈련 문제(세분성, 포괄성, 작업 중심 정렬)로 인해 제안된 아키텍처에 포함되지 않지만, 미래 발전 가능성은 높습니다.
  - **추론**은 VLA 계층 전반에 걸쳐 다른 모든 액션 토큰 생성을 향상시키는 핵심적인 메타 토큰으로 통합될 것입니다.

- **VLA 모델에서 VLA 에이전트로의 진화**:

  - 현재의 VLA 모델은 시각-언어 입력에서 액션 출력으로의 매핑에 초점을 맞추고 있지만, 미래에는 **기억, 계획, 반성**과 같은 인지 아키텍처를 포함하는 **VLA 에이전트**로 발전해야 합니다.
  - 선형적인 아키텍처를 넘어, 모듈과 액션 토큰이 에이전트에 의해 적응적으로 호출되고 관리되는 **복잡한, 양방향, 그래프 구조 토폴로지**가 필요합니다.
  - 다중 에이전트 시스템 및 인간-에이전트 공존에 대한 연구도 중요합니다.

- **모방 학습에서 강화 학습으로의 전환**:

  - 인간의 학습이 시행착오와 자기 주도적 탐색에 크게 의존하므로, VLA 모델도 **강화 학습(RL)**을 통합하여 더 견고하고 능숙하며 성공률이 높은 행동을 학습해야 합니다.
  - 실제 환경에서의 높은 리셋 비용, 낮은 상호작용 효율성, 안전 문제 등 RL 적용의 과제를 해결하기 위한 효율적인 RL 알고리즘 개발이 필요합니다.
  - VLM을 활용하여 **밀도 높은 보상 함수를 자동 생성**하는 연구는 RL 훈련 및 배포를 가속화할 수 있습니다.

- **제한적인 하드웨어에서 완전한 민첩성과 양상으로**:

  - 단순한 그리퍼를 넘어 **민첩한 손**의 통합이 필요하며, 시각, 언어, 액션 외에 **촉각, 청각, 후각** 등 더 광범위한 감각 양상을 활용하여 일반 목적 에이전트를 개발해야 합니다.

- **능력 중심에서 안전 인식으로**:

  - VLA 모델은 디지털 AI 시스템의 안전 문제를 상속할 뿐만 아니라, 실제 세계와의 상호작용으로 인한 물리적 손상 및 인간에게 잠재적 해를 끼칠 수 있으므로, **안전을 최우선적으로 고려**하는 알고리즘 설계가 필수적입니다.

- **데이터 부족에서 데이터 확장성으로**:
  - 로봇 데이터는 수량, 양상 커버리지, 교차 구현 호환성, 품질 등 여러 면에서 심각하게 부족합니다.
  - 시뮬레이션 및 인터넷 규모 자원을 활용하여 확장 가능한 감독을 제공하고, 더 다재다능하고 신뢰할 수 있으며, 다중 양상을 지원하는 **실세계 데이터 수집 시스템** 개발이 필수적입니다.

결론적으로, VLA 모델의 발전은 액션 토큰의 전략적 통합, 에이전트 아키텍처로의 전환, 강화 학습의 채택, 하드웨어 및 데이터의 발전, 그리고 안전에 대한 강조를 통해 이루어질 것입니다.

## 📌 TL;DR

이 서베이 논문은 비전-언어-액션(VLA) 모델 연구의 급속한 발전을 체계적으로 분석하기 위해 '액션 토큰화(action tokenization)'라는 새로운 관점을 제시한다. 논문은 비전 및 언어 입력이 VLA 모듈을 거쳐 실행 가능한 액션으로 이어지는 '액션 토큰' 체인을 생성하는 통합 프레임워크를 제안하고, 기존 VLA 모델들이 사용하는 8가지 주요 액션 토큰 유형(언어 설명, 코드, 어포던스, 궤적, 목표 상태, 잠재 표현, 원시 액션, 추론)을 분류한다. 각 토큰 유형의 장점과 한계를 심층적으로 분석하며, 미래 VLA 모델은 단일 토큰이 아닌 계층적 아키텍처 내에서 여러 토큰 유형을 전략적으로 조합할 것으로 예측한다. 또한, VLA 모델이 단순히 모델에서 '기억, 계획, 반성'을 갖춘 'VLA 에이전트'로 진화해야 하며, 모방 학습에서 강화 학습으로 전환하고, 더 다양하고 민첩한 하드웨어 및 데이터 확장성을 확보해야 함을 강조한다. 궁극적으로, 안전과 인간 정렬을 최우선으로 하여 일반 목적의 실제 세계 지능을 향한 VLA 연구의 미래 방향을 제시한다.
