# SmolVLA: An affordable and efficient vision-language-action model for robotics

Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, Remi Cadene

## 🧩 Problem to Solve
기존의 비전-언어-액션 (VLA) 모델은 대부분 수십억 개의 파라미터를 가진 대규모 모델이며, 이는 높은 훈련 비용과 제한적인 실제 환경 배포 가능성으로 이어진다는 문제가 있습니다. 또한, 이러한 모델들은 주로 학술 및 산업 데이터셋에 의존하여, 저렴한 로봇 플랫폼에서 커뮤니티가 수집한 데이터의 활용이 미흡한 실정입니다. 이 연구는 대규모 VLA 모델의 높은 비용과 접근성 한계를 해결하고, 효율적이면서도 경쟁력 있는 성능을 유지하는 소형 VLA 모델을 개발하는 것을 목표로 합니다.

## ✨ Key Contributions
*   **경량 아키텍처 (Lightweight architecture):** 단일 GPU 훈련 및 일반 소비자용 GPU 또는 CPU 배포에 최적화된 작고 효율적인 SmolVLA 아키텍처를 제안합니다. 이는 VLM 레이어 생략, 최소한의 시각 토큰 사용, 소형 VLM 활용, 교차-자기 어텐션 레이어 인터리빙 등의 설계 선택을 포함합니다.
*   **커뮤니티 기반 데이터셋 사전 학습 (Pretraining on community-driven datasets):** 이전 연구보다 훨씬 적은 약 3만 에피소드 미만의 공개 커뮤니티 기여 데이터셋으로 모델을 엔드투엔드 훈련시켜 강력한 성능을 보여줍니다.
*   **비동기 추론 스택 (Asynchronous inference stack):** 액션 실행과 인지 및 액션 예측을 분리하는 최적화된 비동기 추론 스택을 도입하여 지연 시간을 줄이고 더 높은 제어 속도를 가능하게 합니다.
*   **경쟁력 있는 성능:** 크기가 10배 이상 큰 기존 VLA 모델과 비등하거나 더 나은 성능을 달성합니다.
*   **오픈 소스 공개:** 모든 코드, 사전 학습된 모델 및 훈련 데이터를 공개하여 연구 커뮤니티의 접근성을 높입니다.

## 📎 Related Works
*   **비전-언어 모델 (VLMs):** 이미지와 텍스트를 모두 처리하여 시각적 컨텍스트에 기반한 텍스트를 생성하도록 설계된 모델들입니다. LLM의 성공에 힘입어 사전 학습된 비전 인코더와 LLM을 통합하는 방식으로 발전했습니다. 효율성을 위해 소형 모델 훈련, 작은 데이터셋 사용 또는 파라미터 미세 조정에 대한 연구도 진행되었습니다.
*   **비전-언어-액션 모델 (VLAs):** 로봇 제어를 위한 일반화된 정책을 개발하기 위해 자연어 명령, 시각 관찰, 고유 수용성 입력을 처리하여 제어 동작을 출력하는 모델입니다. Octo, RT-1과 같은 초기 노력은 로봇 데이터셋으로 트랜스포머를 훈련했으며, RT-2는 사전 학습된 VLM을 활용했습니다. OpenVLA, $\pi_0$, DexVLA는 개방성과 연속적인 액션 생성을 위한 확산 기반 디코더를 탐구했습니다. TinyVLA는 경량 모델 훈련에 초점을 맞췄지만 대규모 로봇 데이터 사전 학습의 부재가 한계로 지적됩니다. SmolVLA는 이러한 VLA 모델의 효율성과 성능 개선에 기여합니다.

## 🛠️ Methodology
SmolVLA는 경량 VLM과 플로우 매칭으로 훈련된 액션 전문가(action expert)로 구성됩니다. 여러 이미지와 언어 지침이 주어지면, 모델은 액션 덩어리($A_t = (a_t, ..., a_{t+n})$)를 출력합니다.

1.  **모델 아키텍처:**
    *   **비전-언어 모델 (VLM):** 로봇 환경 인지를 위한 SmolVLM-2(효율적인 멀티-이미지 및 비디오 입력에 최적화된 모델)를 활용합니다.
        *   **시각 토큰 감소:** 이미지 타일링 대신 전역 이미지만 사용하고 픽셀 셔플링을 통해 프레임당 시각 토큰 수를 64개로 제한합니다.
        *   **레이어 스킵을 통한 빠른 추론:** VLM의 전체 $L$ 레이어 대신 첫 $N$개 레이어(실험적으로 $N=L/2$가 속도와 성능 사이의 좋은 균형 제공)의 특징을 사용하며, 이는 LLM 및 액션 전문가의 계산 비용을 절반으로 줄입니다.
    *   **액션 전문가 ($v_\theta$):** VLM 특징을 조건으로 액션 덩어리를 예측하도록 훈련된 트랜스포머 아키텍처입니다.
        *   **인터리빙된 교차 및 인과적 자기 어텐션 레이어:** 기존 VLA 모델과 달리, 각 블록에 교차 어텐션(CA) 또는 자기 어텐션(SA) 레이어를 번갈아 사용하여 VLM 특징과 액션 토큰 간의 상호작용을 촉진합니다. SA 레이어에는 인과적 어텐션 마스크를 적용하여 이전 액션 토큰에만 주의를 기울이도록 합니다.
        *   **플로우 매칭 학습:** 다음 목적 함수를 사용하여 훈련됩니다.
            $$L_{\tau}(\theta)=E_{p(A_t|o_t),q(A_{\tau t}|A_t)}[\|v_{\theta}(A_{\tau t},o_t)-u(A_{\tau t}|A_t)\|^2]$$
            여기서 $o_t$는 VLM 특징, $A_{\tau t} = \tau A_t + (1-\tau)\epsilon$이며, $u(A_{\tau t}|A_t)=\epsilon-A_t$입니다.
    *   **상태, 액션 및 특징 프로젝터:** 선형 프로젝션 레이어를 사용하여 상태, 액션 및 VLM 특징의 차원을 조정합니다.
2.  **커뮤니티 데이터셋 사전 학습:**
    *   **데이터 출처:** Hugging Face에서 481개의 커뮤니티 데이터셋 하위 집합을 사용하여 약 22.9K 에피소드, 10.6M 프레임으로 모델을 사전 학습합니다. 이는 이종 로봇 형태, 센서, 제어 방식, 작업 등을 포괄합니다.
    *   **작업 주석 정규화:** VLM (Qwen2.5-VL-3B-Instruct)을 사용하여 모호하거나 누락된 작업 설명을 자동으로 생성합니다.
    *   **카메라 시점 정규화:** 다양한 카메라 명명 규칙을 'OBS_IMAGE_1', 'OBS_IMAGE_2', 'OBS_IMAGE_3'와 같은 표준화된 시점(상단, 손목, 측면)으로 수동 매핑합니다.
3.  **비동기 추론 (Asynchronous Inference):**
    *   로봇 제어 루프가 이전 액션 덩어리를 소비하는 동안 새로운 액션 덩어리 예측을 트리거하여 액션 실행과 액션 예측을 분리합니다.
    *   큐에 남은 액션 수가 임계값 $g$ 미만일 때 새로운 관찰 $o_{t+1}$을 캡처하고 정책 서버에 보냅니다.
    *   관찰 유사성 필터를 사용하여 중복된 서버 호출을 방지하며, 큐가 비어있는 경우 유사성과 관계없이 최신 관찰을 처리합니다.
    *   이를 통해 로봇이 새로운 액션 덩어리를 기다리는 동안 발생하는 유휴 시간을 줄이고 반응성을 향상시킵니다.

## 📊 Results
*   **시뮬레이션 평가 (LIBERO 및 Meta-World):**
    *   SmolVLA (0.45B 파라미터)는 LIBERO 벤치마크에서 평균 87.3%의 성공률을 달성하여 Octo (0.09B, 75.1%), OpenVLA (7B, 76.5%), $\pi_0$ (3.3B, 86.0%)와 같은 다른 VLA 기반 접근 방식보다 우수하거나 경쟁력 있는 성능을 보입니다.
    *   Meta-World 벤치마크에서도 SmolVLA (0.45B)는 평균 57.3%의 성공률을 기록하여 TinyVLA (31.6%), $\pi_0$ (3.5B, 47.9%)보다 뛰어난 성능을 보였습니다.
    *   SmolVLA는 훈련 속도가 40% 빠르고, 메모리를 6배 적게 소모합니다.
*   **실제 환경 평가 (SO100 및 SO101):**
    *   **SO100 (Pick-Place, Stacking, Sorting):** 멀티태스크 훈련에서 SmolVLA (0.45B)는 평균 78.3%의 성공률을 달성하여 ACT (48.3%), $\pi_0$ (3.5B, 61.7%)를 능가합니다.
    *   **SO101 (Pick-Place-Lego):** in-distribution (90%) 및 out-of-distribution (50%) 설정 모두에서 ACT (70%, 40%)보다 뛰어난 성능을 보였습니다.
*   **사전 학습 및 멀티태스크 학습의 효과:** 커뮤니티 데이터셋 사전 학습은 실제 환경 성능을 40%에서 78.3%로 크게 향상시켰고, 멀티태스크 미세 조정은 추가적인 성능 향상을 가져왔습니다.
*   **비동기 추론:**
    *   비동기 추론은 동기식 추론과 비슷한 성공률을 달성하면서도 작업 완료 시간을 약 30% 단축했습니다 (평균 13.75초에서 9.7초로).
    *   정해진 시간 내에 더 많은 작업을 완료할 수 있었으며 (동기식 9회 vs. 비동기식 19회), 환경 변화에 대한 반응성과 적응성이 향상되었습니다.
*   **제거 연구 (Ablation Study):**
    *   **CA vs. SA:** 교차 어텐션(CA)과 자기 어텐션(SA)의 인터리빙이 가장 좋은 결과를 제공했습니다.
    *   **인과적 vs. 양방향 어텐션:** 인과적 자기 어텐션이 양방향 어텐션보다 성능이 좋았으며, 미래 액션 누출을 방지하는 것이 중요함을 보여줍니다.
    *   **초기 LLM 레이어 사용:** VLM 전체 레이어 대신 첫 $N=L/2$ 레이어를 사용하는 것이 성능과 계산 비용 사이의 좋은 균형을 제공합니다.
    *   **액션 전문가 용량:** VLM 차원 $d$ 대비 0.75배의 은닉 크기를 사용하는 것이 성능과 효율성 사이의 좋은 균형을 이룹니다.
    *   **플로우 매칭 vs. 회귀:** 플로우 매칭이 복잡한 멀티모달 액션 분포 모델링에 더 나은 귀납적 편향을 제공하여 회귀보다 훨씬 우수했습니다.
    *   **상태 정보 위치:** 센서 모터 상태를 VLM에 제공하는 것이 액션 전문가에게 직접 전달하는 것보다 성능이 훨씬 좋았습니다.
    *   **액션 덩어리 크기 ($n$):** 10에서 50 사이의 덩어리 크기가 로봇 반응성과 효율성 사이의 좋은 균형을 제공합니다.
    *   **관찰 업데이트 빈도:** 관찰을 더 자주 업데이트하는 것이 성공률을 크게 향상시켰으며, 추론 속도와 제어 정확도 간의 상충 관계를 보여줍니다.

## 🧠 Insights & Discussion
SmolVLA는 경량화된 VLA 모델임에도 불구하고 기존의 훨씬 더 큰 VLA 모델들과 경쟁할 만한 성능을 보여주어, 저렴한 하드웨어에서 로봇 제어를 가능하게 합니다. 이는 효율적인 아키텍처 설계와 비동기 추론 스택 덕분입니다. 비동기 추론 스택은 모델에 구애받지 않고 액션 덩어리를 출력하는 어떤 정책에도 통합될 수 있어, 실제 로봇 시스템의 반응성과 효율성을 크게 향상시킬 수 있는 잠재력을 가지고 있습니다. 이 연구는 모델 아키텍처를 더욱 개선하기 위한 실용적인 지침을 제공하며, 모든 코드를 오픈 소스로 공개하여 재현성을 높였습니다.

**한계점:**
*   **데이터셋 다양성 및 교차-로봇 훈련:** 현재 사전 훈련 데이터셋이 단일 로봇 유형(SO100)에 집중되어 있어, 다양한 로봇 형태의 데이터를 통합하면 새로운 플랫폼으로의 일반화 능력을 향상시킬 수 있습니다.
*   **데이터셋 크기 및 확장성:** 사용된 23K 에피소드는 기존 VLA 훈련 데이터셋보다 훨씬 작습니다. 데이터셋 크기를 늘리면 성능과 일반화가 크게 향상될 수 있습니다.
*   **모델 크기 및 하드웨어 효율성:** SmolVLA의 효율성은 장점이지만, 속도나 접근성을 희생하지 않고 아키텍처를 더 확장하는 방법 모색이 필요합니다.
*   **VLM 백본 선택:** 문서 읽기 및 OCR 작업에 주로 사전 훈련된 VLM 백본(SmolVLM-2)을 사용했는데, 로봇 상호작용에 최적인지는 불분명하며, 더 특화된 사전 훈련 전략을 탐색할 수 있습니다.
*   **멀티모달 및 로봇 데이터 공동 훈련:** 로봇 특화 데이터와 광범위한 멀티모달 데이터의 공동 훈련은 일반화 및 지침 수행 능력을 향상시킬 수 있습니다.
*   **작업 복잡성 및 장기 계획:** 현재 SmolVLA는 비교적 간단하고 짧은 시간 범위의 작업에서 효과적이지만, 계층적 정책이나 다단계 계획 메커니즘을 통합하여 장기 계획 문제를 해결하는 것이 중요합니다.
*   **학습 패러다임:** 모방 학습에 주로 의존하고 있으나, 강화 학습 기법을 탐색하여 성능 이점과 더 능숙한 정책 적응을 얻을 수 있습니다.

## 📌 TL;DR
SmolVLA는 대규모 VLA 모델의 높은 비용과 접근성 문제를 해결하기 위해, 경량 아키텍처와 커뮤니티 데이터셋으로 훈련된 소형(0.45B 파라미터) 비전-언어-액션 모델입니다. VLM 레이어 스킵, 효율적인 시각 토큰 처리, 플로우 매칭 기반의 액션 전문가(인터리빙된 CA+SA)를 통해 단일 GPU 훈련 및 CPU 배포가 가능하며, 비동기 추론 스택으로 로봇의 반응성과 효율성을 극대화합니다. 실제 환경 및 시뮬레이션 벤치마크에서 크기가 10배 이상 큰 모델들과 경쟁하거나 능가하는 성능을 보이면서도, 훈련 비용과 메모리 사용량을 크게 절감하는 것이 핵심 기여입니다.