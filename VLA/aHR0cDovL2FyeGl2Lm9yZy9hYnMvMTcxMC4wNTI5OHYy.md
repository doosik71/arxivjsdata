# Text2Action: Generative Adversarial Synthesis from Language to Action

Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and Songhwai Oh

## 🧩 Problem to Solve

이 논문은 자연어 문장 설명으로부터 인간의 행동 시퀀스를 생성하는 생성 모델을 개발하는 것을 목표로 합니다. 특히, 로봇이나 가상 에이전트가 주어진 언어 설명에 따라 행동을 수행할 수 있도록 언어와 인간 행동 간의 관계를 학습하는 것이 핵심 문제입니다. 기존 연구들은 행동 생성의 다양성이나 사실성 측면에서 한계가 있었습니다.

## ✨ Key Contributions

- **GAN 기반의 행동 생성 모델 제안**: Sequence-to-Sequence (SEQ2SEQ) 모델을 기반으로 하는 생성적 적대 신경망(GAN)인 Text2Action을 제안하여 언어 설명으로부터 인간 행동 시퀀스를 생성합니다.
- **다양하고 사실적인 행동 생성**: GAN의 이점을 활용하여 동일한 입력 문장에 대해 다양한 랜덤 노이즈 벡터를 통해 여러 가지의 사람과 유사한(human-like) 행동을 생성할 수 있음을 입증했습니다. 이는 이전 SEQ2SEQ 모델의 한계(예: 대칭적이고 덜 역동적인 행동)를 극복합니다.
- **실제 데이터 기반 학습**: 대규모 MSR-Video-to-Text (MSR-VTT) 데이터셋에서 29,770쌍의 행동-문장 주석을 추출하여 3D 상체 포즈 데이터셋을 구축하고 모델 학습에 사용했습니다.
- **로봇으로의 전이 가능성 입증**: 생성된 3D 행동 시퀀스를 Baxter 로봇에 적용하여 로봇이 주어진 문장 설명에 따라 행동을 수행할 수 있음을 시연했습니다.
- **효과적인 학습 전략**: 전체 네트워크를 엔드투엔드로 학습시키는 대신, 자연어와 행동 간의 관계를 학습하는 오토인코더를 먼저 사전 학습시킨 후, 이 인코더를 고정하여 GAN을 학습시키는 방식을 사용했습니다.

## 📎 Related Works

- **데이터셋**:
  - **[2], [3]**: 인간 전신 동작과 해당 단어/문장 주석을 제공하는 기존 데이터셋. 본 연구에서는 MSR-VTT [14]를 사용.
  - **[14] MSR-Video-to-Text (MSR-VTT)**: 웹 비디오 클립과 주석 텍스트를 포함하는 대규모 비디오 데이터셋.
- **언어-행동 매핑 학습**:
  - **[4]**: HMMs [6]를 사용하여 동작 원시 요소를 인코딩하고 단어와 연관시키는 방식.
  - **[5]**: SEQ2SEQ 모델 [7]을 사용하여 자연어와 인간 행동 간의 관계를 학습하는 방식 (본 논문에서 비교 대상).
- **기반 기술**:
  - **[6] Hidden Markov Models (HMMs)**: 순차 데이터 모델링에 사용.
  - **[7] Sequence to Sequence (SEQ2SEQ) 모델**: 입력 시퀀스를 출력 시퀀스로 변환하는 모델.
  - **[8] Generative Adversarial Networks (GANs)**: 생성자(Generator)와 판별자(Discriminator)가 경쟁적으로 학습하며 사실적인 데이터를 생성하는 모델.
  - **[12] Long Short-Term Memory (LSTM)**: RNN의 한 종류로, 장기 의존성 문제를 해결.
  - **[13] Attention Mechanism**: SEQ2SEQ 모델에서 입력 시퀀스의 관련 부분에 집중하여 출력 생성.
  - **[15] Convolutional Pose Machine (CPM)**: 이미지에서 2D 인간 포즈를 추출.
  - **[16] 2D to 3D Pose Conversion**: 2D 포즈를 3D 포즈로 변환.
  - **[17] Word2vec**: 단어 임베딩을 학습.

## 🛠️ Methodology

본 논문에서 제안하는 Text2Action 네트워크는 SEQ2SEQ 모델 기반의 GAN으로 구성됩니다.

1. **네트워크 구성**:
   - **텍스트 인코더 (Text Encoder, $E$)**: RNN (LSTM 셀) 기반으로, 입력 문장의 단어 임베딩 시퀀스 $e = \{e_1, \dots, e_{T_i}\}$를 은닉 상태 $h = \{h_1, \dots, h_{T_i}\}$로 인코딩합니다.
   - **생성자 (Generator, $G$)**: $E$의 은닉 상태 $h$를 어텐션 메커니즘 [13]을 사용하여 언어 특징 벡터 $c = \{c_1, \dots, c_{T_o}\}$로 디코딩합니다. $G$는 이 $c$와 랜덤 노이즈 벡터 $z = \{z_1, \dots, z_{T_o}\}$를 입력으로 받아 인간 행동 시퀀스 $x = \{x_1, \dots, x_{T_o}\}$를 합성합니다 ($G(z,c) = x$).
   - **판별자 (Discriminator, $D$)**: $E$의 은닉 상태 $h$를 어텐션 메커니즘을 사용하여 언어 특징 벡터 $c$로 디코딩합니다. $D$는 이 $c$와 행동 시퀀스 $x$를 입력으로 받아 $x$가 실제 행동인지 가짜 행동인지 ($D(x,c) \in [0,1]$) 판별합니다.
2. **학습 목표**:
   - $G$와 $D$는 미니맥스 게임을 수행합니다. $G$는 $D$를 속일 수 있는 사실적인 행동을 생성하려 하고, $D$는 $G$가 생성한 가짜 행동과 실제 행동을 구별하려 합니다.
   - 가치 함수 $V(D,G)$는 다음과 같습니다:
     $$
     \min_G \max_D V(D,G) = E_{x \sim p_{data}(x)}[\log D(x,c)] + E_{z \sim p_z(z)}[\log(1-D(G(z,c)))]
     $$
3. **구현 및 학습 상세**:
   - **데이터셋**: MSR-VTT [14]에서 추출한 비디오 클립을 사용합니다.
     - Convolutional Pose Machine (CPM) [15]으로 2D 상체 포즈를 추출.
     - 추출된 2D 포즈를 3D 포즈로 변환 [16].
     - 각 포즈는 24차원 벡터 $x_t \in R^{24}$ (목의 3D 위치 및 7개 다른 관절의 3D 벡터).
     - 데이터의 불필요한 정보(예: 옷 색깔)는 수동으로 제거하고 행동 관련 정보만 남겼습니다. 총 29,770쌍의 문장-행동 데이터 사용.
   - **사전 학습 (Pre-training)**:
     - 전체 네트워크를 엔드투엔드로 학습시키는 대신, 먼저 언어와 행동 사이의 관계를 학습하는 오토인코더를 학습시킵니다.
     - 이 오토인코더는 텍스트-액션 인코더 (텍스트를 행동으로 매핑)와 액션-텍스트 디코더 (행동을 텍스트로 재구성)로 구성됩니다.
     - 사전 학습된 오토인코더의 텍스트-액션 인코딩 부분이 제안된 네트워크의 텍스트 인코더 $E$로 사용됩니다.
     - 손실 함수 $L_a$는 행동 시퀀스 추정 손실과 단어 임베딩 재구성 손실의 합으로 정의됩니다:
       $$
       L_a(x,e) = \frac{a_1}{T_o} \sum_{t=1}^{T_o} \|x_t - \hat{x}_t\|_2^2 + \frac{a_2}{T_i} \sum_{t=1}^{T_i} \|e_t - e'_t\|_2^2
       $$
   - **GAN 학습**:
     - 오토인코더 학습 후, $E$는 고정하고, $G$의 가중치와 바이어스 일부는 사전 학습된 값으로 초기화합니다.
     - $G$와 $D$는 GAN 가치 함수 (1)를 사용하여 학습됩니다. 이 과정에서 $E$는 학습되지 않아 사전 학습된 언어 정보가 손상되는 것을 방지합니다.

## 📊 Results

- **다양한 3D 행동 생성**: 고정된 문장('A girl is dancing to the hip hop beat')과 다른 랜덤 노이즈 벡터($z_1, z_2, z_3$)를 입력으로 주었을 때, 같은 춤 동작이지만 미묘하게 다른 다양하고 자연스러운 춤 동작들이 생성되었습니다. 이는 모델이 문장과 노이즈를 효과적으로 결합하여 다양한 행동을 합성할 수 있음을 보여줍니다.
- **조건부 행동 생성**: 고정된 랜덤 노이즈 벡터 $z$와 다른 문장들('A woman drinks a coffee', 'A muscular man exercises in the gym', 'A chef is cooking a meal in the kitchen')을 입력으로 주었을 때, 각 문장에 해당하는 행동(예: 오른손을 입으로 가져가는 동작, 아령을 든 듯한 움직임, 요리하고 맛보는 동작)이 생성되었습니다. 비록 구체적인 도구나 배경 정보 없이 행동만으로는 완벽한 맥락 이해가 어렵지만, 의미에 맞는 행동 시퀀스가 나타났습니다.
- **[5]와의 비교**:
  - 학습 데이터셋에 포함된 문장('Woman dancing ballet with man')에 대해 본 제안 모델은 실제 데이터와 더 유사하고 자연스러운 발레 동작을 생성했습니다. 반면, [5]의 모델은 팔을 벌리는 발레 동작을 생성했지만, 제안 모델에 비해 덜 자연스럽고 역동성이 부족했습니다.
  - 학습 데이터셋에 없는 복합 문장('A drunk woman is stumbling while lifting heavy weights')에 대해 본 모델은 술 취한 여성이 비틀거리며 무거운 것을 들어 올리는 듯한 행동을 생성하여, 유연한 언어-행동 관계 학습 능력을 보여주었습니다. [5]의 모델은 단순히 무게를 들어 올리는 동작만을 생성했습니다.
  - [5]의 모델은 데이터의 우도를 최대화하는 손실 함수를 사용하여 대칭적이고 덜 역동적인 행동을 생성하는 경향이 있는 반면, 제안 모델은 GAN을 통해 실제 데이터에 가까운 다양한 행동을 생성할 수 있었습니다.
- **Baxter 로봇으로의 적용**: 생성된 'A man is throwing something out' 문장에 해당하는 3D 행동 궤적을 Baxter 로봇에 적용하여 로봇이 물건을 던지는 듯한 동작을 성공적으로 수행함을 시연했습니다.

## 🧠 Insights & Discussion

본 논문은 GAN의 장점(다양한 데이터 생성)과 SEQ2SEQ 모델의 장점(시퀀스 매핑)을 결합하여 자연어로부터 인간 행동을 생성하는 효과적인 방법을 제시했습니다.

- **다양성과 사실성**: GAN 기반의 접근 방식이 기존의 우도 최대화 기반 모델보다 더 다양하고, 역동적이며, 실제 데이터에 가까운 행동을 생성할 수 있음을 보여주었습니다. 이는 로봇이 단순히 주어진 명령을 수행하는 것을 넘어, 보다 인간적이고 맥락에 맞는 방식으로 상호작용하는 데 중요한 발전입니다.
- **강건한 학습**: 사전 학습된 오토인코더를 활용하여 텍스트 인코더를 안정화하고, 이후 GAN을 학습하는 단계별 접근 방식은 복잡한 다중 모달 생성 모델의 학습 난이도를 완화하는 데 효과적이었습니다.
- **적용 가능성**: 생성된 3D 행동을 실제 로봇에 성공적으로 전이함으로써, 로봇이 언어 명령을 이해하고 물리적인 행동으로 변환하는 데 있어 제안 모델의 실용적인 가치를 입증했습니다.
- **한계 및 향후 연구**: 현재 모델은 상체 포즈에 중점을 두고 있으며, 행동의 완전한 맥락(예: 사물과의 상호작용, 배경 환경)을 포착하는 데는 한계가 있습니다. 향후 전신 포즈 및 환경 정보까지 고려한 모델링을 통해 보다 풍부하고 상황 인지적인 행동 생성이 가능할 것으로 보입니다. 또한, 생성된 행동의 평가 지표를 더욱 객관화하는 연구도 필요합니다.

## 📌 TL;DR

본 논문은 자연어 설명으로부터 인간 행동 시퀀스를 생성하기 위해 SEQ2SEQ 모델과 GAN을 결합한 'Text2Action' 모델을 제안합니다. MSR-VTT 데이터셋에서 추출한 3D 상체 포즈 데이터를 학습하여, 모델은 하나의 문장으로부터 다양한 (random noise 기반) 행동을 생성할 수 있으며, 이전 SEQ2SEQ 모델보다 더 자연스럽고 역동적인 행동을 합성합니다. 생성된 행동은 Baxter 로봇에 성공적으로 적용되어 언어-행동 이해를 통한 로봇 상호작용의 가능성을 보여줍니다.
