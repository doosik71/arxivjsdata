# Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models

Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang

## 🧩 Problem to Solve

Vision-Language-Action (VLA) 플로우 모델은 범용 로봇 조작에서 우수한 성능을 보이지만, 복잡한 다운스트림 작업에서는 행동 정확도가 불만족스럽습니다. 이는 모델이 모방 학습(Imitation Learning) 패러다임에만 의존하여 데이터 품질 분포에 대한 깊은 이해가 부족하기 때문입니다. 오프라인 강화 학습(RL)을 통해 데이터 품질 특성을 더 깊이 파악하려는 시도가 있었으나, 기존 VLA 플로우 모델(예: $\pi^0$)에 RL return-to-go를 도입한 ReinboT와 같은 방법은 전체 행동 궤적 분포를 벡터 필드를 통해 모델링하기 때문에 최종 행동 예측에 간접적이고 비효율적으로 영향을 미치는 한계가 있었습니다. 따라서 VLA 플로우 모델의 오프라인 RL 미세 조정을 효과적으로 수행하는 방법은 여전히 탐구되지 않은 영역입니다.

## ✨ Key Contributions

- VLA 플로우 모델을 위한 새로운 적응형 오프라인 RL 후학습 방법인 **Adaptive Reinforced Flow Matching (ARFM)**을 제안합니다. 이는 데이터 품질 분포를 적응적으로 조절할 수 있습니다.
- 적응형 스케일링 인자를 위한 이론적 최적화 목표를 수립하고, 이를 실시간으로 업데이트하는 이분법 반복 알고리즘을 도출하여 효율적인 VLA 플로우 모델 미세 조정을 가능하게 합니다.
- 광범위한 시뮬레이션 및 실제 로봇 조작 작업에서 ARFM이 최첨단 일반화 능력, 동적 교란에 대한 강건성, 소수 샷(few-shot) 및 지속적인 학습 성능을 보임을 입증합니다.

## 📎 Related Works

- **RL에서의 플로우 매칭 모델:** 확산 모델과 유사하게, 플로우 매칭(Flow Matching)은 오프라인 RL에서 좋은 결과를 보입니다. 이전 연구들은 확산 모델 기반의 행동 정책 모델링이나 에너지 유도 확산 프로세스에 오프라인 RL 목표를 적용했습니다. 최근에는 에너지 가중 플로우 매칭(Energy-Weighted Flow Matching, EWFM)이 제안되어 중간 에너지 모델 학습 없이 RL 보상을 에너지 함수로 직접 활용할 수 있게 되었습니다. 본 연구는 이 EWFM을 VLA 모델 후학습 설정으로 확장하고 새로운 적응형 에너지 가중 알고리즘을 제안합니다.
- **VLA 모델을 위한 RL:** RL은 최근 VLA 모델의 후학습에서 모방 학습의 한계를 극복하는 핵심 기술로 부상했습니다. 이는 주로 온라인 및 오프라인 RL 미세 조정으로 나뉩니다. 온라인 방법은 PPO와 같은 알고리즘으로 직접 환경과 상호작용하고, 오프라인 방법은 정적 데이터셋에서 인간 선호도나 가치 안내와 같은 다양한 신호를 도입하여 학습합니다. ReinboT와 같은 최근 연구는 RL의 핵심 아이디어를 VLA 모델 후학습에 적용하여 미래 보상 예측을 통해 행동을 안내하려 했습니다. 본 연구는 에너지 가중 VLA 플로우 모델에서 오프라인 RL 후학습을 구현하여 효율적이고 안정적인 정책 최적화를 목표로 합니다.

## 🛠️ Methodology

본 연구는 VLA 플로우 모델을 위한 효율적인 후학습 방법인 ARFM을 제안하며, 이는 Fig. 1에 요약되어 있습니다.

1. **에너지 가중 VLA 플로우 모델 구축:**
   - 스케일링 인자 $\alpha$와 return-to-go 이점(advantage) $R^*(o_t, A_t)$를 사용하여 에너지 가중 정책 분포 $\pi(A_t|o_t) \propto p(A_t|o_t) \exp(\alpha R^*(o_t, A_t))$를 정의합니다.
   - 조건부 에너지 가중 플로우 매칭(CEFM) 손실을 최적화하여 이 분포의 벡터 필드를 학습합니다. 실용적인 손실 함수는 배치 내의 데이터 샘플에 대한 에너지 가중치 $w_i(\alpha)$를 통해 계산됩니다:
     $$L_{\tau}^{1}(\theta) = \sum_{i=1}^{B} w_i(\alpha) ||v_{\theta}(\{A_t^i\}_{\tau}, o_t) - u(\{A_t^i\}_{\tau}|A_t^i)||^2$$
     여기서 $w_i(\alpha) = \frac{\exp(\alpha R^*(A_t^i, o_t))}{\sum_{j=1}^{B} \exp(\alpha R^*(A_t^j, o_t))}$는 RL 이점을 정규화하여 사용한 에너지 가중치입니다.
2. **스케일링 인자 $\alpha$의 적응형 조정:**
   - $\alpha$는 RL 신호(이점) 보존과 손실 기울기 분산 제어 사이의 균형을 맞추기 위해 각 미세 조정 단계에서 적응적으로 조정됩니다.
   - $\alpha$의 조정 방향은 다음 목적 함수 $J(\alpha)$를 최소화하는 것입니다:
     $$J(\alpha) = \text{Var}(\hat{g}(\alpha)) - \lambda S(\alpha)$$
     여기서 $\hat{g}(\alpha)$는 손실 $L_{\tau}^{1}(\theta)$의 기울기이고, $S(\alpha)$는 RL 이점의 효과를 나타내는 점수 함수입니다. $\lambda$는 RL 신호와 기울기 분산 간의 비율을 조절하는 하이퍼파라미터입니다.
   - 목적 함수 $J(\alpha)$를 해결 가능하게 만들기 위해 다음과 같은 가정을 도입합니다:
     - 가정 1: 표준화된 RL 이점 신호 $R^*(A_t, o_t)$는 가우시안 변수 $\mathcal{N}(0, \sigma_R^2)$를 따른다.
     - 가정 2: 조건부 플로우 매칭(CFM) 손실 $L_i^{\text{CFM}}$은 가우시안 변수 $\mathcal{N}(\mu_L, \sigma_L^2)$를 따른다.
     - 가정 3: 배치 크기 $B$가 충분히 클 때, 표본 기반의 기댓값과 분산으로 $\alpha_L, \sigma_A, \sigma_L$을 근사할 수 있다.
3. **$\alpha$의 최적화 목표 해결 및 알고리즘:**
   - 위 가정을 바탕으로 $J(\alpha)$는 $J(\alpha) = \sigma_L^2[e^{2\alpha^2\sigma_R^2} - e^{\alpha^2\sigma_R^2}] - \lambda \alpha \sigma_R^2$로 표현됩니다.
   - $J(\alpha)$를 최소화하는 최적의 $\alpha^*$는 다음 비선형 방정식을 통해 계산됩니다:
     $$4\sqrt{x^*} e^{2x^*} - 2\sqrt{x^*} e^{x^*} - \frac{\lambda \sigma_R}{\sigma_L^2} = 0, \quad \alpha^* = \frac{\sqrt{x^*}}{\sigma_R}$$
   - 이 방정식은 이분법 반복 알고리즘 (Algorithm 1)을 사용하여 실시간으로 효율적으로 해결될 수 있습니다.
   - Algorithm 2 (ARFM)는 최적의 $\alpha^*$를 구하고 이를 에너지 가중 플로우 매칭 손실에 적용하여 VLA 플로우 모델을 후학습하는 전체 과정을 설명합니다.

## 📊 Results

- **다중 작업 학습 (LIBERO):** ARFM은 LIBERO 벤치마크의 네 가지 스위트(Object, Long, Spatial, Goal)에서 평균 92.1%의 가장 높은 성공률을 달성했습니다. 이는 기준선 $\pi^0$보다 4.5% 높고, ReinboT(91.2%) 및 RWR(90.8%)보다도 우수합니다.
- **행동 교란 설정:** 모델 평가 중 행동에 다양한 수준의 가우시안 노이즈(0.1~0.3)를 추가했을 때, ARFM은 평균 48.2%의 성공률로 가장 높은 강건성을 보였습니다. 이는 $\pi^0$(43.3%)보다 11.4% 높습니다.
- **소수 샷 학습 (LIBERO-Long):** 데이터가 부족한 시나리오에서 ARFM은 LIBERO-Long 스위트에서 가장 좋은 성능(36.5%)을 보였습니다. 이는 ReinboT(33.9%) 및 RWR(34.6%)을 능가합니다.
- **지속적인 학습 (LIBERO):** ARFM은 지속적인 학습 설정에서 최종 평균 성공률을 10.5% 향상(61.0%)시켰을 뿐만 아니라, NBT(Negative Backward Transfer) 지표를 38.0% 감소(4.7%)시켜 치명적인 망각을 효과적으로 완화했습니다.
- **실험 설정 연구 (Ablation Study):** ARFM의 성능은 최적화 목표 하이퍼파라미터 $\lambda$에 둔감한 것으로 나타났으며, 이분법 반복 횟수 $M$은 10회 이상에서 안정적인 성능을 보였습니다.
- **실제 로봇 실험 (UR5):** 외부 교란이 있는 실제 집게 및 배치 작업에서 ARFM은 가장 우수한 성능을 달성했으며, $\pi^0$ 대비 교란 저항 능력이 크게 향상되었습니다.

## 🧠 Insights & Discussion

본 연구는 VLA 플로우 모델의 후학습 과정에서 RL 신호와 기울기 분산 사이의 중요한 균형을 이론적으로 정립하고, 이를 통해 모델의 성능과 안정성을 크게 향상시키는 효과적인 방법을 제시했습니다. ARFM은 적응형 스케일링 인자 $\alpha$를 도입하여 고품질 데이터 샘플의 RL 이점을 최대한 활용하면서도, 과도한 가중치로 인한 기울기 폭주를 방지하여 학습 과정을 안정화합니다. 이러한 접근 방식은 VLA 플로우 모델이 더 정확하고 강건한 행동을 학습할 수 있도록 합니다.

ARFM의 성공은 특히 데이터 희소성(few-shot) 및 지속적인 학습 시나리오에서 데이터 활용 효율성을 높이고 치명적인 망각(catastrophic forgetting)을 완화하는 데 중요한 역할을 한다는 점에서 의의가 있습니다. 이는 VLA 모델이 실제 세계의 복잡하고 변화하는 환경에 적응하는 데 필수적인 능력입니다.

주요 제한점은 현재 ARFM이 오프라인 RL 후학습에 초점을 맞추고 있다는 점입니다. 향후 연구 방향으로는 환경과의 상호작용을 통해 새로운 시나리오에 효율적으로 적응하는 온라인 RL 후학습 방법을 탐색하는 것이 유망합니다.

## 📌 TL;DR

VLA 플로우 모델의 행동 정확도와 안정성 문제를 해결하기 위해, 본 논문은 RL 이점 신호와 손실 기울기 분산 간의 균형을 적응적으로 조절하는 **Adaptive Reinforced Flow Matching (ARFM)** 방법을 제안합니다. ARFM은 적응형 스케일링 인자 $\alpha$를 손실 함수에 도입하고, 이 $\alpha$를 최적화하는 비선형 방정식을 이분법 반복으로 실시간 해결하여 VLA 플로우 모델을 미세 조정합니다. 시뮬레이션 및 실제 로봇 실험을 통해 ARFM이 뛰어난 일반화 능력, 동적 교란에 대한 강건성, 그리고 소수 샷 및 지속적인 학습 성능을 보이며 기존 최첨단 모델들을 능가함을 입증했습니다.
