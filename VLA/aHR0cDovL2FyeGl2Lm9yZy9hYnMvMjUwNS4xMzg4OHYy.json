{
  "url": "http://arxiv.org/abs/2505.13888v2",
  "title": "InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning",
  "authors": "Ji Zhang, Shihan Wu, Xu Luo, Hao Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song",
  "year": 2025,
  "abstract": "Leveraging pretrained Vision-Language Models (VLMs) to map language\ninstruction and visual observations to raw low-level actions,\nVision-Language-Action models (VLAs) hold great promise for achieving\ngeneral-purpose robotic systems. Despite their advancements, existing VLAs tend\nto spuriously correlate task-irrelevant visual features with actions, limiting\ntheir generalization capacity beyond the training data. To tackle this\nchallenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet\neffective approach that mitigates the adverse effects of spurious correlations\nby boosting the spatial reasoning ability of VLAs. Specifically, InSpire\nredirects the VLA's attention to task-relevant factors by prepending the\nquestion \"In which direction is the [object] relative to the robot?\" to the\nlanguage instruction and aligning the answer\n\"right/left/up/down/front/back/grasped\" and predicted actions with the\nground-truth. Notably, InSpire can be used as a plugin to enhance existing\nautoregressive VLAs, requiring no extra training data or interaction with other\nlarge models. Extensive experimental results in both simulation and real-world\nenvironments demonstrate the effectiveness and flexibility of our approach. Our\ncode, pretrained models and demos are publicly available at:\nhttps://Koorye.github.io/proj/Inspire.",
  "citation": 2
}