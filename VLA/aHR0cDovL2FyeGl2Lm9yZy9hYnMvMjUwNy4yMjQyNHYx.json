{
  "title": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with\n  Relaxed Acceptance",
  "authors": "Songsheng Wang, Rucheng Yu, Zhihang Yuan, Chao Yu, Feng Gao, Yu Wang, Derek F. Wong",
  "year": 2025,
  "url": "http://arxiv.org/abs/2507.22424v1",
  "abstract": "Vision-Language-Action (VLA) models have made substantial progress by\nleveraging the robust capabilities of Visual Language Models (VLMs). However,\nVLMs' significant parameter size and autoregressive (AR) decoding nature impose\nconsiderable computational demands on VLA models. While Speculative Decoding\n(SD) has shown efficacy in accelerating Large Language Models (LLMs) by\nincorporating efficient drafting and parallel verification, allowing multiple\ntokens to be generated in one forward pass, its application to VLA models\nremains unexplored. This work introduces Spec-VLA, an SD framework designed to\naccelerate VLA models. Due to the difficulty of the action prediction task and\nthe greedy decoding mechanism of the VLA models, the direct application of the\nadvanced SD framework to the VLA prediction task yields a minor speed\nimprovement. To boost the generation speed, we propose an effective mechanism\nto relax acceptance utilizing the relative distances represented by the action\ntokens of the VLA model. Empirical results across diverse test scenarios affirm\nthe effectiveness of the Spec-VLA framework, and further analysis substantiates\nthe impact of our proposed strategies, which enhance the acceptance length by\n44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without\ncompromising the success rate. The success of the Spec-VLA framework highlights\nthe potential for broader application of speculative execution in VLA\nprediction scenarios."
}