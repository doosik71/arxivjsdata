# Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand

Cheng Pan, Kai Junge & Josie Hughes

## 🧩 Problem to Solve

자율 능숙(dexterous) 조작을 발전시키기 위해서는 고수준의 작업 계획, 손목 궤적 생성, 상황에 맞는 파지 유형 선택 및 정밀한 손 제어가 필요합니다. 특히, 최근 개발된 VLA(Vision-Language-Action) 모델은 언어 입력을 기반으로 한 고수준 계획에 효과적이지만, 주로 1자유도(DoF) 핀치 그리퍼(pinch gripper)에서만 시연되었습니다. 복잡한 인간형 다지(multi-fingered) 조작기의 경우, 이러한 일반화된 모델이 요구되는 정밀하고 복잡한 구동 신호를 제공할 수 있을지는 불분명했습니다. 즉, VLA의 일반화 능력과 다지 조작에 필요한 정밀하고 강력한 제어 능력을 결합하는 것이 주요 문제입니다.

## ✨ Key Contributions

- VLA 모델의 일반화 가능한 고수준 계획 능력과 확산 모델(diffusion model)의 정밀하고 견고한 저수준 상호작용 능력을 결합한 하이브리드 제어 방법을 제안했습니다.
- 언어 명령 기반의 픽앤플레이스(pick-and-place) 작업에서 VLA 모델과 확산 모델 간의 이벤트 기반 전환을 가능하게 하는 스위칭 신호(switching signal)를 훈련 데이터에 통합했습니다.
- 13자유도 인간형 로봇 손인 ADAPT Hand 2를 VLA 모델로 제어한 첫 사례를 시연했습니다.
- 이 모델 전환 접근 방식은 VLA 단독 사용 시 40% 미만이던 성공률을 80% 이상으로 크게 향상시켰습니다.
- 확산 모델을 통해 다중 모드 파지(환경 맥락에 따라 파지 전략 변경) 및 오류 복구(객체를 떨어뜨리거나 미끄러졌을 때 재시도) 능력을 시연했습니다.
- 직렬 탄성 구동(series elastic actuation)을 통해 유연성(compliance)을 통합한 하드웨어(ADAPT Hand 2)가 접촉 및 환경 충돌에 대한 복원력과 안정성을 제공함을 강조했습니다.

## 📎 Related Works

- **로봇 조작:** Billard와 Kragic의 연구 [1]는 로봇 조작의 동향과 과제를 제시합니다.
- **대규모 언어 모델(LLM) 및 VLA (Vision-Language-Action) 모델:** Zeng 등 [2], Ma 등 [3]의 연구는 로봇 공학에서 언어 입력 기반의 고수준 계획에 효과적인 접근법으로 LLM과 VLA를 소개합니다.
  - open-X-embodiment [4], Droid [5], BridgeData V2 [6]와 같은 대규모 로봇 조작 데이터셋이 활용됩니다.
  - RT-1 [8], RT-2 [9], Octo [10]와 같은 복잡한 VLA 모델이 개발되었으나, 주로 1자유도 핀치 그리퍼에 적용되었습니다.
  - **openVLA [7]:** 본 연구의 VLA 모델 미세 조정에 사용된 사전 훈련된 모델입니다.
- **시각 서보 제어(Visual Servoing):** Ribeiro 등 [11]은 물체에 접근하고 파지하는 데 널리 사용되지만, 파지 전략을 위한 맥락 정보를 활용하는 데 어려움이 있습니다.
- **확산 모델(Diffusion Models):** Chi 등 [12]의 연구는 로봇 동작 학습 및 계획에 적합하며, 정밀한 제어와 맥락 정보 캡처, 직접적인 모터 제어 신호 생성이 가능합니다.
  - 조작 [13], [14] 및 다지 로봇 손 [15], [16]에 성공적으로 적용되었습니다.
- **ADAPT Hand 2:** Junge와 Hughes [17], [18]가 개발한 커스텀 빌드 인간형 로봇 손으로, 유연한 직렬 탄성 구동을 특징으로 합니다.

## 🛠️ Methodology

1. **VLA 및 확산 모델 전환 프레임워크:**
   - **고수준 계획 (VLA):** 사전 훈련된 openVLA [7] 모델을 미세 조정하여 언어 조건부 로봇 팔 및 손 위치를 제어합니다. 언어 입력 및 시각 피드백을 받아 로봇 팔 말단 장치의 6개 자세 값과 파지 비율(grasp percentage)을 제어하는 1개 신호를 출력합니다.
   - **저수준 파지 (확산 정책):** 확산 정책 모델 [12]을 사용하여 능숙한 파지 제어를 수행합니다. 카메라 1(cam1)과 카메라 2(cam2)의 이미지를 입력받아, 손의 복잡한 움직임을 위한 모터 제어 신호를 생성합니다.
   - **전환 메커니즘:** "이벤트 신호($\sigma$)"를 통해 두 모델 간 자동 전환을 구현합니다.
     - VLA는 객체 근처로 손을 이동시키는 팔 움직임을 생성하며, VLA의 스칼라 출력이 픽앤플레이스 시 목표 위치에 대한 근접성을 나타내는 이벤트 신호로 사용됩니다.
     - VLA의 이벤트 신호가 특정 임계값에 도달하면 확산 모델로 전환됩니다. 언어 입력에 따라 객체별 확산 정책을 선택하는 룩업 테이블(lookup table)을 사용합니다.
     - 확산 정책 훈련 시 성공적인 파지 종료를 나타내는 추가 신호가 기록되어, 성공 시 VLA로 다시 전환됩니다.
2. **로봇 설정:** ADAPT Hand 2 [17] (13자유도 인간형 손)가 UR5 로봇 팔에 부착되어 픽앤플레이스 작업을 수행합니다. 시각 데이터 수집을 위해 고정된 카메라(cam1)와 손에 부착된 카메라(cam2)를 사용합니다. 대상 객체는 붉은 피망, 테이프, 종이이며, 노란색 또는 보라색 플레이트에 놓습니다.
3. **데이터 수집:**
   - **VLA 훈련 데이터 (그림 5A):** 전체 픽앤플레이스 작업 시연을 텔레오퍼레이션(teleoperation)으로 수집합니다. 운영자가 수동으로 이벤트 신호를 기록하며, 파지 기간 전후의 동작만 VLA 훈련에 사용됩니다. 손이 목표물에 가까워질 때 공중에서 손을 의도적으로 닫는 동작을 포함하여 VLA가 "손 닫기" 신호를 학습하도록 합니다.
   - **확산 정책 훈련 데이터 (그림 5B):** 파지 동작만을 수집합니다. 손이 객체 약 5cm 위에 위치한 상태에서 텔레오퍼레이션을 통해 다양한 파지 전략(밀어 집기, 직접 집기)과 오류 복구 동작(일부러 실패하고 다시 잡기)을 시연합니다. 성공적으로 객체를 들어 올릴 때 이벤트 신호가 발생하도록 기록합니다.
4. **모델 훈련:**
   - **VLA:** openVLA [7]를 미세 조정하며, cam1 (224x144)과 cam2 (224x80)의 이미지를 수직으로 연결하여 사용합니다. 훈련 정확도가 95%를 초과하고 수렴할 때까지 훈련합니다.
   - **확산 정책:** 수집된 파지 시연 데이터로 훈련합니다. 두 카메라의 이미지를 320x240으로 리사이즈한 후 288x216으로 무작위 크롭하여 데이터 증강을 수행합니다. 1500 에포크 동안 훈련합니다.

## 📊 Results

- **VLA 단독 성능:**
  - 전체 픽앤플레이스 작업에서 VLA 단독 모델의 성공률은 피망의 경우 0.40~0.45, 테이프의 경우 0.20~0.25에 불과했습니다 (표 I). 종이와 같이 파지하기 어려운 객체는 거의 불가능했습니다. 주로 로봇 팔을 올바른 객체로 이동시키는 능력에 의해 점수가 결정되었습니다.
  - VLA의 "도달" 정밀도: 손 중심과 객체 중심 간의 오프셋이 6cm 이내였습니다 (그림 6).
  - 두 카메라(cam1 및 cam2)의 이미지를 결합하면 위치 인식 정밀도가 크게 향상되었습니다 (예: 테이프의 평균 오프셋이 12cm에서 3cm로 감소) 이는 깊이 정보를 보완해주기 때문입니다.
- **확산 모델 단독 성능:**
  - **다중 모드 파지:** 테이프 및 파란색 블록에 대해 테이블 가장자리로부터의 거리에 따라 "밀어 집기(slide & pick)" 또는 "직접 집기(direct pick)" 전략을 능숙하게 선택했습니다. 모든 위치에서 100% 성공률을 보였습니다 (그림 7).
  - **정밀도 한계:** 손과 객체 간의 초기 오프셋이 15cm를 초과하면 파지 성공률이 50% 미만으로 떨어져(그림 8), VLA가 손을 객체에 충분히 가깝게 가져다 놓는 것이 필수적임을 입증했습니다.
  - **오류 복구:** 파지 실패 시 자동으로 재시도하며, 성공적인 파지 시에만 이벤트 신호가 증가하는 능력을 보여주었습니다 (그림 9).
- **VLA 및 확산 모델 결합 성능:**
  - 모든 객체-배치 조합에서 0.8 이상의 성공 점수를 달성했습니다 (그림 10D).
  - 이는 VLA 단독 시도(피망 0.43, 테이프 0.20)와 비교할 때 **상당히 개선된 결과**이며, 픽앤플레이스 작업의 특정 기술을 목표로 두 모델을 활용하는 것의 장점을 강력하게 시사합니다.
  - 수직 운동 궤적 및 이벤트 신호(그림 10B)와 말단 장치의 x-y 평면 궤적(그림 10C)은 VLA와 확산 모델 간의 명확하고 성공적인 전환 행동을 보여주었습니다.

## 🧠 Insights & Discussion

- 이 연구는 VLA의 언어 기반 고수준 계획 능력과 확산 모델의 맥락적이고 정밀한 저수준 모터 제어 능력의 장점을 효과적으로 결합하여 능숙한 조작을 달성하는 프레임워크를 제시했습니다.
- 다지 인간형 로봇 손에 VLA 모델을 성공적으로 적용한 첫 사례로, 기존 VLA 연구의 주요 한계를 극복했습니다.
- ADAPT Hand 2의 유연한 하드웨어 설계는 접촉에 대한 복원력을 제공하여 학습 기반 제어기의 견고성을 향상시키는 데 기여했습니다.
- **한계점:**
  - 새로운 객체가 추가될 때마다 확산 모델을 개별적으로 훈련해야 하는 점이 일반화에 한계가 있습니다.
  - openVLA 모델이 1자유도 핀치 그리퍼에만 사전 훈련되었기 때문에, 다지 로봇 손에 직접 사용하기 위해서는 신중한 미세 조정과 적용이 필요합니다.
- **향후 연구 방향:**
  - 확산 모델에 대한 보다 일반화된 접근 방식을 개발하거나 기존 데이터셋을 활용하여 객체별 훈련 필요성을 줄이는 것이 필요합니다.
  - 다양한 말단 장치를 지원하기 위해 VLA 모델에 대한 근본적인 변경이나 다양한 로봇 손에 대한 대규모 데이터 수집이 탐구될 수 있습니다.
  - 학습 기반 제어기를 강화하기 위해 가변 강성(variable stiffness)을 제어 입력으로 통합하거나, 상호작용 힘 제어를 개선하기 위해 촉각 피드백을 통합하는 등 하드웨어와의 시너지를 탐색할 수 있습니다.

## 📌 TL;DR

인간형 로봇 손의 능숙한 제어를 위해 VLA(Vision-Language-Action) 모델의 언어 기반 고수준 계획 능력과 확산 모델(Diffusion Model)의 정밀한 저수준 상호작용 능력을 결합한 하이브리드 제어 프레임워크를 제안합니다. 이 프레임워크는 훈련 데이터에 통합된 이벤트 신호를 사용하여 두 모델 간의 전환을 자동화합니다. 13자유도 ADAPT Hand 2에 배포된 이 방법은 VLA 모델이 객체 근처로 정확히 이동하고 확산 모델이 다중 모드 그립 및 오류 복구 기능을 제공하여, VLA 단독 접근법(40% 미만)보다 훨씬 높은 80% 이상의 성공률을 달성합니다. 이는 VLA가 다지 로봇 손을 제어한 첫 사례이며, 정밀한 조작을 위한 일반화된 고수준 계획과 특정 객체 상호작용 간의 시너지를 입증합니다.
