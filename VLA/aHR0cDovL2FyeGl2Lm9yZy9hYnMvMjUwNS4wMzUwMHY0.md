# Task Reconstruction and Extrapolation for $\pi_0$ using Text Latent

Quanyi Li

## 🧩 Problem to Solve

최신 Vision-Language-Action (VLA) 모델들은 시연된 작업에 대해 높은 성능을 보이지만, 서로 다른 작업에서 사용된 기술들을 새로운 방식으로 재조합해야 하는 외삽(extrapolation) 작업에서는 크게 어려움을 겪습니다. 예를 들어, VLA가 "크림 치즈를 그릇에 넣기"와 "그릇을 캐비닛 위에 올리기"는 성공해도, 두 가지 기술을 결합해야 하는 "크림 치즈를 캐비닛 위에 올리기"는 실패할 수 있습니다. 이는 VLA가 단순히 시연된 궤적에 과적합하는 것인지, 아니면 더 넓은 일반화를 지원하는 조합 가능한 내부 표현을 학습하는지에 대한 의문을 제기하며, 본 연구는 이 문제를 탐구합니다.

## ✨ Key Contributions

- **텍스트 잠재 벡터 ($\text{text latent}$) 개념 도입**: 모델의 숨겨진 상태에서 파생된 작업별 벡터인 텍스트 잠재 벡터를 사용하여 작업을 완료하는 데 필요한 의미론을 인코딩하고, 이를 모델의 잔차 스트림(residual stream)에 주입하여 관련 작업 행동을 재구성할 수 있음을 보였습니다.
- **기술 조합을 통한 외삽 능력 증명**: 서로 다른 작업에서 사용된 기술들이 해당 텍스트 잠재 벡터를 혼합함으로써 새로운 행동을 생성할 수 있음을 발견했습니다.
- **$\text{libero-ood}$ 벤치마크 도입**: 표준 LIBERO 작업에서 외삽된 20개의 새로운 작업을 포함하는 $\text{libero-ood}$ 벤치마크를 제안하여, 모델이 실패하지만 잠재적으로 해결할 수 있는 OOD(Out-of-Distribution) 작업에 대한 평가 기준을 마련했습니다.
- **$\pi_0$의 외삽 성능 대폭 향상**: 제안된 텍스트 잠재 벡터 보간(Text Latent Interpolation, TLI)을 $\pi_0$에 적용하여 $\text{libero-ood}$ 벤치마크에서 성공률을 9%에서 83%로 크게 향상시켰습니다. 이는 $\pi_0$에 인코딩된 기술 표현이 개별적이지만 조합 가능하며, $\pi_0$가 이러한 표현을 자율적으로 조합하는 데 실패했음을 시사합니다.
- **VLA의 공간적 과적합 현상 발견**: 다른 최신 VLA 모델들도 $\text{libero-ood}$에서 21% 미만의 성공률을 보였으며, 추가 분석을 통해 VLA가 객체 이름을 시연된 장면에서 객체가 공간적으로 위치했던 곳과 연관시키는 '공간적 과적합(spatial overfitting)'이라는 공통적인 패턴을 보임을 밝혔습니다.

## 📎 Related Works

- **Vision-Language-Action (VLA) 모델**: 대규모 멀티모달 데이터셋으로 사전 학습된 VLM(Vision-Language Models)을 기반으로 하며, 특정 작업에 대한 미세 조정을 통해 의사 결정 능력을 얻습니다. OpenVLA, RT-2, $\pi_0$-fast와 같이 다음 토큰 예측 방식이나 회귀 헤드/확산 모델을 사용하여 액션을 디코딩합니다.
- **기계적 해석 가능성 (Mechanistic Interpretability, MI)**: 모델의 내부 연산을 역설계하여 작동 방식을 이해하고 특정 행동을 재구성하거나 활성화합니다. 초기 작업은 이미지 분류 모델에서 뉴런의 특징 감지 역할을 분석했으며, 최근 LLM 및 VLM 연구는 유도 헤드, 함수 벡터, 특정 객체를 인식하는 뉴런 등 중요한 회로를 식별했습니다. 본 연구는 VLA에 대한 MI 관점의 연구가 제한적인 상황에서 내부 표현과 VLA 행동 간의 인과 관계를 찾아 외삽 작업을 가능하게 합니다.

## 🛠️ Methodology

1. **VLA의 내부 표현 이해**:
   - VLA는 이미지, 텍스트(작업 설명), 로봇 고유 상태에서 얻은 $\text{d}$-차원 임베딩을 $L$개의 트랜스포머 레이어를 통해 처리합니다.
   - 텍스트 임베딩($e_T$)과 그 숨겨진 상태($h_T^l$)는 에피소드 내내 고정되며, 이는 작업별 스킬 표현을 인코딩한다고 가정합니다.
2. **텍스트 잠재 벡터 ($\text{text latent}$) 식별**:
   - 주어진 작업의 $\text{text latent}$는 해당 작업 시연의 모든 타임스텝에서 텍스트 토큰의 숨겨진 상태($h_T(i)$)를 모든 레이어에 걸쳐 평균하여 얻습니다.
   - 훈련 세트의 여러 시연에 걸쳐 평균을 취하여 $T = \frac{1}{\sum_{k=1}^K |B_k|} \sum_{k=1}^K \sum_{i \in B_k} h_T(i)$ 와 같이 계산합니다. 여기서 $B_k$는 $k$-번째 시연의 모든 타임스텝 인덱스입니다.
3. **텍스트 잠재 벡터 보간 (Text Latent Interpolation, TLI)**:
   - 두 기본 작업에서 파생된 외삽 작업을 위해, 두 작업의 $\text{text latent}$($T_1$, $T_2$)를 선형 보간하여 스킬을 조합합니다.
   - 보간된 $\text{text latent}$는 모델의 잔차 스트림($h_T(i)$)에 주입됩니다:
     $$h_T(i) = h_T(i) + ((1-\alpha)T_1 + \alpha T_2) - ((1-\alpha)T_2 + \alpha T_1)$$
     여기서 $\alpha = i/\lambda$ (타임스텝 $i$와 전환 속도 $\lambda$).
   - 이는 에피소드 초반에는 작업 1의 행동을 유도하고, 에피소드가 진행됨에 따라 작업 2의 행동으로 부드럽게 전환되도록 모델을 안내합니다.
4. **텍스트 임베딩 보간 (Text Embedding Interpolation, TEI)**:
   - 대안적으로, 두 기본 작업 프롬프트의 텍스트 임베딩($e_T^1$, $e_T^2$)을 타임스텝 $i$에서 선형 보간하여 연속적인 전환을 근사화합니다:
     $$e_T = e_T(i) = (1-\alpha)e_T^1 + \alpha e_T^2$$
   - TLI와 TEI는 독립적으로 또는 함께 적용될 수 있습니다.

## 📊 Results

- **작업 재구성 ($\pi_0$)**:
  - 원본 프롬프트 없이도, 텍스트 잠재 벡터를 모델의 잔차 스트림에 주입함으로써 LIBERO의 표준 작업들을 80% 이상의 성공률로 재구성할 수 있었습니다 (Blank Prompt + T: $\text{libero-object}$ 0.94, $\text{libero-goal}$ 0.82, $\text{libero-spatial}$ 0.81). 이는 텍스트 잠재 벡터가 필수적인 작업 지식을 포착하고 있음을 확인시켜 줍니다.
  - 텍스트 잠재 벡터를 언임베딩하여 생성된 대체 프롬프트는 읽기 어렵지만, 약 70%의 성공률로 작업을 완료할 수 있었습니다. 이는 비공개 지시나 백도어 공격 가능성을 시사합니다.
- **작업 외삽 ($\text{libero-ood}$ 벤치마크)**:
  - $\pi_0$는 $\text{libero-ood}$에서 기본적으로 9%의 낮은 성공률을 보였습니다.
  - TLI를 $\pi_0$에 적용하자, 성공률이 83%로 크게 향상되었습니다. 이는 $\pi_0$가 새로운 작업을 해결하기 위해 재구성될 수 있는 개별적인 스킬 표현을 학습했음을 증명합니다.
  - 다른 최신 VLA(UniVLA, openvla-oft, $\pi_0$-fast)들은 $\text{libero-ood}$에서 21%를 넘는 성공률을 달성하지 못했습니다.
  - TLI\* (빈 프롬프트 사용)는 TLI보다 성능이 저하되었고, 프롬프트 전환($\pi_S^0$)은 TLI보다 낮은 69%의 성공률을 보였습니다.
- **공간적 과적합 (Spatial Overfitting)**:
  - $\text{libero-goal-ood}$의 일부 작업에서 $\pi_0$-TLI$^+$는 새로운 객체가 나타났음에도 불구하고 훈련 데이터에서 해당 객체가 위치했던 고정된 공간적 위치로 로봇 팔을 움직이는 '공간적 과적합' 현상을 보였습니다. 이는 $\pi_0$가 객체 정체성을 이해하기보다 객체 이름을 고정된 위치에 매핑하고 있음을 나타냅니다.
  - 이러한 공간적 과적합은 $\pi_0$ 외의 다른 VLA에서도 흔히 발견되었으며, $\text{libero-object}$ 스위트에서 잘못된 프롬프트로도 높은 성공률을 유지하거나, 객체가 훈련된 위치가 아닌 곳에 있을 때 실패하는 것으로 확인되었습니다.

## 🧠 Insights & Discussion

- **잠재된 외삽 능력**: VLA, 특히 $\pi_0$는 시연된 궤적에 과적합하는 경향이 있지만, 텍스트 잠재 벡터에 인코딩된 개별적이고 조합 가능한 스킬 표현을 통해 새로운 외삽 작업을 해결할 잠재력을 가지고 있습니다. 이 능력은 모델이 자율적으로 활용하지 못하는 "잠금 해제되지 않은" 상태로 존재합니다.
- **$\text{libero-ood}$의 유효성**: $\text{libero-ood}$ 벤치마크는 VLA가 잠재적으로 해결할 수 있지만 현재는 실패하는 OOD 작업을 포함하도록 설계되었으며, TLI를 통해 $\pi_0$의 성공률이 크게 증가함으로써 그 유효성이 입증되었습니다. 이 벤치마크는 모델의 고유한 외삽 능력을 개발하기 위한 훈련 방식을 모색하는 데 중요합니다.
- **VLA의 근본적인 한계**: $\pi_0$를 포함한 다른 최신 VLA들이 $\text{libero-ood}$에서 낮은 성능을 보인 것은 학습된 표현을 새로운 기술로 재조합하는 능력의 부족 또는 훈련 중 스킬 표현이 제대로 생성되지 않았다는 VLA의 공통된 제한점을 시사합니다.
- **객체 및 목표 이해 부족**: VLA는 진정한 객체 인식이나 목표 이해보다는 시연된 장면에서 객체의 공간적 위치를 기억하는 '공간적 과적합' 현상을 보입니다. 이는 로봇이 특정 객체가 아닌 '해당 위치에 있는 어떤 객체'를 다루는 방식으로 작동함을 의미합니다.
- **미래 연구 방향**: $\text{libero-ood}$는 TLI와 같은 추론 트릭 없이도 모델의 본질적인 외삽 능력을 발휘할 수 있는 훈련 방법을 개발하는 데 중요한 도구가 될 것입니다.

- **한계점**: 텍스트 잠재 벡터의 보편성을 연구하기는 어렵습니다. VLA 아키텍처는 매우 다양하며 LLM만큼 일관적이지 않기 때문입니다. $\pi_0$-fast에 TLI를 구현했을 때 $\text{libero-ood}$ 작업을 완료하려는 경향을 보였지만, FAST 디코더가 모델 출력을 액션으로 변환하는 데 자주 실패하는 문제가 발생했습니다.

## 📌 TL;DR

이 논문은 VLA(Vision-Language-Action) 모델이 시연된 작업에 과적합하여 새로운 외삽(extrapolation) 작업을 실패하는 문제를 다룹니다. 연구자들은 모델의 숨겨진 상태에서 추출한 '텍스트 잠재 벡터($\text{text latent}$)'가 작업에 필요한 스킬 표현을 인코딩한다는 가설을 세웠습니다. 제안된 '텍스트 잠재 벡터 보간(TLI)' 기법을 사용해 두 기본 작업의 텍스트 잠재 벡터를 혼합하여 $\pi_0$ 모델의 잔차 스트림에 주입함으로써, $\pi_0$의 외삽 능력을 크게 향상시켰습니다. 새로운 '$\text{libero-ood}$' 벤치마크(표준 LIBERO 작업에서 외삽된 20개 작업)에서 $\pi_0$의 성공률은 TLI 적용 후 9%에서 83%로 급증했으며, 이는 $\pi_0$가 개별적이지만 조합 가능한 스킬 표현을 학습했지만 이를 자율적으로 활용하지 못했음을 시사합니다. 다른 최신 VLA 모델들은 $\text{libero-ood}$에서 21% 미만의 낮은 성공률을 보였고, 분석 결과 VLA들이 객체 이름을 고정된 위치에 매핑하는 '공간적 과적합' 현상을 공통적으로 보임을 발견했습니다. 이 연구는 VLA가 잠재적으로 외삽 능력을 가지고 있으나 이를 발현하기 위한 새로운 훈련 방식이 필요함을 강조하며, $\text{libero-ood}$ 벤치마크를 공개하여 커뮤니티의 추가 연구를 장려합니다.
