{
  "url": "http://arxiv.org/abs/2503.05833v2",
  "title": "Refined Policy Distillation: From VLA Generalists to RL Experts",
  "authors": "Tobias JÃ¼lg, Wolfram Burgard, Florian Walter",
  "year": 2025,
  "abstract": "Vision-Language-Action Models (VLAs) have demonstrated remarkable\ngeneralization capabilities in real-world experiments. However, their success\nrates are often not on par with expert policies, and they require fine-tuning\nwhen the setup changes. In this work, we introduce Refined Policy Distillation\n(RPD), a novel Reinforcement Learning (RL)-based policy refinement method that\nbridges this performance gap through a combination of on-policy RL with\nbehavioral cloning. The core idea of RPD is to distill and refine VLAs into\ncompact, high-performing expert policies by guiding the student policy during\nRL exploration using the actions of a teacher VLA, resulting in increased\nsample efficiency and faster convergence. We complement our method by\nfine-tuned versions of Octo and OpenVLA for ManiSkill3 to evaluate RPD in\nsimulation. While this is a key requirement for applying RL, it also yields new\ninsights beyond existing studies on VLA performance in real-world settings. Our\nexperimental results across various manipulation tasks show that RPD enables\nthe RL student to learn expert policies that outperform the VLA teacher in both\ndense and sparse reward settings, while also achieving faster convergence than\nthe RL baseline. Our approach is even robust to changes in camera perspective\nand can generalize to task variations that the underlying VLA cannot solve. Our\ncode, dataset, VLA checkpoints, and videos are available at\nhttps://refined-policy-distillation.github.io"
}