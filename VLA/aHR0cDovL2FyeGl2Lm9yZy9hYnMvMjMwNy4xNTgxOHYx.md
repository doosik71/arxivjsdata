# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control

Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich (Google DeepMind)

## 🧩 Problem to Solve

인터넷 규모 데이터로 사전 학습된 비전-언어 모델(VLM)은 자연어 생성, 문제 해결, 시각적 인식 및 복잡한 추론에서 뛰어난 능력을 보여줍니다. 이러한 의미론적 추론 및 문제 해결 능력은 다양한 실제 환경에서 작업을 수행해야 하는 제너럴리스트 로봇에게 매우 유용할 것입니다. 그러나 이러한 VLM의 풍부한 지식을 로봇의 저수준(low-level) 동작 제어에 직접 통합하여 일반화 능력을 높이고 초월적인(emergent) 의미론적 추론을 가능하게 하는 방법은 명확하지 않습니다. 기존 접근 방식은 주로 VLM을 고수준 계획에 사용하고 저수준 제어는 별도의 컨트롤러에 맡기는데, 이는 VLM의 풍부한 지식이 저수준 제어까지 전달되지 않는 한계가 있습니다.

이 논문은 "대규모 사전 학습된 비전-언어 모델이 일반화를 촉진하고 초월적 의미론적 추론을 가능하게 하기 위해 저수준 로봇 제어에 직접 통합될 수 있는가?"라는 핵심 질문을 다룹니다.

## ✨ Key Contributions

- **Vision-Language-Action (VLA) 모델인 RT-2 제시**: 웹 규모 데이터로 사전 학습된 대규모 비전-언어 모델을 일반화 가능하고 의미론적으로 인지하는 로봇 정책으로 직접 미세 조정하는 모델 제품군을 제안합니다.
- **로봇 동작의 텍스트 토큰화**: 로봇 동작(예: 6-DoF 위치/회전 변위)을 텍스트 토큰으로 표현하여, VLM이 자연어 토큰과 동일한 방식으로 로봇 동작을 출력하도록 학습합니다. 이는 VLM을 종단 간 로봇 제어에 통합하는 간단하고 일반적인 방법을 제공합니다.
- **획기적인 일반화 성능 향상**: 새로운 물체, 배경, 환경에 대한 일반화 능력을 크게 향상시키며, 기존 최신 로봇 정책(RT-1, MOO) 대비 약 2배, 다른 베이스라인 대비 약 6배의 성공률 개선을 보여줍니다.
- **초월적 의미론적 추론 능력 발현**: 로봇 훈련 데이터에는 없던 명령(예: 물체를 특정 숫자나 아이콘 위에 놓기)을 해석하고, 기초적인 추론(예: 가장 작거나 가장 큰 물체, 다른 물체에 가장 가까운 물체 집기)을 수행하는 등 인터넷 규모 사전 학습을 통해 얻은 다양한 초월적 능력을 보여줍니다.
- **Chain of Thought 추론 통합**: "Chain of Thought" 프롬프트를 통합하여, 즉흥적인 망치로 사용할 물체(돌)를 찾거나 피곤한 사람에게 적합한 음료(에너지 드링크)를 파악하는 등 다단계 의미론적 추론을 수행할 수 있음을 보여줍니다.
- **광범위한 평가**: 6,000회 이상의 로봇 평가를 통해 RT-2의 강력한 성능과 웹 규모 VLM 사전 학습에서 상속된 광범위한 초월적 능력을 입증합니다.

## 📎 Related Works

- **비전-언어 모델 (VLM)**: CLIP과 같은 표현 학습 모델과 `{vision, text}` $\rightarrow$ `{text}` 형태의 시각 언어 모델이 있습니다. RT-2는 후자 범주(예: PaLI-X, PaLM-E)에 초점을 맞춰 VLM의 기능을 로봇 폐루프 제어에 확장합니다.
- **로봇 학습의 일반화**: 대규모의 다양한 데이터셋을 통해 새로운 물체, 작업, 지시 및 환경에 대한 일반화 능력을 향상시키는 기존 연구들(예: RT-1)이 있습니다. RT-2는 사전 학습된 VLM을 활용하여 이러한 모든 축에 걸쳐 단일 모델로 일반화를 달성하고자 합니다.
- **로봇 조작을 위한 사전 학습**:
  - **시각적 표현 사전 학습**: ImageNet, 데이터 증강 또는 로봇 제어에 맞춤화된 목표를 통해 인코더를 초기화합니다.
  - **언어 모델 사전 학습**: 명령 인코더 또는 고수준 계획(예: Ahn et al., 2022; Driess et al., 2023)에 사용됩니다.
  - **VLM의 로봇 공학 활용**: 시각적 상태 표현, 객체 식별, 고수준 계획, 감독 제공 등 다양한 방식으로 VLM을 활용한 연구들이 있습니다(예: CLIPort, MOO).
  - **RT-2의 차별점**: 기존 VLM 활용 방식(예: CLIPort, MOO)과 달리, RT-2는 언어를 생성하는 VLM을 사용하여 언어와 동작을 위한 통일된 출력 공간을 가지며, 동작 전용 모델 계층 없이 모델 가중치를 언어 및 동작 작업에 완전히 공유합니다.

## 🛠️ Methodology

1. **사전 학습된 VLM 활용**: 시각적 질의응답 및 시각적 대화를 위해 설계된 기존의 최신 VLM인 PaLI-X (Chen et al., 2023a)와 PaLM-E (Driess et al., 2023)를 기반으로 합니다. 이 모델들은 일반적으로 이미지를 입력으로 받아 자연어 텍스트 토큰 시퀀스를 출력합니다.
2. **로봇 동작 토큰화**:
   - 로봇 액션 공간은 로봇 엔드 이펙터의 6-DoF 위치 및 회전 변위, 그리퍼 확장 수준, 에피소드 종료를 위한 이산 명령으로 구성됩니다.
   - 연속적인 차원은 각각 256개의 균일한 빈으로 이산화됩니다. 이로써 로봇 동작은 8개의 정수로 표현될 수 있습니다.
   - 이 이산화된 동작을 VLM의 기존 토큰화와 연결합니다. PaLI-X의 경우 1000까지의 정수는 고유 토큰을 가지므로 해당 정수 토큰을 사용합니다. PaLM-E의 경우 가장 적게 사용되는 256개 토큰을 동작 어휘로 덮어씁니다.
   - 동작 벡터는 단순한 문자열 연결을 통해 단일 문자열로 변환됩니다: "terminate $\Delta \text{pos}_x \Delta \text{pos}_y \Delta \text{pos}_z \Delta \text{rot}_x \Delta \text{rot}_y \Delta \text{rot}_z$ gripper_extension". 예시: "1 128 91 241 5 101 127".
3. **공동 미세 조정 (Co-Fine-Tuning)**:
   - 로봇 데이터와 원래의 웹 스케일 VLM 데이터를 함께 미세 조정합니다. 로봇 데이터셋에 더 높은 샘플링 가중치를 부여하여 각 훈련 배치에서 로봇 데이터와 웹 데이터의 비율을 균형 있게 맞춥니다.
   - 이 방법은 모델이 웹 데이터에서 학습한 추상적인 시각적 개념과 로봇 데이터에서 학습한 저수준 로봇 동작을 모두 접하게 하여 더 일반화 가능한 정책을 만듭니다.
4. **출력 제약**: 로봇 동작 태스크 시에는 유효한 동작 토큰만 출력하도록 모델의 출력 어휘를 제한합니다. 일반적인 비전-언어 태스크에서는 전체 자연어 토큰을 출력할 수 있습니다.
5. **실시간 추론**: 수백억 개의 파라미터를 가진 대규모 VLM의 실시간 제어를 위해, 모델을 멀티-TPU 클라우드 서비스에 배포하고 네트워크를 통해 질의하는 프로토콜을 개발합니다. 이를 통해 55B 파라미터 모델은 1-3 Hz, 5B 파라미터 모델은 약 5 Hz의 제어 주파수를 달성합니다.
6. **Chain of Thought 추론 (선택 사항)**: PaLM-E를 사용한 RT-2 변형에 "Plan" 단계를 추가하여 명령과 동작 사이에 자연어 계획을 포함시킵니다. 이는 VQA 데이터셋과 조작 데이터셋 간의 연결 역할을 하여 더 정교한 추론 행동을 유도합니다.

## 📊 Results

- **일반화 성능**: RT-2-PaLI-X-55B 및 RT-2-PaLM-E-12B 모델은 보지 못한 물체, 배경, 환경에 대한 일반화 실험에서 RT-1, MOO 같은 베이스라인 대비 평균 2배, 다른 베이스라인 대비 6배 높은 성공률을 기록했습니다. 기존에 보았던(seen) 작업에서는 RT-1과 유사한 성능을 보였습니다.
- **초월적 능력**: RT-2는 기호 이해, 추론(시각적, 수학적, 다국어), 인간 인식 등 새로운 카테고리의 작업에서 베이스라인(RT-1 대비 평균 3배 성공률)을 크게 능가했습니다. 특히 PaLM-E 기반 모델은 수학적 추론 작업에서 강점을 보였습니다.
- **설계 선택의 영향**:
  - **모델 크기**: 55B 파라미터 모델은 5B 모델보다 더 나은 일반화 성능을 보였습니다.
  - **훈련 전략**: 사전 학습된 모델을 로봇 동작 데이터로만 미세 조정하는 것보다, 원래의 VLM 훈련 데이터와 로봇 데이터를 **공동 미세 조정(co-fine-tuning)**하는 것이 훨씬 더 나은 일반화 성능을 가져왔습니다. 이는 VLM 훈련 중 학습된 개념을 잊지 않게 하는 효과 때문입니다. 스크래치(scratch)부터 훈련하는 것은 매우 낮은 성능을 보였습니다.
- **Chain of Thought 추론**: 정성적 평가 결과, RT-2는 자연어 계획 단계를 통해 "못을 박기 위한 유용한 물건" (바위) 또는 "피곤할 때 마실 음료" (에너지 드링크)와 같은 보다 복잡한 명령을 이해하고 행동할 수 있음을 보여주었습니다.
- **오픈 소스 Language Table 벤치마크**: 시뮬레이션 환경에서 RT-2-PaLI-3B 모델이 90%의 성공률로 BC-Zero (72%), RT-1 (74%), LAVA (77%) 등 기존 베이스라인을 뛰어넘는 성능을 보였습니다.

## 🧠 Insights & Discussion

- **웹 지식의 강력한 전이**: RT-2는 웹 규모 데이터로 사전 학습된 VLM의 풍부한 의미론적 지식, 시각적 해석, 추론 능력을 로봇 제어에 성공적으로 전이시켰습니다. 이는 로봇이 인간과 유사한 방식으로 세계를 이해하고 상호 작용하는 데 중요한 진전입니다.
- **단순하고 일반적인 접근 방식**: 로봇 동작을 텍스트 토큰으로 표현하고 VLM과 공동 미세 조정하는 것은 기존 VLM을 종단 간 로봇 정책에 통합하는 효과적이고 확장 가능한 방법을 제공합니다.
- **초월적 능력의 중요성**: 로봇이 학습 데이터에 없는 새로운 시나리오나 복잡한 명령에 대응할 수 있는 능력은 일반화된 로봇의 핵심 요소이며, RT-2는 이를 달성할 수 있음을 보여줍니다.

**한계점**:

- **새로운 동작 습득의 한계**: RT-2는 웹 스케일 사전 학습을 통해 새로운 동작을 수행하는 능력을 습득하지 못합니다. 모델의 물리적 기술은 여전히 로봇 훈련 데이터에서 본 기술 분포에 국한되며, 단지 그 기술을 새로운 방식으로 '배치'하는 방법을 학습합니다. 이는 로봇 데이터셋의 기술 다양성 부족에서 비롯된 것으로 보이며, 인간 비디오와 같은 새로운 데이터 수집 패러다임이 필요합니다.
- **높은 계산 비용**: 대규모 VLA 모델의 실시간 추론은 클라우드 서비스를 사용하더라도 계산 비용이 높습니다. 고주파수 제어가 필요한 경우 병목 현상이 발생할 수 있어, 양자화 및 증류(distillation) 기술에 대한 추가 연구가 필요합니다.
- **VLM 모델 가용성**: RT-2 구축에 사용할 수 있는 VLM 모델의 수가 제한적입니다. 더 많은 오픈 소스 모델이나 미세 조정을 위한 API의 개방이 필요합니다.

## 📌 TL;DR

- **문제**: 인터넷 규모 비전-언어 모델(VLM)의 풍부한 의미론적 추론 및 일반화 능력을 로봇의 저수준 제어에 직접 통합하여 현실 세계 작업을 수행할 수 있을까?
- **제안 방법**: RT-2는 기존 VLM (PaLI-X, PaLM-E)을 기반으로, 로봇 동작을 텍스트 토큰으로 인코딩한다. 로봇 궤적 데이터와 인터넷 규모 VLM 데이터를 함께 **공동 미세 조정(co-fine-tuning)**하여 VLM이 자연어뿐만 아니라 로봇 동작도 직접 출력하도록 학습시킨다.
- **주요 결과**: RT-2는 새로운 물체, 배경, 환경에 대한 뛰어난 일반화 능력(기존 모델 대비 최대 2~6배 향상)을 보였으며, 웹 규모 사전 학습에서 상속된 기호 이해, 추론, 인간 인식 등 다양한 **초월적 능력(emergent capabilities)**을 시연했다. Chain of Thought를 활용하면 다단계 의미론적 추론도 가능하다. 모델 크기와 공동 미세 조정 전략이 성능 향상에 핵심적임을 입증했다.
