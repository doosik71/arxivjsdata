{
  "url": "http://arxiv.org/abs/2405.17418v2",
  "title": "A Self-Correcting Vision-Language-Action Model for Fast and Slow System\n  Manipulation",
  "authors": "Chenxuan Li, Jiaming Liu, Guanqun Wang, Xiaoqi Li, Sixiang Chen, Liang Heng, Chuyan Xiong, Jiaxin Ge, Renrui Zhang, Kaichen Zhou, Shanghang Zhang",
  "year": 2024,
  "abstract": "Recently, some studies have integrated Multimodal Large Language Models into\nrobotic manipulation, constructing vision-language-action models (VLAs) to\ninterpret multimodal information and predict SE(3) poses. While VLAs have shown\npromising progress, they may suffer from failures when faced with novel and\ncomplex tasks. To emulate human-like reasoning for more robust manipulation, we\npropose the self-corrected (SC-)VLA framework, which integrates fast system for\ndirectly predicting actions and slow system for reflecting on failed actions\nwithin a single VLA policy. For the fast system, we incorporate\nparameter-efficient fine-tuning to equip the model with pose prediction\ncapabilities while preserving the inherent reasoning abilities of MLLMs. For\nthe slow system, we propose a Chain-of-Thought training strategy for failure\ncorrection, designed to mimic human reflection after a manipulation failure.\nSpecifically, our model learns to identify the causes of action failures,\nadaptively seek expert feedback, reflect on the current failure scenario, and\niteratively generate corrective actions, step by step. Furthermore, a\ncontinuous policy learning method is designed based on successfully corrected\nsamples, enhancing the fast system's adaptability to the current configuration.\nWe compare SC-VLA with the previous SOTA VLA in both simulation and real-world\ntasks, demonstrating an efficient correction process and improved manipulation\naccuracy on both seen and unseen tasks."
}