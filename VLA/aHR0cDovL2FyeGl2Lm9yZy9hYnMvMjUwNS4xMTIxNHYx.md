# Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions

Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang

## 🧩 Problem to Solve

기존 Vision-Language-Action (VLA) 모델들은 주로 텍스트 기반의 언어 지시만 받아들이도록 설계되어 있습니다. 이는 로봇과 인간 간의 상호작용이 언어에만 국한되지 않는 실제 개방형 시나리오에서 VLA 모델의 적용 가능성을 크게 제한합니다. 예를 들어, 사용자는 이미지에 표시된 물체를 가져오거나, 화이트보드에 쓰인 지시를 따르거나, 비디오에 시연된 행동을 모방하는 등 다양한 형태의 멀티모달 지시를 기대할 수 있습니다. 본 연구는 이러한 격차를 해소하고 VLA 모델이 개방형 멀티모달 지시를 처리할 수 있도록 확장하는 것을 목표로 합니다.

## ✨ Key Contributions

- **OE-VLA 도입:** 통일된 신경 아키텍처를 통해 다양한 개방형 멀티모달 인간 지시를 처리할 수 있는 새로운 VLA 모델인 OE-VLA를 제안합니다.
- **데이터 구축 방법론:** 기존 로봇 데이터셋을 활용하여 자유로운 형태의 멀티모달 지시가 포함된 로봇 데이터셋을 구축하는 일반적인 접근 방식을 제안합니다.
- **2단계 커리큘럼 학습:** 언어-시각 기초 모델을 개방형 지시를 위한 VLA 모델로 미세 조정하는 2단계 커리큘럼 학습 알고리즘을 제안합니다.
- **새로운 벤치마크:** CALVIN 스위트를 기반으로 다양한 개방형 지시를 포함하는 OE-CALVIN$_{base}$ 및 OE-CALVIN$_{hard}$라는 두 가지 강력한 벤치마크를 도입하여 제안된 방법론을 평가합니다.

## 📎 Related Works

- **기존 VLA 모델:** RT-2, RT-X, OpenVLA 등은 사전 학습된 VLM을 기반으로 로봇 데이터셋으로 미세 조정하여 언어 지시 하의 로봇 조작에서 뛰어난 일반화 성능을 보여주었습니다.
- **성능 개선 VLA:** RoboFlamingo는 시각적 이력 정보를 통합하고 연속적인 로봇 동작을 예측하며, CogACT, $\pi_0$, GR00T 등은 diffusion policy를 통합하여 최첨단 성능을 달성했습니다. 3D-VLA는 깊이 정보를 활용하고, RT-H는 행동 계층 기술을 도입하며, Embodied-COT는 사고 연쇄 추론을 통합했습니다. 일부 연구는 월드 모델을 활용하여 성능을 더욱 향상시키기도 했습니다.
- **멀티모달 프롬프트 로봇 제어:** VIMA는 로봇 제어를 위한 멀티모달 프롬프트를 처리할 수 있지만, 객체 중심 정책 모델이며, 시각 세계에 대한 지식이 부족한 언어 모델(T5)을 기반으로 하고, 멀티모달 지시가 동일한 작업 공간 내로 제한된다는 점에서 OE-VLA와는 다릅니다. OE-VLA는 전통적인 VLA 모델이며, VLM을 활용하고, 다양한 환경 및 인터넷 출처의 지시를 처리할 수 있습니다.

## 🛠️ Methodology

OE-VLA 모델은 주로 세 가지 구성 요소로 이루어집니다: 비전 인코더, MLP 프로젝터, LLM 백본. LLaVA-Next-Interleave를 기초 모델로 사용하며, 이는 자유로운 멀티-이미지 입력 처리에 강점을 가집니다. 연속적인 로봇 행동은 256개의 이산 토큰으로 양자화됩니다.

1. **전체 아키텍처:**
   - **비전 인코더 (SigLIP-400M Vision Transformer):** 384x384 해상도의 이미지를 입력받아 패치로 분할합니다. 로봇 관측(정적 시점, 손목 카메라)과 지시 이미지로부터 시각 토큰을 생성합니다.
     - $T_{obs} = \text{SigLip}(I_{obs})$
     - $T_{img_1}, T_{img_2}, ..., T_{img_i} = \text{SigLip}(I_{img_1}, I_{img_2}, ..., I_{img_i})$
   - **LLM 백본 (Qwen-1.5):** 최대 32k 토큰의 컨텍스트 길이를 지원합니다. 텍스트 지시는 Qwen 토크나이저로 토큰화됩니다.
     - $T_{lang_1}, T_{lang_2}, ..., T_{lang_j} = \text{Tokenizer}(L_1, L_2, ..., L_j)$
   - **MLP 프로젝터:** 비전 인코더의 시각 토큰을 언어 토큰과 동일한 히든 공간으로 매핑합니다. 모든 토큰은 원래 위치를 유지하며 최종 시퀀스로 연결되어 LLM에 입력됩니다.
     - $T_{final} = \text{Concat}(T_{obs}, T_{lang_1}, T_{img_1}, T_{lang_2}, T_{img_2}, ...)$
   - **액션 토크나이저:** 연속적인 로봇 행동을 256개의 이산 빈으로 이산화하며, 이 빈들은 Qwen 어휘에서 사용 빈도가 낮은 언어 토큰으로 재사용됩니다. 모델은 다음 조건부 확률 분포를 구축하여 로봇 행동을 예측합니다.
     - $P_{\phi}(A_1, A_2, ..., A_n | T_{final}) = \prod_{i=1}^{n} P_{\phi}(A_i | T_{final}, A_{1:i-1})$
2. **훈련 데이터 구축:** 기존 언어 주석이 있는 로봇 조작 데이터셋을 개방형 멀티모달 지시가 포함된 데이터셋으로 변환하는 방법을 제안합니다.
   - **시각 객체 지정 (Visual Object Specification, VOS):** 언어 주석의 객체 설명을 크롭된 이미지로 대체합니다.
   - **시각적 지시 따르기 (Optical Instruction Following, OIF):** 텍스트 지시를 다양한 글꼴, 크기, 색상, 배경, 위치를 사용하여 이미지로 렌더링합니다.
   - **시각적 목표 도달 (Visual Goal Reaching, VGR):** 목표 상태를 나타내는 단일 이미지(마지막 프레임)를 제공합니다.
   - **비디오 시연 학습 (Video Demo Learning, VDL):** 언어 주석을 에피소드 전체에서 균일하게 샘플링된 일련의 이미지 프레임으로 대체합니다.
3. **훈련 파이프라인 (2단계 커리큘럼 학습):**
   - **1단계: 멀티-이미지 그라운딩 (Multi-Image Grounding):** MGrounding 데이터셋을 사용하여 기초 모델의 객체 간 공간 관계 인지 능력을 향상시킵니다.
   - **2단계: 개방형 지시 튜닝 (Open-ended Instruction Tuning):** 1단계 학습 후, 구축된 개방형 작업 사양을 가진 로봇 조작 데이터셋으로 모델을 훈련합니다.

## 📊 Results

- **언어 기반 작업 사양 결과 (CALVIN 벤치마크, ABC→D 분할):**
  - OE-VLA$_{1b}$ 및 OE-VLA$_{7b}$는 순수 언어 입력에서도 매우 경쟁력 있는 성능을 달성했으며, 특히 OE-VLA$_{7b}$는 평균 성공 시퀀스 길이 2.99로 최고의 성능을 기록했습니다. 이는 기존의 강력한 언어 기반 VLA 모델들과 비교할 때 우수하거나 대등한 수준입니다.
- **개방형 멀티모달 지시 결과 (OE-CALVIN$_{base}$ 및 OE-CALVIN$_{hard}$ 벤치마크):**
  - **OE-CALVIN$_{base}$:** OE-VLA$_{1b}$는 평균 성공 시퀀스 길이 2.75로 유망한 결과를 보였으며, 특히 VOS 작업에서 뛰어난 성능을 보였습니다. VGR은 가장 어려운 작업으로 나타났습니다. OE-VLA$_{7b}$로 모델을 확장했을 때, 성능이 크게 향상되어 평균 3.48을 기록하며 기존 언어 기반 모델의 성능을 능가했습니다.
  - **OE-CALVIN$_{hard}$:** 더 어려운 벤치마크에서도 OE-VLA는 challenging한 상황을 처리할 수 있음을 보여주었습니다. 성능은 OE-CALVIN$_{base}$에 비해 다소 저하되었지만, OE-VLA$_{7b}$는 평균 2.68로 기존 언어 기반 VLA와 유사한 성능을 유지했습니다. VGR은 여전히 단일 이미지의 제한된 정보로 인해 어려운 과제로 남았습니다.
- **훈련 파이프라인의 Ablation Study:**
  - 2단계 훈련 파이프라인은 특히 OE-CALVIN$_{base}$의 ABC→D 분할에서 모델 성능을 크게 향상시켰습니다. ABCD→D 분할에서는 향상 폭이 작았는데, 이는 모델이 평가 스위트와 동일한 환경 D의 훈련 데이터로부터 충분한 지식을 습득했기 때문으로 추정됩니다.
  - OE-CALVIN$_{hard}$ 벤치마크에서도 2단계 훈련은 효과적이었으나, OE-VLA$_{7b}$의 경우 기초 모델이 이미 강력한 공간 관계 이해를 가지고 있어 1단계 훈련의 개선 효과가 상대적으로 제한적이었습니다.

## 🧠 Insights & Discussion

- OE-VLA는 로봇이 언어 입력에만 의존하는 기존 VLA 모델의 한계를 극복하고, 인간-로봇 상호작용의 범위를 시각 객체 지정, 시각적 지시 따르기, 시각적 목표 도달, 비디오 시연 학습 등 다양한 멀티모달 지시로 확장할 수 있는 잠재력을 보여줍니다.
- 모델의 크기를 스케일업(1B에서 7B로)하면 멀티모달 작업에서 상당한 성능 향상을 가져오며, 특히 이미지 기반 지시의 양이 텍스트 전용 지시보다 훨씬 많을 때 더 큰 모델이 멀티모달 작업에서 훨씬 개선된 성능을 달성할 수 있음을 시사합니다.
- 시각적 목표 도달 (VGR) 작업은 단일 목표 이미지에 포함된 정보의 제한성 때문에 가장 어려운 작업으로 나타났으며, 이는 미래 연구에서 추가적인 정보 통합(예: 3D 정보, 시퀀스 정보)을 통해 개선될 여지가 있음을 시사합니다.
- 제안된 2단계 커리큘럼 학습 전략은 모델이 개방형 지시를 처리하는 능력을 향상시키는 데 기여하며, 특히 일반화 능력이 요구되는 ABC→D 분할에서 효과적입니다.
- 훈련 데이터 구성 방식(멀티모달 지시 데이터의 비중)에 따라 순수 언어 입력에서의 모델 성능에 영향을 미칠 수 있으며, 향후 데이터 레시피의 최적화된 균형을 찾는 연구가 필요합니다.

## 📌 TL;DR

이 논문은 기존 VLA (Vision-Language-Action) 모델이 언어 지시만 받는 한계를 극복하기 위해, 개방형 멀티모달 지시를 처리하는 **OE-VLA**를 제안합니다. OE-VLA는 시각 객체 지정, 이미지 내 지시 따르기, 비디오 시연 학습, 시각적 목표 도달 등 다양한 형태의 입력(텍스트, 이미지, 비디오)을 통합된 신경 아키텍처로 처리합니다. 연구팀은 기존 로봇 데이터셋을 멀티모달 지시 데이터로 변환하는 방법과 2단계 커리큘럼 학습 전략을 개발했으며, 새로운 OE-CALVIN$_{base}$ 및 OE-CALVIN$_{hard}$ 벤치마크를 도입했습니다. 결과적으로 OE-VLA는 언어 지시뿐만 아니라 다양한 비언어적 입력에서도 뛰어난 성능을 보였으며, 모델 스케일업을 통해 멀티모달 작업 성능이 크게 향상됨을 입증하여 인간-로봇 상호작용의 잠재력을 크게 확장했습니다.
