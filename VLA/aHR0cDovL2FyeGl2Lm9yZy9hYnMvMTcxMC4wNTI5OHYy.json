{
  "title": "Text2Action: Generative Adversarial Synthesis from Language to Action",
  "authors": "Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, Songhwai Oh",
  "year": 2017,
  "url": "http://arxiv.org/abs/1710.05298v2",
  "abstract": "In this paper, we propose a generative model which learns the relationship\nbetween language and human action in order to generate a human action sequence\ngiven a sentence describing human behavior. The proposed generative model is a\ngenerative adversarial network (GAN), which is based on the sequence to\nsequence (SEQ2SEQ) model. Using the proposed generative network, we can\nsynthesize various actions for a robot or a virtual agent using a text encoder\nrecurrent neural network (RNN) and an action decoder RNN. The proposed\ngenerative network is trained from 29,770 pairs of actions and sentence\nannotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video\ndataset. We demonstrate that the network can generate human-like actions which\ncan be transferred to a Baxter robot, such that the robot performs an action\nbased on a provided sentence. Results show that the proposed generative network\ncorrectly models the relationship between language and action and can generate\na diverse set of actions from the same sentence."
}