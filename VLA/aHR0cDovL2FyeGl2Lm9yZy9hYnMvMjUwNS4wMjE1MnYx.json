{
  "title": "Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text\n  Instructions",
  "authors": "Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding",
  "year": 2025,
  "url": "http://arxiv.org/abs/2505.02152v1",
  "abstract": "Vision-Language-Action (VLA) models have shown great promise for generalist\nrobotic manipulation in the physical world. However, existing models are\nrestricted to robot observations and text-only instructions, lacking the\nflexibility of interleaved multimodal instructions enabled by recent advances\nin foundation models in the digital world. In this paper, we present\nInterleave-VLA, the first framework capable of comprehending interleaved\nimage-text instructions and directly generating continuous action sequences in\nthe physical world. It offers a flexible, model-agnostic paradigm that extends\nstate-of-the-art VLA models with minimal modifications and strong zero-shot\ngeneralization. A key challenge in realizing Interleave-VLA is the absence of\nlarge-scale interleaved embodied datasets. To bridge this gap, we develop an\nautomatic pipeline that converts text-only instructions from real-world\ndatasets in Open X-Embodiment into interleaved image-text instructions,\nresulting in the first large-scale real-world interleaved embodied dataset with\n210k episodes. Through comprehensive evaluation on simulation benchmarks and\nreal-robot experiments, we demonstrate that Interleave-VLA offers significant\nbenefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x\ncompared to state-of-the-art baselines, 2) supports flexible task interfaces,\nand 3) handles diverse user-provided image instructions in a zero-shot manner,\nsuch as hand-drawn sketches. We further analyze the factors behind\nInterleave-VLA's strong zero-shot performance, showing that the interleaved\nparadigm effectively leverages heterogeneous datasets and diverse instruction\nimages, including those from the Internet, which demonstrates strong potential\nfor scaling up. Our model and dataset will be open-sourced."
}