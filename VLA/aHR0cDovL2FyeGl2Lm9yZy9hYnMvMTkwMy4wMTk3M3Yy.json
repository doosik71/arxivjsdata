{
  "url": "http://arxiv.org/abs/1903.01973v2",
  "title": "Learning Latent Plans from Play",
  "authors": "Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet",
  "year": 2019,
  "abstract": "Acquiring a diverse repertoire of general-purpose skills remains an open\nchallenge for robotics. In this work, we propose self-supervising control on\ntop of human teleoperated play data as a way to scale up skill learning. Play\nhas two properties that make it attractive compared to conventional task\ndemonstrations. Play is cheap, as it can be collected in large quantities\nquickly without task segmenting, labeling, or resetting to an initial state.\nPlay is naturally rich, covering ~4x more interaction space than task\ndemonstrations for the same amount of collection time. To learn control from\nplay, we introduce Play-LMP, a self-supervised method that learns to organize\nplay behaviors in a latent space, then reuse them at test time to achieve\nspecific goals. Combining self-supervised control with a diverse play dataset\nshifts the focus of skill learning from a narrow and discrete set of tasks to\nthe full continuum of behaviors available in an environment. We find that this\ncombination generalizes well empirically---after self-supervising on unlabeled\nplay, our method substantially outperforms individual expert-trained policies\non 18 difficult user-specified visual manipulation tasks in a simulated robotic\ntabletop environment. We additionally find that play-supervised models, unlike\ntheir expert-trained counterparts, are more robust to perturbations and exhibit\nretrying-till-success behaviors. Finally, we find that our agent organizes its\nlatent plan space around functional tasks, despite never being trained with\ntask labels. Videos, code and data are available at\nlearning-from-play.github.io"
}