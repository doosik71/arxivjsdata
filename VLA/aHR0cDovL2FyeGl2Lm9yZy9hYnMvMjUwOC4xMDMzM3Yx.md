# ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver

Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Haodong Yan, Yuxin Huang, Feilong Tang, Donglin Wang, Haoang Li

## 🧩 Problem to Solve

기존 VLA(Vision-Language-Action) 모델은 시각적 주의를 목표 영역에 정확히 집중하는 데 어려움을 겪습니다. 시각적 주의가 분산되어 있어, 로봇이 조작해야 할 대상 객체를 정확히 인식하지 못하고 잘못된 객체를 조작하거나 정밀한 조작에 실패하는 경우가 많습니다. 특히 복잡한 환경이나 장기적인 작업에서 이러한 문제는 로봇의 조작 능력과 일반화 성능을 저해합니다. 따라서 VLA 모델의 시각적 주의 할당을 개선하고 시각적 접지(visual grounding) 능력을 향상시키는 방법이 필요합니다.

## ✨ Key Contributions

- 암묵적 접지(implicit grounding) 패러다임을 특징으로 하는 재구성적 VLA 모델인 ReconVLA를 제안합니다. 이 모델은 조작 대상 객체에 해당하는 '응시 영역(gaze region)'을 재구성하는 보조 작업을 통해 정확한 시각적 주의 할당과 미세 표현 학습을 유도하여 시각적 접지 능력과 정밀 조작 능력을 향상시킵니다.
- 100,000개 이상의 궤적(trajectories)과 2백만 개 이상의 데이터 샘플을 포함하는 대규모 로봇 사전 학습 데이터셋을 구축했습니다. 이 데이터셋을 활용한 사전 학습을 통해 시각적 재구성 능력의 일반화를 크게 향상시켰습니다.
- 시뮬레이션 및 실제 환경에서의 광범위한 실험을 통해 제안된 암묵적 접지 방법의 우수성을 입증했으며, 정밀 조작 및 미탐지 객체(unseen targets)에 대한 강력한 일반화 능력을 보여주었습니다.

## 📎 Related Works

- **행동 중심 VLA 모델 (Action-centric VLA Models):** RoboFlamingo, OpenVLA, UniVLA 등 기존 VLA 모델들은 주로 행동 출력을 감독하여 실행 가능한 행동을 학습합니다. 반면 ReconVLA는 보조 작업으로 시각적 출력을 감독하여 시각적 인식을 강화하는 데 차이가 있습니다.
- **조작을 위한 생성적 방법 (Generative Methods for Manipulation):** UniPi, SuSIE, GR-1 등은 미래 이미지 또는 비디오 생성을 통해 로봇의 계획 및 제어 능력을 향상시킵니다. 이와 달리 ReconVLA는 현재 이미지의 목표 영역을 재구성하여 정밀한 인식과 조작에 집중합니다.
- **조작을 위한 시각적 접지 방법 (Visual Grounding Methods for Manipulation):**
  - **명시적 접지 (Explicit Grounding, EG):** RoboGround, VIP와 같은 방법들은 외부 객체 감지 전문가 모델(예: LISA, YOLOv11)을 활용하여 접지된 이미지나 잘라낸 이미지를 VLA 모델의 추가 입력으로 제공합니다. 그러나 이는 외부 모델에 의존하며 정책 자체의 주의 할당을 근본적으로 개선하지 못합니다.
  - **CoT 접지 (CoT Grounding, CG):** ECoT, GraspVLA는 CoT(Chain-of-Thought) 방식으로 바운딩 박스 좌표를 먼저 출력한 후 행동을 출력합니다. 이는 접지 능력을 훈련시키지만, 좌표 형태의 바운딩 박스 정보만으로는 정밀 조작에 불충분하며, 정밀한 좌표와 행동을 동시에 출력하는 것은 학습에 어려움이 있습니다.
  - ReconVLA는 이러한 방법들과 달리, VLA 모델의 시각적 출력을 통해 직접 조작 대상 영역을 재구성하여 암묵적으로 접지를 수행하고, 목표 영역의 미세 표현 학습을 촉진합니다. 이는 인간의 시선(gaze) 처리 방식과 유사합니다.

## 🛠️ Methodology

ReconVLA는 기존 VLA 모델의 시각적 주의 분산 문제를 해결하기 위해 **재구성적 VLA 모델**과 **대규모 시각적 사전 학습**을 제안합니다.

1. **VLA 모델의 기본 구조:**
   - VLA 모델 $\Lambda$는 입력 이미지 $I$와 텍스트 지시 $S$를 받아 행동 $A = \Lambda(I, S)$를 예측합니다.
   - 주요 구성 요소는 대규모 언어 모델(LLM), 비전 인코더 $E$, 토크나이저 $T$, 그리고 행동 디토크나이저 $Q$입니다.
   - 입력 $I$와 $S$는 각각 이미지 토큰 $h_I$와 텍스트 토큰 $h_S$로 처리된 후 LLM에 입력되어 행동 토큰 $a$를 생성하고, $Q$를 통해 로봇 제어를 위한 실행 가능한 행동 $A$로 매핑됩니다. 행동 토큰은 자기회귀적으로 생성됩니다: $$p(a) = \prod_{i=1}^{N} p_{LLM}(a_i | a_{1 \sim i-1}; h_I; h_S)$$
2. **재구성적 VLA 모델 (ReconVLA):**
   - **재구성 목표 설정:** 인간의 시선(gaze) 행동에서 영감을 받아, 로봇이 조작할 '응시 영역(gaze region)'을 재구성 목표로 설정합니다. 이는 모델이 올바른 목표에 집중하고, 미세한 인식을 강화하며, 장기 작업에서 암묵적인 서브태스크 계획을 용이하게 합니다.
   - **손실 함수:** ReconVLA의 전체 학습 목표는 행동 예측 손실 $L_{action_{VLA}}$와 시각적 응시 영역 재구성 손실 $L_{visual_{VLA}}$의 합입니다. $L_{action_{VLA}}$는 교차 엔트로피 손실이며, $L_{visual_{VLA}}$는 재구성 토큰 $h_R$과 재구성 목표 이미지 $I'$ 간의 측정입니다.
   - **잠재 시각 재구성:**
     - 연속 변분 오토인코더(VAE)를 시각 토크나이저 $F$로 사용하여 응시 영역 $I'$에서 상세 시각 정보를 보존하는 장면 토큰 $z_0 = F(I')$를 추출합니다.
     - 디퓨전 트랜스포머(denoiser $D$)는 재구성 토큰 $h_R = LLM(h_I)$에 조건화되어 노이즈가 추가된 토큰 $z_t$에서 $z_0$를 복구하도록 학습됩니다.
     - 재구성 목표 함수는 디퓨전 프로세스를 따릅니다: $$L_{visual_{VLA}}(h_R, I') = E_{t,\epsilon} [||D(z_t; h_R, t) - \epsilon||^2]$$ 여기서 $t$는 디퓨전 타임스텝, $\epsilon$는 노이즈입니다.
   - **구현:** ReconVLA는 사전 학습된 LLaVA-7b 모델을 기반으로 하며, Qwen2-7b를 LLM 백본으로, siglip-so400m-patch14-384를 비전 인코더로 활용합니다.
3. **대규모 시각적 사전 학습 (Visual Pretraining):**
   - **데이터셋 구축:** BridgeData V2, LIBERO, CALVIN 등 오픈소스 로봇 데이터셋을 활용하여 사전 학습 데이터셋을 구축합니다. 최신 개방형 객체 감지기인 Grounding DINO를 파인튜닝하여 로봇이 상호작용하도록 지시된 '응시 영역'을 자동으로 분할하고, 원본 이미지와 잘라낸 응시 영역 이미지를 쌍으로 구성합니다. 이를 통해 100,000개 이상의 궤적과 2백만 개 이상의 데이터 샘플을 확보합니다.
   - **학습 과정:** 사전 학습 단계에서는 재구성 손실과 행동 손실 모두에 대해 기울기 역전파를 수행하여 최적화 목표의 일관성을 유지합니다. 이 과정을 통해 모델은 일반화된 시각 재구성 능력을 갖추게 되어 다양한 환경과 작업에 배포될 수 있도록 합니다.

## 📊 Results

- **패러다임 비교:** CALVIN ABC→D 벤치마크에서 ReconVLA의 **암묵적 접지(IG)** 방법이 **명시적 접지(EG)** 및 **CoT 접지(CG)** 대비 가장 높은 성공률을 기록했습니다. 특히 5단계 연속 작업에서 IG는 64.1%의 성공률을 보여 EG(50.2%)와 CG(0.0%)를 크게 능가했습니다. 이는 암묵적 접지 방식이 시각적 정보 처리 효율성과 정밀 조작에 더 효과적임을 입증합니다.
- **심층 분석:**
  - **주의 시각화:** 시각적 주의 맵 시각화 결과, 기존 모델은 주의가 분산되거나 관련 없는 영역에 집중하는 반면, ReconVLA는 $L_{visual_{VLA}}$ 손실을 통해 응시 영역(조작 대상 객체)에 주의를 정확히 집중하는 것을 확인했습니다. 이는 작업 성공률에 직접적인 영향을 미쳤습니다.
  - **정밀 조작:** '블록 쌓기'와 같은 고난이도 정밀 조작 작업에서 ReconVLA의 응시 메커니즘은 기준선 대비 20.2%p 향상된 79.5%의 성공률을 달성하며, 정밀 시각적 접지를 통한 행동 정확도 향상을 입증했습니다.
- **어블레이션 연구:** 재구성 부분, 응시 영역 활용, 대규모 사전 학습의 기여도를 분석한 결과, **사전 학습**이 시각 재구성의 일반화 능력을 크게 향상시켜 성공률에 지대한 영향을 미쳤습니다. 또한, 전체 이미지 대신 **응시 영역**만을 재구성하는 것이 모델의 시각적 주의를 목표 객체에 집중시켜 성능 개선에 더 효과적이었습니다.
- **최신 기술과의 비교:** CALVIN ABC→D 및 ABCD→D 장기 작업에서 ReconVLA는 UniPi, SuSIE, GR-1 등의 생성 모델과 RoboFlamingo, OpenVLA, UniVLA 같은 대규모 VLA 모델들을 능가하는 경쟁력 있는 성능을 보였습니다. 특히, ABC→D의 마지막 서브태스크에서 ReconVLA는 OpenVLA보다 20.6%p, UniVLA보다 7.6%p 높은 성공률을 달성하여, 현재 관찰에 대한 인식을 강화하는 것의 중요성을 강조했습니다.
- **실제 환경 다중 작업 실험:** 실제 환경에서 '과일을 그릇에 넣기', '그릇 쌓기', '컵 뒤집기', '테이블 정리' 등 네 가지 대표적인 작업에서 ReconVLA는 OpenVLA 및 PD-VLA 대비 일관되게 높은 성공률을 기록했습니다. 특히, 훈련 데이터에 없던 **미탐지 객체(unseen objects)**에 대해서도 OpenVLA와 PD-VLA가 거의 0%의 성공률을 보인 반면, ReconVLA는 대규모 혼합 데이터 사전 학습 덕분에 성공적으로 조작을 수행하며 강력한 시각적 일반화 능력을 입증했습니다.

## 🧠 Insights & Discussion

- 이 논문은 기존 VLA 모델의 분산된 시각적 주의가 정밀 조작의 주요 한계임을 분석하고, ReconVLA의 **암묵적 접지 패러다임**이 이 문제를 효과적으로 해결함을 보여줍니다. 응시 영역을 재구성하는 보조 작업을 통해 모델은 목표 객체에 대한 미세한 표현을 학습하고 시각적 주의를 정확하게 할당하게 됩니다.
- 이 재구성적 학습 메커니즘은 인간의 시각 인지 과정, 즉 특정 영역에 집중하고 주변을 흐리게 보는 '시선(gaze)' 행동과 유사하게 작동하여 로봇의 지각 및 조작 능력을 크게 향상시킵니다.
- **대규모 로봇 데이터셋을 활용한 사전 학습**은 ReconVLA의 시각적 재구성 능력을 강화하여 미탐지 객체나 새로운 환경에서도 성공적으로 작업을 수행할 수 있는 뛰어난 일반화 능력을 부여합니다. 이는 ReconVLA가 실제 로봇 시스템에 배포될 때의 견고성과 유연성을 시사합니다.
- ReconVLA는 추가적인 외부 입력이나 복잡한 출력 구조 없이 시각적 출력을 직접 감독하는 단순하면서도 효과적인 학습 및 추론 파이프라인을 제공합니다. 이는 기존의 명시적 접지 방식이나 CoT 접지 방식이 가졌던 한계(예: 정보 중복, 학습 난이도)를 극복합니다.
- 결론적으로, ReconVLA는 로봇이 복잡한 환경에서 정밀하고 일반화된 조작 능력을 갖추도록 돕는 효과적인 접근 방식을 제시하며, VLA 모델의 시각적 인식 역량을 근본적으로 개선할 수 있는 중요한 통찰을 제공합니다.

## 📌 TL;DR

기존 VLA(Vision-Language-Action) 모델의 분산된 시각적 주의로 인한 정밀 조작 문제를 해결하기 위해, 본 논문은 **ReconVLA**라는 재구성적 VLA 모델을 제안합니다. ReconVLA는 모델의 시각적 출력을 기반으로 로봇의 '응시 영역(gaze region)'을 재구성하는 **암묵적 접지(implicit grounding)** 패러다임을 사용합니다. 이 과정은 모델이 정확한 시각적 주의를 할당하고 미세한 표현을 학습하여 정밀 조작 능력을 향상시키도록 유도합니다. 또한, 100K개 이상의 궤적을 포함하는 대규모 로봇 데이터셋으로 사전 학습하여 모델의 시각적 재구성 및 일반화 능력을 강화합니다. 시뮬레이션 및 실제 환경에서의 실험 결과, ReconVLA는 기존 VLA 및 생성 모델 대비 월등한 정밀 조작 성능과 미탐지 객체에 대한 강력한 일반화 능력을 입증했습니다.
