# VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making

Zuojin Tang, Bin Hu, Chenyang Zhao, De Ma, Gang Pan, Bin Liu

## 🧩 Problem to Solve

최근 대규모 사전 학습 모델(LLM 및 VLA)은 멀티모달 작업에서 상당한 발전을 이루었지만, 이들은 `다중 입력 단일 출력(MISO)` 패러다임에 기반을 두고 있습니다. 본 논문은 이러한 MISO 패러다임이 `병렬 작업 실행`이 필요한 `다중 입력 다중 출력(MIMO)` 시나리오에서 성능을 근본적으로 제한한다는 것을 보여줍니다. MISO 아키텍처에서는 작업들이 공유된 출력 채널을 놓고 경쟁하여 상호 배제 효과를 발생시키고, 이는 불균형한 최적화와 성능 저하로 이어집니다. 인간의 인지는 방해 없이 여러 작업을 동시에 처리(예: 대화와 의사 결정)할 수 있으므로, 이 간극을 해결하는 것이 연구의 목표입니다.

## ✨ Key Contributions

- 기존 MISO 모델(예: LLM 및 VLA)이 MIMO 작업을 근본적으로 처리하지 못한다는 최초의 증거를 제시했습니다.
- `MIMO-VLA (VLASCD)`를 제안했습니다. 이는 병렬 멀티태스킹 출력을 지원하는 통합 MIMO 학습 아키텍처입니다. 이 설계는 다음과 같은 검증된 기술들을 통합합니다:
  - 연속적인 행동 값을 생성하기 위한 계산 모듈 및 손실 항.
  - 텍스트 생성 및 의사 결정 과정에서 풍부한 시각 정보를 활용하기 위한 이미지 재구성 손실.
  - 의사 결정 정확도를 높이면서 대화 능력을 보존하기 위한 레이블 스무딩(label smoothing) 전략.
- 광범위한 실험을 통해 MIMO-VLA가 최첨단(SOTA) MISO 기반 LLM, 강화 학습 모델 및 VLA보다 더 정확한 실시간 행동 결정을 달성하며, 실시간 대화 기능을 완벽하게 유지함을 입증했습니다.

## 📎 Related Works

- **의사 결정을 위한 LLM**: ChatGPT, LLaMA와 같은 LLM은 컨텍스트 내 학습 및 추론 능력을 보여주었으며, 주로 고수준 계획(high-level planning)을 수행하고 전문 모듈을 조정하는 계층적 모듈 시스템에 통합됩니다. MIMO-VLA는 행동 결정과 자연어 대화를 동시에 출력하는 멀티모달 GPT 변형으로 이러한 접근 방식과 차별화됩니다.
- **의사 결정을 위한 VLA 모델**: RT-X, OpenVLA와 같은 Vision-Language-Action (VLA) 모델은 환경과 상호작용하는 물리적 에이전트(예: 로봇)를 위한 제어 신호를 생성합니다. 그러나 이들은 연속적인 행동 공간을 이산화(discretization)하여 미세한 제어를 방해하는 한계가 있습니다.
- **MIMO 설정을 위한 LLM**: 기존의 멀티태스킹 접근 방식은 대개 작업별(task-specific) 설계에 의존하거나 MISO 아키텍처에 갇혀 MIMO 출력을 생성할 수 없습니다. 최근 Simlingo 프레임워크와 유사한 개념이 있지만, 본 연구는 (1) MISO 모델의 멀티태스킹 간섭 분석, (2) 종단간(end-to-end) 연속 행동 매핑 도입, (3) 언어, 행동, 이미지 목표를 통합하는 동적 복합 손실(dynamic composite loss) 설계에서 차이를 보입니다.

## 🛠️ Methodology

MIMO-VLA는 LLaMA-7B를 백본으로 사용하며, 시각, 언어, 수치 벡터의 세 가지 입력 양식을 지원합니다.

- **모델 아키텍처**:

  - **입력 인코딩**:
    - **텍스트**: LLaMA-7B의 사전 학습된 임베딩 레이어를 사용합니다.
    - **이미지**: 각 이미지를 $L$개의 패치 $p_{l}$로 나누고, 학습 가능한 2D 컨볼루션 네트워크를 통해 벡터 공간으로 매핑합니다.
    - **행동**: 멀티 레이어 퍼셉트론(MLP)이 행동 값을 동일한 벡터 공간으로 인코딩합니다.
  - **트랜스포머 백본**: 모든 양식의 임베딩을 시퀀스 $\tau_{t}$로 연결하여 트랜스포머에 입력합니다.
  - **출력**:
    - **대화**: LLaMA-7B의 사전 학습된 출력 MLP 레이어와 토크나이저를 활용하여 텍스트를 생성합니다.
    - **행동 결정**: `<EOS>` 토큰 다음에 추가 임베딩 벡터를 생성합니다. OpenVLA와 달리, 행동 공간을 이산화하지 않고 MLP로 구성된 `행동 헤드`가 임베딩을 직접 연속적인 행동 값(가속 및 조향, $D=2$)으로 매핑합니다.

- **학습 절차**:
  - LoRA를 사용하여 트랜스포머 백본을 미세 조정하고, 이미지, 텍스트, 행동 인코딩 및 디코딩 모듈을 전문가 오프라인 데이터셋 $D_{\text{expert}}$(운전 궤적 및 QA 주석 포함)에서 공동으로 학습합니다.
  - **보조 이미지 재구성 작업**: 전치 컨볼루션(transposed convolution) 레이어 $f_{\phi}$를 통해 출력 임베딩에서 입력 이미지 패치를 재구성하여 시각 양식의 특징 학습을 강화합니다.
  - **전체 손실 함수**: 세 가지 손실 항의 가중 합으로 정의됩니다:
    $$ L = \alpha*{1} L*{\text{language}} + \alpha*{2} L*{\text{action}} + \lambda L\_{\text{image}} $$
    - **$L_{\text{language}}$ (텍스트 생성)**: 데이터의 순차적 특성으로 인한 과적합을 방지하기 위해 `레이블 스무딩`을 적용한 교차 엔트로피 손실을 사용합니다.
      $$ q*{k}^{i} = \begin{cases} 1-\epsilon & \text{if } k=y*{i} \\ \frac{\epsilon}{K-1} & \text{otherwise} \end{cases} $$
            $$ L*{\text{language}}(\theta) = \frac{1}{N} \sum*{i} \sum*{k} q*{k}^{i} \log p(k|\tau\_{:i-1},\theta) $$
    - **$L_{\text{action}}$ (행동 예측)**: 실제 행동 $a_{t}$와 예측 행동 $\hat{a}_{t}$ 간의 `평균 제곱 오차(MSE)` 손실을 사용하여 연속적인 행동 값을 직접 예측합니다.
      $$ L*{\text{action}}(\theta) = \frac{1}{T} \sum*{t=1}^{T} \frac{1}{D} \sum*{d=1}^{D} [(a*{d}^{t} - \pi*{\theta}(\hat{a}*{d}^{t}|\tau\_{t}))^{2}] $$
    - **$L_{\text{image}}$ (이미지 재구성)**: 원본 이미지 $o_{t}$와 재구성된 이미지 패치 간의 픽셀 단위 유클리드 거리(MSE)를 사용합니다.
      $$ L*{\text{image}}(\theta,\phi) = \frac{1}{L} \sum*{l=1}^{L} \text{mse}(o*{t},f*{\phi}(\pi*{\theta}(g*{\theta}(\tau*{:p*{l}^{t}}^{t}))))) $$
  - **그레디언트 공간 분리**: $L_{\text{language}}$는 텍스트 토큰 위치에만, $L_{\text{action}}$는 행동 토큰 위치에만, $L_{\text{image}}$는 이미지 패치 위치에만 영향을 미치도록 명시적으로 구현하여 양식별(modality-specific) 표현 학습을 촉진합니다.

## 📊 Results

- **CARLA 자율 주행 플랫폼에서의 성능**:
  - MIMO-VLA는 단일 시간 스텝($H=1$) 및 여러 시간 스텝($H=4$) 모두에서 `Driving Score (DS)`, `Average Reward (AR)`, `Average Safe Driving Distance (ASD)`에서 BC, OpenVLA, DriverGPT4 등 다른 베이스라인 모델들을 크게 능가했습니다. 특히 DriverGPT4는 정확한 행동 값 생성을 자주 실패했습니다.
  - `town03`에서 훈련하고 `town04`에서 테스트했을 때, MIMO-VLA는 뛰어난 일반화 능력을 보였습니다.
- **대화 능력**:
  - MIMO-VLA는 GPT-4o 평가에서 질문-응답 품질 면에서 다른 모델들을 현저히 능가했습니다. OpenVLA와 DriverGPT4는 대화 생성에 어려움을 겪었습니다.
  - `L_{language}` 손실 구성 요소는 대화 능력 향상에 결정적이며, `L_{image}`는 대화 성능에 덜 직접적으로 기여했습니다.
- **손실 함수 설계에 대한 제거 연구 (Ablation Studies)**:
  - `L_{action}`(연속 행동)은 OpenVLA 및 RT2에서 사용되는 이산화된 행동 기반 `L_{action-bins}`보다 우수하며, 이는 MIMO-VLA가 더 미세하고 정확한 제어를 가능하게 하는 이유입니다.
  - `L_{language}`를 포함하는 것은 대화 능력뿐만 아니라 의사 결정 품질도 크게 향상시켰습니다.
  - `L_{image}`는 손실 함수에 추가될 때 모든 의사 결정 성능 지표를 개선하여, 모델이 풍부한 장면 정보를 추출하고 활용하는 데 도움을 줍니다.
- **동시 멀티태스킹 출력의 충돌 해결**:
  - MIMO-VLA는 텍스트 생성과 행동 생성에 별도의 목적 함수를 사용하여, DriverGPT4와 달리 작업 간 간섭 없이 효율적인 병렬 처리를 가능하게 합니다.
- **텍스트 데이터 품질이 의사 결정 능력에 미치는 영향**:
  - 운전과 관련 없는 센서 입력에 노이즈를 주입하면 모델의 의사 결정 성능이 급격히 저하되지만, QA 내용에만 노이즈를 주입했을 때는 성능 저하가 덜 두드러졌습니다. 이는 MIMO-VLA가 인간 운전자와 유사하게 관련 센서 입력에 대한 강건성을 보임을 시사합니다.

## 🧠 Insights & Discussion

- MIMO-VLA는 MISO 아키텍처의 내재된 작업 충돌을 해결하고 병렬 멀티태스킹 출력을 위한 통합 프레임워크를 제공하여 대규모 사전 학습 모델의 새로운 방향을 제시합니다.
- 연속적인 행동 처리, 레이블 스무딩, 이미지 재구성 손실을 포함한 세심한 손실 함수 설계, 그리고 그레디언트 공간 분리는 대화와 의사 결정 능력을 효과적으로 균형 잡는 데 중요합니다.
- 관련 센서 입력에 대한 강건성과 대화 내용의 노이즈에 대한 상대적인 허용치는 모델의 인간과 유사한 의사 결정 행동을 나타냅니다.
- 이 연구는 종단간(end-to-end) 방식으로 대화와 행동 생성을 동시에 처리할 수 있는 통합 생성 모델 개발의 중요한 첫걸음입니다.
- **한계**:
  - 자율 주행에 대한 유효성 검증에 국한되어 있으며, 로봇 공학이나 인간-컴퓨터 상호작용과 같은 다른 도메인으로의 일반화 가능성은 아직 테스트되지 않았습니다.
  - 대화 및 행동 생성의 공동 최적화는 멀티태스킹 조정 및 확장성 측면에서 개선이 필요합니다.
  - 대화 평가는 시나리오 분류 및 언어적 다양성이 부족하여 향후 연구에서 더 상세한 평가가 필요합니다.
  - 긴 텍스트 프롬프트나 큰 이미지 입력은 동기화 지연을 초래할 수 있으므로, 실시간 성능을 지원하기 위한 효율적인 토큰 처리 및 리소스 할당이 필요합니다.

## 📌 TL;DR

**문제**: 기존 LLM 및 VLA는 MISO 아키텍처로 인해 동시 대화 및 의사 결정과 같은 병렬 멀티태스킹 시나리오에서 출력 채널 충돌로 인해 성능이 저하됩니다.

**제안 방법**: MIMO-VLA (VLASCD)는 병렬 멀티태스킹 출력을 위한 통합 MIMO 학습 아키텍처입니다. LLaMA-7B 백본을 기반으로, MLP를 통한 연속 행동 예측, 레이블 스무딩이 적용된 언어 손실, 이미지 재구성 손실을 포함하는 복합 손실 함수를 사용합니다. 각 작업의 손실 그레디언트를 분리하여 작업 간 간섭을 제거합니다.

**주요 결과**: CARLA 자율 주행 시뮬레이션에서 MIMO-VLA는 SOTA MISO 모델(예: OpenVLA, DriverGPT4)을 능가하며, 실시간 행동 의사 결정 정확도를 높이면서 유창한 대화 기능을 유지합니다. 연속 행동 처리 및 모달리티별 손실 설계가 핵심이며, 관련 없는 대화 노이즈에 대한 강건성을 보였습니다.
