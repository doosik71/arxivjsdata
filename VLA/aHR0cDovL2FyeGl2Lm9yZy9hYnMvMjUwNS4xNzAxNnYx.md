# Interactive Post-Training for Vision-Language-Action Models

Shuhan Tan, Kairan Dou, Yue Zhao, Philipp Krähenbühl

## 🧩 Problem to Solve

기존 Vision-Language-Action (VLA) 모델 훈련 파이프라인은 주로 오프라인 전문가 시연 데이터와 지도 모방 학습(Supervised Imitation Learning, SFT)에 크게 의존합니다. 이로 인해 다음과 같은 주요 한계가 발생합니다:

- **환경 상호작용 부재:** 모델이 실제 환경에서 자신의 행동 결과를 직접 경험하지 못하여, 특히 장기적인 작업을 수행할 때 실제 시나리오의 복잡성을 처리하는 데 실패할 수 있습니다.
- **데이터 의존성:** 새로운 작업이나 환경에 적응하기 위해 대규모의 고품질 시연 데이터가 필요하며, 데이터가 부족한 환경(low-data regimes)에서는 성능이 크게 저하됩니다.
- **배포 불일치:** 오프라인 데이터에 대한 최적화가 실제 작업 성공률 향상으로 이어지지 않을 수 있으며, 분포 변화(distribution shift)와 연쇄 오류(cascading errors)로 인해 실제 롤아웃에서 정책이 취약해질 수 있습니다.

이 논문은 VLA 모델이 희소한 이진 성공 보상(sparse binary success rewards)만을 사용하여 새로운 작업과 환경에 효율적으로 적응하고, 실제 상호작용을 통해 성능을 개선할 수 있는 방법을 제시하여 이러한 문제들을 해결하고자 합니다.

## ✨ Key Contributions

- **RIPT-VLA(Reinforcement Interactive Post-Training for VLA) 도입:** 사전 훈련된 VLA 모델을 희소한 이진 성공 보상만으로 미세 조정하는 간단하고 확장 가능한 강화 학습 기반 상호작용 후훈련 패러다임을 제안합니다.
- **안정적이고 효율적인 정책 최적화:** 동적 롤아웃 샘플링(dynamic rollout sampling)과 Leave-One-Out 이점 추정(Leave-One-Out Advantage Estimation)에 기반한 안정적인 정책 최적화 알고리즘을 제안하여, 비평가(critic) 모델 없이도 희소한 보상 환경에서 효과적인 학습을 가능하게 합니다.
- **다양한 VLA 모델에 적용 가능성:** 가벼운 QueST 모델의 성능을 평균 21.2% 향상시키고, 7B OpenVLA-OFT 모델의 성공률을 전례 없는 97.5%까지 끌어올려 다양한 VLA 모델에 대한 일반적인 효과를 입증합니다.
- **높은 데이터 및 연산 효율성:** 단 하나의 시연 데이터만으로도 거의 실패하는 SFT 모델(4%)의 성공률을 15회 반복 내에 97%까지 끌어올려 극도로 낮은 데이터 환경에서의 효율성을 보여줍니다.
- **우수한 일반화 능력 및 강건성:** RIPT-VLA로 학습된 정책이 다양한 작업 및 시나리오에 걸쳐 일반화되며, 초기 상태 컨텍스트에 대해 강건함을 입증합니다.
- **비평가(Critic-free) 최적화:** 학습된 보상 모델이나 비평가 모델에 의존하지 않아 학습 안정성을 높이고 GPU 메모리 사용량 및 훈련 비용을 줄입니다.

## 📎 Related Works

- **Vision-Language-Action (VLA) 모델:** RT-2, RT-1, PaLM-E, Octo, OpenVLA 등과 같은 선행 연구들은 VLA 모델이 멀티모달 입력(시각, 언어)을 해석하여 물리적 세계에서 행동할 수 있도록 합니다. 이들은 주로 대규모 사전 훈련(Stage 1)과 태스크별 지도 미세 조정(Stage 2)의 두 단계로 훈련되지만, 오프라인 데이터 의존성으로 인해 새로운 태스크 적응 및 소규모 시연 데이터에서의 성능 저하라는 한계가 있습니다.
- **대규모 언어 모델(LLM)을 위한 강화 학습:** LLM 훈련 패러다임에서 사전 훈련 및 SFT 이후 강화 학습(RL)을 세 번째 단계로 활용하여 모델의 추론, 계획, 제약 충족 능력을 향상시키는 선례를 제공합니다. RLAIF, Tree-of-Thoughts 등 LLM의 잠재력을 상호작용 피드백을 통해 활성화하는 방식은 RIPT-VLA의 주요 동기가 됩니다.
- **VLA를 위한 강화 학습:** iRe-VLA 및 ConRFT와 같은 최근 연구들은 사전 훈련된 VLA 모델에 강화 학습을 적용하여 SFT의 한계를 극복하고 새로운 시연 데이터 없이 새로운 태스크에 적응하는 방법을 탐색했습니다. 그러나 이들은 여전히 학습된 가치 비평가(value critic) 또는 형상화된 보상 함수(shaped reward functions)에 의존하여 안정화를 위해 오프라인-온라인 단계 간의 세심한 조율이 필요합니다. RIPT-VLA는 이와 대조적으로 완전히 비-비평가(critic-free) 방식이며 희소한 이진 보상만으로 최적화를 수행합니다.

## 🛠️ Methodology

RIPT-VLA는 기존 VLA 훈련의 2단계 패러다임에 세 번째 단계로 강화 상호작용 후훈련을 추가합니다.

1. **Stage 1: 사전 훈련 (Pretraining)**
   - 대규모의 다양한 실제 시연 데이터 $D_{\text{pretrain}}$을 사용하여 VLA 모델 $\pi_{\theta}$를 사전 훈련합니다.
   - 모델은 시각-언어 표현과 일반적인 시각-운동 기술을 학습합니다.
2. **Stage 2: 지도 미세 조정 (Supervised Fine-tuning, SFT)**
   - 더 작은 규모의 다중 태스크 데이터셋 $D_{\text{sft}}$을 사용하여 사전 훈련된 정책을 미세 조정합니다.
   - 특정 대상 태스크에 대한 지시를 따르도록 모델을 적응시킵니다. 이 단계 후에도 정책은 낮은 성공률을 보일 수 있습니다.
3. **Stage 3: 강화 상호작용 후훈련 (Reinforcement Interactive Post-Training, RIPT-VLA)**
   - SFT된 $\pi_{\theta}$를 환경 $E$에서 롤아웃하고, 이진 보상 $R(c,a) \in \{0,1\}$를 받습니다.
   - 훈련은 **롤아웃 수집 (Rollout Collection)**과 **정책 최적화 (Policy Optimization)**의 두 단계로 반복됩니다.
   - **롤아웃 수집:**
     - 컨텍스트 데이터셋 $D_c$에서 컨텍스트 $c = (g, o_1)$를 샘플링합니다. $D_c$는 $D_{\text{sft}}$에서 추출된 초기 상태로 구성됩니다.
     - 현재 정책 $\pi_{\theta}$를 샘플링 정책 $\pi_{\psi}$로 설정하고, 동일한 컨텍스트 $c$에 대해 $K$개의 롤아웃 $\{a_k \sim \pi_{\psi}(\cdot|c)\}_{k=1}^K$를 생성합니다.
     - 각 롤아웃 $a_k$에 대해 환경으로부터 이진 보상 $R_k = R(c, a_k)$를 받습니다.
     - **Leave-One-Out 이점 추정(RLOO) 적용:** 각 롤아웃 $k$에 대한 이점 $A_k$를 계산합니다. 이는 다른 롤아웃들의 보상 평균을 기준으로 해당 롤아웃의 성능이 얼마나 더 좋거나 나쁜지를 나타냅니다.
       $$b_k = \frac{1}{K-1} \sum_{j \neq k} R_j, \quad A_k = R_k - b_k$$
     - **동적 롤아웃 샘플링 (Dynamic Rollout Sampling):** $K$개 롤아웃 모두가 동일한 보상(모두 성공 또는 모두 실패)을 받아 이점 $A_k$가 모두 0이 되는 컨텍스트는 버리고 새로운 컨텍스트를 다시 샘플링합니다. 이는 정보가 없는 컨텍스트를 걸러내고 안정적인 그래디언트 신호를 보장하여 최적화를 어려운 컨텍스트에 집중시킵니다.
     - $B$개의 롤아웃이 수집될 때까지 $(c, a_k, A_k)$ 쌍을 $D_{\text{rollout}}$에 추가합니다.
   - **정책 최적화:**
     - 수집된 $D_{\text{rollout}}$ 데이터셋에 대해 $N$회 반복하여 근접 정책 최적화(PPO)를 사용하여 $\pi_{\theta}$를 업데이트합니다.
     - PPO 손실 함수는 다음과 같이 클리핑된 목적 함수를 사용합니다:
       $$L_{\text{PPO}} = -\min \left(r_i A_i, \text{clip}(r_i, 1-\epsilon, 1+\epsilon)A_i\right)$$
       여기서 $r_i = \pi_{\theta}(a_i|c_i) / \pi_{\psi}(a_i|c_i)$는 중요도 비율(importance ratio)이며, $\epsilon$은 클리핑 임계값입니다.
   - **다양한 VLA 모델에 대한 일반화:**
     - **토큰화된 행동 헤드 (Tokenized Action Head, 예: QueST):** 행동을 이산 토큰으로 예측하며, `softmax` 출력 로짓에서 직접 `log_probability`를 얻습니다.
     - **회귀 행동 헤드 (Regression Action Head, 예: OpenVLA-OFT):** 연속적인 행동을 회귀 방식으로 예측합니다. 기존 출력 헤드가 평균 $\mu_{\theta}$를 제공한다고 가정하고, 가벼운 예측 헤드를 추가하여 행동 값의 스케일 $\sigma_{\theta}$를 추정합니다. 정책을 인자화된 가우시안(Gaussian) 또는 라플라스(Laplace) 분포로 가정하여 `log_probability`를 계산할 수 있습니다.

## 📊 Results

- **표준 다중 태스크 훈련:**
  - LIBERO 벤치마크 4개 스위트에서 QueST 모델의 성능을 평균 10.9% 절대 성공률(SR) 향상시키며, 특히 어려운 LONG 스위트에서는 18.7%의 큰 향상을 보였습니다.
  - OpenVLA-OFT 모델의 경우, 이미 높은 성공률(96.7%)에도 불구하고 평균 실패율을 3.3%에서 2.4%로 추가 감소시켜 평균 성공률을 97.5%로 달성하며 새로운 SOTA를 기록했습니다.
  - LIBERO-90(90개 태스크) 및 MetaWorld45(45개 태스크) 벤치마크에서도 QueST 성능을 각각 5.7% 및 1.2% 향상시키며 새로운 SOTA를 달성하여 대규모 다중 태스크 시나리오에서의 확장성을 입증했습니다.
- **소규모 데이터 다중 태스크 훈련 (Few-shot):**
  - LIBERO-LONG 및 ML45 스위트에서 각 태스크당 5개의 시연 데이터만 사용했을 때, RIPT-VLA는 QueST 성능을 각각 21.2% 및 12.4% 크게 향상시켰습니다.
  - 특히, 태스크당 단 1개의 시연 데이터만 사용한 극도로 낮은 데이터 환경에서도 RIPT-VLA는 20.8%의 절대 성능 향상을 달성했습니다.
- **교차 시나리오 일반화:**
  - 동일한 태스크 목표를 가졌지만 다른 시나리오(배경, 객체 구성이 다름)에서 1개의 시연 데이터로 SFT 모델이 평균 5% 정도의 성공률을 보였으나, RIPT-VLA는 최대 93.7%의 절대 성공률 향상(3.5%에서 97.2%로)을 보였습니다. 3~5개의 시연 데이터만으로도 거의 100%에 가까운 성능에 도달했습니다.
- **교차 목표 일반화:**
  - 동일한 시나리오에서 다른 목표를 가진 태스크 쌍에 대해(예: "오른쪽 접시에 빨간 머그잔 놓기" vs. "왼쪽 접시에 빨간 머그잔 놓기"), 3개의 시연 데이터로 SFT 모델은 평균 0.7%의 성공률을 보인 반면, RIPT-VLA는 평균 59.7%까지 성능을 향상시켰습니다.
- **추가 연구:**
  - **동적 롤아웃 샘플링의 효과:** 동적 롤아웃 샘플링은 모든 태스크 범주와 스위트에서 성능을 크게 향상시켰으며, 비-동적 샘플링(non-dynamic sampling) 변형 대비 평균 3.3%의 절대 성공률 향상을 보이며 수렴을 가속화했습니다.
  - **컨텍스트 데이터셋 크기의 효과:** 롤아웃 컨텍스트 수가 증가할수록 성능이 크게 향상되었으며, 이는 초기 상태 다양성이 증가하여 모델의 일반화 능력을 향상시킵니다.
  - **RLOO 그룹 내 컨텍스트 분산의 효과:** 초기 상태에 가우시안 노이즈를 주입하여 시뮬레이션한 결과, RIPT-VLA는 객체 초기 위치의 표준 편차가 2.5cm(1.0배)까지는 성능이 안정적으로 유지되었고, 7.0배(17.5cm) 분산에서도 SFT 기준선을 크게 능가하는 강건성을 보였습니다.

## 🧠 Insights & Discussion

RIPT-VLA는 사전 훈련된 VLA 모델의 잠재력을 직접적인 환경 상호작용을 통해 효과적으로 발휘하는 강력한 패러다임을 제공합니다.

- **오프라인-온라인 격차 해소:** 기존의 오프라인 지도 훈련 방식의 한계(분포 변화, 연쇄 오류)를 넘어 모델이 실제 롤아웃의 결과를 학습하게 하여 정책의 강건성을 크게 높입니다.
- **데이터 효율성의 극대화:** 대규모 시연 데이터 수집의 부담 없이, 심지어 단 하나의 시연 데이터만으로도 모델의 성능을 비약적으로 향상시킵니다. 이는 실제 로봇 공학 배포에서 데이터 희소성 문제를 완화하는 데 매우 중요합니다.
- **안정적이고 효율적인 학습:** 비평가 모델이나 형상화된 보상 없이도 희소한 이진 보상을 효과적으로 사용하여 안정적인 정책 최적화를 달성합니다. 동적 롤아웃 샘플링은 학습 신호를 일관되게 유지하여 효율성을 증대시킵니다.
- **강력한 일반화 능력:** 교차 시나리오 및 교차 목표 설정에서 RIPT-VLA는 사전 훈련된 시각-운동 기술을 빠르게 활성화하고 새로운 환경 및 태스크 목표에 적응하는 능력을 보여주며, 이는 VLA 모델의 전이 학습 및 일반화 잠재력을 크게 확장합니다.
- **확장 가능한 3단계 훈련 패러다임:** 기존의 사전 훈련 및 SFT 파이프라인을 보완하는 확장 가능한 3단계 훈련 패러다임으로서, 다양한 규모의 VLA 모델에 적용 가능하며 성능 한계를 돌파할 수 있습니다.

향후 연구 방향으로는 RIPT-VLA를 VLA 모델의 추론 및 계획 능력과 결합하여 더욱 정교하고 일반화 가능한 행동을 복잡한 환경에서 구현하는 것이 있습니다.

## 📌 TL;DR

RIPT-VLA는 기존 VLA 모델의 오프라인 훈련 한계를 극복하기 위해, 희소한 이진 성공 보상만을 사용하는 **강화 학습 기반 상호작용 후훈련 패러다임**을 제안합니다. 이 방법은 동적 롤아웃 샘플링과 Leave-One-Out 이점 추정 기반의 **비평가(critic-free) 정책 최적화**를 통해, **단 하나의 시연 데이터**로도 SFT 모델의 성공률을 97%까지 끌어올리는 **매우 높은 데이터 효율성**을 보입니다. 다양한 벤치마크에서 **SOTA 성능**을 달성하고, **교차 시나리오/목표 일반화 능력**을 크게 향상시켜, 사전 훈련된 VLA 모델의 잠재력을 환경 상호작용을 통해 효율적으로 발휘할 수 있음을 입증합니다.
