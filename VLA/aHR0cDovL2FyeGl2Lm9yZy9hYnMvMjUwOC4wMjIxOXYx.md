# CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning

Dongchi Huang, Zhirui Fang, Tianle Zhang, Yihang Li, Lin Zhao, Chunhe Xia

## 🧩 Problem to Solve

시각-언어-행동(VLA) 모델은 실제 로봇 제어를 위한 일반화된 정책 개발에 큰 잠재력을 보여주지만, 강화 학습(RL)을 통한 미세 조정은 다음과 같은 문제에 직면합니다:

1. **샘플 효율성:** VLA 모델은 방대한 데이터셋에서 사전 학습되지만, 특정 작업에 대한 RL 미세 조정 시 샘플 효율성이 낮습니다.
2. **행동 청킹(Action Chunking) 호환성:** VLA 모델의 주요 특징인 행동 청킹 기법이 기존 RL 연구에서는 간과되어 왔습니다.
3. **학습 안정성:** 대규모 모델에 대한 RL 학습의 안정성을 확보하기 어렵습니다.
4. **일반화 부족:** 기존 지도 학습 기반 미세 조정(SFT)은 전문가 시연 데이터의 품질과 양에 크게 의존하며, 분포 외(OOD) 시나리오에 대한 일반화가 어렵습니다.
5. **온라인 RL의 제약:** 온라인 RL은 자율 학습 및 대규모 모델의 안정성을 위한 인프라 구축이 필수적이며, 주로 시뮬레이터에 국한됩니다.
6. **추론 시간 문제:** 테스트 시간 RL(Test-time RL)은 성능 개선이 미미하고 추론 시간을 증가시킵니다.

## ✨ Key Contributions

- **청킹 RL(Chunked RL) 프레임워크 제안:** VLA 모델에 특화된 새로운 강화 학습 프레임워크인 청킹 RL을 제안하여, 시간차(TD) 학습을 행동 청킹 개념과 통합했습니다.
- **CO-RFT 알고리즘 개발:** 제한된 시연(30-60개 샘플)을 사용하여 VLA 모델을 미세 조정하기 위한 2단계 오프라인 RL 알고리즘인 CO-RFT를 제안했습니다.
- **상당한 성능 향상:** 실제 환경에서 기존 지도 학습 방법 대비 성공률 57% 향상, 사이클 시간 22.3% 감소를 달성했습니다.
- **강력한 위치 일반화 능력 입증:** 이전에 보지 못한 위치에서 44.3%의 성공률을 달성하여 강력한 위치 일반화 능력을 보여주었습니다.
- **효율적인 전이 학습:** 그리퍼 로봇용으로 학습된 RoboVLMs를 30~60개의 샘플만으로 정교한 손 로봇에 성공적으로 전이하여 여러 실제 작업을 숙달했습니다.

## 📎 Related Works

- **시각-언어-행동(VLA) 모델:** 방대한 로봇 데이터셋을 모방 학습하여 일반화 가능한 로봇 정책을 개발하며 상당한 잠재력을 보여줍니다 (Kim et al. 2024, AgiBot-World-Contributors et al. 2025 등). 그러나 전문가 시연 수집 비용, 낮은 추론 속도, OOD 시나리오 처리 능력 부족 등의 문제가 있습니다.
- **VLA 모델을 위한 강화 학습:** LLM 및 VLM 미세 조정에서 RL의 성공에 영감을 받아 VLA 모델 성능 향상에 RL을 통합하려는 노력이 진행 중입니다 (ReinboT, Zhang et al. 2025a; TPO, Zhang et al. 2024; iRe-VLA, Guo et al. 2025; ConRFT, Chen et al. 2025 등). 하지만 이들 방법은 **강화 학습과 행동 청킹의 호환성 문제**를 다루지 않습니다.
- **행동 청킹:** 정책이 각 결정 단계에서 일련의 행동을 예측하는 방식으로, 모방 학습(IL)에서 행동 부드러움과 비마르코프적 행동 처리에 효과적임이 입증되었습니다. RL에 청킹을 통합하려는 연구가 있었으며 (Tian et al. 2025; Li et al. 2025; Seo and Abbeel 2025; Li, Zhou, and Levine 2025), 샘플 효율성, 긴 시야 및 희소 보상 처리 능력을 향상시킬 수 있음을 보여주었습니다. 이 연구들은 VLA 미세 조정에서 발생하는 주요 장애물들을 다루지만, VLA 모델의 맥락에서는 이전에 간과되었습니다.

## 🛠️ Methodology

CO-RFT는 두 가지 주요 설계 원칙과 2단계 학습 프레임워크를 따릅니다.

### 청킹 강화 학습 (Chunked Reinforcement Learning)

VLA 모델의 행동 청킹 기법과 호환되도록 시간적으로 확장된 행동 공간에 Q-러닝을 적용합니다.

- **정책($\pi_{\psi}$):** $h$개의 연속적인 행동을 예측합니다: $\pi_{\psi}(a_{t:t+h} | s_t) := \pi_{\psi}(a_t, a_{t+1}, \cdots, a_{t+h-1} | s_t)$
- **가치 함수($Q_{\theta}$):** $h$개의 연속적인 행동에 대한 Q-값을 예측합니다: $Q_{\theta}(s_t, a_{t:t+h}) := Q_{\theta}(s_t, a_t, a_{t+1}, \cdots, a_{t+h-1})$
- **청킹 TD-학습 (Chunked TD-learning):**
  - 크리틱과 액터를 $h$개의 연속적인 행동($a_{t:t+h}$)과 $h$단계 이후의 상태($s_{t+h}$)로 구성된 트랜지션 배치로 업데이트합니다.
  - 타겟은 $G_{(h)}^t (s_t, a_{t:t+h}) \leftarrow \sum_{t'=t}^{t+h-1} \gamma^{t'-t} r_{t'} + \gamma^h \bar{Q}(s_{t+h}, a_{t+h:t+2h})$ 로 정의되며, $\bar{Q}$는 타겟 네트워크입니다.
  - 이를 통해 Q-값이 역방향으로 전파되는 시간 단계 수를 $h$배 빠르게 할 수 있습니다.
- **청킹 크리틱 학습 목표:**
  - 크리틱은 주어진 다음 $n$개 행동에 대해 1부터 $N$까지의 모든 N-스텝 리턴을 출력하도록 학습됩니다.
  - $L(\theta) = \frac{1}{Nh} \sum_{k=1}^N \sum_{i=1}^h \left[ Q_{\theta}(s_t, a_{t:t+i}) - G_{(h)}^t (s_t, a_{t:t+i}) \right]^2$
- **청킹 크리틱 네트워크:** 계산 비용을 줄이기 위해 상태와 행동 청크를 입력으로 받아 자기-어텐션과 인과 마스크를 사용하는 단일 트랜스포머 블록을 사용하여 모든 Q-값을 학습합니다.

### CO-RFT의 2단계 학습 프레임워크

1. **단계 1: 행동 복제(Behavior Cloning, BC):**
   - 기존 VLA 아키텍처는 그리퍼 데이터셋과 그리퍼 로봇으로 학습되었으므로, 먼저 VLM 백본과 액션 헤드를 현재 작업 공간 및 로봇으로 전이합니다.
   - 텔레오퍼레이션을 통해 30개의 시연을 수집하고, 다음 목적 함수를 사용하여 BC를 수행합니다: $L_{BC} = E_{(s_e, a_e) \sim \tau_e} \|a_e - \pi_{BC}(s_e)\|^2$.
2. **단계 2: 청킹 오프라인 RL (Chunked Offline RL):**
   - 사전 학습된 정책을 추가로 최적화하기 위해 행동 청킹을 포함한 오프라인 RL을 적용합니다.
   - **CalQL** 알고리즘(Nakamoto et al. 2024b)을 기반으로 희소 보상 및 OOD 문제에 대응합니다.
   - 크리틱 학습 목표는 다음과 같습니다:
     $$L(\theta) = E_{s,a,s' \sim D} \left[ Q_{\theta}(s, a_{t:t+i}) - B_{\pi}Q(s, a_{t:t+i}) \right]^2 + \alpha \left( E_{s \sim D, a \sim \pi} [\max (Q_{\theta}(s, a_{t:t+i}), V_{\mu}(s))] - E_{s,a \sim D} [Q_{\theta}(s, a_{t:t+i})] \right)$$
   - 정책 학습 목표는 다음과 같습니다:
     $$L(\psi) = - \frac{1}{Nh} \sum_{k=1}^N \sum_{i=1}^h Q_{\theta}(s_t, a_{t:t+i})$$

### 실제 구현 (Practical Implementations)

- **모델 아키텍처:** RoboVLMs(Li et al. 2024) 기반으로 Kosmos2를 VLM 백본으로 사용하며, TD3(Fujimoto, van Hoof, and Meger 2018) 알고리즘을 기반으로 구현되었습니다.
- **보상 업샘플링 (Reward Upsampling):** 희소 보상 문제를 해결하기 위해 인간 텔레오퍼레이션 중 추가적인 성공 단계를 기록하는 데이터 수집 전략을 사용하여 보상 신호를 포함하는 샘플 수를 늘립니다.

## 📊 Results

- **성공률 개선:** CO-RFT는 SFT(지도 학습 미세 조정) 대비 평균 57%의 성공률 향상을 보였으며, 6개 작업 중 4개 작업에서 거의 100%에 가까운 성공률을 달성했습니다. SFT가 실패했던 작업(컵 잡기, 소독제 잡기, 머그컵 가져오기)에서도 유망한 성능을 보였습니다. 이는 제한된 샘플(30-60개)로도 훨씬 더 나은 샘플 효율성을 달성했음을 시사합니다.
- **사이클 시간 단축:** CO-RFT는 평균 22.3%의 사이클 시간 단축을 가져왔으며, 이는 전문가 시연을 능가하는 효율적인 정책 학습 가능성을 보여줍니다. 다만, "큐브 집기" 작업에서는 데이터 다양성 부족으로 개선이 없었습니다.
- **위치 일반화 능력:** OOD(분포 외) 시나리오에서 SFT 대비 38%의 성공률 향상을 보이며 뛰어난 성능을 입증했습니다. 예를 들어, 컵 잡기, 고리 잡기, 바코드 스캐너 들기 작업에서 각각 90%, 50%, 80%의 성공률을 달성했습니다. 이는 오프라인 RL이 온라인 탐색 없이도 뛰어난 일반화 능력을 가질 수 있음을 의미합니다.
- **데이터 다양성의 중요성:**
  - 무작위 초기화 데이터셋으로 학습된 모델은 OOD에서 10-15%의 성능 저하만 보였습니다.
  - 고정 초기화 데이터셋으로 학습된 모델은 OOD에서 평균 55.3%의 치명적인 성능 저하를 겪었습니다.
  - 이는 더 다양한 상태와 행동을 포함하는 데이터셋이 더 정확한 가치 함수와 정책 일반화를 이끌어낸다는 RL의 원칙과 일치합니다.

## 🧠 Insights & Discussion

- CO-RFT는 행동 청킹을 강화 학습 과정에 통합한 Chunk RL을 통해 샘플 효율성을 높이고 강력한 위치 일반화 능력을 제공합니다.
- 제안된 방법은 그리퍼 로봇용으로 학습된 VLA 모델(RoboVLMs)을 30-60개의 샘플만으로 정교한 손 로봇 환경으로 성공적으로 전이시키고 여러 실제 작업을 숙달했습니다.
- 실험 결과는 오프라인 RL이 동일한 데이터셋을 사용하여 성공률, 사이클 시간, 일반화 능력 면에서 모방 학습(IL)을 능가할 수 있음을 보여줍니다. 이는 오프라인 RL이 VLA 모델 미세 조정을 위한 우수한 패러다임이 될 수 있음을 시사합니다.
- **한계점:**
  - 제안된 방법은 과적합(overtraining) 문제를 겪을 수 있어 최적의 체크포인트를 검증하는 추가 기법이 필요합니다.
  - 확정적 정책(deterministic policy)을 사용하여 다중 모드 행동 분포를 정확하게 모델링하지 못할 수 있습니다.
- **향후 연구:** 강화 학습과 확산 기반(diffusion-based) 정책 훈련을 함께 진행할 예정입니다.

## 📌 TL;DR

VLA 모델의 RL 미세 조정이 샘플 효율성, 행동 청킹 호환성, 학습 안정성 문제에 직면하고, 기존 SFT 방식은 일반화에 한계가 있다는 문제를 해결하고자 **CO-RFT**를 제안합니다. 이 2단계 오프라인 RL 알고리즘은 **Chunked RL** 프레임워크를 통해 VLA 모델의 행동 청킹을 TD 학습과 통합하고, 제한된 시연 데이터로 VLA 백본 및 정책을 초기화한 후 오프라인 RL(CalQL 기반)로 최적화합니다. CO-RFT는 실제 로봇 환경에서 SFT 대비 **57% 높은 성공률**과 **22.3% 짧은 사이클 시간**을 달성했으며, **강력한 위치 일반화 능력**을 보여주었습니다. 특히, 데이터 다양성이 일반화에 매우 중요함을 입증했습니다.
