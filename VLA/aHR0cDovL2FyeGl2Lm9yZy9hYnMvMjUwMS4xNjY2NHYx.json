{
  "url": "http://arxiv.org/abs/2501.16664v1",
  "title": "Improving Vision-Language-Action Model with Online Reinforcement\n  Learning",
  "authors": "Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, Jianyu Chen",
  "year": 2025,
  "abstract": "Recent studies have successfully integrated large vision-language models\n(VLMs) into low-level robotic control by supervised fine-tuning (SFT) with\nexpert robotic datasets, resulting in what we term vision-language-action (VLA)\nmodels. Although the VLA models are powerful, how to improve these large models\nduring interaction with environments remains an open question. In this paper,\nwe explore how to further improve these VLA models via Reinforcement Learning\n(RL), a commonly used fine-tuning technique for large models. However, we find\nthat directly applying online RL to large VLA models presents significant\nchallenges, including training instability that severely impacts the\nperformance of large models, and computing burdens that exceed the capabilities\nof most local machines. To address these challenges, we propose iRe-VLA\nframework, which iterates between Reinforcement Learning and Supervised\nLearning to effectively improve VLA models, leveraging the exploratory benefits\nof RL while maintaining the stability of supervised learning. Experiments in\ntwo simulated benchmarks and a real-world manipulation suite validate the\neffectiveness of our method.",
  "citation": 28
}