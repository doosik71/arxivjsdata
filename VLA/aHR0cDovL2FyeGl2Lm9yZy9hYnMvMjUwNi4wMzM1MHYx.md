# Adversarial Attacks on Robotic Vision-Language-Action Models

Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, J. Zico Kolter

## 🧩 Problem to Solve

시각-언어-행동(VLA) 모델은 다중 모드 입력을 융합하고 대규모 언어 모델(LLM) 아키텍처를 활용하여 로봇 공학 분야를 혁신하고 있습니다. 그러나 LLM은 적대적 공격(탈옥 공격)에 취약한 것으로 알려져 있으며, 로봇 공학의 내재된 물리적 위험을 고려할 때 VLA가 이러한 취약점을 얼마나 물려받는지에 대한 의문이 제기됩니다. 본 논문은 이러한 우려에 따라 VLA로 제어되는 로봇에 대한 적대적 공격 연구를 시작하여, 광범위한 배포 전에 잠재적 오용 및 안전하지 않은 행동으로 이어질 수 있는 위험을 이해하는 것을 목표로 합니다.

## ✨ Key Contributions

- **VLA 위협 모델:** VLA 제어 로봇에 대한 현실적인 위협 모델을 식별했습니다. 이는 텍스트 프롬프트를 통해 특정 로봇 행동을 유도하는 것과 관련이 있습니다.
- **VLA 공격 알고리즘:** 타겟 VLA 제어 로봇에서 고정된 행동 또는 행동 시퀀스를 유도하는 토큰 수준 공격 방식을 제안했습니다. 이는 LLM 탈옥 공격을 VLA에 적용한 것입니다.
- **목표 행동 유도 (Targeted Action Elicitation):** 적대적 프롬프트가 VLA의 행동 공간을 거의 모든 목표 행동에 도달할 수 있게 함을 보였습니다. LIBERO 데이터셋의 다양한 작업에 대해 OpenVLA 미세 조정 모델에서 90% 이상의 목표 행동 유도 성공률을 달성했습니다.
- **공격 지속성 (Attack Persistence):** 제안된 공격이 모델이 새로운 시각적 입력을 관찰하더라도 여러 VLA 롤아웃 단계에 걸쳐 지속되는 경향이 있음을 입증했습니다. 공격은 목표 지속성 롤아웃 단계를 기준선 대비 최대 28배 증가시켰습니다.
- **보편성 (Universality):** 제안된 공격이 환경에 구애받지 않고 시뮬레이션 및 실제 환경 모두에서 성공적으로 배포될 수 있음을 보여주었습니다.

## 📎 Related Works

- **로봇 공학을 위한 파운데이션 모델:** 로봇 제어 모델은 크게 두 가지 패러다임으로 나뉩니다.
  - **고수준 플래너 (High-level planners):** LLM이나 VLLM을 사용하여 `walk_forward`와 같은 고수준 명령을 통해 로봇을 제어합니다 (예: PaLM-E, LEO, SayCan).
  - **저수준 액추에이터 (Low-level actuators):** 시각-언어-행동(VLA) 모델로 불리며, 텍스트 목표 설명과 시각적 입력에 기반하여 로봇 팔의 토크나 속도와 같은 저수준 제어 행동을 생성하도록 훈련됩니다 (예: Octo, Google의 RT-1/RT-2/RT-X, OpenVLA, Physical Intelligence의 $\pi^0$, 확산 기반 CogACT).
- **적대적 공격 및 방어:** AI 안전 커뮤니티는 AI 오용을 완화하는 데 주력해왔습니다. 기존 연구는 주로 LLM 및 VLLM에 대한 탈옥(jailbreaking) 공격에 초점을 맞춰 유해한 텍스트 또는 유해한 시각 미디어를 유도하는 것이 목표였습니다. 최근에는 LLM 기반 고수준 플래너에 대한 탈옥 공격(Robey et al. [29], Zhang et al. [30])도 연구되었으나, 본 연구는 **저수준 VLA에 대한 공격을 고려한 첫 번째 연구**입니다.

## 🛠️ Methodology

본 논문은 LLM 탈옥 공격인 GCG(Greedy Coordinate Gradient)를 VLA 환경에 맞춰 적용합니다.

1. **VLA 위협 모델:**
   - VLA는 텍스트 프롬프트(고정)와 이미지 입력(매 단계 업데이트)을 융합하여 로봇의 저수준 행동을 생성합니다.
   - 액션 디토크나이저(action detokenizer)는 LLM 백본의 어휘 중 가장 적게 사용되는 토큰 집합 $A \subset V$를 로봇의 이산화된 행동 공간의 점들과 매핑하는 "심볼 튜닝(symbol tuning)" 방식을 사용합니다.
   - 공격은 텍스트 프롬프트를 수정하는 데 초점을 맞추며, 이는 로봇의 저수준 행동에 대한 **완전한 제어 권한(control authority)**을 획득하는 것을 목표로 합니다.
2. **VLA 공격 알고리즘:** GCG의 손실 함수를 VLA에 맞게 조정합니다.
   - 손실 함수: $\ell(x_{1:n};z_j) \triangleq -\text{log Pr} [x_{n+1:n+d}|x_{1:n};z_j]$
     - 여기서 $x_{1:n}$은 텍스트 프롬프트, $z_j$는 이미지 임베딩, $x_{n+1:n+d}$는 목표 행동을 나타냅니다.
   - **단일 단계 공격 (Single-step attacks):**
     - 하나의 고정된 목표 행동을 유도하는 것을 목표로 합니다.
     - 최적화 문제: $\text{minimize}_{x_i \in V:i \in I} \ell(x_{1:n};z)$
     - $z$는 첫 번째 롤아웃 단계의 이미지 임베딩입니다.
   - **지속성 공격 (Persistence attacks):**
     - 진화하는 이미지 표현에도 불구하고 목표 행동이 더 긴 시간 동안 지속되도록 합니다.
     - 여러 개의 고유한 이미지 임베딩 $z_j$에 대한 손실을 합산하여 목표 함수를 수정합니다: $\text{minimize}_{x_i \in V:i \in I} \sum_{j=1}^{r} \ell(x_{1:n};z_j)$
   - **전이 공격 (Transfer attacks):** 오픈 소스(open-weight) 모델에서 공격 문자열을 최적화한 다음, 다른 타겟 모델에 적용하여 전이 가능성을 평가합니다.
3. **구현 상세:**
   - VLA 프롬프트 템플릿: `In: What action should the robot take to [INSTRUCTION]? Out:` 여기서 공격 문자열은 `[INSTRUCTION]`을 대체하거나 끝에 추가됩니다.
   - 행동은 훈련 데이터셋의 통계를 기반으로 정규화됩니다 (일반적으로 $[-1,1]$ 사이).

## 📊 Results

- **단일 단계 공격:**
  - **높은 성공률 및 효율성:** OpenVLA의 4가지 미세 조정 버전에 대해 평가한 결과, Libero-Goal, Libero-Object, Libero-Spatial 모델은 90% 이상의 전반적인 공격 성공률(ASR)을 달성했습니다. Libero-10은 77.4%를 달성했습니다.
  - **빠른 최적화:** 성공적인 공격은 평균 30-110단계(3-10분) 내에 달성되었으며, 이는 챗봇 탈옥 공격보다 훨씬 효율적입니다.
  - **토큰 예산의 영향:** 공격자의 토큰 예산이 증가함에 따라 성공률이 향상되고 필요한 최적화 단계 수가 감소했습니다.
  - **환경 불가지론:** 시뮬레이션 환경(SIMPLER)과 실제 환경(HYDRA) 모두에서 단일 단계 공격이 성공하여 실제 환경에서의 제어 권한 획득 가능성을 입증했습니다 (OpenVLA 기본 모델의 경우 각각 38.0%, 61.2%의 전반적인 ASR).
- **지속성 공격:**
  - 공격은 VLA 롤아웃의 여러 단계에 걸쳐 지속되는 경향을 보였습니다.
  - 최적화에 더 많은 이미지($r \in \{1,2,3\}$)를 사용할수록 미관찰 이미지(unseen images)에 대한 일반화가 향상되었습니다.
  - 특히 Libero-Spatial 미세 조정 모델의 경우, 지속성 단계 수가 기준선 대비 최대 28배 증가했습니다.
- **전이 공격:**
  - **아키텍처 간 전이의 한계:** OpenVLA 기본 모델에서 최적화된 공격은 TraceVLA, CogACT, OpenPi0과 같은 다른 VLA 아키텍처로의 직접적인 전이 성공률이 낮았으며, 최적화된 명령이 무작위 명령보다 나은 성능을 보이지 않았습니다.
  - **앙상블 전이:** 동일한 OpenVLA 계열 내의 여러 모델(예: 2개의 OpenVLA 미세 조정 모델에 대한 보편적인 접미사 최적화)에 걸쳐 보편적인 공격을 성공적으로 훈련할 수 있음을 보여주었습니다.

## 🧠 Insights & Discussion

- **VLA 방어의 어려움:**
  - 기존 LLM 및 VLLM 방어 기법(예: 시스템 프롬프트, 인컨텍스트 데모) 중 상당수는 VLA에 적용하기 어렵습니다.
  - **Perplexity 필터 방어:**
    - 언어 전용(LLM-Only) perplexity 필터는 ASR을 0%로 낮추는 데 효과적이었으나, 실제 로봇 환경에서는 필요한 perplexity 임계값을 미리 알 수 없어 비실용적입니다.
    - 시각-언어 다중 모드 perplexity 필터는 이미지 입력의 지배력으로 인해 비효과적이었습니다 (ASR 100%).
  - **스무딩(Smoothing) 방어:** ASR을 0%로 만들었지만, 동시에 일반 명령까지 손상시켜 비공격 작업의 성공률도 0%로 만들었습니다.
  - 이는 VLA의 고유한 출력 형식과 제어 특성을 반영하는 새로운 방어 메커니즘이 필요함을 시사합니다.
- **안전 메커니즘의 필요성:**
  - LLM은 SFT(Supervised Fine-tuning), RLHF(Reinforcement Learning from Human Feedback), 적대적 훈련과 같은 정렬(alignment) 기술이 발전했지만, VLA는 아직 유사한 개념의 정렬 연구가 부족합니다.
  - 제어 탈취 시도를 감지했을 때 VLA가 행동을 거부하는 메커니즘이 필요하며, 고전 제어(예: 제어 장벽 함수)와 같은 도구를 통합할 수 있습니다.
- **연구의 의의 및 한계:**
  - 본 연구는 저수준 VLA 액추에이터에 대한 적대적 공격을 처음으로 탐구하여, 최적화된 명령을 통해 VLA에 대한 완전한 제어 권한을 획득할 수 있음을 보여주었습니다.
  - **한계:** GCG 알고리즘의 화이트박스(white-box) 특성 및 상대적인 비용, 확산 기반 모델 및 블랙박스 시나리오에 대한 공격 프레임워크 확장 필요성 등이 있습니다.

## 📌 TL;DR

시각-언어-행동(VLA) 모델은 LLM의 취약점을 계승하여 적대적 공격에 취약합니다. 본 논문은 특수하게 제작된 텍스트 프롬프트를 통해 VLA 제어 로봇에 대한 **완전한 제어 권한**을 획득할 수 있음을 입증했습니다. 이러한 공격은 높은 성공률과 지속성을 보이며, 심지어 다양한 환경에서도 효과적입니다. 현재의 LLM 방어 기법은 VLA에 대부분 비효율적이며, 로봇 공학에서 VLA의 광범위한 배포가 임박함에 따라 VLA 고유의 안전 메커니즘 및 정렬 기술 개발이 시급함을 강조합니다.
