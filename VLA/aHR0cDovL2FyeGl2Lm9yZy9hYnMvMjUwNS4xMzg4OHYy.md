# InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning

Ji Zhang, Shihan Wu, Xu Luo, Hao Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song

## 🧩 Problem to Solve

기존 VLA(Vision-Language-Action) 모델은 사전 학습된 VLM(Vision-Language Models)을 활용하여 언어 지시와 시각적 관찰을 로봇 행동으로 매핑하지만, 훈련 데이터 범위를 넘어 일반화하는 데 한계가 있습니다. 특히, VLA는 **과제와 무관한 시각적 특징과 행동을 잘못 연관시키는 경향(spurious correlations)**이 있어, 시각적 관찰에서 언어 지시나 공간 관계와 같은 중요한 요소를 간과합니다. 이로 인해 모델의 일반화 능력이 저해되고 새로운 시나리오에서 신뢰할 수 없는 성능을 보이게 됩니다.

본 연구는 **추가적인 데이터나 다른 대규모 모델과의 상호작용 없이** VLA의 공간 추론 능력을 강화하여 이러한 잘못된 연관성의 악영향을 완화하고 일반화 성능을 향상시킬 수 있는지에 대한 질문을 제기합니다.

## ✨ Key Contributions

- VLA의 일반화 성능에 대한 잘못된 연관성의 부정적인 영향을 완화하도록 설계된 새로운 접근 방식인 **InSpire**를 제안합니다.
- 추가적인 데이터나 다른 대규모 모델과의 상호작용 없이, InSpire는 **플러그 앤 플레이 방식**으로 VLA에 공간 추론 능력을 부여합니다.
- 시뮬레이션 및 실제 환경 모두에서 수행된 포괄적인 평가는 제안된 InSpire 접근 방식의 **효과성과 유연성**을 입증합니다.

## 📎 Related Works

- **Vision-Language-Action Models (VLA)**: RT-1, RT-2, RT-X, OpenVLA, Octo, $\pi^{0}$-FAST와 같은 모델들이 사전 학습된 VLM을 기반으로 로봇 제어에서 뛰어난 일반화 능력을 보여주었습니다. 특히 $\pi^{0}$-FAST는 효율적인 행동 토큰화를 통해 훈련 효율을 높였습니다.
- **Chain-of-Thought (CoT) Reasoning**: NLP에서 시작되어 복잡한 문제를 순차적인 추론 과정으로 분해하는 CoT 기법은 로봇 시스템의 고수준 작업 계획에 적용되었습니다(예: CoT-VLA, ECoT). 그러나 이러한 방법은 종종 복잡한 중간 계산 과정이나 외부 모델에 의존하여 효율성이 저하될 수 있습니다.
- **Learning Spatial Reasoning**: SpatialVLM, SpatialRGPT, SpatialVLA와 같은 연구들이 VLM이나 LLM의 공간 추론 능력을 활용하여 멀티모달 학습 성능을 향상시켰습니다. SpatialVLA는 3D 정보를 입력 관찰에 주입하여 공간 인지 행동 예측을 달성했습니다. InSpire는 SpatialVLA와 유사하게 3D 정보를 사용하는 대신, VQA(Visual Question Answering) 태스크를 브리징 언어로 사용하여 공간 추론 및 행동 예측을 동시에 가능하게 합니다.

## 🛠️ Methodology

InSpire는 VLA의 공간 추론 능력을 강화하여 잘못된 연관성의 영향을 완화하는 간단하면서도 효과적인 접근 방식입니다. 핵심 아이디어는 다음 두 단계로 모델이 행동을 출력하도록 하는 것입니다.

1. **관련 요인 추출 ($u'$)**:
   - 초기 언어 지시 ($l$)가 주어지면, InSpire 프레임워크는 "로봇을 기준으로 [객체]는 어느 방향에 있습니까?"와 같은 텍스트 질문 ($q$)을 도입하여 객체와 로봇 간의 공간 관계를 탐색합니다. 객체 이름은 자연어 처리 툴킷으로 식별됩니다.
   - VLA는 추출 정책 $\pi_{u'}$로 작동하며, $q$, 현재 시각적 관찰 ($o$), 언어 지시 ($l$)를 입력으로 받아 "right/left/up/down/front/back/grasped"와 같은 사전 정의된 조잡한 방향 옵션 내에서 텍스트 답변 ($g$)을 생성합니다.
   - 이 질문과 답변($[q, g]$)의 조합이 추출된 텍스트 표현 $u'$를 구성합니다.
2. **행동 생성 ($a$)**:
   - 추출된 표현 $u'$는 동일한 VLA로 전달되며, 이제 행동 정책 $\pi_{\theta}$로 작동하여 최종 로봇 행동 ($a$)을 출력합니다.
   - 수식으로 표현하면 다음과 같습니다.
     $$u' = \pi_{u'}(o,l)$$
     $$a = \pi_{\theta}(u',o,l)$$

**자동 규칙 기반 객체 방향 라벨링**:

- 모델 훈련을 위해, Ground-truth 공간 관계는 훈련 데이터셋에 통합됩니다. 각 훈련 데이터 포인트는 $(o_i, a_i, u'_i)$로 확장되며, $u'_i$는 자체 생성 질문과 Ground-truth 공간 관계 답변을 포함합니다.
- 이 Ground-truth 관계는 로봇의 엔드 이펙터(gripper)와 관련 객체의 3D 위치를 기반으로 추론됩니다. 시뮬레이션에서는 객체 위치가 바로 사용 가능하며, 실제 환경에서는 그리퍼가 닫히거나 열리는 순간의 엔드 이펙터 위치를 객체 위치의 대용으로 사용합니다.
- 엔드 이펙터 위치 $[x_i, y_i, z_i]$와 객체 위치 $[x_0, y_0, z_0]$가 주어지면, 위치 차이 $d = [x_i - x_0, y_i - y_0, z_i - z_0]$가 계산됩니다.
- 가장 큰 절대값을 가진 $d$의 구성 요소에 해당하는 축에 따라 조잡한 공간 관계가 결정됩니다. 예를 들어, $|x_i - x_0|$가 가장 큰 경우, $x_i - x_0$의 부호에 따라 "left" 또는 "right"로 분류됩니다. 객체가 잡힌 경우("grasped")로 직접 라벨링됩니다.
- 훈련 중에는 공간 추론 답변에 해당하는 토큰과 주 행동 토큰에 자동회귀 손실이 적용되어, 모델이 공간 관계에 대한 올바른 텍스트 설명을 예측하도록 장려합니다.

## 📊 Results

- **시뮬레이션 환경 (LIBERO 벤치마크)**:
  - miniVLA-VQ에 InSpire를 적용했을 때, 학습된(seen) 작업에서 **6.2%**, 학습되지 않은(unseen) 작업에서 **10%** 절대 성공률 향상을 달성했습니다.
  - $\pi^{0}$-FAST에 InSpire를 적용했을 때, 학습된 작업에서 **1.0%**, 학습되지 않은 작업에서 **2.8%** 절대 성공률 향상을 보였습니다. (miniVLA-VQ가 스크래치부터 학습된 반면, $\pi^{0}$-FAST는 사전 학습된 모델을 파인튜닝한 것이므로 상대적 성능 향상이 적음).
  - 특히 LIBERO-Spatial 및 LIBERO-Object 작업에서 InSpire의 성능 향상이 두드러져 공간 관계 및 객체 상호작용 처리 능력을 입증했습니다.
- **실제 환경**:
  - $\pi^{0}$-FAST에 InSpire를 적용했을 때, 10개의 학습된 실제 작업에서 평균 **25%**, 5개의 학습되지 않은 실제 작업에서 평균 **26%** 성공률 향상을 달성했습니다.
  - InSpire는 학습되지 않은 작업의 4/5에서 기준 모델 대비 **100%**의 상대적 성공률 증가를 보이며, 객체 색상, 객체, 배경의 변화에 대한 강건성을 입증했습니다.
  - InSpire는 단계당 평균 0.18초의 추가 시간 비용이 발생했지만, 성공률의 상당한 개선을 고려할 때 허용 가능한 트레이드오프로 평가되었습니다.
- **최신 기술과의 비교 (InspireVLA-1B)**:
  - InspireVLA-1B(1B 매개변수)는 SpatialVLA-4B(4B 매개변수) 및 CoT-VLA-7B(7B 매개변수)와 같은 추론 기반 VLA 모델을 포함한 5개 VLA 모델을 LIBERO 4개 데이터셋에서 일관되게 능가했습니다.
  - InspireVLA-1B는 SpatialVLA-4B보다 **8.6%**, CoT-VLA-7B보다 **2.8%** 성능을 향상시키면서, 각각 **4배 및 7배 더 높은 매개변수 효율성**을 보였습니다.

## 🧠 Insights & Discussion

- **잘못된 연관성 해결**: InSpire는 다음을 통해 VLA의 잘못된 연관성 학습 문제를 효과적으로 해결합니다.
  - **지름길 학습(Shortcut Learning) 방지**: 과제와 무관한 특징에 기반한 지름길 학습을 방지하고, 언어 지시 및 공간 관계와 같은 필수 요소를 모델이 고려하도록 유도합니다.
  - **방해 요소(Distractors)에 대한 강건성 향상**: 공간 추론 VQA 태스크를 통해 모델이 과제 관련 객체와 방해 요소를 구별할 수 있게 합니다.
  - **지속적인 행동 수정 가능**: 궤적을 따라 각 시각적 관찰에 대해 공간 추론을 수행함으로써, 모델이 작업 실행 상태(예: 목표 객체와 로봇의 상대적 방향)를 지속적으로 인식하고 잘못된 행동을 수정하는 데 도움을 줍니다.
- **VQA 설계의 영향**: "1D Direction" (객체와 로봇의 1차원 방향 예측) 기반 VQA 태스크가 가장 효과적이었으며, 이는 행동 생성 프로세스를 안내하는 데 조잡한 방향 정보가 중요하다는 것을 시사합니다.
- **제한 사항**:
  - InSpire의 확산 정책 기반 VLA(예: Octo, $\pi^{0}$)에 대한 통합 및 효과는 아직 탐구되지 않았습니다.
  - 다양한 VLA 간의 잘못된 연관성의 관계 및 차이에 대한 조사가 부족하며, 이는 향후 연구에서 더 복잡한 로봇 시나리오에서의 효과성 및 강건성을 개선하는 데 도움이 될 수 있습니다.

## 📌 TL;DR

InSpire는 VLA의 **잘못된 연관성(spurious correlations)** 문제와 **일반화 한계**를 해결하기 위한 간단하면서도 효과적인 플러그 앤 플레이 방식입니다. 이 방법은 **"로봇을 기준으로 [객체]는 어느 방향에 있습니까?"**와 같은 공간 추론 VQA(Visual Question Answering) 태스크를 언어 지시 앞에 추가하고, VLA가 이 질문에 답변한 후 행동을 생성하도록 하여 **내재적 공간 추론 능력**을 부여합니다. 추가 데이터나 외부 대규모 모델 없이, InSpire는 시뮬레이션 및 실제 환경에서 기존 VLA의 성공률을 크게 향상시키고, 특히 **miniVLA-VQ에서 학습된/학습되지 않은 작업의 성공률을 각각 6.2%/10% 개선**하며, 최신 추론 기반 VLA보다 **더 높은 효율성**을 보였습니다. 이는 VLA가 과제 관련 요소에 집중하고 지름길 학습 및 방해 요소에 대한 강건성을 높여 지속적인 행동 수정 능력을 가능하게 합니다.
