# Spec-VLA: Vision-Language-Action 모델을 위한 완화된 수용(Relaxed Acceptance)을 통한 추론 디코딩

Songsheng Wang, Rucheng Yu, Zhihang Yuan, Chao Yu, Feng Gao, Yu Wang, Derek F. Wong

## 🧩 Problem to Solve

시각-언어-행동(Vision-Language-Action, VLA) 모델은 강력한 시각-언어 모델(Vision-Language Models, VLM)의 역량을 활용하여 상당한 발전을 이루었지만, VLM의 방대한 파라미터 크기와 자기회귀(Autoregressive, AR) 디코딩 방식은 VLA 모델에 상당한 계산 부담을 줍니다. 추측 디코딩(Speculative Decoding, SD)은 효율적인 초안 작성(drafting) 및 병렬 검증을 통해 여러 토큰을 한 번의 순방향 통과로 생성하여 대규모 언어 모델(Large Language Models, LLM)의 속도를 높이는 데 효과를 보였지만, VLA 모델에 대한 적용은 아직 탐구되지 않았습니다. VLA 모델의 액션 예측 작업의 난이도와 탐욕적 디코딩(greedy decoding) 메커니즘으로 인해, SD 프레임워크를 VLA 예측 작업에 직접 적용하면 속도 향상이 미미했습니다.

## ✨ Key Contributions

- VLA 추론 가속화를 위해 설계된 최초의 SD 프레임워크인 **Spec-VLA**를 제안합니다.
- VLA 모델의 액션 토큰이 나타내는 상대적 거리를 활용하여 토큰 수용 기준을 완화하는 효과적인 메커니즘을 제안합니다. 이 "완화된 수용(Relaxed Acceptance)" 전략은 SD의 직접적인 적용 한계를 극복합니다.
- 다양한 테스트 시나리오에서 Spec-VLA 프레임워크의 효과성을 경험적으로 입증했습니다.
- 제안된 전략이 수용 길이(acceptance length)를 **44% 향상**시키고, OpenVLA 베이스라인 대비 **1.42배의 속도 향상**을 달성하며, 성공률(success rate)은 저하시키지 않음을 분석적으로 보여주었습니다.
- Spec-VLA 프레임워크의 성공은 VLA 예측 시나리오에서 추측 실행(speculative execution)의 광범위한 적용 가능성을 강조합니다.

## 📎 Related Works

- **VLA 모델 가속화:**
  - **토큰 수준 최적화:** FastV, SparseVLM은 시각-언어 토큰 선택을 통해 계산 중복을 줄이지만, 휴리스틱에 크게 의존합니다.
  - **LLM 가속화 기술 적용:** QAIL (양자화), Mope-CLIP (가지치기), DeeR (조기 종료) 등이 VLA에 적용되었지만, 교차 모달 상호작용 품질 저하나 작업별 튜닝이 필요합니다.
  - **구조적 수정:** Robomamba, TinyVLA (모델 백본 재설계), Kim et al. (2025), Song et al. (2025) (Jacobi 반복을 통한 디코딩 재구성) 등이 있지만, 도메인별 데이터 미세 조정 또는 재훈련이 필요하며 시스템 복잡성을 증가시킵니다.
- **LLM을 위한 추측 디코딩 (SD):**
  - **초기 SD 프레임워크:** Medusa, Medusa-CTC는 다중 헤드 디코딩과 트리 어텐션 검증을 통해 병렬 생성 능력을 도입했습니다.
  - **Eagle 시리즈:** Eagle, Eagle-2, Eagle-3은 초안 모델링의 아키텍처 혁신을 통해 더 높은 속도 향상을 달성했습니다.
  - **최근 확장:** 검색 증강 생성(RAG) 및 긴 컨텍스트 생성과 같은 새로운 시나리오에 적용되었습니다.
  - **완화된 수용 (Relaxed Acceptance):** Spec-Dec은 AR 모델의 상위 k개 후보 중 일치하는 모든 초안 토큰을 수용하고, Lantern은 딕셔너리에서 상위 k개 유사 토큰을 수용하여 시각 생성 속도를 향상시킵니다.

## 🛠️ Methodology

- **전반적인 프레임워크 (Spec-VLA):**
  - **초안 생성기 모델 ($M_D$)**로 Llama 디코더 레이어를 사용합니다. 이 모델은 선형 레이어를 통합하여 특징 수준 및 토큰 수준 손실 데이터를 효과적으로 통합합니다.
  - 초안 생성기는 검증 모델($M_V$, 미세 조정된 OpenVLA 모델)로부터 은닉 상태($f_{1:t}$), 텍스트 임베딩($e_p$), 시각 임베딩($e_v$)을 받습니다.
  - $M_D$는 이전 은닉 상태, 임베딩, 액션 토큰에 따라 액션 토큰 $\hat{a}_i$를 자기회귀적으로 예측합니다.
    $$\hat{a}_i = M_D(f_{1:t}, \text{concat}(e_v, e_p), \hat{a}_{t+1:i-1})$$
  - Eagle-2의 동적 초안 트리 전략을 사용하여 $M_D$의 상위 K개 예측이 트리 구조를 형성하고, $M_V$가 이 경로들을 병렬로 검증합니다.
- **직접 적용의 문제점:** SD를 직접 적용했을 때 VLA 예측 작업의 복잡성과 엄격한 탐욕적 디코딩으로 인해 속도 개선이 미미했습니다.
- **수용 완화 (Relaxation of Acceptance):**
  - **완화 임계값 $r$**을 도입하여 초안 액션 토큰 $\hat{a}_i$와 예측된 액션 토큰 $a_i$ 사이의 허용 가능한 거리 $D$를 정량화합니다.
  - 초안 토큰 $\hat{a}_i$는 $D(a_i, \hat{a}_i) \le r$일 경우 수용됩니다. 그렇지 않으면 $a_i$가 다시 샘플링됩니다.
    $$a_i = M_V(\text{a}_{1}\sim\hat{a}_{i-1}, p, \theta), \\ \begin{cases} \text{Accept, } D(a_i,\hat{a}_i)\le r \\ \text{Resample }\hat{a}_i=a_i, D(a_i,\hat{a}_i)> r. \end{cases}$$
  - OpenVLA 및 RT-2와 같은 VLA 모델은 연속적인 차원을 256개의 이산적인 빈으로 나누고 이를 256개의 액션 토큰에 매핑합니다. 토큰 유사성에 대한 정보는 본질적으로 VLA 토큰 표현에 내재되어 있으며, 토큰 간의 거리는 **빈 ID의 절대 차이**에서 직접 유추될 수 있습니다.
  - 이는 수용 영역을 엄격한 빈 $b$에서 $\hat{b} \in (b-r, b+r)$로 확장하여 상위 $2 \times r$개의 유사 토큰을 수용할 수 있게 하며, 거의 추가적인 계산 오버헤드가 없습니다.

## 📊 Results

- **주요 결과:** LIBERO 시뮬레이션 벤치마크(Object, Spatial, Goal, Long 데이터셋)에서 평가되었습니다.
  - **직접 SD 적용:** 성공률 저하 없이 1.08배에서 1.15배의 가속 비율을 달성했습니다.
  - **Spec-VLA (완화된 수용):** 수용 길이를 2.10에서 2.94로 증가시켰으며, 이는 26%에서 44%의 향상에 해당합니다. AR 베이스라인 대비 1.22배에서 1.42배의 생성 속도 향상을 달성하면서도 성공률은 유지했습니다.
- **완화 임계값에 대한 분석:**
  - 완화 임계값이 증가하면 수용 길이가 효과적으로 향상되어 VLA 모델의 생성 속도를 높였습니다 (각 데이터셋에서 50%에서 70% 증가).
  - OpenVLA 모델은 LIBERO-Goal, Object, Spatial 데이터셋에서 높은 견고성을 보여, 성공률 저하 없이 완화 임계값을 5에서 9 (LIBERO-Goal의 경우 15까지)로 설정할 수 있었습니다.
  - 모델 성능이 좋은 시나리오일수록 더 큰 완화 임계값을 허용할 수 있었습니다 (예: LIBERO-Goal은 임계값 15에서도 안정적인 성공률).
- **수용 길이 분포 (표 2):** 완화되지 않은 조건에서는 짧은 시퀀스(길이 0-1)가 지배적이었지만, 완화된 수용에서는 긴 시퀀스(길이 2-5)의 비율이 더 균형 있게 분포되어 효율성을 크게 향상시켰습니다.
- **다중 위치에서의 수용 길이 (표 3):** 완화된 수용은 모든 위치에서 완화되지 않은 수용보다 일관되게 더 긴 평균 길이를 달성하여, 짧은 예측에 대한 편향을 줄였습니다.
- **사례 연구 (그림 5):** 엄격한 검증 모델(Non-Relaxed)은 액션 시퀀스를 점진적으로 축적하는 반면, 완화된 수용은 초기 단계에서 더 넓은 범위의 초안 제안을 수용하여 계획 생성에 필요한 반복 횟수를 크게 줄였습니다.

## 🧠 Insights & Discussion

- SD 프레임워크를 VLA 모델에 직접 적용하는 것은 VLA 예측의 복잡성과 엄격한 탐욕적 디코딩으로 인해 제한적인 이점을 제공합니다.
- 제안된 "완화된 수용" 메커니즘은 VLA 액션 토큰의 내재된 거리 정보를 활용하여, 미미한 액션 예측 편차가 작업 성공률에 영향을 미치지 않는 VLA 모델의 견고성을 활용합니다.
- 이는 VLA 모델이 예측된 액션 토큰의 "약간의 편차"에 대해 높은 견고성을 보인다는 중요한 통찰력을 제공하며, 이러한 특성 덕분에 품질 저하 없이 수용 기준을 완화할 수 있었습니다.
- 이 방법은 검증 모델의 미세 조정이나 재훈련 없이 손실 없는 가속화 솔루션을 제공합니다.
- **한계점:** 시간 및 자원 제약으로 인해 실제 로봇 환경에서의 실험은 수행되지 않았습니다. 또한, 검증 모델의 한계로 인해 액션 청킹(Action Chunking)은 탐구되지 않았습니다.

## 📌 TL;DR

**문제:** VLA 모델은 거대한 VLM과 자기회귀 디코딩으로 인해 추론 속도가 느리며, 기존 추측 디코딩(SD)의 직접 적용은 효과가 미미했습니다.
**방법:** 본 논문은 VLA 모델을 위한 SD 프레임워크인 Spec-VLA를 제안합니다. 핵심은 VLA 액션 토큰의 고유한 빈(bin) ID 거리를 활용한 **"완화된 수용(relaxed acceptance)"** 메커니즘입니다. 이는 검증 모델의 예측과 정확히 일치하는 초안 토큰만 받아들이는 대신, 특정 임계값 내에 있는 유사한 초안 토큰도 수용하여 효율성을 높입니다.
**결과:** Spec-VLA는 OpenVLA 베이스라인 대비 최대 **1.42배의 속도 향상**과 **44% 증가된 수용 길이**를 달성했으며, VLA 모델의 성공률은 **저하시키지 않았습니다**. 이는 VLA 모델이 사소한 액션 예측 오차에 대해 높은 견고성을 보인다는 중요한 발견을 바탕으로 합니다.
