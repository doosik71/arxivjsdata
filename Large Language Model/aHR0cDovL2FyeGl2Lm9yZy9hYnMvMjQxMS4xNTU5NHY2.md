# LLM-as-a-Judgeμ— λ€ν• μ΅°μ‚¬ μ—°κµ¬

JIAWEI GU, XUHUI JIANG, ZHICHAO SHI, HEXIANG TAN, XUEHAO ZHAI, CHENGJIN XU, WEI LI, YINGHAN SHEN, SHENGJIE MA, HONGHAO LIU, SAIZHUO WANG, KUN ZHANG, ZHOUCHI LIN, BOWEN ZHANG, LIONEL NI, WEN GAO, YUANZHUO WANG, JIAN GUO

## π§© ν•΄κ²°ν•λ ¤λ” λ¬Έμ 

μ •ν™•ν•κ³  μΌκ΄€λ ν‰κ°€λ” λ‹¤μ–‘ν• λ¶„μ•Όμ—μ„ μμ‚¬κ²°μ •μ— μ¤‘μ”ν•μ§€λ§, λ³Έμ§μ μΈ μ£Όκ΄€μ„±, λ³€λ™μ„±, ν™•μ¥μ„± λ•λ¬Έμ— μ—¬μ „ν μ–΄λ ¤μ΄ κ³Όμ μ…λ‹λ‹¤. λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ΄ λ‹¤μ–‘ν• μμ—­μ—μ„ λ†€λΌμ΄ μ„±κ³µμ„ κ±°λ‘λ©΄μ„, LLMμ„ λ³µμ΅ν• μ‘μ—…μ ν‰κ°€μλ΅ ν™μ©ν•λ” "LLM-as-a-Judge" ν¨λ¬λ‹¤μ„μ΄ λ“±μ¥ν–μµλ‹λ‹¤. κ·Έλ¬λ‚ LLM-as-a-Judge μ‹μ¤ν…μ **μ‹ λΆ°μ„±μ„ λ³΄μ¥ν•λ” κ²ƒ**μ΄ ν•µμ‹¬ κ³Όμ λ΅ λ‚¨μ•„ μμΌλ©°, μ΄λ¥Ό μ„ν• μ‹ μ¤‘ν• μ„¤κ³„μ™€ ν‘μ¤€ν™”κ°€ ν•„μ”ν•©λ‹λ‹¤. μ΄ λ…Όλ¬Έμ€ μ‹ λΆ°μ„± μλ” LLM-as-a-Judge μ‹μ¤ν…μ„ κµ¬μ¶•ν•λ” λ°©λ²•μ„ ν¬κ΄„μ μΌλ΅ λ‹¤λ£Ήλ‹λ‹¤.

## β¨ μ£Όμ” κΈ°μ—¬

* **κ°λ…μ  μ •μ ν™•λ¦½:** LLM-as-a-Judgeμ— λ€ν• κ³µμ‹μ  λ° λΉ„κ³µμ‹μ  μ •μλ¥Ό μλ¦½ν•μ—¬, μ…λ ¥ λ³€λ™μ„±, λ¨λΈ νΉμ„± λ° λ§¥λ½ μμ΅΄μ„±μ„ ν†µν•©ν• μ‹ λΆ°μ„±μ— λ€ν• λ§¥λ½ν™”λ μ •μλ¥Ό μ μ‹ν•©λ‹λ‹¤.
* **ν†µν•© ν”„λ μ„μ›ν¬ μ μ‹:** ννΈν™”λ κΈ°μ΅΄ λ¬Έν—μ„ 'λ¬΄μ—‡μΈκ°€(What)', 'μ–΄λ–»κ² μ‚¬μ©ν•λ”κ°€(How to use)', 'μ–΄λ–»κ² κ°μ„ ν•λ”κ°€(How to improve)', 'μ–΄λ–»κ² ν‰κ°€ν•λ”κ°€(How to evaluate)'μ λ„¤ κ°€μ§€ μ§λ¬Έμ„ μ¤‘μ‹¬μΌλ΅ μ‹ λΆ°μ„±μ„ ν•µμ‹¬ μ£Όμ λ΅ ν•μ—¬ μ²΄κ³„μ μΌλ΅ μ¬κµ¬μ„±ν•©λ‹λ‹¤.
* **μƒλ΅μ΄ λ©”νƒ€ ν‰κ°€ λ²¤μΉλ§ν¬ μ μ•:** κΈ°μ΅΄ μ ‘κ·Ό λ°©μ‹μ— λ€ν• λΉ„κµ λ¶„μ„μ„ μν–‰ν•κ³ , LLM-as-a-Judge μ‹μ¤ν… ν‰κ°€λ¥Ό μ„ν• μƒλ΅μ΄ λ©”νƒ€ ν‰κ°€ λ²¤μΉλ§ν¬λ¥Ό μ μ•ν•μ—¬, κ°•κ±΄μ„± λ€ λ―Όκ°λ„(robustness vs. sensitivity)μ™€ κ°™μ€ ν•µμ‹¬μ μΈ μƒμ¶© κ΄€κ³„λ¥Ό λ°νκ³  μ‹ λΆ°ν•  μ μλ” ν‰κ°€ ν”„λ μ„μ›ν¬ κµ¬μ¶•μ„ μ„ν• μ‹¤ν–‰ κ°€λ¥ν• ν†µμ°°λ ¥μ„ μ κ³µν•©λ‹λ‹¤.
* **ν¬κ΄„μ μΈ λ―Έλ μ—°κµ¬ λ°©ν–¥ μ μ‹:** LLM-as-a-Judgeμ μ‘μ©, λ„μ „ κ³Όμ  λ° λ―Έλ λ°©ν–¥μ„ ν†µν•©μ μΌλ΅ λ¶„μ„ν•μ—¬, μ‹ λΆ°ν•  μ μκ³  μ‚¬νμ μΌλ΅ μ‹ λΆ°ν•  μ μλ” μ‹μ¤ν… κµ¬μ¶•μ„ μ„ν• μ΄λ΅ μ  κΈ°λ°κ³Ό μ‹¤μ§μ μΈ μ§€μΉ¨μ„ μ κ³µν•©λ‹λ‹¤.

## π“ κ΄€λ ¨ μ—°κµ¬

* **κΈ°μ΅΄ ν‰κ°€ λ°©λ²•μ ν•κ³„:**
  * **μ „λ¬Έκ°€ μ£Όλ„ ν‰κ°€:** ν¬κ΄„μ μ΄κ³  λ―Έμ„Έν• μ΄ν•΄λ¥Ό μ κ³µν•μ§€λ§, λΉ„μ©μ΄ λ§μ΄ λ“¤κ³  ν™•μ¥ν•κΈ° μ–΄λ ¤μ°λ©° μΌκ΄€μ„±μ΄ λ¶€μ΅±ν•©λ‹λ‹¤ (μ: ν•™μ  λ…Όλ¬Έ μ‹¬μ‚¬).
  * **μλ™ μ§€ν‘:** ν™•μ¥μ„±κ³Ό μΌκ΄€μ„±μ΄ λ›°μ–΄λ‚μ§€λ§, ν‘λ©΄μ μΈ μ–΄ν μΌμΉμ— μμ΅΄ν•μ—¬ λ―Έλ¬ν• λ‰μ•™μ¤λ¥Ό ν¬μ°©ν•μ§€ λ»ν•©λ‹λ‹¤ (μ: BLEU [109], ROUGE [85]λ” μ¤ν† λ¦¬ μƒμ„±κ³Ό κ°™μ€ λ³µμ΅ν• μ‘μ—…μ— λ¶€μ ν•©).
* **LLM κΈ°λ° ν‰κ°€μ λ“±μ¥:**
  * GPT-4 [107], ChatGPT, Claude λ“± κ°•λ ¥ν• LLMμ„ μΈκ°„ ν‰κ°€μλ¥Ό λ€μ²΄ν•λ” μλ™ν™”λ ν‰κ°€μλ΅ μ‚¬μ© (Zheng et al. [213], Li et al. [81]).
  * RLHF (Reinforcement Learning from Human Feedback) [108]μ™€ κ°™μ€ ν›λ ¨ ν¨λ¬λ‹¤μ„μ„ ν†µν•΄ LLMμ΄ μΈκ°„ κ°€μΉ λ° μ¶”λ΅  ν”„λ΅μ„Έμ¤μ™€ λ”μ± μ μ •λ ¬λμ–΄ ν‰κ°€ μ‘μ—…μ— ν™μ©λ©λ‹λ‹¤.
* **LLM-as-a-Judge λ―Έμ„Έ μ΅°μ • μ—°κµ¬:**
  * PandaLM [162], JudgeLM [222], Auto-J [79], Prometheus [63] λ“±μ€ Alpaca, Vicuna λ“±μ λ°μ΄ν„°μ…‹κ³Ό GPT-3.5/GPT-4 μ£Όμ„μ„ κΈ°λ°μΌλ΅ LLMμ„ ν‰κ°€μλ΅ λ―Έμ„Έ μ΅°μ •ν•©λ‹λ‹¤.
* **LLM ν‰κ°€ λ²¤μΉλ§ν¬:**
  * MTBench [213], Chatbot Arena [213], FairEval [155], EvalBiasBench [110], CALM [185], MLLM-as-a-Judge [17] λ“±μ€ LLM-as-a-Judgeμ μ„±λ¥, νΈν–¥, μΈκ°„ ν‰κ°€μ™€μ μΌμΉλ„λ¥Ό μΈ΅μ •ν•©λ‹λ‹¤.

## π› οΈ λ°©λ²•λ΅ 

μ΄ λ…Όλ¬Έμ€ μ‹ λΆ°μ„± μλ” LLM-as-a-Judge μ‹μ¤ν… κµ¬μ¶•μ„ μ„ν• μ²΄κ³„μ μΈ μ ‘κ·Ό λ°©μ‹μ„ μ μ‹ν•λ©°, κ·Έ ν•µμ‹¬μ€ ν‰κ°€ νμ΄ν”„λΌμΈμ κ° λ‹¨κ³„μ—μ„ μ‹ λΆ°μ„±μ„ ν–¥μƒν•κ³  ν‰κ°€ν•λ” μ „λµμ— μμµλ‹λ‹¤.

### 1. LLM-as-a-Judge μ •μ λ° λ¶„λ¥

* **λΉ„κ³µμ‹μ  μ •μ:** LLMμ„ μ‚¬μ „ μ •μλ κ·μΉ™, κΈ°μ¤€ λλ” μ„ νΈλ„μ— λ”°λΌ κ°μ²΄, ν–‰λ™ λλ” κ²°μ •μ„ ν‰κ°€ν•λ” λ° μ‚¬μ©ν•λ” κ²ƒ (μ: λ“±κΈ‰ μ±„μ μ, ν‰κ°€μ, λΉ„ν‰κ°€, κ²€μ¦μ, μμ„ λ¨λΈ).
* **κ³µμ‹μ  μ •μ:** LLM-as-a-Judgeμ κΈ°λ³Έμ μΈ κΈ°λ¥μ„ λ‹¤μκ³Ό κ°™μ΄ μ •μν•©λ‹λ‹¤.
    $$ E \leftarrow P_{\text{LLM}}(x \oplus C) $$
  * $E$: μµμΆ… ν‰κ°€ κ²°κ³Ό (μ μ, μ„ νƒ, λ μ΄λΈ”, λ¬Έμ¥ λ“±).
  * $P_{\text{LLM}}$: ν•΄λ‹Ή LLMμ— μν•΄ μ •μλ ν™•λ¥  ν•¨μ (μλ™ νκ·€ μƒμ„± ν”„λ΅μ„Έμ¤).
  * $x$: ν‰κ°€ν•  μ…λ ¥ λ°μ΄ν„° (ν…μ¤νΈ, μ΄λ―Έμ§€, λΉ„λ””μ¤ λ“±).
  * $C$: μ…λ ¥ $x$μ— λ€ν• λ§¥λ½ (ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ, λ€ν™” κΈ°λ΅ λ“±).
  * $\oplus$: μ…λ ¥ $x$μ™€ λ§¥λ½ $C$λ¥Ό κ²°ν•©ν•λ” μ—°μ‚°μ.
* **ν–¥μƒλ κ³µμ‹μ  μ •μ (μ‹ λΆ°μ„± κ°•μ΅°):**
    $$ R \leftarrow f_R(P_{\text{LLM}}, x, C) $$
  * $R$: μΌκ΄€μ„±, κ°•κ±΄μ„±, μΈκ°„ νλ‹¨κ³Όμ μ •λ ¬μ„ λ…μ‹μ μΌλ΅ λ³΄μ¥ν•λ„λ΅ μ„¤κ³„λ ν‰κ°€. μ¶”κ°€μ μΈ κ²€μ¦, λ³΄μ •, ν‘μ¤€ν™” λ‹¨κ³„λ¥Ό ν†µν•΄ μ‹ λΆ°μ„±μ„ κ²€μ¦ν•©λ‹λ‹¤.
  * $f_R$: ν‰κ°€ μ‹ λΆ°μ„±μ„ ν–¥μƒν•κΈ° μ„ν•΄ κΈ°λ³Έ LLM-as-a-Judge ν”„λ μ„μ›ν¬μ— μ²΄κ³„μ μΌλ΅ μ μ©λλ” μΌλ ¨μ μ μ•½ λ° κ²€μ¦ λ°©λ²•.

### 2. LLM-as-a-Judge κµ¬ν„μ κΈ°λ³Έ μ ‘κ·Ό λ°©μ‹

LLM-as-a-Judgeλ” **In-Context Learning (ICL)**, **λ¨λΈ μ„ νƒ (Model Selection)**, **ν›„μ²λ¦¬ (Post-processing)**, κ·Έλ¦¬κ³  μ΄λ¥Ό ν†µν•©ν• **ν‰κ°€ νμ΄ν”„λΌμΈ (Evaluation Pipeline)**μΌλ΅ λ¶„λ¥λ©λ‹λ‹¤.

* **In-Context Learning (ICL):** LLMμ μ¶”λ΅ κ³Ό νλ‹¨μ„ μ•λ‚΄ν•κΈ° μ„ν•΄ μ§€μΉ¨κ³Ό μμ‹λ¥Ό μ κ³µν•λ” λ°©λ²•.
  * **μ μ μƒμ„± (Generating scores):** 1-5, 1-10κ³Ό κ°™μ€ μ΄μ‚° μ μ λλ” 0-1, 0-100κ³Ό κ°™μ€ μ—°μ† μ μλ΅ ν‰κ°€. (μ: G-Eval [93], LLaVA-Critic).
  * **Yes/No μ§λ¬Έ ν•΄κ²° (Solving Yes/No questions):** νΉμ • μ§„μ μ μ •ν™•μ„±μ— λ€ν• μ΄μ§„ νλ‹¨ (μ: Reflexion [128], μκ°€ κ°μ„ ).
  * **μλ€ λΉ„κµ (Conducting pairwise comparisons):** λ‘ μµμ…μ„ λΉ„κµν•μ—¬ μ°μ„λ¥Ό νλ‹¨ (μ: MTBench, Chatbot Arena).
  * **κ°κ΄€μ‹ μ„ νƒ (Making multiple-choice selections):** μ—¬λ¬ μµμ… μ¤‘ κ°€μ¥ μ μ ν•κ±°λ‚ μ •ν™•ν• κ²ƒμ„ μ„ νƒ.
* **λ¨λΈ μ„ νƒ (Model Selection):**
  * **μΌλ° LLM (General LLM):** GPT-4 [107], ChatGPT λ“± κ°•λ ¥ν• μƒμ© λ¨λΈ μ‚¬μ©.
  * **λ―Έμ„Έ μ΅°μ •λ LLM (Fine-tuned LLM):** νΉμ • ν‰κ°€ μ‘μ—…μ„ μ„ν•΄ μμ²΄ λ°μ΄ν„°λ΅ λ―Έμ„Έ μ΅°μ •λ λ¨λΈ (μ: PandaLM [162], JudgeLM [222]).
* **ν›„μ²λ¦¬ (Post-processing):** LLMμ΄ μƒμ„±ν• ν™•λ¥  λ¶„ν¬λ¥Ό μ •μ ν•μ—¬ μ •ν™•ν• ν‰κ°€λ¥Ό λ³΄μ¥ν•λ” λ‹¨κ³„.
  * **νΉμ • ν† ν° μ¶”μ¶ (Extracting specific tokens):** κ·μΉ™ μΌμΉλ¥Ό ν†µν•΄ μ μ, νΉμ • μµμ…, Yes/No λ“±μ ν† ν° μ¶”μ¶.
  * **μ¶λ ¥ λ΅μ§“ μ •κ·ν™” (Normalizing the output logits):** μ—μ΄μ „νΈ λ°©λ²•μ—μ„ Yes/No μ„¤μ • μ‹, μ¶λ ¥ λ΅μ§“μ„ 0μ—μ„ 1 μ‚¬μ΄μ μ—°μ†μ μΈ κ°’μΌλ΅ μ •κ·ν™”ν•μ—¬ ν‰κ°€μ— μ‚¬μ©.
  * **λ¬Έμ¥ μ„ νƒ (Selecting sentences):** LLMμ΄ μƒμ„±ν• λ¬Έμ¥μ΄λ‚ λ‹¨λ½μ„ ν‰κ°€ κ²°κ³Όλ΅ μ¶”μ¶.
* **ν‰κ°€ νμ΄ν”„λΌμΈ (Evaluation Pipeline):** μ΄ λ¨λ“  λ‹¨κ³„λ¥Ό ν†µν•©ν•μ—¬ LLM-as-a-Judge μ‹μ¤ν…μ„ κµ¬μ¶•ν•λ” κ³Όμ •. λ¨λΈ, λ°μ΄ν„°, μ—μ΄μ „νΈ, μ¶”λ΅ /μ‚¬κ³  λ“± λ„¤ κ°€μ§€ μ‹λ‚λ¦¬μ¤μ—μ„ μ μ©λ©λ‹λ‹¤.

### 3. κ°μ„  μ „λµ (Improvement Strategy)

LLM ν‰κ°€μμ μ‹ λΆ°μ„±μ„ λ†’μ΄κΈ° μ„ν• μ„Έ κ°€μ§€ μ£Όμ” μ „λµ.

* **ν”„λ΅¬ν”„νΈ μ„¤κ³„ μ „λµ (Prompt Design Strategy):**
  * **LLMμ μ‘μ—… μ΄ν•΄λ„ ν–¥μƒ:** Few-shot ν”„λ΅¬ν”„νΈ, ν‰κ°€ λ‹¨κ³„/κΈ°μ¤€ λ¶„ν•΄ (μ: G-Eval [93]μ CoT), λ‚΄μ© λ¬΄μ‘μ„ μ„κΈ°(shuffling)λ¥Ό ν†µν•΄ νΈν–¥ μ™„ν™”, μ μ λ§¤κΈ°κΈ° μ‘μ—…μ„ μλ€ λΉ„κµλ΅ μ „ν™.
  * **LLMμ μ¶λ ¥ ν•μ‹ ν‘μ¤€ν™”:** κµ¬μ΅°ν™”λ ν•μ‹ (μ: JSON)μΌλ΅ μ¶λ ¥ κ°•μ , μ„¤λ…κ³Ό ν•¨κ» ν‰κ°€ μ κ³µν•μ—¬ ν•΄μ„ κ°€λ¥μ„± μ¦λ€.
* **μ—­λ‰ ν–¥μƒ μ „λµ (Capability Enhancement Strategy):**
  * **μ „λ¬Έ λ―Έμ„Έ μ΅°μ • (Specialized Fine-tuning):** ν‰κ°€ μ‘μ—…μ„ μ„ν• λ©”νƒ€ ν‰κ°€ λ°μ΄ν„°μ…‹μΌλ΅ LLMμ„ λ―Έμ„Έ μ΅°μ • (μ: OffsetBias [110]λ΅ νΈν–¥ κ°μ†).
  * **ν”Όλ“λ°± κΈ°λ° λ°λ³µ μ •μ  (Feedback-Driven Iterative Refinement):** λ” κ°•λ ¥ν• λ¨λΈμ΄λ‚ μΈκ°„ ν‰κ°€μμ ν”Όλ“λ°±μ„ ν†µν•΄ LLMμ„ μ§€μ†μ μΌλ΅ μ—…λ°μ΄νΈ (μ: INSTRUCTSCORE [176]).
* **μµμΆ… μ¶λ ¥ μµμ ν™” μ „λµ (Final Output Optimization Strategy):**
  * **λ‹¤μ¤‘ μ†μ¤ ν‰κ°€ κ²°κ³Ό ν†µν•© (Integrating Multi-Source Evaluation Results):** μ—¬λ¬ λ²μ ν‰κ°€ μ‹¤ν–‰ λλ” μ—¬λ¬ LLM ν‰κ°€μμ κ²°κ³Όλ¥Ό ν†µν•© (μ: λ‹¤μκ²° ν¬ν‘, ν‰κ·  μ μ).
  * **μ§μ ‘ μ¶λ ¥ μµμ ν™” (Direct Output Optimization):** λ‹¨μΌ LLM ν‰κ°€μμ μ¶λ ¥μ„ μ¶”κ°€ μ²λ¦¬ν•μ—¬ μ‹ λΆ°μ„± ν–¥μƒ (μ: μ μ μ¤λ¬΄λ”©, μκ°€ κ²€μ¦).

### 4. ν‰κ°€ λ°©λ²• (Evaluation of LLM-as-a-Judge)

LLM-as-a-Judgeμ μ‹ λΆ°μ„±μ„ μ²΄κ³„μ μΌλ΅ κ²€μ¦ν•κΈ° μ„ν• μ„Έ κ°€μ§€ μ£Όμ” μ°¨μ›.

* **μΈκ°„ νλ‹¨κ³Όμ μΌμΉλ„ (Agreement with Human Judgments):** LLM ν‰κ°€μκ°€ μΈκ°„ μ£Όμ„μμ™€ μ–Όλ§λ‚ μΌμΉν•λ”μ§€ μΈ΅μ •. (μ: λ°±λ¶„μ¨ μΌμΉ, Cohen's Kappa, Spearman's correlation).
* **νΈν–¥ (Bias):** LLM ν‰κ°€μκ°€ λ‚νƒ€λ‚΄λ” λ‹¤μ–‘ν• νΈν–¥ μ ν•μ„ μ‹λ³„ν•κ³  μΈ΅μ •.
  * **μ‘μ—… λ¶κ°€μ§€λ΅ μ  νΈν–¥ (Task-Agnostic Biases):** λ‹¤μ–‘μ„± νΈν–¥, λ¬Έν™”μ  νΈν–¥, μκΈ° κ°•ν™” νΈν–¥.
  * **νλ‹¨ νΉμ • νΈν–¥ (Judgment-Specific Biases):** μ„μΉ νΈν–¥, λ™μ • κ°μ† νΈν–¥, μ¤νƒ€μΌ νΈν–¥, κΈΈμ΄ νΈν–¥, κµ¬μ²΄μ„± νΈν–¥.
* **μ λ€μ  κ°•κ±΄μ„± (Adversarial Robustness):** μλ„μ μΌλ΅ μ΅°μ‘λ μ…λ ¥μ— λ€ν•΄ ν‰κ°€ μ μκ°€ μ–Όλ§λ‚ μ•μ •μ μΌλ΅ μ μ§€λλ”μ§€ μΈ΅μ •. (μ: μ λ€μ  λ¬Έκµ¬ κ³µκ²©, Null λ¨λΈ κ³µκ²©, λ‹¤μ μκ²¬ κ³µκ²©).

## π“ κ²°κ³Ό

λ…Όλ¬Έμ€ LLM-as-a-Judgeμ κ°μ„  μ „λµμ— λ€ν• λ©”νƒ€ ν‰κ°€ μ‹¤ν—μ„ μν–‰ν•μ—¬ λ‹¤μ–‘ν• LLMκ³Ό κ°μ„  μ „λµμ ν¨κ³Όλ¥Ό λ¶„μ„ν–μµλ‹λ‹¤.

### 1. λ‹¤λ¥Έ LLMκ³Όμ λΉ„κµ

| LLM                 | μΈκ°„ μ •λ ¬ (%) | μ„μΉ νΈν–¥ (%) | κΈΈμ΄ νΈν–¥ (%) | κµ¬μ²΄μ„± νΈν–¥ (%) |
| :------------------ | :------------ | :------------ | :------------ | :-------------- |
| GPT-4-turbo         | 61.54         | 80.31         | 91.18         | 89.29           |
| GPT-3.5-turbo       | 54.72         | 68.78         | 20.59         | 64.29           |
| Qwen2.5-7B-Instruct | 56.54         | 63.50         | 64.71         | 71.43           |
| LLaMA3-8B-Instruct  | 50.72         | 38.85         | 20.59         | 57.14           |
| Mistral-7B          | 55.42         | 59.78         | 26.47         | 67.86           |
| Mixtral-8Γ—7B        | 56.29         | 59.06         | 50.00         | 78.57           |
| gemini-2.0-thinking | 60.75         | 76.84         | 94.12         | 89.29           |
| o1-mini             | 60.16         | 76.73         | 91.18         | 89.29           |
| o3-mini             | 61.66         | 74.63         | 82.35         | 92.86           |
| deepseek r1         | 56.48         | 69.17         | 94.12         | 100.00          |

* **GPT-4μ μ°μ›”μ„±:** GPT-4-turboλ” λ¨λ“  λ©”νƒ€ ν‰κ°€ μ°¨μ›μ—μ„ λ‹¤λ¥Έ LLMμ„ ν¬κ² λ¥κ°€ν•λ©° νΈν–¥μ΄ μ κ² λ‚νƒ€λ‚¬μµλ‹λ‹¤.
* **μ¤ν” μ†μ¤ LLM:** Qwen2.5-7B-Instructλ” λ€λ¶€λ¶„μ μ°¨μ›μ—μ„ GPT-3.5-turboλ¥Ό λ¥κ°€ν•λ” μ°μν• ν‰κ°€ λ¥λ ¥μ„ λ³΄μ€μµλ‹λ‹¤. νΉν Concreteness Bias λ° Content Continuation Biasλ¥Ό μ μ™Έν• λ€λ¶€λ¶„μ νΈν–¥μ—μ„ μ„±λ¥μ΄ μΆ‹μ§€ μ•μ•μΌλ©°, μ‹¬μ§€μ–΄ GPT-4-turboλ„ Empty Reference Bias λ° Nested Instruction Biasμ—μ„ μƒλ‹Ήν• μ„±λ¥ μ €ν•λ¥Ό κ²μ—μµλ‹λ‹¤.
* **μ¶”λ΅  κ°•ν™” LLM:** gemini-2.0-thinking, o1-mini, o3-mini, deepseek r1κ³Ό κ°™μ€ μ¶”λ΅  LLMλ“¤μ€ GPT-4-turboμ— ν•„μ ν•λ” μΌμΉλ„μ™€ μ •ν™•λ„λ¥Ό λ³΄μ€μ§€λ§, μΈκ°„ μ •λ ¬μ΄ ν•„μ”ν• μ‘μ—…μ—μ„μ κ°μ„ μ€ μμƒλ§νΌ λλ ·ν•μ§€ μ•μ•μµλ‹λ‹¤. νΉν gemini-2.0-thinkingμ€ human=model2 μ‹λ‚λ¦¬μ¤μ—μ„ GPT-4-turboλ¥Ό λ¥κ°€ν•λ” 78.27%μ μ •ν™•λ„λ¥Ό κΈ°λ΅ν–μµλ‹λ‹¤.

### 2. λ‹¤λ¥Έ κ°μ„  μ „λµκ³Όμ λΉ„κµ (GPT-3.5-turbo κΈ°λ°)

| κ°μ„  μ „λµ               | μΈκ°„ μ •λ ¬ (%) | μ„μΉ νΈν–¥ (%) | κΈΈμ΄ νΈν–¥ (%) | κµ¬μ²΄μ„± νΈν–¥ (%) |
| :---------------------- | :------------ | :------------ | :------------ | :-------------- |
| GPT-3.5-turbo (κΈ°λ³Έ)    | 54.72         | 68.78         | 20.59         | 64.29           |
| μ„¤λ… ν¬ν•¨               | 52.47         | 48.97         | 35.29         | 60.71           |
| μκ°€ κ²€μ¦               | 54.86         | 69.31         | 23.53         | 60.71           |
| λ‹¤μ¤‘ λΌμ΄λ“ (λ‹¤μκ²°)    | 54.68         | 70.11         | 26.47         | 67.86           |
| λ‹¤μ¤‘ λΌμ΄λ“ (ν‰κ· )      | 54.72         | 69.58         | 11.76         | 57.14           |
| λ‹¤μ¤‘ λΌμ΄λ“ (μµκ³  μ μ) | 51.95         | 58.72         | 5.88          | 42.86           |
| λ‹¤μ¤‘ LLM (μ„ΈνΈ 1)       | 57.66         | 32.28         | 26.47         | 64.28           |
| λ‹¤μ¤‘ LLM (μ„ΈνΈ 2)       | 58.19         | 70.98         | 64.71         | 71.43           |

* **μ„¤λ… ν¬ν•¨ (w/ explanation):** ν‰κ°€ μ„±λ¥κ³Ό νΈν–¥ μ™„ν™”μ— λ¶€μ •μ μΈ μν–¥μ„ λ―Έμ³¤λ”λ°, μ΄λ” μκΈ° μ„¤λ…μ΄ λ” κΉμ€ νΈν–¥μ„ μ λ°ν•  μ μμμ„ μ‹μ‚¬ν•©λ‹λ‹¤.
* **μκ°€ κ²€μ¦ (w/ self-validation):** LLMμ κ³Όμ‹ μΌλ΅ μΈν•΄ μµμ†ν•μ ν¨κ³Όλ¥Ό λ³΄μ€μµλ‹λ‹¤.
* **λ‹¤μ¤‘ λΌμ΄λ“ (λ‹¤μκ²° ν¬ν‘ - majority@5):** μ—¬λ¬ μ°¨μ› (νΉν νΈν–¥)μ—μ„ κ°μ„ μ„ λ³΄μ€μΌλ©°, LLMμ λ¬΄μ‘μ„μ„± μν–¥μ„ μ¤„μ΄λ” λ° ν¨κ³Όμ μ΄μ—μµλ‹λ‹¤.
* **λ‹¤μ¤‘ λΌμ΄λ“ (ν‰κ·  μ μ - mean@5 λλ” μµκ³  μ μ - best-of-5):** ν‰κ°€ μ„±λ¥μ„ κ°μ„ ν•μ§€ λ»ν–μΌλ©°, νΈν–¥μ μν–¥μ„ ν¨κ³Όμ μΌλ΅ μ™„ν™”ν•μ§€ λ»ν–μµλ‹λ‹¤.
* **λ‹¤μ¤‘ LLM (Vote by Multiple LLMs):** LLM μ„ νƒμ— λ”°λΌ μ„±λ¥μ΄ ν¬κ² λ‹¬λΌμ΅μµλ‹λ‹¤. νΉν, μ„ΈνΈ 1(GPT-4, GPT-3.5, LLaMA3)μ€ LLaMA3μ κΈΈμ΄ νΈν–¥ λ•λ¬Έμ— μ „λ°μ μΈ μ„±λ¥μ΄ μ €μ΅°ν–μ§€λ§, μ„ΈνΈ 2(GPT-4, GPT-3.5, Qwen2.5)λ” Qwen2.5μ μ„±λ¥ λ•λ¶„μ— κΈΈμ΄ νΈν–¥μ—μ„ λ” λ‚μ€ κ²°κ³Όλ¥Ό λ³΄μ€μµλ‹λ‹¤.

**μ‹¤ν— μ”μ•½:** LLMμ λ‚΄μ¬λ λ¥λ ¥κ³Ό μ μ¬μ  μ„ν—μΌλ΅ μΈν•΄ μΌλ°μ μΈ κ°μ„  μ „λµμ΄ μ„±λ¥ ν–¥μƒμ΄λ‚ νΈν–¥ μ™„ν™”μ— μ™„μ „ν ν¨κ³Όμ μ΄μ§€λ” μ•μ•μµλ‹λ‹¤. ν„μ¬ μ‹¤ν— λ¶„μ„μ— λ”°λ¥΄λ©΄, μλ€ λΉ„κµ ν‰κ°€ μ‘μ—…μ—μ„λ” κ°•λ ¥ν• LLMμ„ μ„ νƒν•κ³ , ν‰κ°€ λ‚΄μ©μ μ„μΉλ¥Ό λ°”κΎΈλ” μ „λµκ³Ό μ—¬λ¬ λΌμ΄λ“μ λ‹¤μκ²° ν¬ν‘ κ²°κ³Όλ¥Ό μ·¨ν•λ” μ „λµμ΄ νΈν–¥μ„ ν¨κ³Όμ μΌλ΅ μ™„ν™”ν•  μ μμµλ‹λ‹¤. μΈκ°„κ³Όμ μΌμΉλ„ κ°μ„ μ„ μ„ν•΄μ„λ” μ¶”κ°€ νƒκµ¬κ°€ ν•„μ”ν•©λ‹λ‹¤.

## π§  ν†µμ°° λ° λ…Όμ

### 1. μ‹μ‚¬μ 

* **κ°•λ ¥ν• LLMμ μ¤‘μ”μ„±:** GPT-4μ™€ κ°™μ€ κ°•λ ¥ν• LLMμ€ λ” κ°κ΄€μ μ΄κ³  νΈν–¥μ΄ μ μ€ ν‰κ°€λ¥Ό μ κ³µν•λ©°, μ¤ν” μ†μ¤ λ¨λΈ μ¤‘μ—μ„λ” Qwen2.5-7Bκ°€ μ λ§ν• λ€μ•μ΄ λ  μ μμµλ‹λ‹¤.
* **λ³µν•©μ  μ „λµμ ν•„μ”μ„±:** λ‹¨μΌ κ°μ„  μ „λµλ³΄λ‹¤λ” μ—¬λ¬ λΌμ΄λ“μ— κ±ΈμΉ λ‹¤μκ²° ν¬ν‘μ™€ κ°™μ€ λ‹¤μ¤‘ μ†μ¤ ν†µν•© μ „λµμ΄ LLMμ λ¬΄μ‘μ„μ„±κ³Ό νΈν–¥ μν–¥μ„ μ¤„μ΄λ” λ° ν¨κ³Όμ μ…λ‹λ‹¤.
* **νΈν–¥ μΈμ‹ λ° μ™„ν™”:** LLMμ€ λ‹¤μ–‘ν• νΈν–¥(μ„μΉ, κΈΈμ΄, κµ¬μ²΄μ„± λ“±)μ— μ·¨μ•½ν•λ©°, λ¨λΈ μ„ νƒ μ „ μ†κ·λ¨ λ©”νƒ€ ν‰κ°€λ¥Ό ν†µν•΄ μ μ¬μ  νΈν–¥μ„ νμ•…ν•κ³  λ§μ¶¤ν• μ™„ν™” μ „λµμ„ μλ¦½ν•΄μ•Ό ν•©λ‹λ‹¤.
* **μ¶”λ΅  λ¥λ ¥μ μν–¥:** μ¶”λ΅  λ¥λ ¥μ΄ λ›°μ–΄λ‚ LLM(μ: Gemini-thinking)μ€ μΈκ°„ μ •λ ¬ μΈ΅λ©΄μ—μ„ κ°μ„ μ„ λ³΄μ΄μ§€λ§, λ¨λ“  ν‰κ°€ μ‘μ—…μ—μ„ μΌκ΄€λ μ°μ„λ¥Ό μ κ³µν•μ§€λ” μ•μ•„ μ¶”κ°€ μµμ ν™”μ μ—¬μ§€κ°€ μμµλ‹λ‹¤.

### 2. ν•κ³„

* **In-Context Learningμ λ―Όκ°μ„±:** ν”„λ΅¬ν”„νΈμ λ―Έμ„Έν• λ³€ν™”λ‚ μμ‹μ μμ„κ°€ ν‰κ°€ κ²°κ³Όμ μΌκ΄€μ„±μ„ ν•΄μΉ  μ μμµλ‹λ‹¤.
* **κ³Όμ‹  λ° μκΈ° κ°•ν™”:** LLMμ΄ μμ‹ μ΄ μƒμ„±ν• μ‘λ‹µμ— λ€ν•΄ κ³Όλ„ν•κ² λ†’μ€ μ μλ¥Ό λ¶€μ—¬ν•λ” κ²½ν–¥μ΄ μμ–΄, ν‰κ°€μ μ‹ λΆ°μ„±μ„ λ–¨μ–΄λ¨λ¦΄ μ μμµλ‹λ‹¤.
* **λ¨λΈ μ„ νƒ λ° μΌλ°ν™” λ¬Έμ :** μƒμ© LLMμ λΈ”λ™λ°•μ¤ νΉμ„±κ³Ό λ²„μ „ μμ΅΄μ„±μ€ μ¬ν„μ„±μ„ μ €ν•΄ν•λ©°, λ―Έμ„Έ μ΅°μ •λ λ¨λΈμ€ ν›λ ¨ λ°μ΄ν„°μ— λ€ν• κ³Όμ ν•© λ° μΌλ°ν™” ν•κ³„λ¥Ό κ°€μ§ μ μμµλ‹λ‹¤.
* **μ λ€μ  κ³µκ²©μ— λ€ν• μ·¨μ•½μ„±:** ν‰κ°€μμ νΈν–¥μ΄λ‚ μΌκ΄€μ„± λ¶€μ΅±μ„ μ•…μ©ν•μ—¬ ν‰κ°€ μ μλ¥Ό μ΅°μ‘ν•  μ μλ” μ λ€μ  κ³µκ²©μ— LLM-as-a-Judgeκ°€ μ·¨μ•½ν•©λ‹λ‹¤.
* **λ°±λ³Έ λ¨λΈμ ν•κ³„:** λ³µμ΅ν• λ‹¤μ¤‘ λ¨λ‹¬ μ½ν…μΈ  ν‰κ°€λ‚ μ¶”μƒμ /μΈκ³Όμ  μ¶”λ΅  ν†µν•©μ—μ„ LLM λ°±λ³Έ λ¨λΈμ ν•κ³„κ°€ μ΅΄μ¬ν•©λ‹λ‹¤.
* **νλ‹¨μ ν•΄μ„ κ°€λ¥μ„± λ° ν¬λ…μ„± λ¶€μ΅±:** LLM ν‰κ°€ κ³Όμ •μ λ¶ν¬λ…μ„±μ€ μ‚¬μ©μ μ‹ λΆ°λ¥Ό μ ν•ν•λ©°, κ³ μ„ν— λ¶„μ•Όμ—μ„μ μ μ©μ„ μ–΄λ µκ² λ§λ“­λ‹λ‹¤.
* **λ©”νƒ€ ν‰κ°€ λ° μ‹κ°„μ  μΌκ΄€μ„±:** ν‰κ°€μ μμ²΄μ— λ€ν• μ²΄κ³„μ μΈ λ©”νƒ€ ν‰κ°€ λ²¤μΉλ§ν¬κ°€ λ¶€μ΅±ν•λ©°, λ¨λΈ μ—…λ°μ΄νΈμ— λ”°λ¥Έ "ν‰κ°€ λ“λ¦¬ν”„νΈ"μ™€ κ°™μ€ μ‹κ°„μ  μΌκ΄€μ„± λ¬Έμ κ°€ λ°μƒν•©λ‹λ‹¤.
* **μ¤λ¦¬μ  λ° μ‚¬νμ  ν•¨μ:** LLMμ΄ ν›λ ¨ λ°μ΄ν„°μ μ‚¬νμ  νΈν–¥μ„ μ¦ν­μ‹ν‚¤κ±°λ‚, μ„¤λ… λ¶κ°€λ¥μ„±μΌλ΅ μΈν•΄ μ±…μ„ μ†μ¬κ°€ λ¶λ¶„λ…ν•΄μ§€κ±°λ‚, ν‰κ°€ μ£Όλ„ν• μ½ν…μΈ  μλ ΄μΌλ΅ μ°½μμ„±μ„ μ €ν•΄ν•  μ μμµλ‹λ‹¤.

### 3. λ―Έλ μ—°κµ¬ λ°©ν–¥

* **μ¶”λ΅  μ¤‘μ‹¬ ν‰κ°€ (Reasoning-Centric Judgement):** μ¶”λ΅ κ³Ό νλ‹¨μ μ‹λ„μ§€λ¥Ό ν™μ©ν•μ—¬, λ™μ  ν”Όλ“λ°± λ£¨ν”„λ¥Ό ν†µν•΄ LLMμ΄ μ§€μ†μ μΌλ΅ μκ°€ κ°μ„ ν•κ³ , κ¶κ·Ήμ μΌλ΅λ” μ‹κ°„μ΄ μ§€λ‚¨μ— λ”°λΌ ν‰κ°€ λ¥λ ¥μ„ μ¤μ¤λ΅ μ§„ν™”μ‹ν‚¤λ” "μκ°€ μ§„ν™”ν•λ” ν‰κ°€μ(Self-Evolving Judges)"λ΅ λ°μ „ν•λ” κ²ƒμ…λ‹λ‹¤.
* **μ΄λ΅ μ μΌλ΅ κΈ°λ°ν• ν‰κ°€ (Theoretically Grounded Evaluation):** κ²½ν—μ  λ²¤μΉλ§ν¬λ¥Ό λ„μ–΄ ν†µκ³„ν•™ λ° μΈ΅μ • μ΄λ΅ μ—μ„ μ•„μ΄λ””μ–΄λ¥Ό μ°¨μ©ν•μ—¬ μΌκ΄€μ„± λ° κ°•κ±΄μ„±κ³Ό κ°™μ€ κ°λ…μ— λ€ν• κ³µμ‹μ μΈ μ΄λ΅ μ  ν”„λ μ„μ›ν¬λ¥Ό κµ¬μ¶•ν•©λ‹λ‹¤.
* **λ” μ‹ λΆ°ν•  μ μλ” LLM-as-a-Judge:** In-Context Learning, λ¨λΈ μ„ νƒ, ν›„μ²λ¦¬ κΈ°μ  λ° μ „λ°μ μΈ ν‰κ°€ ν”„λ μ„μ›ν¬ μ „λ°μ—μ„ LLM-as-a-Judgeμ μ‹ λΆ°μ„±μ„ κ°μ„ ν•κΈ° μ„ν• κ΄‘λ²”μ„ν• μ—°κµ¬κ°€ ν•„μ”ν•©λ‹λ‹¤.
* **MLLM-as-a-Judge:** ν…μ¤νΈλΏλ§ μ•„λ‹λΌ μ‹κ°, μ²­κ° λ“± λ‹¤μ–‘ν• λ¨λ‹¬λ¦¬ν‹°λ¥Ό ν†µν•©ν•μ—¬ λ³µμ΅ν• μ½ν…μΈ λ¥Ό μ¶”λ΅ ν•κ³  ν‰κ°€ν•  μ μλ” κ°•λ ¥ν• λ©€ν‹°λ¨λ‹¬ ν‰κ°€μ κ°λ°μ΄ ν•„μ”ν•©λ‹λ‹¤.
* **ν‰κ°€ λ²¤μΉλ§ν¬ λ°μ „ (Advancing Evaluation Benchmarks):** ν‰κ°€μ μμ²΄λ¥Ό ν‰κ°€ν•λ” μ²΄κ³„μ μΈ λ©”νƒ€ ν‰κ°€ ν”„λ μ„μ›ν¬μ™€ λ„λ©”μΈλ³„, λ©€ν‹°λ¨λ‹¬ μ½ν…μΈ , μ‹¤μ  λ³µμ΅μ„±μ„ ν¬ν•¨ν•λ” ν¬κ΄„μ μ΄κ³  λ‹¤μ–‘ν• λ²¤μΉλ§ν¬λ¥Ό κ°λ°ν•΄μ•Ό ν•©λ‹λ‹¤.
* **λ°μ΄ν„° μ£Όμ„μ„ μ„ν• LLM-as-a-Judge:** LLMμ΄ μΈκ°„ μ£Όμ„μ„ λ€μ²΄ν•κ±°λ‚ λ³΄κ°•ν•μ—¬ λ€κ·λ¨ λ°μ΄ν„°μ…‹μ„ μλ™ μ£Όμ„ν•κ³  λ°μ΄ν„° ν’μ§μ„ ν‰κ°€ν•λ” λ° ν™μ©λ  μ μμµλ‹λ‹¤.
* **ν™•μ¥μ„ μ„ν• LLM-as-a-Judge:** LLM-as-a-Judgeλ” λ°μ΄ν„° μ£Όμ„μ λ³‘λ© ν„μƒμ„ ν•΄μ†ν•κ³ , RLHFμ™€ κ°™μ€ λ¨λΈ μµμ ν™” ν”„λ΅μ„Έμ¤μ—μ„ μλ™ν™”λ λΉ„ν‰κ°€ λλ” λ³΄μƒ λ¨λΈ μ—­ν• μ„ ν•μ—¬ AI κ°λ°μ„ ν™•μ¥ν•λ” ν•µμ‹¬ λ©”μ»¤λ‹μ¦μ΄ λ  κ²ƒμ…λ‹λ‹¤.
* **κµ¬ν„λ μ§€λ¥(Embodied Intelligence)μ„ μ„ν• LLM-as-a-Judge:** λ΅λ΄‡μ΄λ‚ κ°€μƒ ν™κ²½μ μ—μ΄μ „νΈ ν–‰λ™ λ° λ™μ‘μ„ ν‰κ°€ν•μ—¬, ν–‰λ™ μ‹ν€€μ¤, μ‹κ³µκ°„ κ΄€κ³„, κ³ μμ¤€ λ©ν‘μ™€μ μ •λ ¬μ„ νλ‹¨ν•κ³  ν’λ¶€ν• ν”Όλ“λ°±μ„ μ κ³µν•λ” λ° ν™μ©λ  μ μμµλ‹λ‹¤.
* **LLM μµμ ν™”λ¥Ό μ„ν• LLM-as-a-Judge:** LLM-as-a-Judgeλ” λ‹¤μ¤‘ μ—μ΄μ „νΈ ν”„λ μ„μ›ν¬μ—μ„ μƒνΈμ‘μ©μ„ μ•λ‚΄ν•κ³  ReFT νμ΄ν”„λΌμΈμ—μ„ μ¶”λ΅  ν”„λ΅μ„Έμ¤λ¥Ό ν‰κ°€ν•λ” λ“± LLM μµμ ν™”λ¥Ό μ„ν• ν•µμ‹¬ μ”μ†λ΅ μ‘μ©ν•  μ μμµλ‹λ‹¤.
* **λ„λ©”μΈλ³„ μ‹ λΆ°ν•  μ μλ” μ‘μ© (Domain-Specific Reliable Applications):** μλ£ μ§„λ‹¨, λ²•λ¥  νκ²°, κµμ΅ ν‰κ°€, κ³Όν•™ λ…Όλ¬Έ μ‹¬μ‚¬ λ“± κ° λ„λ©”μΈμ κ³ μ ν• μ”κµ¬ μ‚¬ν•­κ³Ό λ†’μ€ μ‹ λΆ°μ„± κΈ°μ¤€μ„ μ¶©μ΅±ν•λ„λ΅ λ§μ¶¤ν• LLM-as-a-Judge μ‹μ¤ν…μ„ κ°λ°ν•΄μ•Ό ν•©λ‹λ‹¤.

## π“ TL;DR

μ΄ λ…Όλ¬Έμ€ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM)μ„ ν‰κ°€μλ΅ ν™μ©ν•λ” **"LLM-as-a-Judge"** ν¨λ¬λ‹¤μ„μ **μ‹ λΆ°μ„± κµ¬μ¶•** λ¬Έμ λ¥Ό λ‹¤λ£¨λ” ν¬κ΄„μ μΈ μ΅°μ‚¬ μ—°κµ¬μ…λ‹λ‹¤. ν•µμ‹¬μ μΌλ΅ LLM-as-a-Judgeλ¥Ό κ³µμ‹μ μΌλ΅ μ •μν•κ³ , κΈ°μ΅΄ λ¬Έν—μ„ ν†µν•© ν”„λ μ„μ›ν¬λ΅ μ¬κµ¬μ„±ν•λ©°, μ‹ λΆ°μ„± ν‰κ°€λ¥Ό μ„ν• μƒλ΅μ΄ λ©”νƒ€ λ²¤μΉλ§ν¬λ¥Ό μ μ•ν•©λ‹λ‹¤. λ°©λ²•λ΅ μ μΌλ΅λ” In-Context Learning, λ¨λΈ μ„ νƒ, ν›„μ²λ¦¬ λ‹¨κ³„μ—μ„ λ‹¤μ–‘ν• κ°μ„  μ „λµ(ν”„λ΅¬ν”„νΈ μ„¤κ³„, λ¨λΈ μ—­λ‰ κ°•ν™”, μ¶λ ¥ μµμ ν™”)μ„ μ μ‹ν•©λ‹λ‹¤. μ‹¤ν— κ²°κ³Ό, GPT-4κ°€ κ°€μ¥ μ°μν• ν‰κ°€ μ„±λ¥κ³Ό λ‚®μ€ νΈν–¥μ„ λ³΄μ€κ³ , λ‹¤μ¤‘ λΌμ΄λ“ λ‹¤μκ²° ν¬ν‘μ™€ κ°™μ€ ν†µν•© μ „λµμ΄ νΈν–¥ μ™„ν™”μ— ν¨κ³Όμ μ„μ„ μ…μ¦ν–μµλ‹λ‹¤. λ§μ§€λ§‰μΌλ΅ μ‹ λΆ°μ„±, κ°•κ±΄μ„±, ν•΄μ„ κ°€λ¥μ„±, λ©”νƒ€ ν‰κ°€ λ“±μ μ£Όμ” λ„μ „ κ³Όμ λ¥Ό λ…Όμν•κ³ , μ¶”λ΅  μ¤‘μ‹¬ ν‰κ°€, μ΄λ΅ μ  κΈ°λ° λ§λ ¨, MLLM ν™•μ¥, λ„λ©”μΈλ³„ μ‘μ© λ“± λ‹¤μ–‘ν• λ―Έλ μ—°κµ¬ λ°©ν–¥μ„ μ μ‹ν•μ—¬ LLM-as-a-Judgeκ°€ λ”μ± μ‹ λΆ°ν•  μ μκ³  ν™•μ¥ κ°€λ¥ν• μ°¨μ„Έλ€ ν‰κ°€ μ‹μ¤ν…μΌλ΅ λ°μ „ν•  μ μλ” λ΅λ“λ§µμ„ μ κ³µν•©λ‹λ‹¤.
