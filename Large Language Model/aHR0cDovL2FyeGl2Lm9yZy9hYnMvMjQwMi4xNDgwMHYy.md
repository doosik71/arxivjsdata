# Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models

Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li

## 🧩 Problem to Solve

Mixture-of-Experts (MoE) 대규모 언어 모델(LLM)은 적은 활성 매개변수로 높은 성능을 달성하지만, 전체 매개변수 규모가 여전히 막대하여 배포에 상당한 메모리와 스토리지(예: Mixtral 8x7B는 두 개의 A100-80G GPU 요구)가 필요합니다. 기존의 가중치 가지치기(weight pruning) 방법은 주로 비정형(unstructured) 또는 N:M 반정형(semi-structured) 희소성에 초점을 맞추며, 특정 하드웨어 설계가 필요하여 플러그-앤-플레이(plug-and-play) 배포가 어렵습니다. 또한, MoE 모델 내의 모든 전문가(expert)가 동일하게 중요하지 않다는 점은 효율적인 배포를 위한 전문가 수준의 희소화(sparsification) 가능성을 시사합니다.

## ✨ Key Contributions

* **최초의 Post-training 전문가 수준 희소화 기법 제안**: MoE LLM을 위한 하드웨어 친화적인 사후 학습(post-training) 전문가 가지치기(pruning) 및 동적 건너뛰기(skipping) 방법을 최초로 제안하여 배포 효율성을 향상시켰습니다.
* **전문가 가지치기 (Expert Pruning)**: 중요도가 낮은 전문가를 영구적으로 제거하여 모델 크기와 메모리 사용량을 줄이는 방법을 개발했습니다.
  * **작업 불가지론적 (Task-agnostic)**: 일반적인 작업에 대해 사전 학습 데이터셋(예: C4)을 사용하여 가지치기를 수행합니다.
  * **작업 특정적 (Task-specific)**: 특정 도메인(예: 수학)에 최적화된 가지치기를 위해 해당 도메인의 보정 데이터셋(예: MATH)을 활용하는 방법을 제안했으며, 이는 문헌에서 최초로 시도되었습니다.
* **동적 전문가 건너뛰기 (Dynamic Expert Skipping)**: 추론 과정에서 활성화된 전문가 수를 동적으로 조정하여 추론 속도를 높이는 온라인 방법을 제시했습니다. 이는 전문가 가지치기 전략과 상호 보완적으로 작동합니다.
* **효율성 입증**: Mixtral 8x7B (Instruct) 모델에 대한 광범위한 실험을 통해 제안된 방법이 모델 성능을 유지하면서 모델 크기 및 추론 속도를 동시에 향상시킴을 보여주었습니다.
* **플러그-앤-플레이 배포**: 제안된 방법은 특정 하드웨어 설계 없이 기존 프레임워크(예: Huggingface Transformers)를 사용하여 쉽게 배포할 수 있습니다.

## 📎 Related Works

* **MoE 모델**: Jacobs et al., 1991에서 개념이 소개된 이후 RNNs (Shazeer et al., 2017) 및 Transformer (Lepikhin et al., 2020) 구조에 적용되었습니다. 최근에는 디코더 전용 GPT 모델 기반 MoE LLM (예: Mixtral 8x7B, Jiang et al., 2024)이 주목받고 있습니다.
* **MoE 모델을 위한 전문가 가지치기**: 주로 기계 번역(Kim et al., 2021; Koishekenov et al., 2022) 등 특정 자연어 처리(NLP) 도메인에 제한적으로 연구되었습니다. 훈련 중 점진적으로 비전문가 전문가를 드롭하는 방법(Chen et al., 2022)도 있으나, LLM 시대에는 훈련 비용이 높습니다.
* **LLM을 위한 사후 학습 가지치기 (Post-training Pruning for LLMs)**: 최근 신경망 희소화 분야에서 인기 있는 주제입니다(Kwon et al., 2022; Hubara et al., 2021). LLM으로 확장된 연구(Sun et al., 2023; Frantar and Alistarh, 2023)도 있지만, 주로 가중치 행렬의 희소화에 초점을 맞추며 전용 하드웨어 구현이 필요한 경우가 많습니다. 디코더 전용 MoE LLM을 위한 효율적인 전문가 수준의 사후 학습 가지치기 방법은 본 연구에서 처음으로 논의됩니다.

## 🛠️ Methodology

본 논문은 MoE LLM의 배포 효율성을 높이기 위해 전문가 수준의 모델 희소화를 활용하는 두 가지 사후 학습(post-training) 기법을 제안합니다.

### 1. 사후 학습 전문가 가지치기 (Post-training Expert Pruning)

이 방법은 중요도가 낮은 전문가를 영구적으로 제거하여 메모리 사용량을 줄입니다.

* **단계 1: 보정 데이터셋 준비 및 계층별 입출력 캐싱**:
  * 작은 보정 데이터셋(예: C4 데이터셋에서 샘플링한 128개의 2048-토큰 시퀀스)을 준비합니다.
  * 원래 MoE 모델에 대해 보정 데이터셋을 이용한 추론을 수행하고, 각 MoE 계층의 입출력 토큰 쌍을 캐시합니다.
* **단계 2: 전문가 조합 열거 및 재구성 손실 최소화**:
  * 보존할 전문가 수 $r$에 기반하여 가능한 모든 전문가 부분집합 $C$를 열거합니다.
  * 각 부분집합 $C$에 대해, 가지치기된 MoE 계층 $F'( \cdot, C)$의 출력을 계산합니다.
  * 원래 계층의 캐시된 출력 $F(x)$와 가지치기된 계층의 출력 $F'(x, C)$ 간의 Frobenius norm 차이(재구성 손실)를 최소화하는 전문가 부분집합 $C$를 선택합니다.
  * 수학적으로는 다음을 최소화합니다:
        $$ \min_{C} \|F'(x,C) - F(x)\|_F $$
        $$ \text{s.t. } C \subseteq \{\text{expert}_0, \ldots, \text{expert}_{n-1}\}, |C|=r $$
  * 선택된 전문가를 제외한 나머지 $n-r$개의 전문가를 폐기합니다.
* **작업 불가지론적 가지치기**: 일반적인 작업의 경우, C4와 같은 포괄적인 사전 학습 데이터셋을 보정 데이터로 사용합니다.
* **작업 특정적 가지치기**: 특정 도메인(예: 수학) 작업의 경우, MATH와 같은 해당 도메인의 훈련 데이터셋을 보정 데이터로 사용하여 가지치기 프로세스를 도메인에 맞게 조정합니다.

### 2. 추론 중 동적 전문가 건너뛰기 (Dynamic Skipping During Inference)

모델의 고정된 전문가 수 내에서 추론 속도를 높이기 위해 개별 토큰 추론 시 특정 전문가를 동적으로 건너뜁니다.

* **원리**: MoE 계층에서 각 토큰 $x$에 대해 상위 $k$개의 전문가가 라우팅 가중치 $w=\{w_{e_0}, \ldots, w_{e_{k-1}}\}$와 함께 선택됩니다.
* **Mixtral 8x7B (k=2)의 경우**:
  * 선택된 두 전문가가 $e_0$와 $e_1$이고 $w_{e_1} < w_{e_0}$라고 가정합니다.
  * 만약 $w_{e_1} < \beta w_{e_0}$이면, 전문가 $e_1$에게 토큰 $x$를 할당하지 않고 건너뜁니다.
  * 하이퍼파라미터 $\beta$는 보정 데이터셋에 대해 $w_{e_1}/w_{e_0}$ 비율의 중앙값으로 각 MoE 계층별로 설정됩니다.
* **일반화된 k-top 시나리오**: 재구성 손실이 특정 상한 $H$를 넘지 않도록 상위 $i^*$개의 전문가만 유지하는 기준을 제시합니다.
    $$ \sum_{m=i+1}^{k} w_m \leq \beta \cdot \sum_{m=1}^{k} w_m $$
    여기서 $i^*$는 이 조건을 만족하는 가장 작은 $i$입니다.
* 이 방법은 전문가 가지치기 전략과 함께 사용하여 효율성을 더욱 향상시킬 수 있습니다.

## 📊 Results

* **메모리 및 속도 향상**:
  * Mixtral 8x7B (Instruct) 모델에서 2개의 전문가를 가지치기하면 (r=6) 배포에 필요한 GPU 수를 A100-80G GPU 1개로 절반 감소시키고 1.2배의 추론 속도 향상을 달성했습니다.
  * 4개의 전문가를 가지치기하면 (r=4) 메모리 사용량이 약 48% 감소하고, 추론 속도가 1.27배 향상되었습니다.
* **성능 유지**:
  * **작업 불가지론적 가지치기**: 2개의 전문가 가지치기 시 평균 성능 저하가 약 2.9포인트(Mixtral 8x7B Instruct의 경우 69.98에서 67.45로), 4개의 전문가 가지치기 시 약 7.1포인트의 성능 저하를 보였습니다.
  * **작업 특정적 가지치기**: 수학 관련 작업(GSM8K)에서, C4 대신 MATH 데이터셋을 보정 데이터로 사용하여 가지치기하면 성능 저하가 크게 감소했습니다(예: Mixtral 8x7B Instruct, $r=4$의 경우 C4 사용 시 30.40%에서 MATH 사용 시 47.01%로 향상).
  * **가지치기 + 동적 건너뛰기**: 전문가 가지치기만 한 모델 대비 미미한 성능 저하로 추론 속도를 추가로 향상시켰습니다. 예를 들어, Mixtral 8x7B Instruct 모델에서 $r=6$ 가지치기 후 동적 건너뛰기를 결합하면 $r=4$ 가지치기 모델과 동일한 속도 향상(1.27배)을 달성하면서 훨씬 높은 정확도(66.04% vs 63.88%)를 유지했습니다.
* **기존 방법과의 비교**: 본 논문의 전문가 가지치기 방법(r=4)은 유사한 매개변수 감소율을 가진 Wanda의 2:4 구조적 희소성 패턴보다 메모리 사용량과 벤치마크 성능 면에서 모두 우수했습니다. 또한, 활성화 빈도 기반 가지치기나 무작위 가지치기보다 더 나은 성능을 달성했습니다.
* **미세 조정 효과**: 수학 관련 도메인(MetaMathQA)에서 미세 조정을 수행하면 가지치기로 인한 성능 저하를 크게 줄여, 가지치기된 모델도 원본 모델과 유사하거나 때로는 더 나은 성능을 달성할 수 있음을 입증했습니다.

## 🧠 Insights & Discussion

* **하드웨어 독립적인 효율성**: 본 연구의 전문가 수준 희소화 기법은 기존 가중치 가지치기 방법과 달리 특수 하드웨어에 의존하지 않고 MoE LLM의 배포 효율성을 크게 향상시킬 수 있습니다. 이는 더 적은 GPU로 모델 로딩 및 더 빠른 추론을 가능하게 합니다.
* **전문가의 불균등한 중요성 확인**: 모든 전문가가 MoE 모델에서 동일하게 중요하지 않다는 가설을 실험적으로 입증했으며, 중요도가 낮은 전문가를 제거하거나 동적으로 건너뛰는 것이 전체 성능에 큰 영향을 미치지 않으면서 효율성을 높일 수 있음을 보여줍니다.
* **작업 특정적 최적화의 가치**: 가지치기 시 보정 데이터셋 선택의 중요성, 특히 도메인 특정 작업에서 해당 도메인의 데이터셋을 사용하는 것이 더 나은 가지치기 결과를 가져옴을 강조합니다. 이는 LLM의 다양한 응용 분야에 대한 맞춤형 희소화 전략의 필요성을 시사합니다.
* **가지치기 및 건너뛰기의 시너지 효과**: 전문가 가지치기와 동적 건너뛰기 방법을 결합하면 개별 방법보다 더 큰 효율성 향상(속도 및 메모리)을 달성하면서도 성능 저하를 최소화할 수 있음을 입증했습니다.
* **한계**:
  * **가지치기 알고리즘의 확장성**: 현재 MoE LLM(예: Mixtral 8x7B의 8개 전문가)에는 적합하지만, 각 MoE 계층의 전문가 수가 크게 증가할 경우(예: 32개 전문가) 열거 기반 가지치기 알고리즘은 계산 비용이 높아질 수 있습니다.
  * **일반화 및 확장성**: 주로 Mixtral 8x7B 모델에 대한 실험에 초점을 맞추었으므로, 다른 MoE LLM에 대한 방법의 일반화 가능성과 확장성에 대한 추가 분석이 필요합니다.
* **향후 연구 방향**: 가중치 가지치기 또는 매개변수 양자화 전략과 본 방법을 결합하여 MoE LLM을 위한 더욱 효과적인 배포 접근 방식을 모색할 예정입니다.

## 📌 TL;DR

MoE LLM은 활성 매개변수가 적어도 전체 모델 크기가 커서 배포가 어렵습니다. 이 논문은 MoE LLM의 배포 효율성을 높이기 위해 **사후 학습 전문가 가지치기(expert pruning)** 및 **동적 전문가 건너뛰기(dynamic expert skipping)** 방법을 최초로 제안합니다. 가지치기는 재구성 손실을 최소화하는 방식으로 중요하지 않은 전문가를 영구적으로 제거하여 메모리를 절감하고, 작업 특정적 성능을 위해 도메인별 보정 데이터셋을 활용할 수 있습니다. 동적 건너뛰기는 추론 시 라우팅 가중치에 기반하여 중요도가 낮은 전문가를 동적으로 건너뛰어 추론 속도를 높입니다. Mixtral 8x7B 모델에 대한 실험 결과, 제안된 방법은 메모리 사용량을 크게 줄이고(예: 48% 감소), 추론 속도를 향상시키면서(예: 1.33배) 모델 성능을 만족스럽게 유지함을 보여주며, 특수 하드웨어 없이 플러그-앤-플레이 방식으로 MoE LLM의 효율적인 배포를 가능하게 합니다.
