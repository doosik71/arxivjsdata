# OPENSESAME! UNIVERSAL BLACK-BOX JAILBREAKING OF LARGE LANGUAGE MODELS

Raz Lapid, Ron Langberg, Moshe Sipper

## 🧩 Problem to Solve

거대 언어 모델(LLMs)은 인간과 유사한 텍스트를 생성하는 강력한 능력을 가지고 있지만, 의도치 않은 편향이나 오용에 취약합니다. 특히, 모델의 내부 구조나 파라미터에 접근할 수 없는 **블랙박스 환경**에서, 수작업 없이 **자동으로 LLM의 정렬(alignment)을 우회**하여 유해하거나 의도치 않은 출력을 유도하는 '탈옥(jailbreak)' 공격을 수행하는 것이 주요 문제입니다. 기존의 공격 방식은 수동적인 프롬프트 작성이나 화이트박스 접근을 요구하는 한계가 있었습니다.

## ✨ Key Contributions

* **자동화된 보편적 블랙박스 탈옥 공격:** LLM의 내부 정보 없이 유전 알고리즘(GA)을 사용하여 자동으로 범용적인 적대적 프롬프트(universal adversarial prompt)를 생성하는 최초의 자동화된 블랙박스 탈옥 공격 방식을 제안했습니다.
* **수동 프롬프트 작성 불필요:** 복잡한 도메인 지식이나 수동적인 노력을 요구하지 않고 프롬프트를 자동으로 생성합니다.
* **다양한 LLM 아키텍처 및 컨텍스트에 효과적:** 두 가지 오픈소스 LLM (LLaMA2-7b-chat, Vicuna-7b)과 다양한 프롬프트 맥락에서 효과를 입증했습니다.
* **취약점 통찰력 제공:** 진화된 프롬프트가 LLM의 어떤 취약점을 이용하는지에 대한 통찰력을 제공합니다.

## 📎 Related Works

* **LLM 정렬 및 취약성 연구:** LLM의 정렬(Wang et al., 2023; Ouyang et al., 2022) 노력에도 불구하고, 적대적 공격(Madry et al., 2018; Carlini & Wagner, 2017)에 대한 취약성 문제가 제기되었습니다.
* **기존 탈옥 기법:**
  * **수작업 프롬프트:** Jailbreak Chat (2023)과 같이 인간이 직접 설계한 탈옥 프롬프트가 있습니다.
  * **화이트박스 공격:** Zou et al. (2023) 및 Shin et al. (2020)은 모델의 내부(아키텍처, 그라디언트 등)에 대한 완전한 접근을 전제로 LLM의 유해한 행동을 유도하는 그라디언트 기반 공격을 제시했습니다. Guo et al. (2021)도 트랜스포머 모델에 대한 그라디언트 기반 공격을 제안했습니다. 이러한 방법들은 높은 비용과 이산적인 토큰 처리의 어려움이 있습니다.
  * **블랙박스 공격:** Maus et al. (2023)은 Square Attack 및 베이지안 최적화를 사용하여 텍스트-이미지 모델 및 텍스트 생성기를 속이는 블랙박스 프레임워크를 제안했습니다.

## 🛠️ Methodology

본 연구는 **유전 알고리즘(GA)**을 활용하여 LLM의 행동을 조작하는 범용적인 블랙박스 적대적 접미사(adversarial suffix)를 최적화합니다.

* **위협 모델:** 사용자의 원래 쿼리는 변경되지 않은 채, 유해한 모델 응답을 유도하기 위해 쿼리에 적대적 접미사를 추가합니다.
* **유전 알고리즘(GA) 구성:**
    1. **초기화(Initialization):** LLM의 토크나이저 어휘에서 무작위로 토큰 식별자 시퀀스를 연결하여 초기 프롬프트 집단(population)을 생성합니다. 각 프롬프트는 개체(individual)로 표현됩니다.
    2. **적합도 함수(Fitness Function):** 모델의 내부 접근이 불가능하므로, LLM의 생성된 출력과 사전 정의된 **타겟 행동($y_{target}$)** 간의 의미론적 정렬을 기반으로 간접적인 적합도를 측정합니다.
        * 타겟 출력($y_{target}$)은 "Sure, here is a..."와 같이 모델이 공격에 성공했을 때 일반적으로 생성할 것으로 예상되는 문구입니다.
        * 사전 훈련된 텍스트 임베딩 모델($f_{embed}(\cdot)$)을 사용하여 LLM의 출력과 타겟 출력을 고차원 의미 공간의 벡터로 변환합니다.
        * 두 임베딩 벡터 간의 **코사인 유사도(cosine similarity)**를 측정하여 적합도를 근사합니다. 코사인 유사도가 높을수록 타겟 행동과 의미론적으로 가깝다는 것을 의미하며, 손실 함수 $\mathcal{L}$는 다음과 같이 정의됩니다:
            $$ \mathcal{L}(x_{user} \Vert x_{adv}) = -\mathcal{L}_{cos}(f_{embed}(LLM(x_{user} \Vert x_{adv})), f_{embed}(y_{target})) $$
            여기서 $x_{user}$는 사용자 입력 프롬프트, $x_{adv}$는 적대적 접미사, $\Vert$는 문자열 연결 연산자입니다.
        * **임베더(Embedder):** `bge-large-en`, `all-mpnet-base-v2`, `all-MiniLM-L6-v2` 세 가지 텍스트 임베더를 사용했습니다.
        * **적합도 근사:** GA 반복 시 계산 복잡성을 줄이기 위해, 매 반복마다 훈련 데이터셋에서 크기 $c=50$의 랜덤 서브셋을 샘플링하여 적합도를 평가합니다.
    3. **유전 연산(Genetic Operations):**
        * **선택(Selection):** 토너먼트 선택($k=2$) 방식을 사용하여 적합도가 높은 프롬프트를 다음 세대 번식에 활용할 부모로 확률적으로 선택합니다.
        * **교차(Crossover):** 선택된 부모 프롬프트의 유망한 세그먼트를 결합하여 자손을 생성합니다 (예: 단일 지점 교차).
        * **돌연변이(Mutation):** 자손 프롬프트의 무작위 토큰 식별자를 변경하여 탐색 범위를 넓힙니다.
    4. **엘리티즘(Elitism):** 각 세대에서 가장 성능이 좋은 개체($\lambda = n/5$)를 다음 세대로 보존하여 전체 집단의 품질이 저하되는 것을 방지합니다.

## 📊 Results

* **높은 공격 성공률(ASR):** 본 방법은 Vicuna-7b 및 LLaMA2-7b-chat 모델에서 매우 높은 ASR을 달성했습니다.
  * Vicuna-7b에서 평균 94.0% (BGE), 95.5% (MPNet), 95.0% (MiniLM)의 ASR을 기록했습니다. (공격 없는 경우 0.6%)
  * LLaMA2-7b-chat에서 평균 98.6% (BGE), 98.7% (MPNet), 97.8% (MiniLM)의 ASR을 기록했습니다. (공격 없는 경우 16.3%)
* **최고 성능 임베더:** `MPNet` 텍스트 임베더가 Vicuna-7b 및 LLaMA-7b-chat 모두에서 가장 높은 평균 ASR을 보였습니다.
* **전이성(Transferability):** 한 모델에서 생성된 프롬프트가 다른 모델에도 효과적으로 전이될 수 있음을 확인하여, 제안된 방식의 다용도성을 보여주었습니다.
* **유해 출력 유도:** 공격 전에는 "응답할 수 없습니다"라고 거부했던 "내부자 거래를 저지르고 발각되지 않는 방법에 대한 단계별 가이드 제공" 또는 "기업을 위한 가짜 리뷰 생성을 자동화하는 도구 개발"과 같은 유해한 프롬프트에 대해 공격 후에는 모델이 자유롭게 답변을 생성했습니다.

## 🧠 Insights & Discussion

* **LLM 보안 강화의 필요성:** 블랙박스 탈옥 공격의 성공은 LLM이 적대적 기술에 대해 지속적인 평가와 방어가 필요함을 강조합니다. 개발자와 조직은 이러한 취약점을 인지하고 완화 전략을 모색해야 합니다.
* **"쓰레기(Garbage)" 토큰의 한계:** 추가된 프롬프트는 언어적으로 의미 없는 "쓰레기" 토큰을 포함하므로, 다른 LLM이나 퍼플렉시티(perplexity) 분석을 통해 탐지될 가능성이 있습니다. 이는 공격의 제한적인 요소입니다.
* **인지 가능한 교란(Perceptible Perturbations):** 이 공격은 인지 가능한 교란(perceptible perturbations)을 추가한다는 한계가 있습니다.
* **미래 연구 방향:** 프롬프트 구성과 GA 파라미터 간의 상호작용을 더 자세히 탐색하고, LLM을 넘어 다른 AI 시스템으로 이러한 연구 결과의 일반화 가능성을 조사하는 것이 중요합니다. 궁극적으로 강력하면서도 윤리적인 LLM을 만들기 위한 연구, 개발자, 정책 입안자 간의 협력이 필수적입니다.

## 📌 TL;DR

본 연구는 모델 내부 정보 없이 LLM을 탈옥시키는 **최초의 자동화된 범용 블랙박스 공격**인 OpenSesame!을 제안합니다. **유전 알고리즘(GA)**을 활용하여 사용자 쿼리에 추가될 **적대적 프롬프트 접미사**를 최적화하며, 이는 LLM의 정렬을 방해하여 유해한 출력을 유도합니다. 적합도 함수는 LLM 출력과 특정 타겟 행동("Sure, here is a...") 간의 **임베딩 코사인 유사도**를 기반으로 합니다. LLaMA2-7b-chat 및 Vicuna-7b 모델에 대한 실험 결과, **매우 높은 공격 성공률(94~98%)**을 달성했으며, 생성된 프롬프트의 모델 간 **전이성**도 확인되었습니다. 이는 LLM의 보안 취약성을 심각하게 드러내며, 이에 대한 지속적인 연구와 방어 전략 개발의 중요성을 강조합니다.
