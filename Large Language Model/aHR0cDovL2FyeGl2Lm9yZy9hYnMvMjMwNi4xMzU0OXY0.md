# A Survey on Multimodal Large Language Models

Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen, Fellow, IEEE

---

## 🧩 Problem to Solve

최근 GPT-4V와 같은 다중 모달 대규모 언어 모델(Multimodal Large Language Models, MLLM)이 부상하며, 강력한 대규모 언어 모델(Large Language Models, LLM)을 '두뇌'로 활용하여 다중 모달 작업을 수행하고 있습니다. 기존 LLM은 텍스트만 이해하여 '맹목적'이며, 대규모 비전 모델(Large Vision Models, LVM)은 시각 능력이 뛰어나지만 추론 능력이 부족합니다. MLLM은 이러한 상호 보완성을 활용하여 이미지 기반 스토리 작성, OCR 없는 수학 추론 등 기존 다중 모달 방식에서는 보기 힘든 놀라운 새로운 능력을 보여주며 인공 일반 지능(Artificial General Intelligence, AGI)의 잠재적인 경로를 제시하고 있습니다. 이 연구는 MLLM의 급속한 발전을 체계적으로 추적하고 요약하여 연구자들이 이 분야의 기본 개념, 주요 방법, 현재 진행 상황을 파악하도록 돕는 것을 목표로 합니다.

## ✨ Key Contributions

- **기본 구성 요소 정리:** MLLM의 기본 구성(아키텍처, 훈련 전략 및 데이터, 평가 방법)을 포괄적으로 제시합니다.
- **확장성 연구 주제 소개:** MLLM의 세분화된 지원, 더 많은 모달리티, 언어, 시나리오 등으로 확장하는 연구 주제를 다룹니다.
- **주요 문제 및 해결책 분석:** 다중 모달 환각(hallucination) 문제와 이를 완화하는 기술들을 소개합니다.
- **핵심 기법 설명:** 다중 모달 In-Context Learning (M-ICL), 다중 모달 Chain of Thought (M-CoT), LLM-Aided Visual Reasoning (LAVR)과 같은 확장된 핵심 기법들을 설명합니다.
- **미래 방향 제시:** MLLM의 기존 과제들을 논의하고 유망한 연구 방향을 제시합니다.
- **최신 정보 제공:** 관련 GitHub 링크를 제공하여 최신 논문들을 지속적으로 업데이트합니다.
- **최초의 MLLM 서베이 주장:** MLLM에 대한 최초의 종합적인 서베이 논문임을 명시합니다.

## 📎 Related Works

- **기존 다중 모달 방법론:**
  - **판별(Discriminative) 패러다임:** CLIP [13], UNITER [15], Align before fuse [14] 등 시각 및 텍스트 정보를 통합된 표현 공간에 투영하여 하위 다중 모달 작업을 위한 다리를 구축.
  - **생성(Generative) 패러다임:** OFA [16], Unifying V&L via Text Generation [17], SimVLM [18] 등 다중 모달 작업을 sequence-to-sequence 방식으로 통합.
- **대규모 언어 모델 (LLM):** GPT-3 [7], ChatGPT [2], InstructGPT [95], FLAN [19], OPT-IML [96], LLaMA [5], Vicuna [4], LLaMA-2 [57], Qwen [58] 등 데이터 및 모델 크기 확장을 통해 놀라운 emergent abilities를 보임.
- **대규모 비전 모델 (LVM):** Segment Anything [9], DINO [11], DINOv2 [12] 등 시각 정보 처리에 특화됨.
- **MLLM 관련 연구:** BLIP-2 [59], InstructBLIP [60], LLaVA [20], MiniGPT-4 [21], mPLUG-Owl [81], Flamingo [74], CogVLM [75], LLaMA-Adapter [76], ImageBind-LLM [30], NExT-GPT [32], Shikra [28], Ferret [141], Osprey [29], LISA [142] 등 다양한 MLLM 아키텍처 및 기법.
- **훈련 기법:** Reinforcement Learning with Human Feedback (RLHF) [110], Direct Preference Optimization (DPO) [113].
- **추론 기법:** In-Context Learning (ICL) [7], Chain of Thought (CoT) [8].
- **주요 데이터셋:** CC-3M [84], LAION-5B [87], ScienceQA [116], COCO [133] 등 사전 학습, 지시 튜닝, 정렬 튜닝, 평가 등에 사용된 다양한 데이터셋들.

## 🛠️ Methodology

MLLM은 주로 3가지 모듈로 구성되며, 3단계의 훈련 과정을 거쳐 개발됩니다.

### 아키텍처

MLLM은 사전 학습된 모달리티 인코더, 사전 학습된 LLM, 그리고 이 둘을 연결하는 모달리티 인터페이스로 구성됩니다. 선택적으로 다른 모달리티 출력을 위한 제너레이터가 포함될 수 있습니다.

1. **모달리티 인코더:**
   - 이미지(CLIP, EVA-CLIP, ConvNext-L), 오디오(CLAP), 다중 모달(ImageBind) 등 사전 학습된 인코더를 사용하여 원시 정보를 압축합니다.
   - 일반적으로 인코더는 고정된 상태로 유지하거나, 더 나은 정렬을 위해 일부 모듈을 학습시킵니다.
   - 높은 해상도의 입력(직접 스케일링, 패치 분할)이 성능 향상에 중요합니다.
2. **사전 학습된 LLM:**
   - Flan-T5, LLaMA, Vicuna, Qwen 등 이미 풍부한 세계 지식과 강력한 추론 능력을 갖춘 LLM을 활용합니다.
   - LLM의 파라미터 크기를 늘리는 것이 성능 향상에 기여합니다.
   - MoE(Mixture of Experts) 아키텍처가 계산 비용 증가 없이 파라미터 크기를 확장하는 데 사용될 수 있습니다.
3. **모달리티 인터페이스 (커넥터):**
   - LLM이 텍스트만 인식할 수 있으므로, 다른 모달리티 정보를 LLM이 이해할 수 있는 공간으로 투영하여 연결하는 역할을 합니다.
   - **학습 가능한 커넥터:**
     - **토큰 수준 융합:** 인코더의 출력을 토큰으로 변환하여 텍스트 토큰과 연결합니다 (예: BLIP-2의 Q-Former, LLaVA의 MLP).
     - **특징 수준 융합:** LLM의 트랜스포머 레이어 내에 추가 모듈을 삽입하여 텍스트 특징과 시각 특징 간의 깊은 상호작용 및 융합을 가능하게 합니다 (예: Flamingo의 Cross-Attention, CogVLM의 Visual Expert).
   - **전문가 모델:** 이미지 캡셔닝 모델과 같은 외부 전문가 모델을 사용하여 다중 모달 입력을 텍스트로 변환합니다. (정보 손실 가능성 있음)

### 훈련 전략 및 데이터

MLLM은 일반적으로 사전 학습, 지시 튜닝, 정렬 튜닝의 세 단계를 거칩니다.

1. **사전 학습 (Pre-training):**
   - **목표:** 다른 모달리티들을 정렬하고 다중 모달 세계 지식을 학습합니다.
   - **방법:** 이미지/오디오/비디오의 캡션을 autoregressively 예측하도록 훈련하며, 교차 엔트로피 손실을 사용합니다. 인코더와 LLM은 고정하고 커넥터 모듈만 학습시키는 것이 일반적이지만, 더 많은 모듈을 학습시키기도 합니다.
   - **데이터:**
     - **거친 입도(Coarse-grained) 캡션 데이터:** 대량의 웹 스크랩 데이터 (예: CC-3M, LAION-5B), 짧고 노이즈가 많아 정제 과정이 필요합니다.
     - **미세 입도(Fine-grained) 캡션 데이터:** GPT-4V 등 강력한 MLLM을 통해 생성된 고품질 데이터 (예: ShareGPT4V-PT), 더 길고 정확한 설명을 포함합니다.
2. **지시 튜닝 (Instruction-tuning):**
   - **목표:** 모델이 사용자 지시를 더 잘 이해하고 요구되는 작업을 수행하도록 가르쳐 zero-shot 성능을 향상시킵니다.
   - **방법:** 지시, 다중 모달 입력, 정답 응답 쌍 (I, M, R)을 사용하여 LLM의 auto-regressive 목적 함수를 통해 다음 토큰을 예측하도록 훈련합니다. 다중 라운드 대화로도 확장될 수 있습니다.
   - **데이터 수집:**
     - **데이터 적응(Data Adaptation):** 기존 VQA, 캡셔닝 데이터셋을 지시 형식으로 변환합니다. 수동 또는 GPT 지원으로 지시문을 생성합니다.
     - **자체 지시(Self-Instruction):** 소수의 수동 주석 샘플을 사용하여 LLM (ChatGPT/GPT-4V)이 새로운 지시-응답 데이터를 생성하도록 유도합니다 (예: LLaVA-Instruct-150k).
     - **데이터 혼합(Data Mixture):** 다중 모달 지시 데이터와 언어 전용 대화 데이터를 함께 사용하여 대화 능력을 향상시킵니다.
   - **데이터 품질:** 지시 다양성 및 작업 범위(특히 시각 추론)가 모델 성능과 일반화 능력에 중요합니다.
3. **정렬 튜닝 (Alignment tuning):**
   - **목표:** 모델 응답이 인간의 선호도(예: 환각 감소)에 더 잘 부합하도록 정렬합니다.
   - **방법:**
     - **RLHF (Reinforcement Learning with Human Feedback) [110]:** 지도 학습 파인튜닝, 보상 모델 훈련 (인간 선호도 기반), PPO 알고리즘을 사용한 강화 학습의 세 단계로 진행됩니다 (예: LLaVA-RLHF).
     - **DPO (Direct Preference Optimization) [113]:** 명시적인 보상 모델 없이 인간 선호 레이블을 사용하여 이진 분류 손실로 직접 학습합니다 (예: RLHF-V, Silkie).
   - **데이터:** 인간 피드백/선호도 데이터를 수집하여 모델 응답의 품질을 비교합니다 (비용이 많이 들고 데이터 양이 적습니다).

### 확장 기법

- **세분화 지원:** 이미지 전체에서 특정 영역(바운딩 박스) 또는 픽셀 수준(클릭)으로 사용자 프롬프트의 제어 수준을 높입니다 (예: Shikra, Ferret, Osprey, LISA). 응답에서도 박스/마스크 주석을 통한 접지(grounding)를 지원합니다.
- **모달리티 지원:** 3D 포인트 클라우드와 같은 더 많은 입력 모달리티를 지원하고, 이미지, 오디오, 비디오와 같은 다양한 출력 모달리티를 생성합니다 (예: NExT-GPT).
- **언어 지원:** 다국어 모델 개발 (예: VisCPM, Qwen-VL)을 통해 영어가 아닌 언어 사용자도 지원합니다.
- **시나리오/작업 확장:** 자원 제한 환경(MobileVLM), GUI 에이전트(CogAgent, AppAgent), 실제 세계 상호작용 에이전트(Embodied agents)와 같은 특정 시나리오에 MLLM을 적용합니다. 의료(LLaVA-Med) 및 문서 이해(mPLUG-DocOwl, TextMonkey)와 같은 도메인별 전문 지식을 강화합니다.

### 다중 모달 환각 (Multimodal Hallucination) 완화

- **환각 유형:** 존재 환각, 속성 환각, 관계 환각으로 분류됩니다.
- **평가 방법:** CHAIR, POPE, MME, HaELM, Woodpecker (GPT-4V 기반), FaithScore, AMBER 등 다양한 지표 및 벤치마크가 사용됩니다.
- **완화 방법:**
  - **사전 교정 (Pre-correction):** 특정 데이터(부정 지시, 인간 선호도)를 수집하여 모델을 미세 조정합니다 (예: LRV-Instruction, LLaVA-RLHF).
  - **진행 중 교정 (In-process-correction):** 아키텍처 설계 또는 특징 표현을 개선하여 생성 과정에서 환각을 완화합니다 (예: HallE-Switch, VCD, HACL).
  - **사후 교정 (Post-correction):** 생성된 출력 후 환각을 수정합니다 (예: Woodpecker, LURE).

### 확장된 기술

- **다중 모달 In-Context Learning (M-ICL):**
  - 소수의 예시를 통해 학습하며, 추론 단계에서 추가 훈련 없이 작동합니다. 지시 튜닝과 결합되거나, 비전 디코더를 통해 이미지 출력을 지원합니다.
  - **개선 노력:** MIMIC-IT, Emu, Link-context learning, MMICL, 일관성 없는 이미지/텍스트 제거를 위한 pre-filtering.
  - **응용:** 시각 추론 작업 해결, 외부 도구 사용법 학습.
- **다중 모달 Chain of Thought (M-CoT):**
  - LLM이 최종 답뿐만 아니라 추론 과정도 출력하도록 유도하여 복잡한 추론 작업을 처리합니다.
  - **학습 패러다임:** 미세 조정 (예: ScienceQA), few/zero-shot 학습.
  - **체인 구성:** 단일 체인 또는 트리 형태 (예: DDCoT), 적응형 또는 사전 정의된 길이.
  - **생성 패턴:** infilling-based(주변 컨텍스트 사이의 논리적 공백 채우기) 및 predicting-based(이전 추론 기록을 기반으로 체인 확장).
- **LLM-Aided Visual Reasoning (LAVR):**
  - 외부 도구 또는 비전 파운데이션 모델을 호출하기 위해 LLM을 '도우미'로 활용하는 시스템입니다.
  - **특징:** 강력한 일반화 능력, 새로운 능력 (밈 이해 등), 더 나은 상호작용 및 제어.
  - **훈련 패러다임:** Training-free (few-shot, zero-shot) 및 Fine-tuning.
  - **LLM의 기능/역할:**
    - **컨트롤러:** 복잡한 작업을 더 간단한 하위 작업으로 분해하고 적절한 도구/모듈에 할당합니다 (예: VisProg).
    - **의사 결정자:** 현재 컨텍스트와 히스토리 정보를 요약하고, 정보의 충분성을 판단하며, 답변을 사용자 친화적인 방식으로 구성합니다.
    - **의미 정제자:** 풍부한 언어 및 의미론적 지식을 활용하여 정보를 일관되고 유창한 자연어 문장으로 통합하거나 특정 요구에 맞는 텍스트를 생성합니다.

## 📊 Results

이 서베이 논문은 직접적인 실험 결과를 제시하기보다는, 다중 모달 대규모 언어 모델(MLLM) 연구의 전반적인 동향과 주요 발견들을 종합하여 제시합니다.

- **놀라운 새로운 능력:** MLLM은 이미지 기반 스토리 작성, OCR 없는 수학 추론, 밈 이해 등 전통적인 다중 모달 방법에서는 드물었던 놀라운 새로운 능력(emergent capabilities)을 보여줍니다.
- **성능 향상 요인:**
  - **데이터 및 모델 스케일링:** 데이터 크기와 모델 크기를 확장할수록 LLM과 유사하게 탁월한 능력이 발현됩니다.
  - **고해상도 입력:** 높은 해상도 이미지를 사용할 때 (예: LLaVA-1.5, Monkey, SPHINX) 상당한 성능 향상을 가져옵니다.
  - **고품질 지시 튜닝 데이터:** 단순히 데이터 양을 늘리는 것보다 다양하고 복잡한 시각 추론 지시문을 포함한 고품질 데이터가 모델 성능과 일반화 능력을 크게 향상시킵니다 (예: Lynx, Wei et al.).
  - **LLM 파라미터 스케일링:** LLM의 파라미터 크기를 키우면 (예: 7B에서 13B, 34B로 확장) 다양한 벤치마크에서 포괄적인 개선이 이루어지며, 학습 데이터에 없던 언어(예: 중국어)에서도 zero-shot 능력이 나타납니다.
  - **MoE 아키텍처:** Mixture of Experts (MoE) 아키텍처 (예: MM1, MoE-LLaVA)는 계산 비용 증가 없이 총 파라미터 크기를 확장하며, 대부분의 벤치마크에서 기존 Dense 모델보다 우수한 성능을 보입니다.
- **환각 완화의 중요성:** MLLM의 주요 문제인 환각(hallucination)을 줄이기 위한 다양한 사전/과정 중/사후 교정 방법들이 개발되고 있으며, GPT-4V를 통한 평가(예: Woodpecker)가 더 정확한 분석을 가능하게 합니다.
- **최신 MLLM의 비교:** GPT-4V와 Gemini 같은 최신 상업용 모델들이 비교 평가되었으며, 서로 다른 응답 스타일에도 불구하고 시각 추론 능력에서 유사한 수준을 보였습니다.

## 🧠 Insights & Discussion

MLLM은 LLM의 강력한 추론 능력과 LVM의 뛰어난 시각 능력을 결합하여 전례 없는 다중 모달 과제 수행 능력을 보여주고 있습니다. 이러한 "새로운 능력"은 인공 일반 지능(AGI)으로 가는 길을 제시하며 학계와 산업계 모두에서 GPT-4V와 경쟁하거나 이를 능가하는 모델 개발에 집중하고 있습니다.

핵심적인 통찰은 MLLM이 인코더, LLM, 그리고 이 둘을 잇는 인터페이스라는 모듈형 아키텍처를 기반으로 한다는 것입니다. 이 모델들은 사전 학습, 지시 튜닝, 정렬 튜닝의 다단계 훈련을 통해 점진적으로 능력을 강화합니다. 특히 고품질 데이터, 고해상도 입력, LLM의 파라미터 스케일링은 MLLM 성능 향상에 결정적인 역할을 합니다.

하지만 MLLM은 아직 초기 단계에 있으며 몇 가지 중요한 한계를 가지고 있습니다.

- **장문 컨텍스트 처리의 한계:** 현재 MLLM은 긴 컨텍스트의 다중 모달 정보를 처리하는 데 제한적입니다. 이는 긴 비디오 이해나 이미지와 텍스트가 섞인 장문 문서 이해와 같은 고급 모델 개발을 저해합니다.
- **복잡한 지시 이해 능력 부족:** 고품질 질문-답변 데이터를 생성하는 주류 방식이 여전히 폐쇄형 GPT-4V에 의존하는 것은 다른 MLLM들이 고급 지시 이해 능력이 부족함을 시사합니다. 모델이 더욱 복잡하고 미묘한 지시를 따르도록 개선해야 합니다.
- **M-ICL 및 M-CoT 능력의 미성숙:** 다중 모달 In-Context Learning (M-ICL) 및 Chain of Thought (M-CoT)와 같은 고급 추론 기법들은 아직 초보적인 수준이며 MLLM의 관련 능력도 미약합니다. 이러한 기법들의 근본적인 메커니즘을 탐구하고 잠재적인 개선점을 찾는 것이 중요합니다.
- **현실 세계 상호작용의 어려움:** MLLM 기반의 embodied agent를 개발하여 현실 세계와 상호작용하는 것은 유망하지만, 이는 인지, 추론, 계획, 실행 등 비판적인 능력을 요구합니다.
- **안전 문제:** LLM과 마찬가지로 MLLM도 조작된 공격(adversarial attacks)에 취약하여 편향되거나 바람직하지 않은 응답을 생성할 수 있습니다. 모델 안전성을 향상시키는 것은 중요한 연구 주제입니다.

이러한 한계들을 극복하기 위한 연구는 MLLM 분야의 미래를 형성할 것입니다.

## 📌 TL;DR

다중 모달 대규모 언어 모델(MLLM)은 텍스트 전용 LLM의 한계와 비전 전용 모델의 추론 부족을 극복하기 위해 등장했습니다. 이 서베이 논문은 MLLM의 기본 아키텍처(모달리티 인코더, LLM, 커넥터), 훈련 전략(사전 학습, 지시 튜닝, 정렬 튜닝), 확장성(세분화, 모달리티, 언어, 시나리오), 그리고 다중 모달 환각 문제 완화 및 M-ICL, M-CoT, LAVR 같은 확장 기술을 종합적으로 정리했습니다. MLLM은 이미지 기반 스토리 생성, OCR 없는 수학 추론 등 놀라운 능력을 보여주지만, 장문 컨텍스트 처리, 복잡한 지시 이해, 고급 추론 능력, 그리고 안전성 측면에서 아직 많은 발전 가능성과 과제를 안고 있습니다.
