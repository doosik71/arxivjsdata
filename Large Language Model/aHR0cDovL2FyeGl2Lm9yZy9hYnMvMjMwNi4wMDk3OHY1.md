# AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION FOR ON-DEVICE LLM COMPRESSION AND ACCELERATION

Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han

## 🧩 Problem to Solve

대규모 언어 모델(LLM)을 엣지 디바이스에 배포하는 것은 막대한 모델 크기와 제한된 하드웨어 자원으로 인해 상당한 어려움을 겪습니다. 기존 양자화 방법들은 다음과 같은 한계를 가집니다:

* **QAT(Quantization-aware training)**: LLM의 높은 학습 비용으로 인해 비효율적입니다.
* **PTQ(Post-training quantization)**: 낮은 비트 설정에서 정확도 저하가 큽니다.
* **GPTQ**: 재구성 과정에서 캘리브레이션 데이터셋에 과적합되어, LLM의 다양한 도메인 및 양상에 대한 일반화 능력을 저해할 수 있습니다.
* **하드웨어 비효율성**: 특정 가중치를 FP16으로 유지하는 혼합 정밀도 방식은 하드웨어 구현이 어렵습니다.

## ✨ Key Contributions

* **AWQ (Activation-aware Weight Quantization)**: LLM을 위한 하드웨어 친화적인 저비트 가중치 전용 양자화 방법론을 제안합니다.
  * LLM의 모든 가중치가 동일하게 중요하지 않다는 사실을 발견하고, 활성화 분포를 참조하여 "주목할 만한(salient)" 가중치 채널을 식별합니다.
  * 하드웨어 비효율적인 혼합 정밀도 방식 대신, 주목할 만한 채널에 스케일링을 적용하여 양자화 오류를 줄이는 방법을 수학적으로 도출합니다.
  * 백프로파게이션이나 재구성 없이 오프라인에서 활성화 통계를 수집하여 최적의 스케일을 결정하며, 이로 인해 캘리브레이션 데이터셋에 과적합되지 않고 LLM의 일반화 능력을 보존합니다.
* **TinyChat**: 4비트 온디바이스 LLM/VLM 배포를 위한 효율적인 추론 프레임워크를 개발했습니다.
  * 커널 퓨전 및 플랫폼 인지 가중치 패킹을 통해 이론적 메모리 절약을 실제 속도 향상으로 전환합니다.
* **성능 우수성**:
  * AWQ는 기존 양자화 방법(RTN, GPTQ) 대비 다양한 언어 모델링, 명령 튜닝, 멀티모달 벤치마크에서 뛰어난 성능을 보입니다.
  * TinyChat은 데스크톱 및 모바일 GPU에서 Huggingface FP16 구현 대비 3배 이상의 속도 향상을 제공하며, Llama-2-70B 모델을 모바일 GPU에서 배포할 수 있게 합니다.

## 📎 Related Works

* **모델 양자화 방법**:
  * **QAT (Quantization-Aware Training)**: 벤지오 외(2013), 골라미 외(2021) 등. 학습 과정에서 양자화를 고려합니다.
  * **PTQ (Post-Training Quantization)**: 제이콥 외(2018), 나겔 외(2019) 등. 학습 후 양자화를 수행하며, LLM 양자화에 주로 사용됩니다.
* **LLM 양자화**:
  * **W8A8 (가중치 및 활성화 8비트)**: 데트머스 외(2022), 샤오 외(2022) 등.
  * **저비트 가중치 전용 양자화 (W4A16)**: 본 연구의 초점. 메모리 요구 사항을 줄이고 토큰 생성 속도를 높입니다.
    * **GPTQ (프란타르 외, 2022)**: 2차 정보를 사용하여 오류 보정을 수행하는 가장 유사한 선행 연구. 캘리브레이션 데이터셋에 과적합될 수 있다는 단점이 있습니다.
* **저비트 양자화 LLM을 위한 시스템 지원**:
  * GPTQ는 OPT 모델을 위한 INT3 커널을 제공하며, GPTQ-for-LLaMA는 INT4 재정렬 양자화를 지원합니다.
  * FlexGen, llama.cpp, exllama는 그룹별 INT4 양자화를 통해 I/O 비용을 줄입니다.
  * MLC-LLM은 TVM 백엔드를 사용하여 엣지 플랫폼에서 강력한 결과를 보여줍니다.

## 🛠️ Methodology

1. **주목할 만한 가중치 식별**:
    * LLM 가중치의 0.1~1%만이 성능에 결정적으로 중요함을 확인합니다.
    * 이러한 '주목할 만한' 채널은 가중치 크기가 아닌 **활성화(activation) 분포**를 기준으로 식별해야 한다고 제안합니다. 활성화 크기가 큰 특징을 처리하는 가중치 채널이 더 중요합니다.
2. **활성화 인지 스케일링으로 주목할 만한 가중치 보호**:
    * 하드웨어 비효율적인 혼합 정밀도를 피하면서, 주목할 만한 가중치 채널의 양자화 오류를 줄이기 위해 **채널별 스케일링(per-channel scaling)** 방법을 제안합니다.
    * **양자화 오류 분석**: 가중치 $w$와 활성화 $x$의 선형 연산 $y=wx$ 및 양자화된 $y=Q(w)x$를 고려합니다. 양자화 함수는 $Q(w) = \Delta \cdot \text{Round}(\frac{w}{\Delta})$로 정의됩니다.
    * 가중치 $w$를 $s > 1$로 스케일하고 $x$를 $1/s$로 역 스케일하면 $Q(w \cdot s)(\frac{x}{s})$가 됩니다. 이 경우 양자화 오류 비율은 $\frac{\Delta'}{\Delta} \cdot \frac{1}{s}$가 됩니다. $\Delta' \approx \Delta$이므로, 주목할 만한 가중치 $w$의 상대적 오류는 $1/s$만큼 줄어듭니다.
    * 주의: 너무 큰 $s$ 값은 주목할 만하지 않은(non-salient) 채널의 오류를 증가시킬 수 있습니다.
3. **최적 스케일링 인자 탐색**:
    * 최종 목표는 양자화 후 출력 차이를 최소화하는 $s$를 찾는 것입니다:
        $$s^{*} = \text{arg min}_{s} \|Q(W \cdot \text{diag}(s))(\text{diag}(s)^{-1} \cdot X) - WX\|$$
    * $W$는 FP16 원본 가중치, $X$는 캘리브레이션 데이터셋에서 얻은 입력 특징입니다.
    * 양자화 함수 $Q$가 미분 불가능하므로 직접적인 역전파 최적화는 어렵습니다.
    * 간단한 탐색 공간: $s = s_{X}^{\alpha}$ ($s_{X}$는 채널별 활성화의 평균 크기). 하이퍼파라미터 $\alpha \in [0, 1]$은 빠른 그리드 탐색을 통해 최적화됩니다.
    * 추가적으로 가중치 클리핑을 적용하여 MSE 오류를 최소화합니다.
4. **TinyChat 추론 시스템**:
    * **온더플라이(On-the-fly) 가중치 역양자화**: 4비트 정수와 FP16 간의 곱셈을 지원하지 않는 하드웨어를 위해, 역양자화 커널과 행렬 곱셈 커널을 퓨전하여 역양자화된 가중치를 DRAM에 쓰는 것을 방지합니다.
    * **SIMD-aware 가중치 패킹**: ARM NEON과 같은 SIMD 아키텍처를 위해 디바이스의 SIMD 단위 비트 폭에 맞춰 가중치를 재정렬하고 패킹하여 런타임 시 효율적인 언패킹(AND 및 시프트 비트 연산)을 가능하게 합니다.
    * **커널 퓨전**: 레이어 정규화, QKV 프로젝션, 위치 임베딩 계산, KV 캐시 업데이트 등 여러 연산자를 단일 커널로 융합하여 커널 실행 오버헤드와 중간 DRAM 접근을 최소화합니다.

## 📊 Results

* **언어 모델링 성능**: LLaMA, Llama-2 (7B-70B), Mistral, Mixtral 모델에서 INT3/INT4 양자화 시 RTN 및 GPTQ 대비 AWQ가 일관되게 낮은 Perplexity(PPL)를 달성합니다. INT4-g128 설정에서 AWQ는 FP16에 준하는 PPL을 보여줍니다.
* **명령 튜닝 모델**: Vicuna 7B 및 13B 모델에서 GPT-4 평가 프로토콜을 사용했을 때, AWQ는 RTN 및 GPTQ 대비 INT3-g128 양자화 성능을 개선하여 일반화 능력을 입증합니다.
* **멀티모달 언어 모델**:
  * OpenFlamingo-9B 모델의 COCO 캡셔닝 태스크에서 AWQ는 기존 방법보다 우수하며, INT4-g128에서 양자화로 인한 성능 저하(32샷 기준)를 4.57에서 1.17로 줄입니다.
  * VILA-7B/13B 모델에 대해 11가지 시각-언어 벤치마크에서 INT4-g128 설정 시 손실 없는 성능을 달성합니다.
  * LLaVA-13B의 시각적 추론 예시에서 RTN보다 합리적인 답변을 생성합니다.
* **프로그래밍 및 수학 태스크**: CodeLlama-7b-Instruct-hf (MBPP) 및 Llama-2 (GSM8K)에서 AWQ는 기존 방법 대비 우수하며, INT4-g128 설정에서 FP16과 유사한 성능을 보입니다.
* **극저비트 양자화 (INT2)**: INT2-g64 양자화에서 RTN은 완전히 실패하고, AWQ는 GPTQ와 결합 시 상당한 PPL 개선을 가져옵니다.
* **데이터 효율성 및 일반화**: AWQ는 GPTQ보다 10배 적은 캘리브레이션 데이터셋으로도 좋은 성능을 달성하며, 캘리브레이션 데이터셋 분포 변화에 대해 훨씬 더 강건합니다 (GPTQ의 PPL 2.3-4.9 증가 대비 AWQ는 0.5-0.6 증가).
* **TinyChat을 통한 속도 향상**:
  * RTX 4090 (데스크톱) 및 Jetson Orin (모바일) GPU에서 TinyChat은 Huggingface FP16 구현 대비 2.7~3.9배의 속도 향상을 제공합니다.
  * RTX 4070 (8GB 메모리) 노트북 GPU에서 Llama-2-13B 모델을 33 토큰/초로 실행 가능하게 합니다 (FP16은 메모리 부족으로 불가능).
  * 기존 엣지 LLM 추론 시스템(AutoGPTQ, llama.cpp, exllama) 대비 1.2~3.0배의 속도 향상을 보입니다.
  * 매우 제한적인 자원의 Raspberry Pi 4B에서도 7B LLM을 0.7 토큰/초로 실행 가능하게 합니다.

## 🧠 Insights & Discussion

* **활성화 인지(Activation-awareness)의 중요성**: 이 연구의 핵심 통찰은 가중치 양자화 시 가중치 자체의 크기보다는 활성화 분포를 통해 '주목할 만한' 가중치를 식별하는 것이 양자화 오류를 줄이는 데 훨씬 효과적이라는 것입니다. 이는 LLM의 일반화 능력을 보존하는 데 결정적인 역할을 합니다.
* **하드웨어 친화적 설계**: 주목할 만한 가중치를 보호하기 위해 혼합 정밀도 데이터 타입을 사용하지 않고 채널별 스케일링을 적용함으로써, 하드웨어 구현 복잡성을 피하고 엣지 디바이스 배포의 실용성을 크게 높였습니다.
* **뛰어난 일반화 능력**: AWQ는 캘리브레이션 데이터에 대한 최소한의 의존성 덕분에 과적합 문제를 해결하고, 명령 튜닝, 멀티모달, 코딩 및 수학 등 다양한 도메인과 작업에서 LLM의 광범위한 지식을 효과적으로 보존합니다. 이는 기존의 재구성 기반 방법론에 비해 큰 장점입니다.
* **시스템 공동 설계의 중요성**: TinyChat 프레임워크는 소프트웨어와 하드웨어의 공동 설계가 이론적인 메모리 절약을 실제 추론 속도 향상으로 전환하는 데 필수적임을 보여줍니다. SIMD-aware 패킹 및 커널 퓨전과 같은 최적화는 엣지 환경에서 성능을 극대화하는 데 중요합니다.
* **LLM의 대중화**: AWQ와 TinyChat은 Llama-2-70B와 같은 대규모 LLM을 자원이 제한적인 엣지 디바이스에 배포할 수 있게 함으로써, 클라우드 컴퓨팅 비용을 절감하고 사용자 프라이버시를 강화하며, AI 애플리케이션의 광범위한 대중화에 기여합니다.

## 📌 TL;DR

**문제**: LLM을 엣지 디바이스에 배포하기 어렵다 (거대한 모델 크기, 제한된 자원, 기존 양자화 방법의 정확도 손실 및 일반화 문제).

**제안 방법**: **AWQ (Activation-aware Weight Quantization)**는 LLM 가중치가 균등하게 중요하지 않다는 관찰에서 출발한다. 가중치 분포가 아닌 *활성화 분포*를 기반으로 "주목할 만한(salient)" 채널을 식별하고, 이 채널들의 상대적 양자화 오류를 줄이기 위해 수학적으로 유도된 *채널별 스케일링*을 적용한다. 이 스케일링은 오프라인에서 활성화 통계를 수집하여 결정되며, 백프로파게이션이나 재구성이 필요 없어 오버피팅을 방지하고 일반화 능력을 유지한다.

**주요 결과**: AWQ는 기존 양자화 방법(RTN, GPTQ)보다 언어 모델링, 명령 튜닝 모델, 멀티모달 모델, 코딩/수학 벤치마크에서 우수한 성능을 보인다. 또한, 동반 개발된 추론 프레임워크인 **TinyChat**은 커널 퓨전 및 플랫폼 인지 가중치 패킹을 통해 4비트 양자화된 LLM/VLM에 대해 Huggingface FP16 구현 대비 3.2-3.3배의 속도 향상을 제공하며, 70B Llama-2 모델을 모바일 GPU에 배포할 수 있게 하여 LLM의 엣지 배포를 대중화한다.
