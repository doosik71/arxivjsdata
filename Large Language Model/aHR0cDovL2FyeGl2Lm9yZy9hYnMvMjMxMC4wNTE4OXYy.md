# 대규모 언어 모델 시대의 사실성 문제 (Factuality Challenges in the Era of Large Language Models)

Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, and Giovanni Zagni

## 🧩 Problem to Solve

ChatGPT와 같은 대규모 언어 모델(LLM) 기반 도구는 자연어 생성에서 상당한 발전을 이루었지만, 거짓, 오류 또는 오해의 소지가 있는 콘텐츠(환각)를 생성하는 경향이 있습니다. 더욱이 LLM은 사실적이지만 그럴듯한 콘텐츠와 프로필을 대규모로 생성하는 등 악의적인 목적으로 악용될 수 있습니다. 이로 인해 사용자 기만 및 부정확한 정보 확산이라는 심각한 사회적 문제가 발생합니다. 본 논문은 이러한 위험에 비추어 볼 때, 생성형 AI 시대에 진실성 문제를 해결하기 위해 사실 확인 기관, 언론 기관 및 광범위한 연구 및 정책 커뮤니티에서 필요한 기술 혁신, 규제 개혁, 그리고 AI 리터러시 이니셔티브의 종류를 모색합니다.

## ✨ Key Contributions

* **LLM의 사실성 및 악성 사용 위험 식별:** LLM의 "환각" 능력과 악의적인 사용 가능성으로 인해 발생하는 구체적인 위험과 임박한 위협을 상세히 설명합니다.
* **다차원적 해결책 탐색:** 이러한 문제에 대응하기 위한 기술적 혁신(정렬, 지식 기반 프레임워크, 검색 증강 생성, 환각 제어, 노출 편향 완화, 개선된 평가), 규제 개혁, AI 리터러시 이니셔티브 등 다양한 측면의 솔루션을 제시합니다.
* **사실 확인 기회 분석:** LLM이 사실 확인 전문가와 저널리스트에게 제공할 수 있는 유망한 기회(예: 요약, 주장 식별, 입장 탐지)를 논의하며, 책임감 있는 활용의 중요성을 강조합니다.
* **포괄적인 의제 제안:** 개인, 정부, 민주 사회가 생성형 AI의 이점을 활용하면서 위험을 줄이기 위한 시급한 의제(조정 및 협력, 규제, AI 리터러시 증진, 기술 개발)를 제안합니다.

## 📎 Related Works

* **초기 언어 모델 및 LLM 발전:** 클로드 섀넌의 정보 이론(1948), 초기 GPT 시리즈(GPT, GPT-2, GPT-3), GPT-4, LLaMA 2, Alpaca, Vicuna, Claude, Falcon, Jurassic, Jais 등 LLM의 역사와 진화.
* **LLM의 능력 및 한계 연구:** LLM의 상식 추론, 지식 일반화, 비정상 능력(emergent abilities), 환각(hallucination), 인과적 추론, 신뢰성 부족 등에 대한 연구.
* **악성 사용 및 오정보 확산:** LLM을 이용한 스캠 이메일, 가짜 뉴스 웹사이트, 소셜 봇 생성, 딥페이크(Dall-E, MidJourney, Stable Diffusion) 등 악의적인 애플리케이션에 대한 논의.
* **LLM 평가 방법:** BIG-bench, GLUE, SuperGLUE와 같은 기존 벤치마크와 TruthfulQA, FActScore, GPTScore, G-Eval, SelfCheckGPT 등 LLM의 사실성 및 신뢰성을 평가하기 위한 새로운 방법론.
* **지식 편집 및 제어:** LLM의 사실적 오류를 수정하기 위한 지식 편집(knowledge editing) 및 환각 제어 기술.
* **정책 및 규제 노력:** 중국의 AI 생성 콘텐츠 워터마크 규제, 이탈리아의 ChatGPT GDPR 관련 일시적 금지, 유럽연합의 AI Act, 미국 연방거래위원회(FTC)의 경고, 캐나다의 자동화된 의사결정 지침.
* **AI 리터러시 및 교육:** 대중의 AI 인식 및 교육의 중요성에 대한 연구.

## 🛠️ Methodology

본 논문은 새로운 모델이나 알고리즘을 제안하는 것이 아닌, LLM으로 인한 사실성 문제와 관련된 광범위한 위험, 위협, 해결책, 그리고 기회를 **종합적으로 분석하고 검토하는 접근 방식**을 채택합니다.

* **문제 정의:** LLM이 생성하는 부정확하거나 오해의 소지가 있는 콘텐츠(환각)와 악의적인 사용 가능성을 핵심 문제로 정의합니다.
* **위험 및 위협 분류:** LLM의 직접적인 사실성 문제(출처 부족, 진실성, 확신에 찬 어조, 유창한 스타일, 사적인 사용, 쉬운 접근성, 후광 효과, 대중의 인식, 신뢰할 수 없는 평가)와 악의적 사용 위협(개인화된 공격, 스타일 모방, 탐지 우회, 가짜 프로필 생성)을 체계적으로 분류하고 설명합니다.
* **해결 전략 제안:** 이러한 위협에 대응하기 위한 기술적 솔루션(정렬 및 안전, 모듈화된 지식 기반 프레임워크, 검색 증강 생성, 환각 제어 및 지식 편집, 노출 편향 완화, 개선된 평가, 개인 정보 보호)과 비기술적 솔루션(AI 생성 콘텐츠 인식, 콘텐츠 진위 및 출처, 규제, 대중 교육)을 제시합니다.
* **기회 탐색:** LLM이 사실 확인을 지원할 수 있는 잠재적인 역할(사실 확인 지원, 입장 탐지, 도메인별 챗봇 지원)을 모색하고 그 한계를 지적합니다.
* **의제 제시:** 분석을 바탕으로 개인, 정부, 민주 사회를 위한 조정 및 협력, 규제, AI 리터러시 증진, 기술 개발의 네 가지 시급한 의제를 제안하며 결론을 내립니다.

## 📊 Results

* **LLM의 사실성 문제점:** LLM은 유창하고 그럴듯한 텍스트를 생성하지만, 종종 출처가 불분명하거나, 사실과 다르거나, 비일관적이며, 근거 없는 주장을 확신에 찬 어조로 제시하는 경향이 있습니다. 특히 민감한 분야(예: 보건, 금융)에서 부정확한 답변을 제공할 수 있습니다.
* **악의적인 LLM 사용의 위협:** LLM은 개인화된 피싱 공격, 특정 인물의 글쓰기 스타일 모방, 사실 확인 시스템을 우회하는 콘텐츠의 무한 변형 생성, 대규모 가짜 소셜 미디어 프로필 생성을 통해 사회적 기만과 여론 조작을 강화할 수 있습니다.
* **현재 해결책의 한계:** LLM의 환각을 줄이기 위한 정렬(alignment), 검색 증강 생성(RAG), 지식 편집(knowledge editing) 등 기술적 노력이 진행 중이지만, 사실성 평가의 어려움, 모델의 훈련 데이터 오염, 오픈 소스 LLM의 확산으로 인한 통제 어려움 등의 한계가 있습니다.
* **규제 및 교육의 필요성:** AI 생성 콘텐츠 탐지 및 워터마킹 기술은 완벽하지 않으며, 악의적인 사용자가 쉽게 우회할 수 있습니다. 따라서 개인 정보 보호, 콘텐츠 진위 증명, 국제적 규제 협력, 대중의 AI 리터러시 증진이 필수적입니다.
* **LLM을 통한 사실 확인 기회:** LLM은 사실 확인 전문가에게 방대한 문서를 요약하고, 중요한 주장을 식별하며, 주장에 대한 입장을 탐지하는 등 보조적인 역할을 제공할 수 있습니다. 그러나 AI 기반 사실 확인의 오용 가능성과 비의도적 결과에 대한 주의가 필요합니다.

## 🧠 Insights & Discussion

* **심각한 사회적 영향:** LLM의 확산은 잘못된 정보와 악성 콘텐츠의 대규모 생성 및 확산을 가능하게 하여 정보 생태계의 신뢰성을 근본적으로 훼손하고 사회적 기만을 심화시킬 수 있습니다. 특히, LLM의 유창하고 확신에 찬 어조는 사용자가 잘못된 정보를 사실로 오인하게 만드는 위험을 가중시킵니다.
* **다각적인 접근의 필요성:** LLM의 사실성 문제는 기술적인 해결책만으로는 불충분하며, 규제, 교육, 사회적 협력이 결합된 다차원적이고 포괄적인 전략이 요구됩니다. 단일 솔루션으로는 이 복잡한 문제를 완전히 해결할 수 없습니다.
* **오픈 소스 LLM의 양면성:** 오픈 소스 LLM은 기술 민주화와 혁신을 촉진하지만, 동시에 악의적인 사용자가 규제를 우회하고 새로운 형태의 위협을 생성하는 것을 더 어렵게 만들 수 있습니다.
* **평가 방법의 한계:** LLM의 "사실성"과 "진실성"에 대한 객관적이고 신뢰할 수 있는 평가는 여전히 어려운 과제이며, 기존 벤치마크만으로는 충분하지 않습니다. 지속적인 연구를 통해 특정 도메인에 특화된 평가 및 개선된 메트릭 개발이 필요합니다.
* **AI 리터러시의 중요성:** 대중이 LLM의 작동 방식, 한계, 그리고 잠재적 위험을 이해하는 AI 리터러시를 갖추는 것이 필수적입니다. 포토샵으로 조작된 이미지에 대한 회의적 태도처럼, AI가 생성한 콘텐츠에 대한 비판적 사고 능력을 함양해야 합니다. 그러나 이로 인해 신뢰할 수 있는 정보원에 대한 불신이 생기는 역효과도 경계해야 합니다.

## 📌 TL;DR

대규모 언어 모델(LLM)은 뛰어난 언어 생성 능력을 가졌지만, **환각(hallucination)을 통해 사실과 다른 정보를 유창하게 제공하고 악의적인 목적으로 대규모 가짜 콘텐츠를 생성**할 수 있어 사회적 혼란과 기만의 위험이 커지고 있습니다. 본 논문은 LLM의 **출처 부족, 진실성 문제, 확신에 찬 어조, 악성 사용(개인화 공격, 가짜 프로필 등) 등의 다양한 위험을 식별**하고, 이를 해결하기 위한 **기술적 개선(검색 증강 생성, 환각 제어 등), 엄격한 규제, 대중의 AI 리터러시 교육, 국제적 협력**이 필수적임을 강조합니다. 또한, LLM이 사실 확인 보조, 주장 식별 등 긍정적인 역할도 할 수 있으나, 그 한계와 비의도적 결과에 대한 주의를 당부하며 다각적인 접근이 필요하다고 결론 내립니다.
