# CAN LLMS EXPRESS THEIR UNCERTAINTY? AN EMPIRICAL EVALUATION OF CONFIDENCE ELICITATION IN LLMS

Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi

## 🧩 Problem to Solve

거대 언어 모델(LLM)이 답변에 대한 불확실성(확신도)을 정확하게 표현하는 능력은 신뢰할 수 있고 책임감 있는 의사 결정을 위해 필수적입니다. 기존의 확신도 추출 방법들은 주로 모델의 내부 정보(화이트박스 접근)에 의존하거나 미세 조정(fine-tuning)을 필요로 했으나, 이는 특히 GPT-4와 같은 상용 폐쇄형 LLM API에는 적합하지 않습니다. 따라서, 모델의 내부 정보에 접근할 수 없는 **블랙박스 환경에서 LLM의 불확실성을 효과적으로 추정하는 새로운 방법론을 모색**할 필요가 있습니다.

## ✨ Key Contributions

* **체계적인 프레임워크 정의**: LLM 불확실성 추정을 위한 프롬프팅 전략, 샘플링 방법, 취합(aggregation) 기술을 포함하는 블랙박스 프레임워크를 제시했습니다.
* **포괄적인 벤치마킹**: 확신도 보정(calibration)과 실패 예측(failure prediction)이라는 두 가지 주요 평가 작업을 5가지 데이터셋 유형(상식, 산술, 상징, 윤리, 전문 지식)과 5가지 LLM(GPT-4, LLaMA 2 Chat 등)에 걸쳐 벤치마킹했습니다.
* **주요 발견**:
  * LLM은 확신도를 표현할 때 **과신 경향**을 보이며, 이는 인간의 확신도 표현 패턴을 모방하는 것일 수 있습니다.
  * 모델의 역량이 커짐에 따라 보정 및 실패 예측 성능이 향상되지만, 여전히 이상적인 수준에는 미치지 못합니다.
  * 제안된 전략(인간에게서 영감을 받은 프롬프트, 다중 응답 간 일관성, 개선된 취합 전략)은 다양한 관점에서 과신 문제를 완화하는 데 도움을 줍니다.
  * 화이트박스 방법이 더 나은 성능을 보이지만, 그 격차는 AUROC에서 0.522에서 0.605로 좁습니다.
  * 어떤 단일 기술도 모든 상황에서 다른 기술보다 지속적으로 뛰어나지 않으며, 조사된 모든 방법은 전문 지식이 필요한 어려운 작업에서 여전히 어려움을 겪습니다.
* **강력한 베이스라인 제공**: 본 연구는 블랙박스 LLM에서 확신도를 추출하기 위한 강력한 베이스라인과 통찰력을 제공합니다.
* **코드 공개**: 관련 코드는 공개적으로 제공됩니다.

## 📎 Related Works

* **LLM의 확신도 추출**: Lin et al. (2022)의 언어화된 확신도(verbalized confidence) 개념 도입(주로 미세 조정에 초점), Mielke et al. (2022)의 외부 보정기(내부 모델 표현 의존), Zhou et al. (2023)의 프롬프트 내 확신도 영향 분석(직접적인 확신도 점수 제공 아님). Tian et al. (2023)의 동시 연구(주로 프롬프팅 전략에 초점).
* **보정(Calibration)**: Guo et al. (2017), Minderer et al. (2021), Xiong et al. (2023) 등의 현대 신경망 보정에 대한 연구. LLM 관련으로는 Jiang et al. (2021), Chen et al. (2022) 등이 LLM의 보정 문제를 다루었으나, 주로 로짓 기반 확률에 초점을 맞춰 폐쇄형 LLM에는 적용하기 어려움.
* **불확실성 추정**: MCDropout (Gal & Ghahramani, 2016) 및 Deep Ensemble (Lakshminarayanan et al., 2017)과 같은 앙상블 기반의 화이트박스 접근 방식에서 영감을 받았습니다.

## 🛠️ Methodology

저자들은 LLM 확신도 추출을 위한 블랙박스 프레임워크를 제안하며, 이는 세 가지 핵심 구성 요소로 이루어집니다.

1. **프롬프팅 전략 (Prompting Strategy)**: 모델이 확신도를 언어로 표현하도록 유도하는 방법.
    * **Vanilla**: 질문에 대한 답변과 확신도를 직접 요청.
    * **CoT (Chain-of-Thought)**: 단계별 추론 과정을 통해 답변과 확신도를 제공하도록 유도.
    * **Self-Probing**: 모델이 자체 생성한 답변의 정확성을 평가하도록 질문하여 확신도를 얻음. (두 개의 독립적인 채팅 세션 사용)
    * **Multi-Step**: 추론 과정을 여러 단계로 나누어 각 단계의 확신도를 추출하고, 이를 취합하여 최종 확신도를 도출합니다.
        $$C_{\text{multi-step}} = \prod_{i=1}^{n} C_i$$
        (단, $n$은 추론 단계의 총 수, $C_i$는 각 단계의 확신도)
    * **Top-K**: `K`개의 가장 좋은 추측과 각 추측의 확률을 요청하여 여러 가능한 답의 존재를 모델에 인지시킵니다.

2. **샘플링 전략 (Sampling Strategy)**: 동일한 질문에 대해 모델로부터 여러 응답을 생성하는 방법.
    * **Self-random**: 모델의 내재된 무작위성(예: 온도 매개변수 조정)을 활용하여 동일한 프롬프트를 여러 번 입력.
    * **Prompting**: 질문을 다르게 의역하여 여러 응답 생성.
    * **Misleading**: 모델에 오해의 소지가 있는 단서를 제공하여 불확실성 평가.

3. **취합 전략 (Aggregation Strategy)**: 여러 응답을 통합하여 최종 답변과 관련 확신도를 계산하는 방법.
    * **Consistency**: `M`개의 후보 응답 간의 일치도를 확신도로 사용합니다.
        $$C_{\text{consistency}} = \frac{1}{M} \sum_{i=1}^{M} \mathbb{I}{\{\hat{Y}_i = \tilde{Y}\}}$$
        (단, $\hat{Y}_i$는 $i$번째 후보 응답, $\tilde{Y}$는 원본 답변, $\mathbb{I}{\{\cdot\}}$는 지시 함수)
    * **Avg-Conf**: 언어화된 확신도와 응답 간의 일관성을 결합합니다.
        $$C_{\text{conf}} = \frac{\sum_{i=1}^{M} \mathbb{I}{\{\hat{Y}_i = \tilde{Y}\}} \times C_i}{\sum_{i=1}^{M} C_i}$$
        (단, $C_i$는 $i$번째 응답의 언어화된 확신도)
    * **Pair-Rank**: Top-K 프롬프트에 특화된 전략으로, `K`개의 추측 순위 정보를 활용하여 범주형 분포 $P$를 추론합니다.
        $$
        \min_{P} - \sum_{i=1}^{N} \sum_{S_u \in A} \sum_{S_v \in A} \mathbb{I}{\{S_u^{(i)} \succ S_v\}} \cdot \log \frac{P(S_u)}{P(S_u) + P(S_v)} \quad \text{s.t.} \sum_{S_u \in A} P(S_u) = 1
        $$

**실험 설정**:

* **데이터셋**: 상식 추론 (Sports Understanding, StrategyQA), 산술 추론 (GSM8K, SVAMP), 상징 추론 (Date Understanding, Object Counting), 전문 지식 (Professional Law), 윤리 지식 (Business Ethics) 등 5가지 유형의 8개 데이터셋.
* **모델**: Vicuna 13B, GPT-3 175B, GPT-3.5-turbo, GPT-4, LLaMA 2 70B.
* **평가 지표**: 보정(Calibration)을 위한 ECE (Expected Calibration Error), 실패 예측(Failure Prediction)을 위한 AUROC (Area Under the Receiver Operating Characteristic Curve), AUPRC-Positive (PR-P), AUPRC-Negative (PR-N).

## 📊 Results

* **과신 경향**: LLM은 확신도를 언어로 표현할 때 심각한 과신 경향을 보이며, 확신도 값은 주로 80%~100% 범위 내에서 5의 배수로 나타나 인간의 표현 패턴과 유사합니다. 실제 정확도는 이에 훨씬 못 미쳐 신뢰성이 낮습니다.
* **모델 역량 스케일링**: GPT-3에서 GPT-4로 모델 역량이 확장될수록 ECE는 감소하고 AUROC는 증가하는 경향을 보이며 성능이 향상되지만, 여전히 최적의 성능에는 미치지 못합니다. GPT-4는 낮은 ECE를 보이지만, AUROC는 평균 62.7%로 무작위 추측(50%)에 가까워 실패 예측 능력이 제한적입니다.
* **프롬프팅 전략의 효과**: 인간에게서 영감을 받은 프롬프팅 전략은 모델의 정확도와 보정 성능을 향상시켜 과신을 부분적으로 완화합니다. 그러나 GPT-4와 같은 고성능 모델에서는 효과가 점차 줄어듭니다. 모든 데이터셋과 모델에서 일관되게 가장 좋은 단일 프롬프트 전략은 없으며, GPT-4에서는 Self-Probing이, GPT-3.5에서는 Top-K가 좋은 성능을 보입니다. ECE는 적절한 프롬프팅으로 효과적으로 줄일 수 있지만, 실패 예측은 여전히 어려운 과제입니다 (AUROC는 0.5에 근접).
* **다중 응답을 통한 예측 실패 개선**: `M=5`개의 샘플링된 응답과 일관성(Consistency) 취합 전략은 언어화된 확신도(M=1)에 비해 실패 예측 및 보정 성능을 일관되게 개선하며, 특히 산술 과제(예: GSM8K에서 AUROC가 54.8%에서 92.7%로 향상)에서 큰 폭의 개선을 보였습니다. 샘플링된 응답 수가 증가함에 따라 성능은 크게 향상된 후 수렴합니다.
* **취합 시 언어화된 확신도 활용**: 취합 전략에 언어화된 확신도를 도입(Avg-Conf, Pair-Rank)하는 것이 일관성만 활용하는 것보다 더 효과적입니다. Pair-Rank는 보정(최저 ECE 0.028)에서 뛰어난 반면, Avg-Conf는 실패 예측에서 더 우수했습니다.
* **화이트박스 vs. 블랙박스**: 화이트박스 방법이 전반적으로 더 나은 성능을 보이지만, 그 격차는 AUROC에서 0.522 대 0.605로 상대적으로 좁으며, 심지어 최상의 화이트박스 방법도 만족스러운 결과를 얻지 못했습니다.

## 🧠 Insights & Discussion

* **의미 및 시사점**: LLM의 과신 경향은 안전한 배포에 잠재적 위험을 야기합니다. 현재의 블랙박스 확신도 추출 방법은 이러한 과신을 어느 정도 완화하는 데 도움이 되지만, 특히 전문 지식이 요구되는 과제에서는 여전히 한계가 있습니다. 이는 LLM의 불확실성 추정 분야에서 지속적인 연구가 필요함을 강조합니다.
* **현재 알고리즘의 만족도**: 최적의 알고리즘이 ECE를 0.028까지 낮출 수 있지만, 전문 법률과 같이 전문 지식이 필요한 과제에서는 오답 예측에 여전히 어려움을 겪습니다.
* **실무자를 위한 권장 사항**: 효율성, 단순성, 효과성의 균형을 고려하여, **Top-K 프롬프트 + Self-Random 샘플링 + Avg-Conf 또는 Pair-Rank 취합** 조합을 권장합니다. Pair-Rank는 정확한 확신도 값을 요구하는 작업에, Avg-Conf는 실패 예측과 관련된 작업에 더 적합합니다. 앙상블 방법은 단일 Top-K 프롬프트의 정확도 한계를 보완할 수 있습니다.
* **전략 작동/미작동 논의**:
  * **샘플링**: 더 많은 쿼리 샘플링은 모델의 내부 분포를 직접적으로 근사하여 산술 과제 등에서 효과적입니다. 모델의 보정 부족이나 계산 제약으로 인한 부정확한 추정이 단점으로 작용할 수 있습니다.
  * **취합**: 답변과 언어화된 확신도를 모두 기반으로 하는 취합(Avg-Conf, Pair-Rank)은 답변만 기반으로 하는 취합보다 더 미세한 확신도 값을 제공하여 보정 성능을 향상시킵니다.
  * **언어화된 확신도**: 인간의 불확실성 표현이 본질적으로 부정확할 수 있다는 점이 훈련 데이터에 반영되어, LLM의 언어화된 확신도 성능이 아직 최적화되지 못했을 수 있습니다.
  * **프롬프팅 전략**: CoT, Multi-Step, Top-K 등의 프롬프트는 주로 모델의 정확도를 향상시켜 평균 확신도와 실제 정확도 간의 간극을 줄임으로써 ECE를 감소시킵니다. 이는 모델이 정답과 오답을 더 잘 구별하는 능력 자체를 크게 향상시키는 것과는 다를 수 있습니다.
* **제한 사항 및 향후 연구**:
    1. **데이터셋 범위**: 본 연구는 주로 고정 형식 및 자유 형식의 질의응답(QA) 작업에 중점을 두었으며, 요약 및 개방형 QA와 같은 작업은 향후 연구 과제로 남겨두었습니다.
    2. **블랙박스 설정**: 블랙박스 접근 방식은 여전히 최적의 성능을 달성하지 못하며, 더 풍부한 정보에 접근할 수 있는 화이트박스 설정이 더 유망할 수 있습니다. 제한된 화이트박스 접근 데이터(예: GPT-3에서 제공하는 로짓)와 블랙박스 방법을 통합하는 것이 유망한 방향이 될 수 있습니다.

## 📌 TL;DR

LLM이 답변에 대한 불확실성을 정확히 표현하는 블랙박스 방법의 부재를 해결하기 위해, 본 연구는 프롬프팅, 샘플링, 취합 전략을 포함하는 체계적인 블랙박스 프레임워크를 제안하고 다양한 LLM과 데이터셋에서 평가했습니다. 주요 발견으로, LLM은 확신도를 표현할 때 **과신 경향**을 보이지만, 제안된 인간에게서 영감을 받은 프롬프트와 다중 응답 샘플링 및 취합 전략이 이러한 과신을 완화하고 예측 실패 성능을 개선하는 데 효과적임을 확인했습니다. 화이트박스 방법이 더 나은 성능을 보이지만 그 격차는 좁으며, 전문 지식 과제에서는 모든 방법이 여전히 어려움을 겪는다는 한계를 드러냈습니다.
