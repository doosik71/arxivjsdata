# PB-LLM: PARTIALLY BINARIZED LARGE LANGUAGE MODELS

Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong

## 🧩 Problem to Solve

거대 언어 모델(LLM)은 뛰어난 성능을 보이지만, 그 방대한 크기로 인해 메모리 제약이 있는 장치에 배포하기 어렵습니다. 모델 압축을 위한 양자화 기법이 널리 사용되지만, 기존 최첨단 LLM 양자화 방법들은 4비트 이하로 내려갈 경우 품질 저하가 심합니다. 특히, 가중치를 1비트로 압축하는 극한의 양자화 방식인 네트워크 이진화(binarization)는 LLM에 직접 적용 시 모델 성능이 무작위 추측보다도 낮아지는 "붕괴" 현상이 발생합니다. 따라서 이진화된 LLM의 언어 추론 능력을 유지하면서 극도로 낮은 비트의 양자화를 달성하는 것이 주요 도전 과제입니다.

## ✨ Key Contributions

* **부분 이진화 LLM(PB-LLM) 제안:** LLM의 언어 추론 능력을 유지하면서도 극도로 낮은 비트의 양자화(부분 이진화)를 달성하는 새로운 접근 방식을 제시합니다.
* **기존 이진화 알고리즘의 비효율성 규명:** 기존 이진화 알고리즘이 LLM에 비효율적임을 밝히고, 저비트 양자화에서 **중요 가중치(salient weights)의 필수적인 역할**을 강조합니다.
* **중요 가중치 부분 보존 전략:** 소수의 중요 가중치는 더 높은 비트 정밀도로 저장하고 나머지 가중치는 이진화하는 **부분 이진화(partially-binarization)** 개념을 도입합니다.
* **PTQ(Post-Training Quantization) 관점에서의 성능 회복:** GPTQ에서 영감을 받아 Hessian 행렬 가이드를 통해 이진화된 가중치 행렬을 재구성하여 저비트에서 PB-LLM의 추론 능력을 성공적으로 복원합니다 (PB-GPTQ).
* **QAT(Quantization-Aware Training) 관점에서의 성능 회복:**
  * 학습 중 중요 가중치를 고정(freeze)하여 효율적인 훈련을 가능하게 합니다.
  * 양자화 오차 최소화를 위한 최적 스케일링 인자를 분석적으로 도출하고, 이 스케일링 전략을 잔여 이진화 가중치에 적용하는 메커니즘을 제안합니다.
* 저비트 양자화 LLM의 성능 향상에 크게 기여하고, LLM을 위한 네트워크 이진화 분야에 상당한 진전을 이룹니다.

## 📎 Related Works

* **네트워크 이진화:** BNN (Hubara et al., 2016), XNOR (Rastegari et al., 2016), Bi-Real (Liu et al., 2020b), ReCU (Xu et al., 2021a), FDA (Xu et al., 2021b) 등 컴퓨터 비전 분야에서 성공적이었던 다양한 1비트 가중치/활성화 양자화 기법들이 있습니다. 그러나 이 방법들은 주로 소규모 모델(BERT-base 등)에 적용되었으며, LLM에는 제한적이었습니다.
* **거대 언어 모델 양자화:**
  * **QAT:** LLM-QAT (Liu et al., 2023a), QLORA (Dettmers et al., 2023a) 등이 LLM의 훈련 데이터 문제를 해결하거나 효율적인 미세 조정을 가능하게 합니다.
  * **PTQ:** GPTQ (Frantar et al., 2022), QuIP (Chee et al., 2023)는 높은 압축률을 달성하는 계층별 양자화 기법입니다. SqueezeLLM (Kim et al., 2023), SpQR (Dettmers et al., 2023b), AWQ (Lin et al., 2023) 등은 특히 큰 양자화 오차를 유발하는 가중치(outliers)를 더 높은 정밀도로 저장하여 정확도 저하를 완화하는 데 중점을 둡니다. SmoothQuant (Xiao et al., 2022) 등은 활성화 양자화 문제를 가중치로 전환합니다.
본 논문은 이러한 기존 연구들을 바탕으로 PTQ 및 QAT 프레임워크 내에서 **가중치 이진화**에 초점을 맞춥니다.

## 🛠️ Methodology

1. **기존 이진화 방법의 한계 분석:** BNN, XNOR, Bi-Real, ReCU, FDA 등 5가지 대표적인 이진화 방법을 OPT-1.3B LLM에 적용했을 때, 무작위 추측보다도 낮은 성능을 보이며 기존 방법들이 LLM 이진화에 부적합함을 확인했습니다 (Fig. 2 참조).
2. **부분 이진화 가중치 행렬 구성:**
    * **중요 가중치(Salient Weight) 정의:**
        * **기준:** Magnitude(크기) 기반 방식이 Hessian 기반 방식과 유사한 효과를 보이며 더 간단하여 중요 가중치 식별 기준으로 채택되었습니다.
        * **세분성(Granularity):** 중요 가중치가 행렬 내에 무작위로 분산되어 있으므로 (Fig. 3), 열(column) 단위가 아닌 **요소(element) 단위**로 선택하여 높은 비트로 유지합니다.
    * **혼합 정밀도 저장:** 전체 가중치 중 소수의 비율($r_{binary}$)을 차지하는 중요 가중치는 높은 비트($N_{salient-bit}$, 예: 8비트)로 저장하고, 나머지 ($1 - r_{binary}$) 가중치는 1비트로 이진화합니다. 추가적으로 인덱스 저장 비용을 1비트로 가정하여 총 비트 수($N_{bit}$)를 계산하며, 예를 들어 10%의 중요 가중치를 8비트로 저장하고 90%를 이진화하면 최대 2.7비트가 됩니다 (Fig. 4).
3. **PTQ(Post-Training Quantization) - PB-GPTQ:**
    * GPTQ의 아이디어를 부분 이진화 설정으로 확장합니다.
    * 가중치 행렬 $W$를 중요 가중치 $W_{sal}$와 이진화될 비중요 가중치 $W_{unsal}$로 나눕니다.
    * GPTQ와 유사하게, 계층별 양자화 오차를 최소화하기 위해 Hessian 행렬 가이드에 따라 비중요 가중치를 이진화하고 중요 가중치를 높은 비트로 양자화하는 과정을 반복적으로 수행하며, 나머지 가중치에 보정(compensation)을 적용합니다.
    * 비대칭 채널별(per-channel) 양자화를 사용하며, 비중요 가중치에는 최적 스케일링 인자(3.4.2절에서 정의)를, 중요 가중치에는 MinMax 스케일링을 적용합니다.
4. **QAT(Quantization-Aware Training) - PB-LLM:**
    * **중요 가중치 고정(Salient Weights Frozen):** 양자화 인식 학습(QAT) 시작 시, 미리 훈련된 모델의 중요 가중치(예: 상위 2%의 magnitude를 가진 가중치)를 식별하여 원래의 고정밀 상태로 유지하고, 학습 과정 내내 이들을 업데이트하지 않고 고정합니다. 이는 학습 효율성을 높이고 빠른 수렴을 돕습니다 (Fig. 5).
    * **이진화 가중치를 위한 최적 스케일링 인자:**
        * 이진화 가중치에 대한 양자화 오차를 최소화하기 위해 열(column) 단위의 최적 스케일링 인자 $\alpha^*$를 **분석적으로 도출**합니다.
        * $\alpha^* = \frac{||w_{F}||_{1}}{n_{w_{F}}}$ ($w_{F}$는 원본 Full-Precision 가중치, $n_{w_{F}}$는 $w_{F}$의 요소 수)로 정의됩니다 (Eq. 9).
    * 이 두 가지 메커니즘(중요 가중치 고정 및 최적 스케일링 인자 적용)만으로도 재훈련 없이 LLM의 언어 능력을 어느 정도 유지할 수 있습니다 (Fig. 6).
    * 학습은 AdamW 옵티마이저를 사용하며, 10K iteration 동안 미세 조정합니다.

## 📊 Results

* **기존 이진화 방법의 실패:** OPT-1.3B에 대한 실험에서 BNN, XNOR 등 기존 이진화 알고리즘들은 무작위 추측보다 낮은 성능을 보여 LLM 이진화에 부적합함을 재확인했습니다 (Fig. 2, Table 4).
* **PB-GPTQ (PTQ) 성능:** RTN(Round-To-Nearest) 방식보다 Perplexity(PPL)가 현저히 낮아 더 우수한 성능을 보였습니다. 특히 중요 가중치 비율이 30% 이상일 때 LLaMA-7B에서 좋은 성능을 보였으나, 10% 이하로 감소하면 성능이 크게 저하되었습니다 (Table 1, Table 3). Hessian 기반 PB-GPTQ가 Magnitude 기반보다 일반적으로 더 나은 결과를 보였습니다.
* **PB-LLM (QAT) 성능:**
  * **빠른 수렴:** 중요 가중치를 고정하는 전략이 학습 손실을 더 빠르게 수렴하게 함으로써 학습 효율성을 입증했습니다 (Fig. 5).
  * **재훈련 없는 성능 유지:** 중요 가중치 고정 및 최적 스케일링 메커니즘만 적용해도 재훈련 없이 OPT-1.3B의 언어 능력을 일부 유지함을 확인했습니다 (Fig. 6).
  * **최고 수준의 QAT 성능:** LLaMA-7B 모델에서 30%의 중요 가중치를 보존한 PB-LLM은 기존 LLM-QAT (4비트 설정)와 유사하거나 더 나은 추론 성능(BoolQ, PIQA 등 7개 Common Sense Reasoning 태스크 평균 정확도 66.9% vs 66.6%)을 보였습니다 (Table 2).
  * **극저비트 조건에서의 우수성:** 5%의 중요 가중치를 보존한 경우와 같은 극도로 낮은 비트 조건에서도 PB-LLM은 기존 LLM QAT 방법들이 완전히 실패하는 반면, LLM의 추론 능력을 성공적으로 복원했습니다 (Fig. 7 i-p).
  * **뛰어난 학습 효율:** PB-LLM은 불과 1-10K번의 학습 반복(iteration)으로 양자화된 LLM의 성능을 복구하여, 100K번의 반복이 필요한 LLM-QAT에 비해 학습 효율성이 크게 향상됨을 입증했습니다 (Fig. 7, Sec. 3.4).

## 🧠 Insights & Discussion

* LLM에 대한 순수한 이진화는 성능 붕괴로 이어지므로, **부분 이진화**와 같은 보다 정교한 접근 방식이 필수적입니다.
* 가중치 중 소수의 **중요 가중치를 식별하고 이들을 높은 정밀도로 보존**하는 것이 극심한 양자화 조건에서 LLM의 언어 추론 능력을 유지하는 데 결정적인 역할을 합니다.
* 제안된 중요 가중치 고정 및 이진화 가중치에 대한 분석적으로 도출된 최적 스케일링 인자 전략은 PTQ와 QAT 모두에서 이진화된 LLM의 성능을 크게 향상시킵니다.
* 특히 QAT 프레임워크에서 PB-LLM은 놀라운 학습 효율성을 보여주며, 기존 LLM QAT 방식보다 훨씬 적은 컴퓨팅 자원과 시간으로 높은 성능을 달성합니다.
* 이 연구는 LLM 양자화의 한계를 극도로 낮은 비트까지 확장하는 데 중요한 진전을 이루었으며, 최종적으로 완전한 1비트 이진화 LLM 개발의 가능성을 시사합니다.
* 한계점으로는 계산 자원 제약으로 OPT-1.3B에 대한 탐색이 주를 이뤘고, 인덱스 저장 비용 최적화에 대한 상세한 논의는 다루지 않았다는 점이 있습니다.

## 📌 TL;DR

LLM 메모리 문제를 해결하기 위해 극심한 양자화(이진화)를 시도했으나 기존 방법들은 LLM을 붕괴시켰습니다. 본 논문은 소수의 **중요 가중치(salient weights)**는 높은 비트로 유지하고 나머지는 이진화하는 **부분 이진화 LLM (PB-LLM)**을 제안합니다. PTQ(PB-GPTQ)에서는 Hessian 가이드를, QAT에서는 중요 가중치 고정 및 최적 스케일링 인자 분석적 도출을 통해 LLM의 추론 능력을 성공적으로 회복했습니다. PB-LLM은 기존 4비트 LLM 양자화 모델과 유사하거나 더 나은 성능을 훨씬 적은 학습 반복(1-10K vs 100K)으로 달성하여, 극도로 낮은 비트의 LLM 개발에 중요한 진전을 이뤘습니다.
