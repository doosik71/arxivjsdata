# LLM 후학습: 추론에 대한 심층 분석 (LLM Post-Training: A Deep Dive into Reasoning Large Language Models)

Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip H.S. Torr, Fahad Shahbaz Khan, Salman Khan

## 🧩 해결하려는 문제

사전 학습된 대규모 언어 모델(LLM)은 뛰어난 능력을 보여주지만, 환각(hallucination), 논리적 비일관성, 사용자 의도 및 윤리적 기준과의 불일치와 같은 심각한 한계를 가지고 있습니다. LLM의 추론은 인간과 같은 논리적 추론보다는 데이터의 통계적 패턴에 기반하기 때문에, 겉으로는 일관된 응답을 생성하더라도 단순한 논리적 과제에서 어려움을 겪을 수 있습니다. 기존 연구들은 주로 강화 학습(RL)의 특정 측면(예: RLHF)에 초점을 맞추어 후학습 기법의 통합적인 관점을 간과했습니다. 따라서 사전 학습 이후 모델의 지식을 정제하고, 추론 능력을 향상하며, 사실적 정확도를 높이고, 사용자 의도에 효과적으로 부합하도록 하는 전문화된 후학습 전략이 필요합니다.

## ✨ 주요 기여

* **종합적이고 체계적인 후학습 방법론 검토**: LLM 모델 최적화의 필수 구성 요소로서 미세 조정(fine-tuning), 강화 학습(RL), 스케일링(scaling)을 포괄하는 후학습 방법론에 대한 종합적인 검토를 제공합니다.
* **후학습 기술의 구조화된 분류 체계 제시**: 각 기술의 역할과 상호 연결성을 명확히 하고, 실제 배포를 위한 LLM 최적화의 당면 과제와 미래 연구 방향에 대한 통찰력을 제시합니다.
* **실용적인 가이드 제공**: 후학습의 효과를 평가하는 데 필수적인 핵심 벤치마크, 데이터셋, 평가 지표를 소개하여 실제 응용을 위한 구조화된 프레임워크를 제시합니다.

## 📎 관련 연구

기존 LLM 및 강화 학습에 대한 연구들은 주로 RLHF(Reinforcement Learning from Human Feedback), RLAIF(Reinforcement Learning from AI Feedback), DPO(Direct Preference Optimization)와 같은 특정 강화 학습 기술에 초점을 맞추었습니다. 이들은 미세 조정, 추론 시 스케일링, 그리고 실제 적용에 필요한 중요한 벤치마크를 간과하는 경향이 있었습니다. 또한, LLM 추론에 대한 연구들은 학습-추론 기술을 논의하지만, 미세 조정, 강화 학습, 스케일링을 결합하는 데 대한 구조화된 지침이 부족했습니다. 본 연구는 이러한 간극을 메우고, 미세 조정, 강화 학습, 스케일링을 상호 연결된 최적화 전략으로 체계적으로 다루어 종합적인 관점을 제공합니다.

## 🛠️ 방법론

이 논문은 LLM 후학습을 세 가지 주요 범주로 나누어 설명합니다: 미세 조정(Fine-tuning), 강화 학습(Reinforcement Learning), 추론 시 스케일링(Test-time Scaling).

### 1. 미세 조정 (Fine-tuning)

사전 학습된 LLM을 특정 작업이나 도메인에 맞게 조정합니다.

* **명령 미세 조정 (Instruction Finetuning)**: 사용자 지침을 따르도록 모델을 훈련합니다.
* **대화형 미세 조정 (Dialogue Finetuning)**: 다중 턴 대화를 더 잘 처리하도록 합니다.
* **사고의 사슬 추론 미세 조정 (CoT Reasoning Finetuning)**: 단계별 추론 과정을 생성하도록 학습하여 복잡한 작업의 정확도와 해석 가능성을 높입니다.
* **도메인 특화 미세 조정 (Domain-Specific Finetuning)**: 특정 분야(예: 의학, 금융)에 모델을 특화시킵니다.
* **지식 증류 기반 미세 조정 (Distillation-Based Finetuning)**: 대규모 '교사' 모델의 지식을 소규모 '학생' 모델로 전달하여 효율성을 높입니다.
* **선호도 및 정렬 SFT (Preference and Alignment SFT)**: RLHF와 같은 정렬 과정의 초기 단계로, 인간의 선호도 데이터를 사용하여 모델을 훈련합니다.
* **효율적인 미세 조정 (Efficient Finetuning)**: LoRA(Low-Rank Adaptation), QLoRA(Quantized LoRA)와 같은 PEFT(Parameter-Efficient Fine-Tuning) 기술을 사용하여 계산 및 메모리 비용을 절감합니다.

### 2. 강화 학습 (Reinforced LLMs)

모델 행동을 정제하고 인간 의도에 맞게 출력을 정렬하여 LLM의 한계를 완화합니다.

* **기본 3단계 프로세스**:
    1. **지도 미세 조정(SFT)**: 고품질 데이터셋으로 사전 학습된 모델을 정제합니다.
    2. **보상 모델(RM) 훈련**: 미세 조정된 모델의 출력에 대한 인간 선호도 라벨을 수집하고, 이를 기반으로 스칼라 보상 함수를 학습합니다.
    3. **강화 학습 미세 조정**: 정책 경사(Policy Gradient) 알고리즘(예: PPO)을 사용하여 보상 모델의 출력을 최대화하도록 LLM을 최적화합니다.
* **보상 모델링 (Reward Modeling)**:
  * **목표**: 쿼리($x$)와 응답($y$) 쌍을 스칼라 점수 $R_\theta(x,y)$로 매핑합니다.
  * **학습**: 인간 선호도 라벨을 사용하여 순위 기반 손실(예: 쌍대 선호도에 대한 Bradley-Terry 모델, 순위에 대한 Plackett-Luce 모델)을 최소화합니다.
  * **유형**:
    * **명시적 보상 모델링**: 사전 정의된 규칙, 휴리스틱 또는 인간 주석을 기반으로 보상 함수를 직접 정의합니다.
    * **암묵적 보상 모델링**: 관찰된 행동, 상호 작용 또는 선호 신호로부터 보상을 간접적으로 추론합니다.
  * **보상 기능의 초점**:
    * **결과 기반 보상 모델링 (Outcome Reward Modeling)**: 최종 결과(예: 최종 답변의 정확성)를 측정합니다.
    * **과정 기반 보상 모델링 (Process Reward Modeling)**: 중간 추론 단계에 피드백을 할당하여 일관되고 논리적인 사고의 흐름을 장려합니다.
* **정책 최적화 (Policy Optimization)**: 보상 모델 $R_\theta(x,y)$를 사용하여 정책 $\pi_\phi$를 최적화합니다.
  * **PPO (Proximal Policy Optimization)**: 확률 비율을 클리핑하는 목적 함수를 통해 탐색과 안정성 균형을 맞춥니다. KL 발산 페널티 $J(\pi) = E[r(x,y) - \beta KL(\pi(\cdot|x) || \rho(\cdot|x))]$를 사용하여 과최적화를 방지합니다.
  * **RLHF (Reinforcement Learning from Human Feedback)**: 인간의 선호도 신호를 사용하여 LLM을 정제합니다. SFT, 보상 모델 훈련, PPO를 포함합니다.
  * **RLAIF (Reinforcement Learning from AI Feedback)**: 인간 주석 대신 AI가 생성한 피드백을 사용합니다.
  * **TRPO (Trust Region Policy Optimization)**: KL 발산 제약을 통해 정책 업데이트가 신뢰 영역 내에 유지되도록 합니다.
  * **DPO (Direct Preference Optimization)**: 별도의 보상 모델 없이 인간 선호 신호를 모델 훈련 목표에 직접 통합합니다. $L_{DPO}(\theta) = E[\sigma(\beta\log\frac{\pi_\theta(y^+|x)}{\pi_{ref}(y^+|x)} - \beta\log\frac{\pi_\theta(y^-|x)}{\pi_{ref}(y^-|x)})]$
  * **OREO (Offline Reasoning Optimization)**: 오프라인 강화 학습으로, 희소 보상과 Bellman 방정식을 사용하여 정책 모델 $\pi_\theta$와 가치 함수 $V_\phi$를 공동으로 훈련하여 다단계 추론을 향상합니다.
  * **GRPO (Group Relative Policy Optimization)**: PPO 프레임워크를 단순화하여 별도의 가치 함수 없이 여러 샘플링된 출력의 평균 보상에서 기준선을 추정합니다.
  * **ORPO (Odds Ratio Preference Optimization)**: 별도의 보상 모델 없이 쌍대 선호도에서 정책을 직접 최적화하여 선호되는 응답의 가능성을 높입니다.
* **순수 RL 기반 LLM 정제 (DeepSeek-R1 사례)**:
    1. **콜드 스타트 RL 단계**: 소량의 큐레이션된 데이터로 초기 모델을 미세 조정한 후 RL을 수행합니다.
    2. **거부 샘플링 및 미세 조정**: 고품질 응답을 생성하고 필터링한 다음 추가 데이터셋과 혼합하여 SFT용 대규모 코퍼스를 생성합니다.
    3. **추론 지향 RL**: GRPO를 활용하여 그룹 기반 샘플링과 보상 계산을 통해 고품질의 구조화된 출력을 우선시합니다.
    4. **인간 정렬을 위한 2차 RL 단계**: 추가 보상 신호를 도입하여 모델을 더 넓은 인간 선호도에 맞게 정렬합니다.
    5. **소규모 모델을 위한 지식 증류**: 주요 모델의 정제된 기능을 소규모 아키텍처로 전송합니다.

### 3. 추론 시 스케일링 (Test-time Scaling, TTS)

모델 업데이트 없이 추론 과정에서 추론 성능과 효율성을 향상시킵니다.

* **빔 검색 (Beam Search)**: 각 단계에서 확률이 가장 높은 $N$개의 부분 시퀀스(빔)를 유지하여 근사 최적 시퀀스를 찾습니다.
* **Best-of-N 검색 (Rejection Sampling)**: $N$개의 후보 출력을 생성하고 선택된 기준(예: 보상 모델)에 따라 가장 좋은 하나를 선택합니다.
* **계산 최적 스케일링 (Compute-Optimal Scaling, COS)**: 문제 난이도에 따라 계산 리소스를 동적으로 할당합니다. 쉬운 프롬프트는 순차적 정제를, 어려운 프롬프트는 병렬 샘플링이나 빔 검색을 사용합니다.
* **사고의 사슬 프롬프팅 (Chain-of-Thought Prompting, CoT)**: LLM이 최종 답변에 직접 도달하기보다는 중간 추론 단계를 생성하도록 유도합니다.
* **자기 일관성 디코딩 (Self-Consistency Decoding)**: 모델에서 다양한 추론 체인을 샘플링하고, 이러한 다중 추론 경로에서 가장 일관된 답변을 선택합니다.
* **사고의 나무 (Tree-of-Thoughts, ToT)**: 단일 선형 체인 대신 여러 가능한 사고 시퀀스로 분기하여 추론을 트리 검색으로 공식화합니다.
* **사고의 그래프 (Graph of Thoughts, GoT)**: ToT를 확장하여 그래프 기반 구조를 통해 보다 유연하고 효율적인 추론 프로세스를 가능하게 합니다.
* **확신도 기반 샘플링 (Confidence-based Sampling)**: 모델 자체의 확신도를 기반으로 여러 후보 솔루션이나 추론 경로의 우선순위를 정하거나 선택합니다.
* **검증기 기반 검색 (Search Against Verifiers)**: 여러 후보 응답을 생성하고 자동화된 검증 시스템(결과 보상 모델/과정 보상 모델)을 사용하여 최상의 응답을 선택합니다.
* **자기 개선을 통한 정제 (Self-Improvement via Refinements)**: LLM이 자기 평가 및 반복적인 수정을 통해 출력을 개선하는 능력입니다 (Self-Refine, Self-Polish).
* **몬테카를로 트리 검색 (Monte Carlo Tree Search, MCTS)**: 몬테카를로 시뮬레이션을 통해 의사결정 트리를 구축하여 최적의 동작 시퀀스를 찾습니다.
* **행동-사고의 사슬 추론 (Chain-of-Action-Thought reasoning, Satori)**: 2단계 훈련 패러다임(형식 튜닝, RL을 통한 자기 개선)을 통해 LLM 추론을 향상합니다.
* **사전 학습 vs. 추론 시 스케일링**: 사전 학습은 모델 매개변수나 훈련 데이터를 스케일링하는 반면, TTS는 추론 시 계산을 최적화하여 기본 모델 수정 없이 성능을 향상합니다. 이 둘은 상호 보완적입니다.

## 📊 결과

* LLM은 코드 및 수학 콘텐츠를 포함한 다양한 데이터로 학습될 때 **새로운 추론 능력**을 보여줍니다.
* RLAIF는 RLHF에 비해 **확장 가능하고 효율적인 대안**으로, 성능과 정렬을 유지하면서 데이터 수집 비용과 시간을 절감합니다.
* GRPO는 PPO에 비해 **메모리 사용량을 크게 줄이고 정책 학습을 안정화**하며, 고전적인 액터-크리틱 프레임워크의 효율적인 대안으로 작용합니다.
* 지식 증류는 소규모 모델이 고급 추론 능력을 상속받아 **대규모 RL 훈련 비용 없이 경쟁력 있는 성능**을 달성하도록 합니다.
* PEFT 기술은 **메모리 사용량을 크게 줄이고** 제약된 환경에서 LLM 훈련을 더 실용적으로 만듭니다.
* Compute-Optimal Scaling (COS)는 기존 Best-of-N 샘플링에 비해 **동일한 성능을 유지하면서 계산 사용량을 4배 낮춥니다**.
* CoT 프롬프팅은 수학 문제, 논리 퍼즐, 다단계 QA와 같은 복잡한 작업에서 **성능을 크게 향상**시킵니다.
* 자기 일관성 디코딩은 산술 및 추론 문제에서 **정확도를 크게 향상**시킬 수 있습니다.
* ToT는 솔루션 공간을 체계적으로 탐색하여 단순 CoT 방법보다 **복잡한 문제에서 뛰어난 성능**을 보입니다. GoT는 ToT보다 문제 해결 효율성과 적응성을 향상시킵니다.
* 자기 개선 방법론은 **다양한 응용 분야에서 모델의 일관성과 정확도를 높입니다**.
* MCTS 기반 추론은 단순 샘플링이나 탐욕적 방법이 놓치는 문제를 **해결할 수 있음**을 보여줍니다.
* 추론 시 스케일링(TTS)은 **쉬운~중간 난이도 작업에서 14배 더 큰 모델과 유사한 결과**를 달성하면서 추론 비용을 4배 절감할 수 있습니다.

## 🧠 통찰력 및 논의

LLM 후학습은 모델의 기본 능력과 실제적이고 정렬된 견고한 응용 프로그램 간의 격차를 해소하는 데 필수적입니다. 이 분야는 더 상호 작용적이고, 적응적이며, 효율적인 방법으로 발전하고 있습니다.

* **한계**:
  * **미세 조정**: **치명적인 망각(catastrophic forgetting)**, 개인 정보 유출 위험(데이터 기억), 데이터 유출 등이 여전히 해결해야 할 과제입니다.
  * **강화 학습**: **보상 해킹(reward hacking)**, 보상 모델 드리프트, 다단계 작업에서의 **희소 보상(sparse rewards)**으로 인한 신용 할당(credit assignment) 문제, 샘플 비효율성, 높은 계산 비용, 언어 품질과 추론 개선 간의 균형 문제가 존재합니다.
  * **인간 피드백**: 비용이 많이 들고 주관적이며 범위가 제한적입니다.
  * **추론 시 스케일링**: 주어진 쿼리에 얼마나 많은 계산이 필요한지 동적으로 조율하는 복잡성과 불확실성 추정기의 신뢰성 문제가 있습니다.
  * **일반적인 문제**: 적대적 취약성(예: 프롬프트 주입 공격), 벤치마크 포화(실제 성능과의 낮은 상관관계)가 있습니다.
* **시사점**: 미래의 LLM 시스템은 소규모 기본 모델을 필수 지식으로 사전 학습하고, TTS를 통해 적응적이고 온디맨드 계산으로 응답을 동적으로 향상하는 **하이브리드 접근 방식**을 채택할 수 있습니다. 이를 통해 비용 효율적이고 효율적인 대규모 모델 배포가 가능해집니다. 안전한 미세 조정, AI 피드백, 적응형 추론 시 계산 할당 등은 LLM을 발전시키는 데 핵심적인 방향입니다.

## 📌 TL;DR

사전 학습된 LLM은 환각, 비일관성, 정렬 부족 등의 문제에 직면해 있어 **후학습**이 필수적입니다. 이 논문은 **미세 조정, 강화 학습, 추론 시 스케일링**이라는 세 가지 핵심 후학습 패러다임을 체계적으로 검토합니다. PPO, DPO, GRPO와 같은 강화 학습 기법과 CoT, ToT, MCTS와 같은 추론 시 스케일링 방법론을 통해 모델의 **추론 능력, 사실적 정확도, 인간 선호도 정렬**을 크게 향상시킬 수 있음을 보여줍니다. 이러한 방법들은 모델 매개변수만 스케일링하는 것보다 **높은 계산 효율성**으로 더 작은 모델이 경쟁력 있는 성능을 달성하도록 돕습니다. 그러나 치명적인 망각, 보상 해킹, 안전성, 효율성, 동적 계산 할당과 같은 과제는 여전히 남아있으며, 이는 미래 연구의 핵심 방향이 될 것입니다.
