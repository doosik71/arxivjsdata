# MACAW-LLM: 이미지, 오디오, 비디오, 텍스트 통합을 통한 다중 모달 대규모 언어 모델

Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, Zhaopeng Tu

## 🧩 해결하고자 하는 문제

* 기존 대규모 언어 모델(LLM)은 텍스트 데이터 처리에는 뛰어나지만, 텍스트 외 다른 모달리티(시각, 오디오 등)에 대한 효과적인 활용 및 그 능력에 대한 연구가 부족합니다.
* 다양한 모달리티(이미지, 비디오, 오디오)의 정보를 텍스트 정보와 통합하여 LLM의 기능을 확장하고 복잡한 실제 시나리오를 처리하는 능력을 향상시켜야 합니다.
* 기존 다중 모달 시스템은 일반적으로 정렬 계층 훈련 및 명령어 미세 조정을 위한 2단계 학습 절차를 요구하여 복잡하며, 잠재적인 오류 전파 위험이 있습니다.
* 현재 다중 모달 데이터셋은 특정 작업 유형에만 치중되어 있으며, 생성된 텍스트가 인간의 지시 스타일과 잘 맞지 않아 모델이 인간의 지시를 효과적으로 따르기 어렵다는 한계가 있습니다.

## ✨ 주요 기여

* 이미지, 비디오, 오디오, 텍스트의 네 가지 모달리티를 단일 모델로 원활하게 통합하는 새로운 다중 모달 LLM인 **MACAW-LLM**을 제안했습니다.
* 다중 모달 특징을 LLM의 텍스트 임베딩에 원활하게 연결하여 모달리티 모듈에서 인지 모듈로의 적응 과정을 간소화하는 **새로운 정렬 모듈(alignment module)**을 개발했습니다.
* 다중 모달 특징 정렬과 명령어 미세 조정을 단일 단계로 통합하는 **원스텝 명령어 미세 조정(one-step instruction fine-tuning)** 방식을 도입하여 기존 2단계 훈련 방식의 복잡성과 잠재적 오류 전파 위험을 줄였습니다.
* GPT-3.5-TURBO를 활용하여 이미지(69K 인스턴스) 및 비디오(50K 인스턴스) 데이터를 포함하는 대규모 **다중 모달 명령어 데이터셋**을 구축했습니다. 이 데이터셋은 다양한 명령어 유형과 다중 턴 대화(향후 확장 예정)를 포함합니다.
* 향후 다중 모달 LLM 연구를 촉진하고 LLM의 능력을 확장하기 위해 모델, 코드 및 데이터를 공개적으로 제공했습니다.

## 📎 관련 연구

* **명령어 튜닝 대규모 언어 모델 (Instruction-Tuned LLMs):** Ouyang et al. (2022), Wei et al. (2022) 등은 명령어 튜닝 LLM의 뛰어난 영샷(zero-shot) 일반화 능력을 입증했으며, Wang et al. (2022c)는 기계 생성 명령어를 통해 명령어 튜닝이 가능함을 보여주었습니다 (예: Alpaca, Vicuna).
* **다중 모달리티 (Multi-Modality):**
  * CLIP (Radford et al., 2021), data2vec (Baevski et al., 2022), ImageBind (Girdhar et al., 2023)와 같이 여러 모달리티 간의 지식을 공유 잠재 공간에 정렬하는 연구가 활발합니다.
  * Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023c)처럼 사전 훈련된 비전 전용 및 언어 전용 모델을 결합하여 영샷(zero-shot) 능력을 시연하는 연구도 진행되었습니다.
  * 최근에는 MiniGPT-4 (Zhu et2 al., 2023), InstructBLIP (Dai et al., 2023), LLaVA (Liu et al., 2023)와 같이 다중 모달 LLM이 지시를 따르도록 하는 연구가 진행 중입니다.
  * MultiInstruct (Xu et al., 2022)는 다중 모달 명령어 튜닝 벤치마크 데이터셋을, PandaGPT (Su et al., 2023)는 LoRA(Low-Rank Adaptation)를 사용하여 6가지 모달리티를 지원하는 연구를 진행했습니다.

## 🛠️ 방법론

MACAW-LLM은 다음과 같은 세 가지 주요 모듈로 구성됩니다.

* **모달리티 모듈 (Modality Module):**
  * **시각 모달리티 인코더:** 이미지 및 비디오 프레임 인코딩을 위해 CLIP-VIT-B/16을 활용합니다.
  * **오디오 모달리티 인코더:** 오디오 신호 인코딩을 위해 WHISPER-BASE를 활용합니다.
  * **텍스트 모달리티 인코더:** 사전 훈련된 LLM인 LLAMA-7B가 인지 모듈 역할을 하면서 동시에 텍스트 정보를 처리합니다.

* **정렬 모듈 (Alignment Module):**
  * **목표:** 각 모달리티 인코더에서 생성된 이질적인 표현을 통일된 공유 공간으로 정렬하여 LLM 임베딩에 자연스럽게 주입될 수 있도록 합니다.
  * **과정:**
        1. **인코딩:** CLIP 및 WHISPER와 같은 사전 훈련된 모델을 사용하여 다중 모달 특징($h_i, h_v, h_a$)을 인코딩합니다.
            $$
            h_i = \text{CLIP}(x_i), h_v = \text{CLIP}(x_v), h_a = \text{WHISPER}(x_a)
            $$
        2. **변환 (Transformation):** 1D 컨볼루션 계층을 사용하여 다중 모달 특징의 길이를 고정된 작은 값 $L'$으로 압축하고, 선형 계층을 사용하여 특징의 숨겨진 크기를 LLM 임베딩 차원 $d_e$와 일치시킵니다.
            $$
            h'_i = \text{Linear}(\text{Conv1D}(h_i)), h'_v = \text{Linear}(\text{Conv1D}(h_v)), h'_a = \text{Linear}(\text{Conv1D}(h_a))
            $$
        3. **정렬 (Alignment):** 어텐션 메커니즘을 사용하여 변환된 모달리티 표현($h'$)을 LLM의 텍스트 임베딩 공간($E$)과 정렬합니다. 이는 $h'$를 LLM의 "소프트 토큰"으로 간주하여 수행됩니다.
            $$
            h^a = \text{Attn}(h', E, E)
            $$
        4. **통합 (Integration):** 정렬된 모달리티 표현($h^a_i, h^a_v, h^a_a$)을 텍스트 명령어 임베딩($\text{Embed}(x_t)$)과 연결하여 다중 모달 명령어를 형성합니다.
            $$
            x = [h^a_i : h^a_v : h^a_a : \text{Embed}(x_t)]
            $$

* **인지 모듈 (Cognitive Module):**
  * 사전 훈련된 LLAMA-7B를 기반으로 하며, 인간의 지시를 이해하고 따르는 역할을 합니다.

* **원스텝 명령어 미세 조정 (One-Step Instruction Fine-Tuning):**
  * 기존의 2단계 훈련 방식과 달리, MACAW-LLM의 모든 파라미터 $\theta$를 단일 단계에서 미세 조정합니다.
  * 응답 $y$에 대한 음의 로그 가능도($\mathcal{L}(y;\theta) = -\sum_{j=1}^{N} \log P(y_j|x;\theta)$)를 최소화하는 것을 목표로 하여 모델의 효과적인 조화를 이끌어냅니다.

## 📊 결과

* **MACAW-LLM 명령어 데이터셋 구축:**
  * GPT-3.5-TURBO (GPT-4 프롬프트 사용)를 활용하여 대규모 다중 모달 명령어 데이터셋을 성공적으로 구축했습니다.
  * COCO 이미지 캡션을 기반으로 약 69K개의 이미지 명령어-응답 쌍을 생성했습니다.
  * Charades 및 AVSD 비디오 캡션을 기반으로 약 50K개의 비디오 명령어-응답 쌍을 생성했습니다.
  * 이 데이터셋은 기존 데이터셋의 한계를 극복하고 다양한 명령어 유형을 포함하여 모델이 인간의 지시를 효과적으로 따르도록 돕습니다.
* **다중 모달 이해 및 응답 생성 능력 시연 (예시를 통해):**
  * **이미지 기반 질의응답:** 이미지 속 객체(기린, 모자, 티셔츠 등), 색상, 시간대(낮), 장소(방의 위치), 사람의 나이 추정 등 시각적 내용을 이해하고 추론하여 유창한 답변을 생성하는 능력을 보여줍니다 (그림 4, 5, 6).
  * **비디오 기반 질의응답:** 비디오 속 객체(보트)의 수, 시간 경과에 따른 행동, 그리고 문 밖의 작은 "흰색" 부분만 보고 "눈(snow)"의 존재를 추론하는 시각적 추론 등 비디오 정보를 효과적으로 이해하고 답변을 생성합니다 (그림 7, 8).
  * **비디오 및 오디오 기반 질의응답:** 비디오 영상과 오디오 트랙을 동시에 활용하여 비디오 속 개의 짖음 여부를 정확하게 식별하는 등, 여러 모달리티 정보를 동시에 처리하고 통합하는 능력을 보여줍니다 (그림 9).
* 제공된 예시들은 MACAW-LLM이 다양한 시각 콘텐츠에 대한 질문에 대해 최고 수준의 상황 적합하고 논리적으로 일관된 응답을 자연어 대화 형태로 생성하는 인상적인 능력을 가지고 있음을 보여줍니다.

## 🧠 통찰 및 논의

* **함의:** MACAW-LLM은 이미지, 비디오, 오디오, 텍스트 등 여러 모달리티를 효과적으로 통합하여 인간과 유사한 대화 에이전트를 만들 수 있는 잠재력을 보여주었습니다. 특히, 새로운 정렬 모듈과 원스텝 미세 조정 방식은 다중 모달 LLM의 개발을 간소화하고 성능을 향상시키는 데 기여하며, 이는 복잡한 실제 환경에서 LLM의 활용성을 확장합니다. 이러한 발전은 자연어 처리(NLP) 능력과 다중 모달 정보 통합 능력을 결합하여 효율적인 인간-기계 통신 인터페이스 개발에 중요한 진전을 가져올 것입니다.
* **한계점:**
  * **평가:** 제시된 예시들은 모델의 능력을 시연하지만, 모델의 실제 능력을 정확하고 포괄적으로 보여주기에는 불충분할 수 있습니다. Gudibande et al. (2023)의 연구처럼 명령어 튜닝 LLM이 보고된 평가 결과만큼 잘 수행되지 않을 수 있다는 우려가 존재합니다.
  * **단일 턴 대화:** 현재 훈련 데이터는 주로 "대화형" 명령어로 구성되어 있지만, 단일 턴 상호 작용에 국한되어 있습니다. MACAW-LLM은 다중 턴 대화나 장거리 문맥을 효과적으로 활용하는 데 최적화되어 있지 않습니다.
  * **환각, 유해성 및 공정성 (Hallucination, Toxicity and Fairness):** 적절한 평가 도구의 부재로 인해 이러한 윤리적 측면에 대한 모델 평가가 이루어지지 않았습니다.

## 📌 TL;DR

MACAW-LLM은 텍스트에만 국한되었던 LLM의 한계를 극복하고 이미지, 비디오, 오디오, 텍스트를 통합하는 혁신적인 다중 모달 LLM입니다. 이 모델은 다중 모달 특징을 LLM의 텍스트 임베딩과 직접적으로 정렬하는 새로운 정렬 모듈과 간소화된 원스텝 명령어 미세 조정 방식을 제안합니다. GPT-3.5-TURBO로 구축된 대규모 다중 모달 명령어 데이터셋을 활용하여, MACAW-LLM은 이미지, 비디오, 오디오 기반의 질의응답에서 뛰어난 이해 및 응답 생성 능력을 시연하며, 다중 모달 LLM 분야의 발전을 위한 중요한 기반을 제공합니다. 다만, 심층적인 정량적 평가, 다중 턴 대화 처리 능력, 그리고 환각 및 공정성 문제 해결은 향후 연구 과제로 남아있습니다.
