# Small LLMs Are Weak Tool Learners: A Multi-LLM Agent

Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, Fei Huang

## 🧩 Problem to Solve

거대 언어 모델(LLM) 에이전트는 외부 도구를 사용하여 다양한 작업을 수행함으로써 LLM의 능력을 크게 확장합니다. 그러나 도구 사용 작업은 LLM이 사용자 질의를 정확하게 이해하고 답변을 생성하는 것 외에도 작업 계획, 도구 호출 및 결과 요약 능력을 요구합니다. 기존의 단일 LLM 접근 방식은 이러한 모든 능력을 하나의 모델에 학습시키지만, 특히 소규모 LLM에서는 성능 제약이 나타납니다. 또한, 도구가 자주 업데이트될 때 전체 LLM을 재학습시켜야 하는 비효율성도 존재합니다.

## ✨ Key Contributions

* **α-UMi 프레임워크 제안:** 도구 학습 능력을 플래너, 호출자, 요약자로 분해하여 각 구성 요소를 전담 LLM이 수행하도록 하는 Multi-LLM 에이전트 프레임워크인 α-UMi를 제안합니다. 이는 기존 단일 LLM 접근 방식보다 우수한 성능을 보이며, 특히 소규모 LLM의 도구 활용 능력을 향상시킵니다.
* **GLPFT(Global-to-Local Progressive Fine-Tuning) 전략 도입:** Multi-LLM 프레임워크를 효과적으로 훈련하기 위한 2단계 미세 조정 전략을 제시합니다. 이는 프레임워크의 성공에 필수적임을 입증합니다.
* **심층적인 분석:** 데이터 스케일링 법칙 연구를 포함하여 프레임워크의 우수한 성능에 대한 근본적인 원인을 철저히 분석합니다.
* **소규모 LLM의 잠재력 입증:** Multi-LLM 프레임워크 내에서 소규모 LLM을 활용하여 단일 대규모 LLM 에이전트의 성능을 능가할 수 있음을 보여줍니다.

## 📎 Related Works

* **도구 학습 (Tool Learning):** Toolformer를 시작으로 실시간 지식, 멀티모달 이해 및 생성, 코드 및 수학 추론, 특정 AI 모델 및 API 활용 등 다양한 외부 도구와 LLM을 통합하는 연구가 진행되었습니다. 본 연구는 단일 LLM에 의존하는 기존 방식과 달리 소규모 오픈소스 LLM을 위한 새로운 Multi-LLM 협업 도구 학습 프레임워크를 제시합니다.
* **LLM 기반 에이전트 (LLM-powered Agents):** ChatGPT, GPT-4와 같은 LLM의 능력을 활용하여 BabyAGI, AutoGPT (일상 문제), Voyager, Ghost (게임), MetaGPT, ChatDev, AutoGen (소프트웨어 개발, 문제 해결)과 같은 다양한 에이전트 시스템이 개발되었습니다. 이러한 에이전트의 능력을 강화하기 위해 Chain-of-Thought, Reflexion (추론), MemoryBank (장기 기억)와 같은 기술들이 제안되었습니다. 기존 연구들은 주로 단일 LLM의 미세 조정에 초점을 맞추는 반면, 본 연구는 도구 학습을 위한 Multi-LLM 에이전트를 효과적으로 미세 조정하는 방법을 탐구합니다.

## 🛠️ Methodology

본 논문은 도구 학습 작업을 세 가지 하위 작업으로 분해하고 각 하위 작업을 전담 LLM에 할당하는 **α-UMi 프레임워크**와 이를 효과적으로 학습시키는 **GLPFT(Global-to-Local Progressive Fine-Tuning) 전략**을 제안합니다.

### α-UMi 프레임워크

α-UMi는 세 가지 구별된 LLM 구성 요소로 이루어져 있습니다: 플래너 ($M_{plan}$), 호출자 ($M_{call}$), 요약자 ($M_{sum}$).

1. **플래너 ($M_{plan}$):**
    * **역할:** 에이전트 프레임워크의 "두뇌" 역할을 하며 계획 및 의사 결정을 담당합니다.
    * **입력:** 시스템 프롬프트 ($P_{plan}$), 사용자 지시 ($q$), 이전 실행 궤적 ($\tau_{t-1}$)을 기반으로 추론 ($r_t$)을 생성합니다.
    * **결정:** 다음 단계로 호출자 활성화 ("Next: Caller"), 요약자 활성화 ("Next: Summarizer") 또는 실행 중단 ("Next: Give up") 중 하나를 결정합니다.
    $$r_t = M_{plan}(P_{plan}, \tau_{t-1}, q)$$
2. **호출자 ($M_{call}$):**
    * **역할:** 도구와의 상호 작용을 담당하며 유효하고 유용한 도구 호출 액션 ($a_t$)을 생성합니다.
    * **입력:** 사용자 지시 ($q$), 이전 실행 궤적 ($\tau_{t-1}$), 그리고 현재 단계의 플래너 추론 ($r_t$)에 집중하도록 설계된 프롬프트 ($P_{call}$)를 받습니다.
    $$a_t = M_{call}(P_{call}, \tau_{t-1}, q, r_t)$$
3. **요약자 ($M_{sum}$):**
    * **역할:** 사용자에게 유익하고 도움이 되는 최종 답변 ($a_n$)을 생성합니다.
    * **입력:** 간결한 프롬프트 ($P_{sum}$)와 최종 실행 궤적 ($\tau_{n-1}$), 사용자 지시 ($q$), 마지막 추론 ($r_n$)을 사용하여 답변을 요약하도록 유도됩니다.
    $$a_n = M_{sum}(P_{sum}, \tau_{n-1}, q, r_n)$$

### GLPFT (Global-to-Local Progressive Fine-Tuning) 전략

α-UMi Multi-LLM 시스템을 효과적으로 미세 조정하기 위한 2단계 전략입니다.

1. **글로벌 미세 조정 (Global Fine-tuning):**
    * 백본 LLM (예: LLaMA-2)을 전체 훈련 데이터셋에 미세 조정합니다. 이때 하위 작업을 구분하지 않고 전체 작업에 대한 포괄적인 이해를 높입니다.
    * 이 단계에서 LLM은 추론, 액션, 답변을 순차적으로 출력하도록 학습됩니다.
    * 이후, 이 미세 조정된 백본 LLM의 세 복사본을 각각 플래너, 호출자, 요약자로 지정합니다.
2. **로컬 미세 조정 (Local Fine-tuning):**
    * 훈련 데이터셋을 각 LLM의 역할에 맞춰 재구성합니다.
    * 플래너, 호출자, 요약자를 각각의 데이터셋에 대해 지속적으로 미세 조정하여 특정 능력(추론 생성, 액션 생성, 최종 답변 생성)을 더욱 향상시킵니다.
    * 다른 텍스트 구간의 그래디언트는 중단되며, 각 LLM에 대한 시스템 프롬프트가 세분화됩니다.

## 📊 Results

* **전반적인 성능 (표 1):**
  * α-UMi는 ToolBench 및 ToolAlpaca 벤치마크에서 ChatGPT 및 ToolLLaMA를 포함한 기존 단일 LLM 에이전트보다 대부분의 지표 (Plan ACC, Act. EM, Hallu., Arg. F1, R-L)에서 우수한 성능을 보였습니다.
  * 7B 백본을 사용하는 α-UMi는 13B 백본을 사용하는 Single-LLM보다 뛰어난 성능을 보이며, 이는 소규모 LLM이 분해된 작업에 집중할 때 경쟁력 있는 전체 성능을 달성할 수 있음을 입증합니다.
  * GLPFT 전략이 없는 `Multi-LLM_{one-stage}`나 단일 LLM이 다중 역할을 수행하는 `Single-LLM_{multi-task}`보다 α-UMi가 훨씬 우수하여, GLPFT의 필수성과 소규모 LLM의 제한된 용량 문제를 극복하는 효과를 보여줍니다.
  * GLPFT의 두 번째 단계에서 사용자 지시를 재사용하는 `α-UMi_{w/reuse}`가 재사용하지 않는 `α-UMi_{w/o reuse}`보다 더 좋은 성능을 보였습니다.
* **실시간 평가 (표 2):**
  * ToolBench의 실시간 평가에서 α-UMi (7B)는 ChatGPT와 ToolLLaMA를 Pass Rate 및 Win Rate에서 상당한 차이로 앞섰습니다.
  * GPT-4에 비해서는 Win Rate가 낮았지만, 특정 테스트 그룹에서는 GPT-4와 동등하거나 더 높은 Pass Rate를 기록했습니다.
* **데이터 스케일링 법칙 (그림 4):**
  * 다양한 훈련 데이터 규모에서 α-UMi는 Single-LLM 대비 Plan ACC, Act. EM, Hallu., Arg. F1 지표에서 일관되고 상당한 성능 향상을 보였습니다. 이는 주로 플래너와 호출자의 분리 덕분임을 시사합니다.
  * Single-LLM은 데이터 규모에 따라 최적 성능을 달성하는 지표가 달랐지만, α-UMi는 모든 지표에서 데이터 규모가 증가함에 따라 꾸준히 성능이 향상되었습니다.
* **수학 추론 벤치마크 (표 4):**
  * MATH 및 GSM8K와 같은 수학 추론 벤치마크에서도 α-UMi는 기준선을 능가하여 그 효과성을 검증했습니다.

## 🧠 Insights & Discussion

* **α-UMi의 성공 요인:**
  * **소규모 LLM의 능력 및 용량 한계 극복:** 복잡한 도구 학습 작업을 더 간단한 하위 작업으로 분해하여 개별 LLM의 부담을 줄입니다.
  * **프롬프트 설계 유연성:** 각 LLM에 특화된 프롬프트 및 모델 입력을 설계하여 하위 작업 능력을 최대한 활용합니다.
  * **GLPFT 전략의 효과:** 전체 도구 학습 작업과 개별 하위 작업 간의 미세 조정 격차를 해소하여 Multi-LLM 시스템의 성공적인 학습을 이끌어냅니다. 훈련 손실 분석 결과 (그림 5), α-UMi는 Single-LLM이 정체되는 지점을 넘어 지속적으로 추론 및 액션 관련 손실을 줄이며 성능 상한을 뛰어넘음을 보여줍니다.
* **비용 효율성 (표 3):**
  * α-UMi는 단일 LLM과 동일한 백본 크기를 사용할 경우 3배의 저장 공간과 1.3배의 계산 리소스, 1.5배의 훈련 시간이 필요합니다.
  * 그러나 7B 백본의 α-UMi가 13B 백본의 Single-LLM보다 우수한 성능을 보이면서도 총 훈련 및 추론 비용이 더 낮아, α-UMi의 비용 효율성을 강조합니다. 추론 시간은 추가적인 콘텐츠 생성을 강제하지 않기 때문에 비슷합니다.
* **사례 연구 (표 5-8):** α-UMi의 플래너는 복잡한 작업에서 사용자 의도를 빠르게 이해하고 효과적인 계획을 수립하여 Single-LLM에서 발생할 수 있는 잘못된 API 호출이나 루프를 피할 수 있음을 보여줍니다.

## 📌 TL;DR

**문제:** 소규모 LLM은 작업 계획, 도구 호출, 결과 요약 등 도구 학습에 필요한 다양한 능력을 단일 에이전트로 학습할 때 성능 제약에 직면합니다.

**방법:** 본 논문은 이러한 도구 학습 작업을 `플래너 (Planner)`, `호출자 (Caller)`, `요약자 (Summarizer)`의 세 가지 전문 LLM으로 분해하는 `α-UMi`라는 Multi-LLM 에이전트 프레임워크를 제안합니다. 이 프레임워크는 `GLPFT (Global-to-Local Progressive Fine-Tuning)`라는 2단계 학습 전략을 사용합니다. 첫 번째 단계에서는 백본 LLM을 전체 도구 사용 데이터셋에 미세 조정하여 포괄적인 이해를 높이고, 두 번째 단계에서는 해당 백본의 복사본들을 각 하위 작업에 맞춰 지속적으로 미세 조정합니다.

**발견:** `α-UMi`는 다양한 도구 사용 벤치마크에서 단일 LLM 에이전트보다 일관되게 우수한 성능을 보였습니다. 특히, 7B `α-UMi`는 13B 단일 LLM보다 뛰어난 성능을 달성하면서도 더 낮은 총 비용을 보여주어, 작업을 분해하고 소규모 모델을 전문적으로 훈련하는 것이 도구 학습에서 효과적이고 효율적임을 입증했습니다.
