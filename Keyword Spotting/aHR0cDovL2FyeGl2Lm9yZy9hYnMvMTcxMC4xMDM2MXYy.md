# DEEP RESIDUAL LEARNING FOR SMALL-FOOTPRINT KEYWORD SPOTTING

Raphael Tang, Jimmy Lin

## 🧩 Problem to Solve

이 논문은 모바일폰이나 스마트 홈 기기와 같은 저전력, 성능 제한적인 장치에서 온디바이스(on-device) 키워드 스포팅(Keyword Spotting, KWS)의 정확도를 높이면서도 모델의 크기(footprint)를 최소화하는 문제를 다룹니다. 클라우드 기반 음성 인식의 잠재적인 개인 정보 보호 문제와 전송 지연 문제를 해결하기 위해, 장치 내에서 "Hey Siri" 또는 "켜줘", "꺼줘" 같은 짧은 명령어를 효율적으로 감지하는 소형 모델이 필요합니다. 기존 연구들은 정확도와 모델 크기 사이의 절충점에 초점을 맞추었으나, 더 효율적인 솔루션이 요구됩니다.

## ✨ Key Contributions

- **딥 잔여 학습(Deep Residual Learning) 및 팽창 컨볼루션(Dilated Convolutions) 적용**: 키워드 스포팅 작업에 딥 잔여 네트워크(ResNet)와 팽창 컨볼루션을 최초로 적용했습니다.
- **최고 수준의 정확도 달성**: Google Speech Commands Dataset에서 제안된 ResNet 모델(`res15`)이 Google의 이전 CNN 모델 대비 95.8%의 정확도를 달성하여, 91.7%였던 기존 최고 기록을 크게 능가했습니다.
- **소형 모델의 성능 향상**: 모델 깊이와 너비 조정을 통해, Google의 이전 최고 CNN 모델에 비해 파라미터는 50배, 추론 시 곱셈 연산은 18배 감소한 컴팩트 모델(`res8-narrow`)이 기존 소형 모델들보다 훨씬 우수한 성능을 보여주었습니다.
- **오픈 소스 최첨단 기준 제시**: KWS를 위한 새로운 오픈 소스 최첨단(state-of-the-art, SOTA) 참조 모델을 구축하여 향후 음성 기반 인터페이스 개발을 지원합니다.

## 📎 Related Works

- **딥 잔여 네트워크(ResNets) [2]**: He et al.이 이미지 인식 분야에서 깊은 네트워크 훈련을 가능하게 하며 SOTA 성능을 이끌어낸 획기적인 기술입니다. 이 논문은 ResNet을 KWS 작업에 적용합니다.
- **CNN 기반 KWS [1]**: Sainath와 Parada는 CNN을 사용하여 이전 HMM(Hidden Markov Model) 기반 접근 방식보다 뛰어난 KWS 성능을 달성했으며, 소형 모델 크기를 주요 동기로 언급했습니다.
- **MLP 기반 KWS [6]**: Chen et al.은 표준 다층 퍼셉트론(MLP)을 KWS에 적용하여 상당한 개선을 이루었습니다.
- **순환 신경망(RNN) 기반 KWS [7, 8]**: 최근에는 RNN 기반 모델들이 KWS에 적용되고 있으나, 본 연구는 CNN 모델의 단순성, 튜닝 용이성, 그리고 공개 구현의 가용성을 이유로 CNN에 초점을 맞추었습니다.

## 🛠️ Methodology

1. **특징 추출 및 입력 전처리**:
   - 입력 오디오에 20Hz/4kHz 대역통과 필터를 적용하여 노이즈를 줄입니다.
   - 30ms 윈도우와 10ms 프레임 시프트를 사용하여 40차원 Mel-Frequency Cepstrum Coefficient (MFCC) 프레임을 구성합니다.
   - 모든 프레임은 1초 간격으로 쌓여 모델의 2차원 입력으로 사용됩니다.
   - 훈련 데이터 생성 시, 0.8의 확률로 배경 노이즈(핑크 노이즈, 백색 노이즈 등)를 추가하고, $\text{UNIFORM}[-100, 100]$ms 범위의 무작위 시간 이동(time-shift)을 적용합니다.
2. **모델 아키텍처 (ResNet 기반)**:
   - He et al. [2]의 잔여 학습(residual learning) 개념을 따릅니다. 이는 깊은 네트워크에서 항등 함수(identity mapping)를 학습하기 어려운 문제점을 해결하기 위해 $H(x) = F(x) + x$와 같은 잔여 매핑 $F(x)$를 학습하는 방식입니다.
   - **잔여 블록(Residual Block)**: 편향이 없는 컨볼루션 레이어로 시작하여, ReLU 활성화 함수와 배치 정규화(Batch Normalization) 레이어가 뒤따릅니다. 입력 $x$는 여러 레이어의 출력이 더해져 잔여 연결을 형성합니다.
   - **팽창 컨볼루션(Dilated Convolutions) [12]**: 네트워크의 수용 필드(receptive field)를 확장하기 위해 $(d_w, d_h)$ 컨볼루션 팽창을 사용합니다. `res15` 모델에서는 $d_w = d_h = 2^{b_{i}/3}$와 같이 지수 스케줄을 사용하여 1초 입력 전체를 적은 레이어 수로 고려할 수 있게 합니다.
   - **모델 변형**:
     - `res15`: 기본 모델 (6개의 잔여 블록, 45개의 특징 맵).
     - `res8`: 컴팩트 모델 (3개의 잔여 블록, 첫 컨볼루션 레이어 후 $4 \times 3$ 평균 풀링, 팽창 컨볼루션 미사용).
     - `res26`: 깊은 모델 (12개의 잔여 블록, 26개 레이어, 잔여 블록 체인 앞에 $2 \times 2$ 평균 풀링, 팽창 컨볼루션 미사용).
     - `-narrow` 변형: 특징 맵 수를 45개에서 19개로 줄여 모델 너비를 감소시킵니다. (`res15-narrow`, `res8-narrow`, `res26-narrow`)
3. **훈련**:
   - 옵티마이저: 모멘텀 0.9를 사용한 확률적 경사 하강법(SGD).
   - 학습률: 초기 학습률 0.1, 정체 구간에서 0.1씩 감소.
   - 미니 배치 크기: 64.
   - 가중치 감소(Weight decay): $L_2$ 정규화를 위해 $10^{-5}$.
   - 총 26 에포크 훈련 (약 9,000 학습 단계).
4. **평가**:
   - Google Speech Commands Dataset (12개 클래스: "yes", "no", "up", "down", "left", "right", "on", "off", "stop", "go", unknown, silence)를 사용합니다.
   - 데이터셋은 훈련(80%), 검증(10%), 테스트(10%)로 분할됩니다.
   - 주요 평가 지표는 정확도(accuracy)와 ROC(Receiver Operating Characteristic) 곡선(FAR/FRR)입니다.

## 📊 Results

- **최고 성능 모델 `res15`**: Google의 최고 CNN 모델인 `tpool2` (91.7%)보다 월등히 높은 95.8%의 테스트 정확도를 달성했습니다. `res15`는 더 적은 파라미터를 사용했지만, 더 많은 곱셈 연산이 필요했습니다.
- **컴팩트 모델 `res8`**:
  - `res8` (와이드 버전, 94.1%)는 모든 Google 모델을 능가하며, 더 작은 모델 크기로 훨씬 높은 정확도를 보였습니다.
  - `res8-narrow` (90.1%)는 `tpool2`보다 약간 낮은 정확도를 보였지만, 파라미터 수는 50배, 곱셈 연산은 18배 감소하여 Google의 컴팩트 모델인 `one-stride1` (77.9%)보다 훨씬 우수했습니다.
- **깊은 모델 `res26`**: `res15`보다 낮은 정확도(95.2%)를 보였으며, 이는 네트워크 깊이를 지나치게 늘려 최적화하기 어려워졌음을 시사합니다.
- **깊이 vs 너비**: 모델 너비(특징 맵 수)가 깊이보다 정확도에 더 큰 영향을 미치는 것으로 나타났습니다.
- **ROC 곡선**: `res15`가 모든 작동 지점에서 다른 모델들을 압도하는 성능을 보여주었습니다.

## 🧠 Insights & Discussion

- 이 연구는 잔여 학습과 팽창 컨볼루션이 키워드 스포팅 작업에 매우 효과적임을 입증했습니다.
- Google Speech Commands Dataset을 벤치마크로 활용하여, 이전 연구들이 사설 데이터셋에 의존하여 비교가 어려웠던 한계를 극복하고 공개적으로 비교 가능한 SOTA 참조 모델을 제시했습니다. 이는 향후 KWS 연구의 기반이 될 것입니다.
- 모델의 깊이와 너비 조정을 통해 정확도와 모델 크기 사이의 최적의 절충점을 찾을 수 있음을 보여주었습니다. 특히, `res8-narrow`와 같은 컴팩트 모델은 저전력 장치에 배포하기에 매우 실용적인 솔루션을 제공합니다.
- 너무 깊은 네트워크(`res26`)는 오히려 성능 저하를 가져올 수 있으며, 모델의 너비(특징 맵 수)가 깊이보다 정확도에 더 큰 영향을 미칠 수 있다는 점을 시사합니다.
- 향후 연구에서는 순환 신경망(RNN) 기반 접근 방식과 비교할 계획이지만, 현재 공개된 참조 구현과 공통 벤치마크의 부족이 과제로 남아있습니다.

## 📌 TL;DR

소형 키워드 스포팅(KWS)의 정확도 및 모델 크기 문제를 해결하기 위해, 잔여 학습(ResNet)과 팽창 컨볼루션(Dilated Convolutions)을 적용한 CNN 모델을 제안했습니다. Google Speech Commands 데이터셋에서 95.8%의 정확도로 기존 Google CNN(91.7%)을 능가하는 SOTA 성능을 달성했습니다. 특히, `res8-narrow` 모델은 이전 컴팩트 모델보다 우수한 정확도를 유지하면서도 파라미터 수를 50배, 연산량을 18배 줄여 저전력 온디바이스 KWS를 위한 새로운 고효율 기준을 제시했습니다.
