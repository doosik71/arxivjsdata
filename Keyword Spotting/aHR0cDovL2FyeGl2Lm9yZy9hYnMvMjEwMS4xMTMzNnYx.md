# LOW-POWER AUDIO KEYWORD SPOTTING USING TSETLIN MACHINES

Jie Lei, Tousif Rahman, Rishad Shafik, Adrian Wheeldon, Alex Yakovlev, Ole-Christoffer Granmo, Fahim Kawsar, Akhil Mathur

## 🧩 Problem to Solve

기존의 인공신경망(NN) 기반 키워드 스포팅(KWS) 파이프라인은 최종 사용자 장치, 특히 사물 인터넷(IoT) 및 임베디드 시스템에서 종단 간 에너지 효율성, 메모리 사용량, 시스템 복잡성 측면에서 심각한 문제를 안고 있습니다. NN은 산술 집약적인 경사 하강법 계산과 다수의 하이퍼파라미터를 요구하여 에너지 효율적이지 못하며, 클라우드 기반 처리 방식은 실시간 응답에 대한 네트워크 의존성과 사용자 데이터 프라이버시 문제를 야기합니다. 따라서 저전력, 저복잡도를 유지하면서 높은 학습 효율을 제공하는 새로운 머신러닝 솔루션이 필요합니다.

## ✨ Key Contributions

- 테슬린 머신(Tsetlin Machine, TM)을 활용한 KWS 파이프라인을 개발했습니다.
- TM 파이프라인에서 특징 세분성(feature granularity)을 제어하기 위한 데이터 인코딩 기법을 탐구했습니다.
- TM의 파라미터와 아키텍처 구성 요소를 조정하여 KWS 성능을 향상시키는 방법을 제시했습니다.
- TM이 NN에 비해 더 빠른 수렴 속도와 훨씬 적은 파라미터로 높은 학습 효율을 유지하며 저전력 온칩 KWS 가능성을 입증했습니다.

## 📎 Related Works

- **초기 KWS:** MFCC(Mel-frequency cepstrum coefficients)를 특징 추출에 사용하고 HMM(Hidden Markov Models)을 분류기로 활용했으나, HMM은 계산 집약적이고 Viterbi 디코딩이 필요했습니다.
- **신경망 기반 KWS:**
  - **RNN(Recurrent Neural Networks):** HMM보다 성능은 우수하지만, 문제 규모가 커질수록 지연이 발생합니다.
  - **DNN(Deep Neural Networks):** HMM보다 메모리 사용량이 적고 실행 시간이 짧지만, MFCC의 시간적 상관관계를 효율적으로 모델링하지 못하며, 가지치기(pruning)나 양자화(quantization) 시 정확도 손실이 큽니다.
  - **CNN(Convolutional Neural Networks):** MFCC 특징을 2D 이미지처럼 처리하여 시간적/공간적 의존성을 보존하지만, DNN과 마찬가지로 수십만에서 수백만 개의 파라미터를 요구하여 높은 계산 복잡도와 메모리 요구사항을 가집니다.
- **하드웨어 가속기:** 제한된 메모리 및 연산 능력의 마이크로컨트롤러에서는 DNN이 CNN보다 적은 연산으로 유리한 경우가 있으며, 양자화된 CNN(QCNN) 가속기 등은 52 μW의 저전력으로 KWS를 수행합니다.
- **테슬린 머신 (TM):**
  - Mikhail Tsetlin이 1950년대에 개발한 학습 자동자(Learning Automaton) 기반으로, 명제 논리(propositional logic)를 사용하여 학습합니다.
  - TM은 순수 논리 연산에 의존하여 하드웨어 구현에 매우 적합하며, Mignon이라는 65nm CMOS ASIC 구현에서 63 Tera Operations per Joule (Tops/Joule)의 에너지 효율을 달성했습니다.
  - 적은 수의 하이퍼파라미터와 빠른 수렴 속도를 특징으로 합니다.

## 🛠️ Methodology

본 논문은 KWS를 위한 KWS-TM 파이프라인을 제안하며, 이는 데이터 인코딩(Data Encoding)과 분류(Classification) 두 가지 주요 단계로 구성됩니다.

1. **데이터 인코딩 (Data Encoding)**

   - **MFCC(Mel-frequency cepstrum coefficients) 기반 오디오 특징 추출:**
     - 음성 데이터에서 불필요한 노이즈와 중복성을 제거하고 키워드의 언어적 특징을 강조합니다.
     - **단계:** Pre-Emphasis (고주파 강조), Framing & Windowing (신호 분할 및 평활화), Fast Fourier Transform (FFT, 시간 도메인에서 주파수 도메인으로 변환), Mel Filter Banks (인간 청각 시스템에 맞춰 주파수 조정), Logarithm Energy Extraction (데이터 값 압축), Discrete Cosine Transform (DCT, MFCC 특징 벡터 계산).
   - **양자화(Quantile-based binning) 기반 특징 Boolean화:**
     - TM에 적합하도록 MFCC 특징을 불리언(Boolean) 형태로 변환합니다.
     - 데이터의 통계적 분포를 보존하면서 데이터를 그룹화(binning)하여 데이터의 세분성을 제어하고, 불리언 특징 공간의 크기를 최소화합니다.
     - 이를 통해 TM의 크기와 처리 요구 사항을 줄입니다.

2. **분류 (Classification)**

   - **Tsetlin Machine (TM) 사용:**
     - 불리언화된 특징을 입력으로 받아 키워드를 분류합니다.
     - **주요 구성 요소:**
       - **Tsetlin Automata (TA):** 각 절(clause) 내에서 특징과 그 보수(complement)의 포함 또는 제외를 결정하여 명제 논리 관계를 형성하는 유한 상태 머신(FSM)입니다.
       - **Conjunctive Clauses:** TA 결정에 기반하여 논리곱($\text{AND}$) 연산을 통해 특징 간의 관계를 나타내는 논리식을 생성합니다.
       - **Summation and Threshold Module:** 여러 절의 출력을 합산하고 임계값을 넘는지 판단하여 최종 출력 클래스를 결정합니다.
       - **Feedback Module:** 예측된 클래스와 실제 목표 클래스를 비교하여 TA의 상태를 업데이트(보상 또는 벌칙 부여)합니다.

3. **파이프라인 최적화:**
   - **MFCC 생성 파라미터 조절:** 윈도우 스텝(Window Step)과 윈도우 길이(Window Length)를 조절하여 생성되는 MFCC 계수 수를 제어합니다.
   - **Boolean 특징의 세분성 제어:** 양자화(quantile) 개수를 변경하여 특징당 불리언 개수를 조절합니다.
   - **TM 하이퍼파라미터 튜닝:** $s$ 값 (TA 상태 전환 유동성 제어) 및 Threshold ($T$, 절 선택에 사용되는 임계값)를 조절하여 학습 안정성과 에너지 효율 간의 균형을 찾습니다.
   - **절 개수 조절:** 각 키워드 클래스에 할당되는 절의 개수를 변경하여 성능과 에너지 소비 간의 균형을 탐색합니다.

## 📊 Results

- **MFCC 설정의 영향:**
  - 윈도우 스텝(Window Step)을 증가시키면 MFCC 계수 수가 크게 감소하지만, 윈도우 길이가 충분하다면 정확도 저하가 미미합니다. 윈도우 스텝을 0.03초에서 0.10초로 증가시켰을 때 정확도는 약 90.5%로 유지되었습니다.
- **양자화 개수의 영향:**
  - MFCC 특징당 1 비트(2개 양자화)만 사용해도 4 비트(10개 양자화)를 사용하는 경우와 유사하게 약 90% 이상의 높은 테스트/검증 정확도를 유지했습니다. MFCC 특징의 분산이 충분히 커서 적은 불리언으로도 클래스 구분이 가능함을 시사합니다. 이는 TM의 크기(전체 불리언 수)를 획기적으로 줄이는 데 기여합니다.
- **키워드 수 증가의 영향:**
  - 키워드 수가 증가할수록 훈련, 테스트, 검증 정확도가 선형적으로 감소했으며, 과적합(overfitting) 현상이 심화되었습니다 (최대 4% 증가). 이는 키워드 간 특징 상관관계 증가로 TM이 고유한 논리적 특징을 만들기 어려워지기 때문입니다.
- **음향적으로 유사한 키워드의 영향:**
  - 'Yes', 'No', 'Stop' 3개 키워드에 'Seven'을 추가했을 때 정확도가 소폭 하락(92.6% $\to$ 90.1%)했으나, 음향적으로 유사한 'Go'를 추가했을 때 정확도가 크게 하락(92.6% $\to$ 82.6%)했습니다. 이는 유사한 키워드 간의 클래스 중첩이 심화되기 때문입니다.
- **클래스당 절 개수:**
  - 절 개수를 늘리면 성능이 향상되지만, 특정 지점(약 190-200개 절)을 넘어서면 과적합이 심화되고 거짓 양성(false positives)이 증가하여 오히려 정확도 향상이 둔화됩니다. 낮은 절 개수에서도 Threshold 하이퍼파라미터 조정으로 정확도를 높일 수 있으며, 이는 전력 소비를 줄이는 데 유리합니다. Raspberry Pi 실험 결과, 절 개수 증가가 전류, 시간, 에너지 사용량을 선형적으로 증가시켰습니다.
- **학습 수렴 및 복잡도 분석:**
  - TM은 4가지 '바닐라' NN 구현(shallow/deep, small/large)에 비해 훨씬 빠른 수렴 속도를 보였습니다. TM은 10 에폭 미만으로 90.5%의 정확도에 수렴한 반면, NN은 동일 정확도 달성에 약 100 에폭 이상이 필요했습니다.
  - TM은 NN보다 훨씬 적은 수의 파라미터(NN은 수십만에서 수백만 개, TM은 960개의 절과 2개의 하이퍼파라미터)를 요구하여 낮은 복잡도를 가집니다.

## 🧠 Insights & Discussion

- **저전력 KWS의 실현 가능성:** Tsetlin Machine(TM)은 기존 NN의 에너지 효율, 메모리 사용량, 시스템 복잡도 문제를 해결할 수 있는 강력한 대안입니다. 논리 기반 연산을 사용하고 필요한 파라미터 수가 현저히 적다는 장점 덕분에 저전력 온칩 KWS 솔루션에 매우 적합합니다.
- **빠른 학습과 자원 효율성:** TM의 빠른 수렴 속도(NN 대비 1/10 수준의 에폭)는 학습 과정의 에너지 소비를 크게 줄일 수 있습니다. 또한, MFCC 특징의 분산 특성을 활용하여 Boolean화를 1비트/특징으로만 수행함으로써 TM의 크기를 최소화할 수 있습니다.
- **파이프라인 최적화의 중요성:** MFCC 특징 추출 단계에서 윈도우 스텝을 늘리거나, TM의 하이퍼파라미터(Threshold)를 최적화하여 적은 수의 절(clause)로도 높은 정확도를 유지하면서 에너지 효율을 극대화할 수 있습니다. 이는 KWS 애플리케이션의 성능 저하를 최소화하면서도 시스템의 에너지 소비를 줄일 수 있는 실용적인 설계 가이드라인을 제공합니다.
- **확장성 및 강건성 한계와 개선 방향:** 키워드 수가 증가하거나 음향적으로 유사한 키워드가 있을 때 TM의 성능이 저하되는 경향은 과적합과 클래스 중첩에 기인합니다. 이를 해결하기 위해서는 더 다양하고 풍부한 훈련 데이터셋을 확보하거나, TM의 하이퍼파라미터 및 절 구성을 더욱 정교하게 튜닝하는 연구가 필요합니다. 또한, 향후 연구에서는 백그라운드 노이즈의 영향을 고려한 강건성 평가가 중요합니다.
- **하드웨어 가속기의 잠재력:** TM의 순수 논리 기반 연산 특성은 ASIC과 같은 전용 하드웨어 가속기 구현에 매우 유리합니다. Raspberry Pi를 이용한 실험은 낮은 절 개수에서의 에너지 효율적인 작동 가능성을 보여주며, 이는 향후 온칩 학습 기반 KWS 솔루션으로의 발전에 중요한 기반이 될 것입니다.

## 📌 TL;DR

기존 신경망(NN) 기반 KWS(키워드 스포팅) 시스템의 높은 에너지 소비와 복잡도 문제를 해결하기 위해, 논리 기반 Tsetlin Machine(TM)을 활용한 KWS 파이프라인이 제안되었습니다. 이 파이프라인은 MFCC 특징 추출 및 양자화 기반 Boolean화를 통해 입력 특징을 최소화하고, TM의 하이퍼파라미터 및 절 개수를 최적화하여 작동합니다. 실험 결과, TM은 NN보다 훨씬 빠른 수렴 속도(10 에폭 미만)와 NN의 수십만 분의 1에 불과한 적은 파라미터로 90.5%의 높은 정확도를 달성하며, 저전력 온칩 KWS를 위한 새로운 에너지 효율적인 대안임을 입증했습니다.
