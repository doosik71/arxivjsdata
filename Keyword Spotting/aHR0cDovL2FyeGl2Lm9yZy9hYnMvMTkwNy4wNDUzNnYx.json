{
  "title": "Multi-layer Attention Mechanism for Speech Keyword Recognition",
  "authors": "Ruisen Luo, Tianran Sun, Chen Wang, Miao Du, Zuodong Tang, Kai Zhou, Xiaofeng Gong, Xiaomei Yang",
  "year": 2019,
  "url": "http://arxiv.org/abs/1907.04536v1",
  "abstract": "As an important part of speech recognition technology, automatic speech\nkeyword recognition has been intensively studied in recent years. Such\ntechnology becomes especially pivotal under situations with limited\ninfrastructures and computational resources, such as voice command recognition\nin vehicles and robot interaction. At present, the mainstream methods in\nautomatic speech keyword recognition are based on long short-term memory (LSTM)\nnetworks with attention mechanism. However, due to inevitable information\nlosses for the LSTM layer caused during feature extraction, the calculated\nattention weights are biased. In this paper, a novel approach, namely\nMulti-layer Attention Mechanism, is proposed to handle the inaccurate attention\nweights problem. The key idea is that, in addition to the conventional\nattention mechanism, information of layers prior to feature extraction and LSTM\nare introduced into attention weights calculations. Therefore, the attention\nweights are more accurate because the overall model can have more precise and\nfocused areas. We conduct a comprehensive comparison and analysis on the\nkeyword spotting performances on convolution neural network, bi-directional\nLSTM cyclic neural network, and cyclic neural network with the proposed\nattention mechanism on Google Speech Command datasets V2 datasets. Experimental\nresults indicate favorable results for the proposed method and demonstrate the\nvalidity of the proposed method. The proposed multi-layer attention methods can\nbe useful for other researches related to object spotting.",
  "citation": 3
}