# Efficient Continual Learning in Keyword Spotting using Binary Neural Networks

Quynh Nguyen-Phuong Vu, Luciano Sebastian Martinez-Rau, Yuxuan Zhang, Nho-Duc Tran, Bengt Oelmann, Michele Magno and Sebastian Bader

## 🧩 Problem to Solve

자원 제한적인 스마트 장치에서 키워드 스포팅(KWS) 모델은 일반적으로 정적이며, 새로운 키워드가 추가되는 등의 새로운 시나리오에 적응할 수 없습니다. 기존의 연속 학습(CL) 방법들은 모델 복잡성과 메모리 소비가 높아 리소스 제약이 있는 마이크로컨트롤러(MCU)에 적합하지 않습니다.

## ✨ Key Contributions

- **BNN 기반 KWS 모델에 CL 알고리즘 통합 및 평가:** 리소스 제약이 있는 MCU에 최적화된 이진 신경망(BNN) 모델에 7가지 최신 CL 알고리즘을 구현하고 성능(분류 정확도 및 역전파 중 FLOPs)을 평가했습니다. 이를 통해 이전에 학습된 지식을 유지하면서 새로운 키워드를 지속적으로 학습할 수 있는 확장 가능하고 에너지 효율적인 KWS 접근 방식을 개발하는 것을 목표로 합니다.
- **새로운 클래스 수의 영향 분석:** KWS 시스템에 도입되는 새로운 클래스 수의 영향을 조사하여, 기존 키워드에 대한 성능을 유지하면서 새로 추가된 키워드에 대한 일반화 능력을 평가했습니다.
- **CL 학습 샘플 수의 영향 조사:** CL 단계에서 학습 샘플 수의 영향을 조사하여, 분류 정확도와 새로운 학습 데이터 요구 사항 간의 균형을 평가했습니다.

## 📎 Related Works

- **리소스 제약 KWS 모델:** Zhang et al. (8비트 양자화 CNN), LiCoNet (int8 선형 연산자), Cerutti et al. (KWS를 위한 BNN) 등의 연구가 진행되었으나, 이 모델들은 대부분 초기 배포 후 변경되지 않는 정적 모델이라는 한계가 있습니다.
- **KWS를 위한 연속 학습 (CL):** Huang et al. (네트워크 인스턴시에이터 및 공유 메모리), Michieli et al. (통계적 풀링 및 가우시안 기반 분류기), Yang et al. (리플레이 기반 이중 메모리 다중 모달 구조) 등의 방법이 제안되었지만, 대부분 상당한 모델 복잡성과 높은 메모리 소비를 필요로 하여 MCU에 적합하지 않습니다.
- **리소스 효율적인 CL 알고리즘:** TinyOL, TinyOL with batches, TinyOL v2, TinyOL v2 with batches, Learning Without Forgetting (LwF), LwF with batches, Copy Weight with Reinit (CWR)와 같이 마지막 모델 레이어의 파라미터만 조정하는 방식들이 최근 제안되었습니다.

## 🛠️ Methodology

- **데이터셋:** Google Speech Commands V2 데이터셋을 사용했습니다. 10개의 명령어, 4개의 숫자, "silence", "unknown"을 포함한 총 16개 클래스의 61,487개 음성 샘플을 사용했습니다.
  - 데이터셋 분할: 3%는 테스트 세트로, 나머지 97%는 학습용으로 사용되었습니다. 초기 모델은 12개 클래스로 사전 학습되었고, 나머지 데이터는 CL 태스크에 사용되었습니다.
  - 음성 전처리: 25ms 윈도우, 10ms 홉 크기, 64개 멜 필터를 사용하여 로그-멜 주파수 도메인으로 변환되었습니다.
- **BNN 모델:** [9]에서 제안된 아키텍처를 기반으로 합니다.
  - 첫 번째 및 마지막 컨볼루션 레이어는 전체 정밀도(full-precision) 입력 및 가중치를 사용합니다.
  - 나머지 컨볼루션 레이어는 이진화(binarized)됩니다.
  - 각 컨볼루션 레이어 뒤에는 Batch Normalization 레이어와 ReLU 활성화 함수가 따릅니다.
  - 평균 풀링(Average Pooling) 후, CL 학습에 사용될 완전 연결(Fully Connected) 레이어가 최종 출력 레이어로 추가됩니다.
  - 최적화 도구: Adam ($10^{-4}$의 초기 학습률), 손실 함수: Categorical Cross Entropy.
- **연속 학습(CL) 과정:**
  - 사전 학습된 BNN 모델에 CL 알고리즘을 적용하여 마지막 완전 연결 레이어의 가중치와 바이어스만 역전파를 통해 업데이트합니다. 다른 레이어들은 고정됩니다.
  - 새로운 샘플이 수신될 때마다 마지막 레이어의 가중치와 바이어스가 업데이트됩니다.
  - 7가지 CL 알고리즘 (TinyOL, TinyOL with batches, TinyOLv2, Tiny OL v2 with batches, LwF, LwF with batches, CWR)이 평가되었습니다.
  - 배치 기반 알고리즘의 배치 크기는 32로 설정되었고, 학습률은 0.05였습니다.
  - 평가 시나리오: 1개부터 4개까지 새로운 클래스를 추가하는 경우를 모든 가능한 조합에 대해 평가했습니다.
  - 데이터 볼륨에 대한 민감도를 평가하기 위해, CL 단계에서 64개부터 16,384개까지 다양한 수의 학습 샘플을 사용하여 실험을 수행했습니다.

## 📊 Results

- **정확도 성능:**
  - 모든 CL 알고리즘은 초기 12개 클래스에 대해 높은 정확도를 성공적으로 유지했습니다. 4개의 새로운 클래스가 추가된 최악의 시나리오에서도 TinyOL with batches는 88.3%, LwF with batches는 86.4%의 정확도를 보였습니다. 이는 사전 학습된 모델에 비해 약 3~5%의 성능 감소만 있었습니다.
  - 새로운 키워드가 단 하나인 경우, TinyOL, LwF, LwF with batches는 95%를 초과하는 가장 높은 정확도를 달성하여 초기 사전 학습 모델의 정확도를 능가했습니다.
  - 전체 클래스를 고려할 때, TinyOL with batches와 CWR은 4개의 새로운 키워드에서 가장 좋은 성능을 보였고, TinyOL, LwF, LwF with batches는 1개의 새로운 키워드에서 가장 좋은 성능을 보였습니다. 전반적으로 새로운 클래스가 잘 통합되지 않아 초기 12개 클래스보다는 정확도가 낮았지만, 최악의 경우에도 82.9%의 높은 정확도를 유지했습니다.
  - 개별 샘플 기반 알고리즘은 1개의 새로운 클래스 시나리오에서 높은 정확도를 보였지만, 새로운 클래스 수가 증가함에 따라 성능이 빠르게 감소했습니다. 반면, 배치 기반 알고리즘은 클래스 수에 덜 민감했습니다.
- **데이터 볼륨 민감도:**
  - TinyOL, TinyOL v2, LwF (배치 유무와 상관없이)는 약 2048개의 샘플에서 안정적인 정확도를 달성했습니다.
  - 배치 기반 알고리즘(LwF with batches 제외)은 더 많은 학습 데이터를 요구했습니다.
  - TinyOL과 TinyOL v2는 제한된 데이터에서도 일관되게 높은 정확도를 보였습니다.
  - CWR은 학습 샘플 수가 적을 때 가장 낮은 성능을 보였지만, 샘플 수가 증가하면 크게 개선되었습니다.
- **계산 복잡도 (역전파 단계의 FLOPs):**
  - LwF 변형은 701 및 707 FLOPs로 가장 높은 계산 복잡도를 보였으며, 이는 다른 알고리즘(436-479 FLOPs)에 비해 약 55% 증가한 수치입니다. 이는 LwF가 복사 레이어와 학습 레이어 모두에서 손실을 계산해야 하기 때문입니다.
  - 하지만 모델의 순방향 패스(forward pass)에 필요한 약 291 MFLOPs와 비교할 때, 역전파의 계산 복잡도는 모든 알고리즘에서 무시할 수 있는 수준이었습니다.

## 🧠 Insights & Discussion

- **시사점:** 평가된 모든 CL 알고리즘은 BNN 기반 KWS 접근 방식에 잠재적으로 사용될 수 있으며, 새로운 키워드를 학습하는 동시에 기존 키워드의 정확한 분류 능력을 유지할 수 있음을 입증했습니다. 새로운 클래스 수가 증가할수록 모델 정확도는 감소하지만, 최악의 조건에서도 82% 이상의 정확도가 관찰되었습니다. 가장 적합한 알고리즘의 선택은 새로운 클래스 수에 따라 달라집니다. 새로운 키워드가 적을 때는 TinyOL 및 LwF가, 새로운 클래스 수가 많을 때는 TinyOL v2 및 CWR이 더 일관된 성능과 높은 정확도를 제공했습니다. 배치 기반 알고리즘은 우수한 성능을 위해 더 많은 학습 샘플을 요구하므로, 제한된 데이터나 리소스 제약이 있는 실제 애플리케이션에서는 유용성이 제한될 수 있습니다. 계산 복잡도는 역전파 단계에서 LwF가 더 많은 FLOPs를 필요로 하지만, 이는 순방향 패스의 계산량에 비해 무시할 수 있는 수준이므로 평가 기준으로 중요하지 않습니다.
- **한계 및 향후 연구:** CL 데이터셋의 최소 크기에 대한 더 자세한 조사를 통해 적은 데이터로 효과적인 학습을 위한 추가 지식을 얻을 수 있습니다. 특히 배치 크기 및 동일한 데이터를 여러 에포크에서 활용하는 효과를 탐색할 수 있습니다. 또한, 제안된 접근 방식을 실제 리소스 제약이 있는 MCU에 배포하고 평가하는 것이 필요합니다.

## 📌 TL;DR

**문제:** 리소스 제약이 있는 장치의 KWS 모델은 정적이며, 새로운 키워드에 적응하지 못합니다. 기존 연속 학습(CL) 방법은 높은 복잡도와 메모리 요구사항으로 인해 이러한 장치에 부적합합니다.
**방법:** 본 연구에서는 이진 신경망(BNN)을 사용하여 리소스 효율적인 KWS를 위한 CL 접근 방식을 제안합니다. 사전 학습된 BNN 모델의 마지막 레이어만 업데이트하는 방식으로 7가지 CL 알고리즘을 평가했습니다.
**결과:** 모든 CL 알고리즘은 기존 키워드에 대한 정확한 분류 능력을 유지하면서 새로운 키워드를 성공적으로 학습하며, 최악의 경우에도 82% 이상의 정확도를 보였습니다. CL 단계의 계산 복잡도는 전체 모델의 순방향 패스에 비해 무시할 수 있는 수준이었습니다. 배치 기반 알고리즘은 좋은 성능을 위해 더 많은 학습 데이터를 필요로 하는 경향이 있습니다.
