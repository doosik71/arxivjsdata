# QUERY-BY-EXAMPLE KEYWORD SPOTTING SYSTEM USING MULTI-HEAD ATTENTION AND SOFTTRIPLE LOSS

Jinmiao Huang, Waseem Gharbieh, Han Suk Shim, Eugene Kim

## 🧩 Problem to Solve

음성 비서와의 대화 시작점인 키워드 탐지(KWS) 시스템에서, 사전에 정의된 키워드("Hey Siri", "Alexa") 대신 **사용자 정의 키워드를 탐지(User-defined KWS)**하는 것이 목표입니다. 이는 장치 개인화에 매력적이지만, 다음과 같은 문제에 직면합니다:

- 사용자 지정 발화가 학습 데이터 분포에서 벗어날 수 있습니다.
- 장치에서 효과적으로 실행하기 위한 낮은 지연 시간과 적은 메모리 사용량 요구사항을 충족해야 합니다.
  기존의 대규모 어휘 연속 음성 인식(LVCSR) 시스템은 어휘 외 키워드에서 성능 저하를 보이고 계산 오버헤드가 큽니다. 최근 **예시 기반 쿼리(Query-by-Example, QbyE)** 접근 방식이 사용자 정의 KWS에 유망한 대안으로 부상했지만, 그 성능을 더욱 향상시키는 것이 중요합니다.

## ✨ Key Contributions

이 논문은 사용자 정의 예시 기반 키워드 탐지(QbyE KWS)를 위한 신경망 아키텍처를 제안하며, 다음과 같은 주요 기여를 합니다:

- **효과적인 특징 추출:** 다중 계층 GRU 위에 **멀티 헤드 어텐션(Multi-head attention)** 모듈을 추가하여 모델 크기를 작게 유지하면서 특징을 효과적으로 추출합니다.
- **간단한 특징 통합 메커니즘:** 특징 추출기에서 얻은 정보를 집약하기 위해 간단한 **정규화된 멀티 헤드 어텐션(Normalized Multi-Head Attention, NMH-A) 계층**을 제안합니다.
- **Softtriple 손실의 적용:** 소프트맥스(softmax) 손실과 트리플릿(triplet) 손실의 장점을 결합한 **Softtriple 손실**을 사용하여 임베딩 공간에서 클래스 간 분리를 보장하고 그 효과를 입증합니다.
- **공개 데이터셋 벤치마크 제공:** Hey-Snips 공개 데이터셋에 대한 모델 성능을 시연하여 향후 연구를 위한 벤치마크를 제시합니다.

## 📎 Related Works

- **사전 정의 KWS:** 딥 뉴럴 네트워크 기반의 작은 풋프린트 KWS 시스템들([1], [2]).
- **사용자 정의 KWS 접근법:**
  - **LVCSR 기반:** [3]은 LVCSR 시스템을 사용했으나, 어휘 외 키워드와 계산 오버헤드 문제가 있었습니다.
  - **DTW 기반 QbyE:** 초기 QbyE 접근 방식은 동적 시간 왜곡(DTW)을 사용하여 키워드 샘플과 테스트 발화 간의 유사성을 결정했습니다([4], [5]).
  - **NN 기반 QbyE:** 변수 길이 오디오 신호를 고정 길이 벡터로 매핑하는 신경망 방식. LSTM 모델을 사용한 토큰 분류 및 임베딩 추출([6]), 비지도 학습 denoising seq2seq autoencoder([7]), seq2seq GRU autoencoder와 CNN-RNN 언어 모델([8]), CTC 빔 서치([9]), RNN 트랜스듀서 모델([10]), Siamese 네트워크와 트리플릿 힌지 손실([11]) 등이 있습니다.
  - **어텐션 기반 QbyE:** 어텐션 메커니즘을 QbyE 태스크에 적용한 연구들도 있습니다([12], [13]).
- **어텐션 메커니즘:** 이 논문은 Vaswani et al. [14]의 멀티 헤드 셀프 어텐션 메커니즘을 특징 추출에 활용합니다.
- **Softtriple Loss:** [15]에서 제안된 Softtriple 손실은 각 클래스에 여러 중심을 할당하여 트리플릿 샘플링 없이 딥 메트릭 학습을 가능하게 합니다.
- **데이터셋:** Hey-Snips 데이터셋 [16]이 공개 KWS 벤치마크로 사용됩니다.

## 🛠️ Methodology

이 시스템은 "인코더-디코더" 구조를 사용하여 QbyE 문제를 해결합니다. 인코더는 입력 신호를 임베딩 벡터로 매핑하고, 디코더는 입력 임베딩이 주어졌을 때 클래스 내(intra-class) 거리를 최소화하고 클래스 간(inter-class) 거리를 최대화하는 역할을 합니다. 추론 시에는 디코더가 사용되지 않고, 등록된 키워드 임베딩과 쿼리 임베딩 간의 거리를 계산합니다.

1. **Base Encoder:**

   - 입력 파형에서 25ms 윈도우와 12ms 스트라이드를 사용하여 160차원 **멜 필터뱅크(FBank) 특징**을 계산합니다.
   - FBank 특징은 배치 정규화 계층을 통과한 후 **다중 계층 GRU**에 입력되어 임베딩을 추출합니다.
   - GRU의 출력은 은닉 상태 벡터 시퀀스 $h = [h_1, h_2, ..., h_t]$이며, 각 $h_i \in \mathbb{R}^n$는 마지막 GRU 계층의 은닉 상태를 나타냅니다.

2. **Multi-Head Feature Extractor (MH-E):**

   - [14]의 멀티 헤드 셀프 어텐션 메커니즘을 GRU 출력 $h$ 위에 적용하여 추가적인 특징 추출기로 사용합니다.
   - 입력을 여러 부분 공간으로 투영하여 다른 위치의 특징들 간의 관계를 포착합니다.
   - 쿼리 $Q_j$, 키 $K_j$, 값 $V_j$는 $h$에 가중치 행렬 $W_{q_j}, W_{k_j}, W_{v_j}$를 곱하여 생성됩니다.
   - $Q_j$와 $K_j$ 간의 스케일드 닷 어텐션(scaled dot attention)으로 점수 $a_j$를 생성하고, $a_j$와 $V_j$를 곱하여 $x_j$를 얻습니다.
   - 모든 어텐션 헤드의 $x_j$를 연결하여 최종 출력 $x \in \mathbb{R}^{t \times md}$를 생성합니다.

3. **Normalized Multi-Head Feature Aggregator (NMH-A):**

   - $x$의 차원을 줄이기 위한 특징 통합기(feature aggregator)로 간단한 정규화된 멀티 헤드 어텐션 메커니즘을 제안합니다.
   - 학습 가능한 가중치 행렬 $W \in \mathbb{R}^{n \times m}$를 정의하고, 각 헤드의 쿼리 벡터 $q_j = W_j^T x$를 계산합니다.
   - **$L_2$ 정규화** $||W^*||_2 = 1$을 $W$에 적용하여, 어텐션 점수가 가중치 행렬의 노름(norm)에 지배되지 않도록 합니다.
   - 어텐션 모델은 다음 수식으로 구현됩니다:
     $$q_{ij} = \frac{\exp(W_j^{*T} x_i)}{\sum_{k=1}^t \exp(W_k^{*T} x_k)}$$
     $$v_j = \sum_{i=1}^t q_{ij} x_i$$
     여기서 $x \in \mathbb{R}^{t \times n}$, $W_j^* \in \mathbb{R}^n$, $q \in \mathbb{R}^{t \times m}$, $v \in \mathbb{R}^{m \times n}$ 입니다. $W^*$는 학습 가능한 파라미터입니다.
   - $q$를 시간 차원(time dimension)을 따라 소프트맥스 함수로 정규화하여 멀티 헤드 정렬 벡터를 생성합니다.
   - $q$와 $x$를 곱하여 가중치 벡터를 생성하고, 시간 차원을 따라 합산하여 멀티 헤드 가중치 합 벡터 $v = [v_1, v_2, ..., v_m]$를 생성합니다.
   - 최종 출력 벡터는 특징 차원(feature dimension)을 따라 $v_i$를 연결한 것입니다.

4. **Decoder with Softtriple Loss:**
   - 클래스 내 거리를 최소화하고 클래스 간 거리를 최대화하는 목표를 달성하기 위해 **Softtriple 손실** [15]을 사용합니다.
   - Softtriple 손실은 각 클래스에 여러 중심을 연결하여 트리플릿 샘플링의 어려움을 해결합니다.
   - 예시 $x_i$와 클래스 $c$ 간의 유사도 $S'_{i,c}$는 다음 수식(원본 논문에서 주어진 형태)으로 정의됩니다:
     $$S'_{i,c} = \frac{\sum_k \exp(\frac{1}{\gamma} x_i^T w_k^c)}{\sum_k \exp(\frac{1}{\gamma} x_i^T w_k^c)} x_i^T w_k^c$$
   - 손실 함수 $\mathcal{L}(x_i)$는 다음과 같습니다:
     $$\mathcal{L}(x_i) = -\log \frac{\exp(\lambda(S'_{i,y_i} - \delta))}{\exp(\lambda(S'_{i,y_i} - \delta)) + \sum_j \exp(\lambda S'_{i,j})}$$
   - 원래 Softtriple 손실은 과적합을 피하기 위해 중심 크기를 적응적으로 설정하는 정규화 장치를 포함하지만, 10k 클래스와 같은 대규모 태스크에서는 계산 집약적이므로 이 논문에서는 생략하고 많은 수의 중심을 사용합니다.
   - 실험을 통해 $\lambda$가 클 때 $\gamma$의 효과가 무시될 수 있음을 확인하여 $\gamma=1$로 설정합니다.

## 📊 Results

- **데이터셋:** Librispeech [19]로 학습된 모델을 내부 영어/한국어 데이터셋(50명의 화자, 8개 영어/11개 한국어 키워드, 24시간 TV 프로그램 부정 샘플), Hey-Snips [16] 공개 데이터셋으로 평가했습니다.
- **모델 구성:** 292k(작은 모델) 및 582k(큰 모델) 파라미터를 가진 두 가지 모델. MH-E에 20개 헤드, NMH-A에 15개 헤드. Softtriple 손실에서 중심 개수 6개, $\lambda=70$, $\delta=0.04$. (하이퍼파라미터는 ASHA를 사용한 대규모 검색으로 선정)
- **평가 결과:**
  - 582k 모델이 292k 모델보다 모든 데이터셋과 노이즈 조건에서 우수한 성능을 보였습니다.
  - 학습은 영어 발화로 진행되었지만, 깨끗한 한국어 데이터에서 깨끗한 영어 데이터보다 더 나은 성능을 보였습니다. 이는 한국어 발화 길이가 영어보다 평균적으로 길기 때문이며, 더 긴 발화가 탐지하기 쉽다는 경험적 관찰과 일치합니다. 노이즈가 있는 데이터에서의 성능은 두 언어에서 거의 유사했습니다.
  - Hey-Snips 데이터셋에서의 성능은 내부 데이터셋보다 좋지 않았는데, 이는 Hey-Snips 발화가 상대적으로 짧기 때문일 수 있습니다.
- **기준선 비교 (Table 1):** 제안된 모델은 [6]에서 제안된 GRU 기반 기준선 시스템(마지막 5개 타임스텝 사용)을 훨씬 뛰어넘는 성능을 보였습니다 (예: 내부 영어 데이터(Clean)에서 기준선(Small) FRR 17.87% 대비 제안 모델(Small) FRR 1.76% @ 0.3 FA/hour).
- **기여도 분석 (Ablation Study) (Table 2):** (FRR @ 0.3 FA/hour, 내부 영어 데이터, 10dB 잡음 조건)
  - **Softtriple 손실의 효과:** Softmax 손실을 Softtriple 손실로 대체했을 때, 작은 모델의 FRR이 8.5% 이상, 큰 모델의 FRR이 5% 감소하여 성능이 크게 향상되었습니다.
  - **MH-E의 효과:** Multi-Head attention feature Extractor (MH-E)를 추가하면 FRR이 약 2% 감소했습니다.
  - **NMH-A의 정규화 효과:** Multi-Head Aggregator에 $L_2$ 정규화를 적용하는 것(NMH-A)이 $L_2$ 정규화가 없는 MH-A나 [13]의 tanh 활성화 기반 어텐션보다 2% 이상 FRR을 추가로 감소시켰습니다.
  - **NMH-A의 헤드 개수:** 멀티 헤드 통합기에서 헤드 개수를 늘리는 것(1개 vs 15개)이 성능을 더욱 향상시켜, 모델이 어텐션 메커니즘의 다차원적인 측면에서 이점을 얻는다는 것을 시사합니다.

## 🧠 Insights & Discussion

- 이 논문은 사용자 정의 예시 기반 키워드 탐지(QbyE KWS)를 위한 강력하고 간단한 신경망 아키텍처를 제안합니다.
- 멀티 헤드 어텐션 기반 특징 추출기와 새로 제안된 정규화된 멀티 헤드 어텐션 특징 통합기의 조합이 효과적인 임베딩 학습에 중요함을 입증했습니다.
- 특히, 이미지 분류 태스크에서 검증된 Softtriple 손실을 KWS 문제에 적용하고 그 효과를 성공적으로 보여주었으며, 대규모 클래스 수에도 불구하고 간단화된 형태로 잘 작동했습니다.
- $L_2$ 정규화된 가중치를 사용하는 NMH-A와 다중 헤드의 사용이 모델 성능 향상에 핵심적인 역할을 함이 기여도 분석을 통해 명확히 드러났습니다.
- 모델은 학습 언어와 다른 언어(영어 학습 -> 한국어 평가)에서도 좋은 성능을 보여 언어에 대한 일반화 가능성을 시사합니다. 노이즈 환경에서도 강건한 성능을 보입니다.
- 이 연구는 Hey-Snips 공개 데이터셋에 대한 새로운 벤치마크를 설정하여 향후 연구의 비교 기반을 제공합니다.
- 주요 한계점은 Softtriple 손실의 정규화 장치 생략으로, 향후 대규모 데이터셋에서 계산 효율적인 대안을 모색할 수 있으며 짧은 발화에 대한 성능 개선도 향후 연구 과제입니다.

## 📌 TL;DR

이 논문은 사용자 정의 키워드 탐지(KWS)를 위한 **예시 기반 쿼리(QbyE)** 시스템을 제안합니다. **멀티 헤드 어텐션 기반 특징 추출기**와 새롭게 제안된 **정규화된 멀티 헤드 어텐션 특징 통합기**를 사용하여 효율적으로 특징을 추출하고 집약합니다. 임베딩 공간에서 클래스 간 분리를 강화하기 위해 **Softtriple 손실**을 적용했습니다. 제안된 모델은 내부 데이터셋과 Hey-Snips 공개 데이터셋에서 기준선 시스템을 크게 능가하는 성능을 보였으며, Softtriple 손실, 멀티 헤드 어텐션, 그리고 정규화된 통합기가 성능 향상에 필수적임을 기여도 분석을 통해 입증했습니다. 이는 언어 및 노이즈에 강건하며, 간단하면서도 강력한 QbyE KWS 시스템을 제공합니다.
