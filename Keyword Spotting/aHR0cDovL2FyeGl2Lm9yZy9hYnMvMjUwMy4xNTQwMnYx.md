# 스파이크 기반 시간 차이 인코더를 활용한 효율적인 키워드 스포팅 연구
Alejandro Pequeño-Zurro, Lyes Khacef, Stefano Panzeri, and Elisabetta Chicca

## 🧩 Problem to Solve
엣지 디바이스에서 음성 활성화 비서가 널리 사용되면서 키워드 스포팅(Keyword Spotting, KWS)의 중요성이 커지고 있습니다. 그러나 대상 임베디드 시스템의 극심한 저전력 제약으로 인해 KWS 시스템 배포가 제한됩니다. 기존 CPU/GPU 아키텍처는 에너지 소비 및 지연 시간 요구 사항을 충족하기 어렵고, 현재 인공지능 알고리즘과 하드웨어의 한계는 기술적으로 지속 불가능해질 것으로 예측됩니다. 따라서 에너지 효율적이고 실시간으로 작동하는 저전력 키워드 스포팅 시스템을 개발하는 것이 주요 과제입니다.

## ✨ Key Contributions
*   신경모방 프로세서에서 효율적인 키워드 스포팅을 위해 Temporal Difference Encoder (TDE) 뉴런 모델의 성능을 탐색했습니다.
*   주파수 변환된 음성 숫자의 스파이크 트레인(spike trains)이 시간 도메인에 많은 양의 정보를 포함하고 있음을 보여, 해당 작업에서 시간 인코딩의 중요성을 강조했습니다.
*   TDE 기반 Spiking Neural Network (SNN)가 재귀적 CuBa-LIF (Current-Based Leaky Integrate-and-Fire) 네트워크와 유사한 분류 정확도(TDE 89% vs. CuBa-LIF 91%)를 달성하면서도 훨씬 적은 시냅스 연산(synaptic operations, 92% 절감)을 수행함을 입증했습니다.
*   TDE 네트워크의 결과가 데이터셋 내 음성 키워드의 주파수 및 시간 스케일 특징과 높은 상관관계를 보이며 해석 가능성이 높음을 발견했습니다.
*   데이터 기반 네트워크 스케일링(가지치기) 방법론을 제시하고, TDE 네트워크가 더 적은 훈련 데이터셋에서도 강건한 성능을 보임을 확인했습니다.
*   신경모방 하드웨어의 에너지 소비를 더 정확하게 측정하기 위한 새로운 시냅스 연산(SynOps) 메트릭을 제안했습니다.

## 📎 Related Works
*   **엣지 AI 및 저전력 컴퓨팅:** IoT 혁명에 따른 실시간, 상시 작동 컴퓨팅 수요 증가와 엣지 디바이스의 에너지 제약 문제를 다룬 연구들(Murshed et al., 2021; Sulieman et al., 2022; Thompson et al., 2020, 2021; Rabaey et al., 2019).
*   **스파이킹 신경망(SNN) 및 신경모방 하드웨어:** SNN을 3세대 신경망으로 정의하고(Maass, 1997) IBM TrueNorth(Merolla et al., 2014) 및 Intel Loihi(Davies et al., 2018)와 같은 플랫폼에서 저전력/저지연 잠재력을 보여준 연구들(Khacef et al., 2018; Roy et al., 2019; Ivanov et al., 2022).
*   **음성 인식:** Mel Frequency Cepstrum Coefficients (MFCCs)를 사용한 Hidden Markov Model (HMM) 기반 접근 방식(Rabiner, 1989; Wong and Sridharan, 2001; Pan et al., 2018).
*   **포먼트(Formant) 분해:** 음성 인식 알고리즘 성능 향상을 위해 주파수 스펙트럼의 포먼트 분해를 활용한 연구들(Welling and Ney, 1998; Prica and Ilic, 2010; Stanek and Polak, 2013; Coath et al., 2014; Laszko, 2016).
*   **뇌의 시간 기반 인코딩:** 감각 피질 뉴런의 스파이크 타이밍이 행동 식별에 중요함을 강조한 연구들(Engineer et al., 2008; Yang et al., 2008; Zuo et al., 2015; Chong et al., 2020; Panzeri et al., 2010, 2014). 시각(Victor, 2000), 청각(Kayser et al., 2010), 체감각(Panzeri et al., 2001) 등 다양한 감각 양상에서 시간 관련 인코딩 메커니즘이 발견됨.
*   **TDE 모델:** 생체 시각 시스템의 광학 흐름 신경 회로에서 영감을 받은 모델(Milde et al., 2015; D’Angelo et al., 2020)로, 시각, 청각, 촉각, 후각 등 다양한 감각 양상으로 확장 가능성 탐구(Schoepe et al., 2021; Mastella and Chicca, 2021; Gutierrez-Galan et al., 2022).
*   **SNN 훈련 방법:** Back-Propagation Through Time (BPTT)과 서러게이트(surrogate) 경사하강법을 사용한 SNN 훈련(Paszke et al., 2017; Neftci et al., 2019; Zenke and Vogels, 2021).
*   **SNN 에너지 효율 측정:** 시냅스 연산(SynOps)을 에너지 소비의 프록시로 사용하는 연구들(Caccavella et al., 2023; Yik et al., 2023; Lemaire et al., 2022).

## 🛠️ Methodology
1.  **데이터셋 및 스파이크 인코딩:**
    *   **데이터셋:** TIdigits 음성 숫자 데이터셋(11개 클래스, 20 kHz 샘플링) 사용.
    *   **전처리:** 음성 신호에서 보컬 트랙의 공명 주파수를 나타내는 3개의 주요 포먼트(formant)를 추출. `Matlab`의 `sinewave speech analysis toolbox`를 활용.
    *   **주파수 대역화:** 0-4 kHz의 인간 음성 주파수 범위를 32개의 125Hz 폭 채널로 양자화. 각 채널은 3개 주요 포먼트의 최대 진폭을 시간 시퀀스로 인코딩.
    *   **스파이크 인코딩 (L0):** CuBa-LIF 뉴런으로 구성된 L0 계층에서 각 주파수 채널의 진폭 시퀀스를 스파이크 트레인으로 변환.

2.  **SNN 아키텍처 비교:**
    *   모든 네트워크는 3개의 계층(L0, L1, L2)으로 구성. L0(인코딩) 및 L2(출력) 계층은 모든 아키텍처에서 CuBa-LIF 뉴런으로 고정. L2는 11개 클래스 각각에 대한 뉴런을 포함하며, 가장 활성적인 뉴런이 최종 클래스로 디코딩됨.
    *   **L1 (은닉 계층) 구성:**
        *   **(i) TDE 네트워크:** L1이 TDE 뉴런으로 구성. 각 TDE 뉴런은 L0의 두 주파수 밴드 입력(촉진/트리거)에 연결.
        *   **(ii) LIFrec 네트워크:** L1이 CuBa-LIF 뉴런과 L1 내 재귀적 연결로 구성.
        *   **(iii) LIF 네트워크:** L1이 CuBa-LIF 뉴런과 순방향 연결만으로 구성.
    *   **네트워크 규모 균형:** 서로 다른 아키텍처 간의 비교를 위해 L1 계층의 뉴런 수를 조정하여 총 시냅스 연결 수(하드웨어 구현의 메모리 사용량 프록시)를 동일하게 유지.

3.  **오프라인 훈련 (BPTT):**
    *   **훈련 방법:** Back-Propagation Through Time (BPTT)에 기반한 지도 학습 방법을 사용하여 네트워크 파라미터 최적화. 스파이크 불연속성 문제를 우회하기 위해 미분 가능한 `fast sigmoid` 서러게이트 함수($\sigma(U^{(l)}_{i}) = \frac{U^{(l)}_{i}}{1 + \lambda|U^{(l)}_{i}|}$) 사용.
    *   **손실 함수:** 출력 계층(L2)의 스파이크 수를 기반으로 `cross-entropy loss` 함수 사용.
    *   **옵티마이저:** `ADAM` 옵티마이저를 사용하며, `dropout` 및 `weight decay`를 적용하여 과적합 방지 및 일반화 속도 향상.
    *   **훈련 파라미터:** TDE 네트워크는 $\tau_{g}$ (TDE 세포의 시간 상수)와 $W_{2}$를 훈련. LIFrec 네트워크는 $W_{1}$, $W_{11}$ (재귀적), $W_{2}$를 훈련. LIF 네트워크는 $W_{1}$, $W_{2}$를 훈련.
    *   **성능 측정:** 25개의 최고 테스트 정확도 결과를 평균하여 보고.

4.  **성능 비교:**
    *   **컴퓨팅 복잡성:** 시냅스 연산(SynOps) 수를 측정하여 에너지 소비량 예측. `SynOps = $\sum_{j} S_{in,j} + \sum_{j} S_{out,j}$`로 정의하며, 뉴런 입력 스파이크 통합 및 출력 스파이크 발생 연산을 모두 포함.
    *   **해석 가능성:** TDE 네트워크의 $W_{2}$ 연결 행렬과 훈련된 $\tau_{g}$ 값을 분석하여 각 키워드 클래스에 기여하는 주파수 쌍 및 시간 상수를 식별.
    *   **데이터 기반 가지치기:** L0에서 포먼트의 스파이크 인코딩에 대한 주파수 쌍의 교차 상관 관계를 측정하여 L1의 TDE 뉴런을 가지치기하고 네트워크 크기 조정을 안내.

## 📊 Results
*   **시간 패턴 vs. 비율 정보:** 인코딩된 스파이크 데이터는 스파이크 비율($I_{rate}$) 정보만큼이나 스파이크 타이밍($I_{pattern}$) 정보를 풍부하게 포함하고 있음을 확인. 특히 작은 시간 창에서는 스파이크 타이밍 정보가 더 중요하거나 유사한 양의 정보를 전달함. 이는 시간 정보를 활용하는 SNN의 중요성을 뒷받침합니다.
*   **데이터 기반 TDE 네트워크 아키텍처:**
    *   교차 상관 관계 분석을 통해 데이터 기반 가지치기가 가능하며, TDE 네트워크에서 45.5%의 뉴런 및 연결을 줄였을 때 정확도 손실이 1.22%에 불과했습니다 (992개 셀에서 540개 셀로 감소 시 89.08%에서 87.86%로 감소). 데이터 기반 가지치기는 무작위 가지치기보다 항상 우수한 성능을 보였습니다.
*   **분류 정확도 및 추론 효율성:**
    *   동일한 시냅스 연결 수(약 7000개)로 균형을 맞춘 네트워크에서 LIFrec 네트워크가 가장 높은 정확도(90.04%)를 보였고, TDE 아키텍처(87.86%)가 그 뒤를 이었습니다. 순방향 LIF 네트워크는 71.59%로 가장 낮은 성능을 보였습니다.
    *   **스파이크 수:** TDE 네트워크는 LIFrec 네트워크에 비해 전체적으로 53% 적은 스파이크를 생성했습니다.
    *   **시냅스 연산(SynOps):** TDE 네트워크는 LIFrec 네트워크보다 92% 적은 시냅스 연산을 수행했습니다. 이는 L1 계층뿐만 아니라 L2 계층에서도 상당한 SynOps 감소를 가져와 TDE의 압도적인 에너지 효율성을 보여줍니다.
*   **훈련 데이터 효율성:**
    *   훈련 데이터셋 크기를 줄였을 때, TDE 아키텍처는 LIFrec 아키텍처보다 더 큰 강건성(robustness)을 보였습니다. 훈련 데이터셋을 75%로 줄였을 때 TDE는 통계적으로 유의미한 성능 변화가 없었으나, LIFrec은 통계적으로 유의미한 감소를 보였습니다. 이는 TDE 아키텍처가 더 작은 데이터셋에서도 강하고 빠르게 일반화됨을 시사합니다.
*   **네트워크 파라미터 해석:**
    *   TDE 네트워크는 L1의 모든 TDE 셀이 고유한 주파수 쌍과 $\tau_{g}$ (시간 상수)에 연결되어 있어 해석 가능성이 높습니다.
    *   훈련된 $W_{2}$ 가중치를 통해 각 키워드 클래스에 가장 크게 기여하는 주파수 쌍과 $\tau_{g}$ 분포를 식별할 수 있었습니다. 각 클래스에 대한 특징은 명확하게 구분되었고, 주파수 대역의 절반 이하에 집중되어 있었습니다 (0-1500 Hz).
    *   흥미롭게도, 훈련된 네트워크가 분류에 사용하는 가장 관련성 높은 주파수 쌍은 초기 데이터 분석에서 가장 높은 교차 상관 관계를 보인 주파수 쌍과 일치하지 않는 경우가 많았습니다. 이는 네트워크가 단순한 동시성 검출을 넘어 복잡한 특징을 학습한다는 것을 의미합니다.

## 🧠 Insights & Discussion
*   **포먼트 분해의 중요성:** 음성 인식 작업에서 포먼트 분해는 매우 효과적이며, 생체학적 달팽이관과 유사한 주파수 분해 기능을 가진 신경모방 장치와의 연계 가능성을 제시합니다.
*   **시간 정보의 활용:** 제안된 데이터셋에서 스파이크의 시간 패턴에 중요한 정보가 포함되어 있음이 확인되었고, 이는 TDE 및 재귀적 LIF 네트워크와 같이 시간 정보를 활용하는 SNN의 성능 우수성을 뒷받침합니다.
*   **TDE의 에너지 효율성:** TDE 네트워크는 재귀적 LIF 네트워크보다 약간 낮은 정확도에도 불구하고 시냅스 연산에서 92%의 극적인 감소를 보여, 엣지 디바이스와 더 깊은 신경망 아키텍처에 매우 유리한 에너지 효율성을 제공합니다. 이는 TDE 뉴런 모델의 복잡성 증가가 시냅스 연결 수 감소로 상쇄되어 전체적인 연산 이점을 가져옴을 의미합니다.
*   **SynOps 메트릭의 개선:** 입력 스파이크 통합뿐만 아니라 출력 스파이크 생성 연산까지 포함하는 SynOps 측정은 신경모방 하드웨어의 실제 에너지 소비를 더 정확하게 예측할 수 있습니다.
*   **확장성 및 강건성:** TDE의 고정된 입력 연결은 네트워크의 데이터 기반 스케일링을 가능하게 하고, 더 작은 훈련 데이터셋에서도 LIFrec보다 더 강건하며 빠르게 일반화되는 훈련 효율성을 제공합니다.
*   **해석 가능성:** TDE 모델은 학습된 파라미터(주파수 쌍, $\tau_{g}$)와 데이터셋의 핵심 특징 간의 직접적인 상응 관계를 제공하여 일반적인 신경망 훈련의 주요 과제인 해석 가능성을 크게 향상시킵니다.
*   **향후 연구 방향:** 각 네트워크 아키텍처에 대한 최적화된 하이퍼파라미터 탐색, 데이터셋의 시간 정밀도(binning) 조절을 통한 시간 패턴 정보 극대화, 그리고 Dynaps나 Texel과 같은 신경모방 하드웨어 플랫폼에서의 실제 구현 및 에너지 소비 검증이 필요합니다.

## 📌 TL;DR
**문제:** 엣지 디바이스를 위한 저전력 고효율 키워드 스포팅 시스템이 필요합니다.
**방법:** 음성 포먼트 분해를 통해 인코딩된 스파이크 데이터를 사용하고, 은닉 계층에 Temporal Difference Encoder (TDE) 뉴런을 사용하는 Spiking Neural Network (SNN)를 제안했습니다. 이 TDE 네트워크를 순방향 및 재귀적 Current-Based Leaky Integrate-and-Fire (CuBa-LIF) 네트워크와 비교하며, Back-Propagation Through Time (BPTT)으로 훈련했습니다.
**결과:** TDE 네트워크는 재귀적 LIF 네트워크와 유사한 정확도(89% vs. 91%)를 달성하면서도 시냅스 연산(SynOps)을 92% 절감하여 탁월한 에너지 효율성을 보였습니다. 또한, TDE 네트워크는 더 적은 훈련 데이터셋에서도 강건했으며, 학습된 파라미터가 키워드의 주파수-시간 스케일 특징과 직접 연관되어 높은 해석 가능성을 제공했습니다. 이는 TDE가 시공간 패턴의 효율적인 이벤트 기반 처리를 위한 유망한 뉴런 모델임을 시사합니다.