# Few-Shot Keyword Spotting from Mixed Speech

Junming Yuan, Ying Shi, LanTian Li, Dong Wang, Askar Hamdulla

## 🧩 Problem to Solve

이 논문은 소수의 학습 샘플만으로 이전에 알려지지 않은 키워드를 탐지하는 소수 데이터 키워드 탐지(Few-Shot Keyword Spotting, KWS)와, 여러 키워드 및 화자가 동시에 발화하는 혼합 음성(Mixed Speech) 환경에서의 KWS라는 두 가지 실제적인 문제를 동시에 해결하는 것을 목표로 합니다. 기존의 소수 데이터 KWS 연구들은 주로 단일 화자 환경에 집중했으며, 혼합 음성 환경에서는 효과가 불분명했습니다.

## ✨ Key Contributions

- 소수 데이터 KWS 및 혼합 음성 환경 모두에서 믹스 트레이닝(Mix-Training, MT) 접근 방식의 적용 가능성을 성공적으로 입증했습니다.
- MT가 사전 학습 단계 또는 미세 조정 단계에서 사용될 때 이 태스크에 매우 효과적임을 보여주었습니다.
- 자기 지도 학습(SSL) 기반의 대규모 사전 학습 모델(HuBert)과 MT 미세 조정을 결합했을 때, 모든 테스트 조건에서 매우 강력한 성능을 달성함을 발견했습니다.
- MT 사전 학습이 혼합 음성 문제를 효과적으로 해결할 수 있으며, 특히 MT 미세 조정과 결합될 때 그 효과가 증대됨을 입증했습니다.

## 📎 Related Works

- **소수 데이터 KWS:**
  - 데이터 증강: [9, 10]
  - 메타 학습: MAML [11], 프로토타입 네트워크 [12, 13, 27]
  - 모델 사전 학습: 멀티-헤드 구조 [14], EfficientNet [16], Wav2Vec2.0과 같은 SSL 모델 [19, 20]
  - 레이블이 없는 데이터 활용 [19, 20, 21]
- **혼합 음성 환경에서의 KWS:**
  - 데이터 증강(DA): 잡음 추가 방식 [23, 29]
  - 믹스업(Mixup): 두 키워드를 선형 보간하여 혼합 [25]. Manifold Mixup [30], CutMix [31], PixMix [32], CoMixup [33] 등 다양한 변형이 있습니다.
  - 믹스 트레이닝(Mix-training, MT): Shi et al. [26]이 제안했으며, 혼합된 음성 신호에서 k개의 키워드가 존재함을 나타내는 k-핫 레이블을 사용하여 저에너지 키워드 탐지 문제를 해결합니다.

## 🛠️ Methodology

이 연구는 사전 학습-미세 조정(pre-training and fine-tuning) 프레임워크를 기반으로 하며, 혼합 음성 환경을 위한 Mix-training (MT) 전략을 통합합니다.

1. **믹스 트레이닝 (MT) 전략 재정의:**
   - **레이블 통합 (Label Union):** 혼합된 음성 신호에서 $k$개의 키워드가 존재함을 나타내기 위해 $k$-핫 레이블을 사용합니다.
     $$x_{\text{mixed}} = \omega_1 x_i + \omega_2 x_j$$
     $$y_{\text{mixed}} = y_i \oplus y_j$$
     여기서 $x_i, x_j$는 두 개의 음성 샘플, $y_i, y_j$는 그들의 원-핫(one-hot) 레이블, $\omega_1, \omega_2$는 무작위 변수이며, $\oplus$는 논리 합(logical addition)을 나타냅니다.
   - **균등 샘플링 (Uniform Sampling):** $\omega_1, \omega_2$는 $(0.1, 0.9)$ 범위의 균등 분포에서 개별적으로 샘플링된 후, $\omega_1 + \omega_2 = 1$이 되도록 정규화됩니다. 이는 혼합된 음성 구성 요소의 에너지가 다양하게 조절될 수 있도록 합니다.
   - **손실 함수:** 각 키워드에 해당하는 이진 분류기(binary classifier)와 이진 교차 엔트로피(Binary Cross-Entropy, BCE) 손실을 사용하여 모델을 훈련합니다.
2. **MT 사전 학습:**
   - 대규모 키워드 세트와 많은 샘플을 사용하여 KWS 모델을 사전 학습합니다.
   - EfficientNet-B0 아키텍처를 임베딩 백본(embedding backbone)으로 사용하고, 사전 학습 세트의 각 단어에 대한 사후 확률을 예측하는 선형 레이어를 추가합니다.
   - LibriSpeech 960 코퍼스를 사전 학습 데이터셋으로 활용하여 약 100만 개의 (오디오, 키워드) 쌍을 구축합니다.
3. **미세 조정:**
   - 사전 학습된 EfficientNet-B0 백본의 파라미터는 고정하고, 새로운 키워드(Google Speech Command v2 데이터셋의 10개 키워드)를 탐지하기 위해 새로운 선형 레이어를 초기화하고 미세 조정합니다.
   - 소수 데이터 환경(50-샷, 30-샷, 15-샷)을 시뮬레이션하기 위해 GSC v2 훈련 세트에서 각 단어당 샘플을 무작위로 추출하여 미세 조정 데이터셋을 구성합니다.
4. **훈련 전략 조합:** 사전 학습 및 미세 조정 단계 모두에서 Clean Training, Mixup Training, Mix Training의 세 가지 훈련 전략을 조합하여 총 9가지 시나리오를 비교 분석합니다.
5. **SSL 모델 비교:** Wav2Vec 2.0 및 HuBert와 같은 대규모 SSL 모델을 사전 학습 모델로 사용하여 제안된 방법과 성능을 비교합니다.

## 📊 Results

- **사전 학습 결과:**
  - **클린 테스트(Test.clean):** Mixup이 MT보다 약간 우수한 성능을 보였습니다 (Top-1 ACC).
  - **혼합 테스트(Test.mixed):** MT가 Mixup에 비해 혼합 음성 환경에서 Top-2 ACC에서 뚜렷한 성능 우위를 보였습니다. 이는 MT가 혼합된 신호에서 클린 음성 패턴을 추출하는 데 효과적임을 시사합니다.
- **소수 데이터 KWS 결과 (미세 조정):**
  - **전반적 경향:** 어떤 사전 학습 모델을 사용하든지, MT 미세 조정이 Clean/Mixup 미세 조정에 비해 뚜렷한 이점을 보였습니다. 특히 2-Mix 테스트에서 이러한 이점이 더욱 두드러졌습니다.
  - **사전 학습 모델 비교:**
    - **Clean/Mixup/MT 사전 학습:** Clean 테스트에서는 Mixup 사전 학습 모델이 가장 좋았지만, 2-Mix 테스트에서는 MT 사전 학습 모델이 훨씬 우수했습니다.
    - **SSL 모델 (Wav2Vec2.0 및 HuBert):**
      - 클린 테스트에서는 두 SSL 모델 모두 매우 우수한 성능을 보였고, 미세 조정 전략에 따른 성능 차이는 미미했습니다.
      - 2-Mix 테스트에서는 Wav2Vec2.0 모델이 크게 실패한 반면, HuBert 모델은 강력한 결과를 보였습니다. 이는 두 SSL 모델 간의 혼합 음성 처리 능력에 상당한 차이가 있음을 나타냅니다.
      - **HuBert + MT 미세 조정** 조합이 모든 테스트 조건(클린 및 혼합 음성)에서 보편적으로 우수한 결과를 달성하며, 가장 강력한 솔루션임을 입증했습니다.
  - **MT + MT 전략의 잠재력:** MT 사전 학습과 MT 미세 조정을 결합한 전략(MT + MT)은 HuBert + MT에 비해 모델 크기와 학습 데이터 양이 훨씬 적었음에도 불구하고 매우 근접한 성능을 보이며 큰 잠재력을 시사했습니다.

## 🧠 Insights & Discussion

- **MT의 중요성:** MT는 소수 데이터 KWS, 특히 혼합 음성 환경에서 필수적인 기술이며, 미세 조정 단계에서 항상 사용하는 것이 유리합니다.
- **사전 학습 전략의 선택:** 사전 학습 단계에서 MT를 사용할지 여부는 예상되는 테스트 조건(혼합 음성 비중)에 따라 달라집니다. 혼합 음성 비중이 높으면 MT가 확실히 유리하지만, 그렇지 않으면 Mixup이 더 나은 사전 학습 전략일 수 있습니다.
- **SSL 모델과의 시너지:** HuBert와 같은 대규모 자기 지도 학습 모델과 MT 미세 조정을 결합하면 클린 및 혼합 음성 환경 모두에서 탁월한 성능을 달성할 수 있는 강력하고 보편적인 솔루션을 제공합니다.
- **Wav2Vec2.0과 HuBert의 차이:** 두 SSL 모델의 혼합 음성 처리 능력에서 상당한 차이가 관찰되었으며, 이는 향후 추가 연구가 필요한 부분입니다.
- **MT 사전 학습의 잠재력:** MT 사전 학습은 현재 실험 환경에서 작은 모델과 적은 데이터로도 매우 유망한 결과를 보여주었으며, 향후 대규모 모델 구조 또는 자기 지도 학습 기반의 MT에 대한 추가 연구의 가치가 있습니다.

## 📌 TL;DR

이 논문은 제한된 샘플과 혼합 음성 환경에서 키워드를 탐지하는 소수 데이터 키워드 탐지(KWS) 문제를 다룹니다. 연구진은 믹스 트레이닝(MT) 전략을 사전 학습 및 미세 조정 프레임워크에 통합하여 이 문제를 해결하고자 했습니다. 실험 결과, MT는 혼합 음성 환경에서 KWS 성능을 크게 향상시키는 데 매우 효과적임을 입증했습니다. 특히, HuBert와 같은 대규모 자기 지도 학습(SSL) 모델과 MT 미세 조정을 결합한 방법이 클린 및 혼합 음성 환경 모두에서 탁월한 성능을 달성하는 강력한 솔루션임을 보여주었습니다.
