{
  "title": "Orthogonality Constrained Multi-Head Attention For Keyword Spotting",
  "authors": "Mingu Lee, Jinkyu Lee, Hye Jin Jang, Byeonggeun Kim, Wonil Chang, Kyuwoong Hwang",
  "year": 2019,
  "url": "http://arxiv.org/abs/1910.04500v1",
  "abstract": "Multi-head attention mechanism is capable of learning various representations\nfrom sequential data while paying attention to different subsequences, e.g.,\nword-pieces or syllables in a spoken word. From the subsequences, it retrieves\nricher information than a single-head attention which only summarizes the whole\nsequence into one context vector. However, a naive use of the multi-head\nattention does not guarantee such richness as the attention heads may have\npositional and representational redundancy. In this paper, we propose a\nregularization technique for multi-head attention mechanism in an end-to-end\nneural keyword spotting system. Augmenting regularization terms which penalize\npositional and contextual non-orthogonality between the attention heads\nencourages to output different representations from separate subsequences,\nwhich in turn enables leveraging structured information without explicit\nsequence models such as hidden Markov models. In addition, intra-head\ncontextual non-orthogonality regularization encourages each attention head to\nhave similar representations across keyword examples, which helps\nclassification by reducing feature variability. The experimental results\ndemonstrate that the proposed regularization technique significantly improves\nthe keyword spotting performance for the keyword \"Hey Snapdragon\".",
  "citation": 13
}