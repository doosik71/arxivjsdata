# CONVMIXER: FEATURE INTERACTIVE CONVOLUTION WITH CURRICULUM LEARNING FOR SMALL FOOTPRINT AND NOISY FAR-FIELD KEYWORD SPOTTING

Dianwen Ng, Yunqi Chen, Biao Tian, Qiang Fu, Eng Siong Chng

## 🧩 Problem to Solve

키워드 스포팅(KWS) 시스템을 스마트 기기와 같은 저사양 기기에 배포하기 위해서는 효율적인 아키텍처 구축이 필수적입니다. 그러나 경량 모델은 시끄러운 원거리(far-field) 환경(낮은 신호 대 잡음비, 잔향 포함)에서 노이즈 강건성을 달성하기 어렵다는 문제가 있습니다. 기존의 어텐션 기반 모델(예: 트랜스포머)은 성능이 좋지만, 높은 계산 및 메모리 복잡도로 인해 소형 장치에 적합하지 않습니다. 본 연구는 적은 계산량으로 노이즈에 강건한 경량 KWS 모델을 구축하는 것을 목표로 합니다.

## ✨ Key Contributions

- **ConvMixer 모델 제안**: 약 100K개의 파라미터만으로 노이즈가 많은 원거리 환경에 대응하는 새로운 특징 상호작용 컨볼루션 모델인 ConvMixer를 제안합니다.
- **효율적인 믹서 유닛 도입**: 어텐션 모듈을 대체하여 전역 채널의 가중 특징 상호작용을 계산하는 믹서 유닛(MLP 기반)을 제안, 정보 흐름을 더욱 효율적으로 촉진합니다.
- **커리큘럼 기반 다중 조건 훈련**: 모델의 노이즈 강건성을 높이기 위해 커리큘럼 기반 다중 조건 훈련 전략을 채택하여 기존 다중 조건 학습 방식보다 우수한 성능을 달성합니다.
- **SOTA 성능 달성**: Google Speech Command V2-12 데이터셋에서 98.2%의 SOTA 정확도를 달성하며, 설계된 노이즈 조건에서 훨씬 큰 트랜스포머 모델과도 경쟁력 있는 성능을 보여줍니다.

## 📎 Related Works

- **경량 키워드 스포팅 (Small Footprint KWS)**: 심층 신경망(DNN) [9], 컨볼루션 신경망(CNN) [10]이 KWS에 효과적임이 입증되었으며, Depthwise Separable Convolution (DWS) [11, 12]을 통해 메모리 및 계산 효율성이 크게 향상되었습니다.
- **노이즈 강건한 음성 모델 (Noise Robust Speech Model)**: 다중 조건 훈련이 노이즈 강건성을 위한 단순한 전략으로 사용되었지만, 매우 낮은 SNR(예: -10 dB)에서 깨끗한 신호까지 다양한 노이즈 범위에서는 비효율적입니다 [13]. 커리큘럼 학습 [13, 14]은 깨끗하거나 높은 SNR 오디오에서 시작하여 점진적으로 노이즈 수준을 높이는 방식으로 노이즈 강건성을 향상시키는 효과적인 방법으로 제안되었습니다.
- **어텐션 메커니즘**: 오디오 네트워크의 효율성을 높이는 데 사용되었으며 [2, 5, 6], 트랜스포머와 같은 Self-attention 모델 [7, 8]은 컨볼루션 네트워크-어텐션 하이브리드 모델을 능가하는 성능을 보였으나 높은 계산 및 메모리 복잡성이 단점입니다.
- **MLP-Mixer**: 어텐션 대신 토큰 채널별 혼합(mixing)을 통해 특징 통신을 수행하는 방법이 제안되었습니다 [17, 18].

## 🛠️ Methodology

ConvMixer 네트워크는 크게 세 부분으로 구성됩니다: Pre-convolutional block, Convolution-mixer block, Post-convolutional block.

1. **모델 아키텍처 (ConvMixer)**:

   - **Depthwise Separable (DWS) Convolution**: 적은 파라미터로 효율적인 계산을 위해 DWS 컨볼루션을 기반으로 설계되었습니다.
   - **Pre 및 Post 컨볼루션 블록**: 1D DWS, Batch Normalization, Swish 활성화 함수로 구성됩니다.
   - **ConvMixer 블록**:
     - 이전 채널 $\times$ 시간 특징을 입력으로 받습니다.
     - **주파수 도메인 추출**: 2D DWS와 2D 컨볼루션을 통해 주파수 도메인 정보를 추출합니다.
     - **차원 유지**: Pointwise 컨볼루션을 사용하여 이전 입력과 동일한 형태로 압축합니다.
     - **시간 도메인 특징 추출**: 1D DWS 블록을 통해 시간 도메인 특징을 추출합니다.
     - **믹서 레이어**: 전역 특징 채널을 통해 정보 흐름을 허용합니다.
     - **스킵 연결 (Skip Connections)**: 이전 출력과 2D 특징을 블록의 출력에 연결합니다.
     - ConvMixer 블록의 연산은 다음 방정식으로 표현됩니다:
       $$z = \sigma \circ f_1(\sigma \circ f(x))$$
       $$y_1 = \sigma \circ \text{BatchNorm}(f(z))$$
       $$y_2 = \sigma \circ \text{BatchNorm}(f_2(y_1))$$
       $$\tilde{y} = x + y_1 + f_3(y_2)$$
       여기서 $f_1$은 2D-DWS, $f$는 2D 컨볼루션 함수, $f_2$는 1D-DWS, $f_3$은 믹서 레이어, $\sigma$는 Swish 활성화 함수를 나타냅니다.

2. **믹서 레이어 (Mixer Layer)**:

   - 어텐션 레이어의 대안으로, 훨씬 효율적인 계산을 제공합니다.
   - 두 가지 유형의 다층 퍼셉트론(MLP)을 활용하여 특징 공간 간의 상호작용을 유도합니다:
     - **시간 채널 혼합 (Temporal Channel Mixing)**: 모든 주파수 $i$에 걸쳐 공유되는 MLP.
     - **주파수 채널 혼합 (Frequency Channel Mixing)**: 모든 시간 $j$에 걸쳐 공유되는 MLP (변환 과정 포함).
   - 각 MLP는 두 개의 선형 레이어와 GELU 활성화 유닛으로 구성됩니다.
   - 수학적으로는 다음과 같이 정의됩니다:
     $$u^{*,i} = x^{*,i} + W_2 \cdot \delta(W_1 \cdot \text{LayerNorm}(x)^{*,i})$$
     $$y^{j,*} = u^{j,*} + W_4 \cdot \delta(W_3 \cdot \text{LayerNorm}(u)^{j,*})$$
     여기서 $\delta$는 GELU 유닛을, $W_1, W_2$는 시간 채널에 대한 학습 가능한 가중치를, $W_3, W_4$는 주파수 채널에 대한 학습 가능한 가중치를 나타냅니다.

3. **커리큘럼 기반 다중 조건 훈련 (Curriculum Based Multi-condition Training)**:
   - **점진적 학습 단계**: 훈련 과정은 점진적으로 난이도가 높아지는 다섯 단계로 나뉩니다.
     1. 노이즈 없는 깨끗한 샘플로 훈련.
     2. 깨끗한 샘플 + 0dB SNR 노이즈 추가.
     3. 깨끗한 샘플 + 0dB SNR + -5dB SNR 노이즈 추가.
     4. 깨끗한 샘플 + 0dB SNR + -5dB SNR + -10dB SNR 노이즈 추가.
     5. 데이터셋의 절반에 Room Impulse Response (RIR) 데이터를 적용하여 원거리 오디오 추가.
   - **진행 기준**: 각 단계에서 검증 정확도와 손실을 정규화하여 진행 기준 $c = \text{Norm}(\text{acc}_m) - \text{Norm}(\text{loss}_m)$을 계산합니다.
   - 만약 $c$가 10 epoch 동안 현재 최고 기준보다 높아지지 않으면, 이전 단계의 최적 모델을 로드하고 다음 난이도 단계로 진행합니다.

## 📊 Results

- **Google Speech Command V2-12 (공식 데이터셋)**: ConvMixer는 98.2%의 SOTA 정확도를 달성했으며, MatchboxNet-6x2x64 (140K 파라미터)와 유사한 경량 모델 중에서 가장 우수했습니다. 또한, 훨씬 큰 KWT-1 (607K 파라미터), AST-Tiny (5.8M 파라미터) 모델에 비해서도 적은 파라미터(119K)와 MACs(22.2M)로 경쟁력 있는 성능을 보였습니다.
- **노이즈가 많은 원거리 조건**:
  - 기존 다중 조건 훈련 방식의 ConvMixer는 유사한 메모리 크기의 MatchboxNet 대비 3%의 절대적인 성능 향상을 보였습니다.
  - 커리큘럼 기반 훈련이 적용된 ConvMixer†는 MatchboxNet 대비 7.4%의 성능 향상을 달성하여, 이 조건에서 훨씬 큰 트랜스포머 기반 모델(KWT-3, AST-Tiny)과도 경쟁력 있는 결과를 보였습니다. 예를 들어, -10dB SNR에서 ConvMixer†는 71.88%의 정확도를 기록하여 AST-Tiny† (72.32%) 및 KWT-3 (71.08%)와 거의 동등한 수준이었습니다.
- **어블레이션 연구 (Ablation Studies)**:
  - **MLP 믹서의 중요성**: MLP 믹서 레이어를 제거했을 때, 노이즈가 많은 원거리 조건에서 정확도가 약 7% 감소하여, MLP 믹서가 모델의 강건성 확보에 크게 기여함을 입증했습니다.
  - **커리큘럼 학습의 효과**: AST-Tiny에 커리큘럼 학습을 적용했을 때 (AST-Tiny†) 정확도가 약 3% 향상되었습니다. ConvMixer에 커리큘럼 학습을 적용했을 때 (ConvMixer†)는 특히 낮은 SNR에서 약 5.5%의 정확도 향상을 보여, 작은 모델에서 커리큘럼 학습이 더욱 효과적임을 시사했습니다.

## 🧠 Insights & Discussion

- ConvMixer는 MLP 믹서를 통한 특징 상호작용으로 효율적인 컨볼루션 아키텍처가 KWS 작업에서 어텐션 기반 트랜스포머 모델과 효과적으로 경쟁할 수 있음을 보여줍니다. 특히 리소스 제약이 있는 환경에 강점이 있습니다.
- MLP 믹서는 기존 어텐션보다 훨씬 낮은 계산 비용으로 정보 흐름을 효과적으로 촉진하여, 시끄럽고 원거리 조건에서 모델의 노이즈 강건성을 크게 향상시킵니다.
- 커리큘럼 기반 다중 조건 훈련은 노이즈 강건성을 높이는 데 매우 효과적인 전략이며, 특히 경량 모델과 낮은 SNR 시나리오에서 학습 효율을 높입니다.
- 본 연구에서 제안된 시스템은 경량 KWS 분야에서 SOTA 성능을 달성했으며, 50배 더 많은 파라미터를 사용하는 모델과도 견줄 만한 성능을 보여 실제 에지 디바이스 배포 및 노이즈 환경에서의 응용 가능성을 강조합니다.

## 📌 TL;DR

**문제**: 경량 키워드 스포팅(KWS) 모델은 적은 리소스 제약 하에 시끄럽고 원거리 환경에서 노이즈 강건성을 달성하기 어렵습니다.
**방법**: 본 연구는 어텐션 모듈을 대체하는 MLP 기반의 특징 상호작용 믹서 유닛을 갖춘 효율적인 ConvMixer 모델을 제안합니다. 또한, 노이즈 강건성을 향상시키기 위해 점진적으로 난이도를 높이는 커리큘럼 기반 다중 조건 훈련 전략을 적용합니다.
**결과**: ConvMixer는 119K개의 파라미터로 Google Speech Command V2-12에서 98.2%의 SOTA 정확도를 달성했습니다. 특히, 시끄러운 원거리 조건에서 기존 경량 모델 대비 최대 7.4%의 성능 향상을 보였으며, 50배 더 큰 트랜스포머 기반 모델과도 견줄 만한 강건한 성능을 입증했습니다.
