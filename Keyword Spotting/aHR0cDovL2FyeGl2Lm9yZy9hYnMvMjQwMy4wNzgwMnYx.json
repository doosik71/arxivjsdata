{
  "title": "Boosting keyword spotting through on-device learnable user speech\n  characteristics",
  "authors": "Cristian Cioflan, Lukas Cavigelli, Luca Benini",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.07802v1",
  "abstract": "Keyword spotting systems for always-on TinyML-constrained applications\nrequire on-site tuning to boost the accuracy of offline trained classifiers\nwhen deployed in unseen inference conditions. Adapting to the speech\npeculiarities of target users requires many in-domain samples, often\nunavailable in real-world scenarios. Furthermore, current on-device learning\ntechniques rely on computationally intensive and memory-hungry backbone update\nschemes, unfit for always-on, battery-powered devices. In this work, we propose\na novel on-device learning architecture, composed of a pretrained backbone and\na user-aware embedding learning the user's speech characteristics. The\nso-generated features are fused and used to classify the input utterance. For\ndomain shifts generated by unseen speakers, we measure error rate reductions of\nup to 19% from 30.1% to 24.3% based on the 35-class problem of the Google\nSpeech Commands dataset, through the inexpensive update of the user\nprojections. We moreover demonstrate the few-shot learning capabilities of our\nproposed architecture in sample- and class-scarce learning conditions. With\n23.7 kparameters and 1 MFLOP per epoch required for on-device training, our\nsystem is feasible for TinyML applications aimed at battery-powered\nmicrocontrollers."
}