# Matching Latent Encoding for Audio-Text based Keyword Spotting

Kumari Nishu, Minsik Cho, Devang Naik

## 🧩 Problem to Solve

음성-텍스트 기반 키워드 스포팅(KWS)은 높은 성능을 보여주지만, 다양한 길이의 다중 단어 키워드에 대해 음성 임베딩과 텍스트 임베딩을 의미론적으로 정렬하는 것이 주요 과제로 남아있습니다. 특히, 음성 시퀀스와 텍스트 시퀀스 간의 길이가 다를 때, 두 이질적인 모달리티(음성 프레임과 단어 토큰)를 효과적으로 매핑하고 비교하는 방법이 부족했습니다.

## ✨ Key Contributions

- 음성 시퀀스를 텍스트 단어 시퀀스와 같은 길이로 분할하기 위한 새로운 동적 프로그래밍 기반 알고리즘인 **동적 시퀀스 분할(Dynamic Sequence Partitioning, DSP) 알고리즘**을 제안했습니다. 이 알고리즘은 단조 정렬(monotonically aligned)된 콘텐츠에서 짧은 시퀀스의 인덱스가 긴 시퀀스의 인덱스와 일대다 관계를 가지며, 긴 시퀀스의 인덱스는 짧은 시퀀스의 인덱스와 일대일 관계를 가질 때 특히 효과적입니다.
- 두 개의 독립적으로 사전 학습된 인코더로부터 생성된 음향 임베딩 시퀀스를 해당 텍스트 단어 임베딩 시퀀스에 매핑하는 유연한 임베딩 매칭 설정을 제시했습니다. 이를 통해 동일한 텍스트에 대한 음성 시퀀스가 유사하게 임베딩될 수 있도록 합니다.
- 인코더, 프로젝터, 그리고 새로운 DSP 기반 음성-텍스트 정렬 모듈로 구성된 종단 간(end-to-end) KWS 모델을 단일 및 다중 단어 키워드 모두에 대해 학습시키는 판별적 설정(discriminative setting)을 제안했습니다.
- 제안된 아키텍처가 AUC(Area Under the ROC Curve) 및 EER(Equal-Error-Rate) 측면에서 최신 기술(state-of-the-art) 대비 각각 14.4%와 28.9%의 성능 향상을 달성했습니다.

## 📎 Related Works

- **DTW(Dynamic Time Warping) 기반 Flexible KWS:** 음성(homogeneous) 모달리티 비교에는 효과적이나, 음성과 텍스트처럼 이질적인 모달리티(음성 프레임 대 단어 토큰)에서는 일대다 매핑이 양방향으로 적용되기 어려워 한계가 있습니다.
- **고정 시퀀스 길이 투영(Fixed Sequence Length Projection):** [4, 11, 12] 등에서는 음성과 텍스트 시퀀스를 고정된 길이로 투영하여 거리 계산을 단순화했지만, 투영 과정에서 중요한 의미론적 맥락을 손실하고 다중 단어 키워드 탐지 성능이 저하되는 경향이 있습니다.
- **음성-텍스트 기반 Flexible KWS (Phoneme-based):** [13]은 음소(phoneme) 기반 임베딩을 사용했지만, 단어 수준의 의미를 효과적으로 포착하지 못해 단어 식별 능력이 떨어졌습니다.
- **최신 기술 [5]:** 음성과 텍스트 간의 일치도를 측정하여 최신 성능을 보고했지만, 텍스트 인코더가 음소/음가(grapheme/phoneme) 시퀀스에 기반하여 텍스트 키워드의 의미를 제대로 포착하지 못해 유사한 소리에 취약했습니다. 또한, 단조 매칭 손실(monotonic matching loss)을 사용하여 정렬 유연성을 저해할 수 있었습니다.

## 🛠️ Methodology

본 논문은 인코딩 매칭 기반 키워드 스포터(EMKWS)라는 종단 간 아키텍처를 제안하며, 이는 인코더(Encoder), 프로젝터(Projector), 그리고 음성-텍스트 정렬기(Audio-Text Aligner)의 세 가지 모듈로 구성됩니다.

1. **인코더 (Encoder):**

   - **음성 인코더:** Conformer 아키텍처 [16]를 사용하여 음성 입력 신호의 임베딩을 생성합니다. Automatic Speech Recognition (ASR) 작업으로 사전 학습된 후 마지막 선형 계층을 제거하고 파라미터를 고정하여 사용합니다. 입력 음성 $a'$를 $\hat{a} = (\hat{a}_1, ..., \hat{a}_n)$으로 인코딩합니다.
   - **텍스트 인코더:** 자연어 이해 모델인 DistilBERT [17]를 사용하여 입력 텍스트의 의미론적 임베딩을 얻습니다. 입력 텍스트 $t'$를 $\hat{t} = (\hat{t}_1, ..., \hat{t}_m)$으로 인코딩합니다.

2. **프로젝터 (Projector):**

   - 인코더에서 생성된 음성 및 텍스트 임베딩을 공통 임베딩 공간(차원 $d$)으로 투영합니다. 각 프로젝션 블록은 두 개의 레이어 정규화(layer norm)와 두 개의 선형 계층으로 구성됩니다. 투영된 음성 임베딩은 $a = (a_1, ..., a_n)$, 텍스트 임베딩은 $t = (t_1, ..., t_m)$으로 표현됩니다. 여기서 $a_i \in \mathbb{R}^d$, $t_k \in \mathbb{R}^d$ 입니다.

3. **음성-텍스트 정렬기 (Audio-Text Aligner):**

   - **동적 시퀀스 분할 (Dynamic Sequence Partitioning, DSP) 알고리즘:** 음성 임베딩 $a$와 텍스트 임베딩 $t$의 길이가 다를 때 (일반적으로 $n > m$), 음성 시퀀스를 $m$개의 청크(chunk)로 최적으로 분할합니다. DSP는 음성과 텍스트 간의 단조로운(monotonic) 콘텐츠 정렬 특성을 활용합니다.
     - DSP는 동적 프로그래밍을 사용하여, 분할된 음성 청크의 평균 벡터와 해당 텍스트 단어 임베딩 간의 $\text{L}_2$ 거리를 최소화하는 최적의 분할을 찾습니다:
       $$z(a,t) = \min_{p \in Partition(a,m)} \left( \frac{1}{m} \sum_{k=1}^{m} \| \overline{p_k} - t_k \|_2 \right)$$
       여기서 $p_k$는 $k$번째 음성 청크이고, $\overline{p_k}$는 해당 청크의 평균 임베딩 벡터입니다.
     - DSP의 시간 복잡도는 $O(mn^2)$이며, $m$이 작으므로 $O(n^2)$에 가까워집니다.
   - **시퀀스 집계 (Sequence Aggregation):** DSP로 분할된 각 음성 청크는 평균 연산을 통해 하나의 벡터로 집계되어, 텍스트 임베딩과 직접 비교할 수 있는 $m$개의 벡터 시퀀스를 생성합니다.

4. **대조 손실 (Contrastive Loss):**
   - 훈련 목표는 대조 손실 [18]을 사용하며, 이는 긍정 예제($l'=1$)에 대해서는 거리를 최소화하고, 부정 예제($l'=0$)에 대해서는 거리를 최대화합니다.
     $$L = l' \cdot \max(z - m_{\text{pos}}, 0) + (1 - l') \cdot \max(m_{\text{neg}} - z, 0)$$
     여기서 $m_{\text{pos}}$는 긍정 마진, $m_{\text{neg}}$는 부정 마진입니다.

## 📊 Results

- **성능 비교:** 제안된 EMKWS 모델은 Libriphrase Easy (LE) 및 Libriphrase Hard (LH) 데이터셋에서 최신 기술 [5] 대비 우수한 성능을 보였습니다 (표 1).
  - **LE 데이터셋:** AUC 점수 1.2% 향상 (97.83% vs 96.7%), EER 12.6% 향상 (7.36% vs 8.42%).
  - **LH 데이터셋:** AUC 점수 14.4% 향상 (84.21% vs 73.58%), EER 28.9% 향상 (23.36% vs 32.9%). 특히 LH 데이터셋은 유사한 소리 또는 의미를 가진 키워드들로 구성되어 모델의 차별 능력을 평가하는 데 중요하며, 여기서 큰 폭의 개선을 달성했습니다.
- **DSP의 효과:** 무작위 분할(random partitioning) 및 균등 분할(equal partitioning)과 비교했을 때, DSP 알고리즘은 AUC 및 EER 측면에서 일관되게 우수한 성능을 보여주었습니다 (그림 2). 특히 다중 단어 키워드 예제에서 DSP의 이점이 두드러졌습니다.
- **임베딩 시각화:** 투영된 음성 및 텍스트 임베딩 간의 상관관계 시각화 (그림 3)를 통해 DSP 알고리즘의 효과를 확인했습니다. "The Old Man"과 같은 긍정 예제에서 텍스트-음성 상관관계 (그림 3a)는 음성 임베딩 축을 따라 3개의 뚜렷한 단조 상관 블록을 보여주며, 각 블록은 텍스트의 각 단어에 정렬됩니다. 음성-음성 임베딩은 각 단어에 대해 완전히 자체 상관(self-correlated)을 보였습니다 (그림 3b). 이는 DSP가 단어별 음성 길이에 비례하여 시퀀스를 효과적으로 분할했음을 시사합니다.

## 🧠 Insights & Discussion

- **이질적 모달리티 정렬의 성공:** DSP 알고리즘은 음성(프레임 시퀀스)과 텍스트(단어 시퀀스)라는 이질적인 모달리티 간의 길이 차이 문제를 효과적으로 해결하여, 다중 단어 키워드에 대한 높은 정확도의 매칭을 가능하게 했습니다.
- **의미론적 맥락 유지:** 기존 투영 방식들이 시퀀스 길이 고정으로 인해 의미론적 맥락을 손실했던 것과 달리, 본 접근 방식은 임베딩 차원만 공통 공간으로 투영하고 시퀀스 길이는 DSP를 통해 동적으로 정렬함으로써 핵심 정보를 보존했습니다.
- **하드 샘플에 대한 강점:** Levenshtein 거리를 기준으로 유사한 음성 콘텐츠를 가진 "하드" 부정 예제(Libriphrase Hard)에서 특히 큰 성능 향상을 보인 것은, 제안된 모델이 유사한 발음의 키워드를 더 효과적으로 구별할 수 있음을 의미합니다. 이는 DistilBERT를 통한 강력한 텍스트 의미 임베딩과 DSP의 정밀한 정렬 능력 덕분으로 해석됩니다.
- **확장성 및 효율성:** 추론 시 텍스트 인코더를 제거하고 사전 계산된 키워드 임베딩을 사용하여 효율성을 높였으며, 모델 파라미터 수가 총 3.7M개로 경량화되어 실제 환경에 적용하기에 유리합니다.

## 📌 TL;DR

본 논문은 다양한 길이의 음성 및 텍스트 키워드를 효율적으로 비교하기 위한 종단 간 키워드 스포팅 모델 EMKWS를 제안합니다. 핵심은 음성 임베딩 시퀀스를 텍스트 시퀀스 길이에 맞춰 최적으로 분할하는 **동적 시퀀스 분할(DSP)** 알고리즘입니다. Conformer 기반 음성 인코더와 DistilBERT 기반 텍스트 인코더, 그리고 공통 잠재 공간으로 임베딩을 투영하는 모듈을 사용하여 음성 및 텍스트 임베딩을 생성한 후, DSP를 통해 최적의 $\text{L}_2$ 거리를 계산하고 대조 손실로 학습합니다. 실험 결과, 제안된 EMKWS 모델은 기존 최신 기술 대비 AUC 14.4%, EER 28.9%의 상당한 성능 향상을 달성하여, 특히 유사한 키워드에 대한 탐지 성능을 크게 개선했습니다.
