# Hello Edge: Keyword Spotting on Microcontrollers

Yundong Zhang, Naveen Suda, Liangzhen Lai and Vikas Chandra

## 🧩 Problem to Solve

스마트 기기에서 음성 기반 사용자 상호작용을 가능하게 하는 핵심 구성 요소인 키워드 스팟팅(KWS)은 실시간 응답과 높은 정확도를 요구합니다. KWS는 상시 동작(always-on)하는 특성 때문에 전력 예산이 매우 제한적이며, 제한된 메모리와 연산 능력을 가진 소형 마이크로컨트롤러(MCU)에서 실행되는 경우가 많습니다. 이 논문은 이러한 자원 제약적인 MCU 환경에 적합한 신경망 아키텍처를 설계하여, 정확도를 희생하지 않으면서도 메모리 및 연산 제약을 충족시키는 방법을 탐색하는 것을 목표로 합니다.

## ✨ Key Contributions

- 문헌에 발표된 인기 있는 KWS 신경망 모델(DNN, CNN, RNN, CRNN)들을 Google 음성 명령 데이터셋에 학습시켜 정확도, 메모리 사용량 및 추론당 연산량(Ops)을 비교했습니다.
- 자원 효율적인 MobileNet에서 영감을 받아 깊이 분리형 합성곱(Depthwise Separable Convolution)을 사용하는 새로운 KWS 모델인 DS-CNN을 구현했으며, 이 모델이 다른 기존 모델들보다 정확도, 모델 크기 및 연산량 모든 면에서 우수한 성능을 보여주었습니다.
- 일반적인 마이크로컨트롤러의 컴퓨팅 및 메모리 제약 조건 내에서 다양한 신경망 아키텍처에 대한 자원 제약적 탐색을 수행하고 포괄적인 비교 결과를 제시했습니다.
- 훈련된 32비트 부동소수점 KWS 모델을 8비트 고정소수점 버전으로 양자화하여, 재훈련 없이도 정확도 손실 없이 배포 가능함을 입증했습니다.

## 📎 Related Works

- **전통적인 KWS**: 은닉 마르코프 모델(HMM)과 비터비(Viterbi) 디코딩 [11,12] 방식을 사용했으나, 학습 및 추론 시 계산 비용이 높았습니다.
- **초기 딥러닝 KWS**: 식별 모델(Discriminative models) [13] 또는 순환 신경망(RNN) [14]이 탐색되었으나, 탐지 지연 시간이 길다는 단점이 있었습니다.
- **DNN 기반 KWS**: 완전 연결 계층(Fully-connected layers)과 ReLU 활성화 함수를 사용하는 심층 신경망(DNN) [5]이 HMM 모델보다 높은 정확도와 낮은 지연 시간을 보여주었습니다.
- **CNN 기반 KWS**: 음성 특징의 지역적 시간/스펙트럼 상관관계를 활용하는 합성곱 신경망(CNN) [6]이 DNN보다 높은 정확도를 달성했습니다.
- **CRNN 기반 KWS**: CNN과 RNN의 장점을 결합한 합성곱 순환 신경망(CRNN) [7]은 잡음에 대한 모델의 견고성을 입증했습니다.
- **LSTM 기반 KWS**: 긴 단기 기억(LSTM) 네트워크에 최대 풀링 손실 함수를 적용한 KWS 모델 [8]은 DNN 및 교차 엔트로피 손실로 훈련된 LSTM보다 더 나은 정확도를 달성했습니다.
- **모델 압축**: 저랭크 근사(Low-rank approximation) 기법 [15,16]을 사용하여 DNN 모델 가중치를 압축하여 하드웨어 자원을 절약하려는 시도가 있었습니다.

## 🛠️ Methodology

1. **데이터셋 및 특징 추출**:
   - **데이터셋**: Google 음성 명령 데이터셋 [9] (30개 키워드의 65,000개 1초 길이 오디오 클립).
   - **분류 대상**: "Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop", "Go" 10개 키워드와 "silence", "unknown" 단어.
   - **데이터 분할**: 훈련, 검증, 테스트 셋을 80:10:10 비율로 분할.
   - **특징**: 40개의 멜 주파수 켑스트럼 계수(MFCC)를 40ms 프레임 길이, 20ms 스트라이드로 추출하여 1초 오디오당 1960($49 \times 40$)개의 특징을 생성.
2. **신경망 아키텍처 탐색**:
   - **평가 모델**: DNN, CNN, RNN (Basic LSTM, LSTM, GRU), CRNN, Depthwise Separable CNN (DS-CNN) [10].
   - **훈련**: Google TensorFlow [31] 프레임워크, Adam 옵티마이저 [32], 표준 교차 엔트로피 손실 함수.
   - **훈련 설정**: 배치 크기 100, 20K 반복, 초기 학습률 $5 \times 10^{-4}$ (10K 반복 후 $10^{-4}$로 감소).
   - **데이터 증강**: 배경 잡음 및 최대 100ms의 무작위 시간 이동.
3. **자원 제약 조건 정의**:
   - MCU 시스템 구성을 바탕으로 'Small (S)', 'Medium (M)', 'Large (L)' 세 가지 모델 클래스를 정의했습니다 (메모리: 80KB/200KB/500KB, 추론당 연산량: 6MOps/20MOps/80MOps). 이는 10회/초 추론 및 8비트 가중치/활성화 기준입니다.
4. **하이퍼파라미터 탐색**:
   - 특징 추출 하이퍼파라미터(MFCC 특징 수 $F$, 프레임 스트라이드 $S$)와 각 신경망 모델의 하이퍼파라미터(계층 수, 뉴런 수, 필터 크기, 스트라이드 등)를 반복적으로 탐색하여 최적의 모델을 찾았습니다.
5. **8비트 양자화**:
   - 32비트 부동소수점 가중치 및 활성화를 8비트 고정소수점으로 양자화하는 방법 [34]을 사용했습니다. 각 계층에 대해 정확도 손실을 최소화하는 최적의 분수 길이 $N$을 찾아 점진적으로 양자화를 수행했습니다. 8비트 고정소수점 값 $v$는 $v = -B_7 \cdot 2^{7-N} + \sum_{i=0}^{6} B_i \cdot 2^{i-N}$로 표현됩니다.

## 📊 Results

- **초기 모델 비교 (표 2)**:
  - DNN은 연산량은 적지만 정확도가 가장 낮고 메모리 집약적입니다 (84.3%, 288KB, 0.57MOps).
  - CNN-1은 가장 높은 정확도(90.7%)를 보였지만, 가장 많은 메모리와 연산량을 요구했습니다 (556KB, 76.02MOps).
  - LSTM 및 CRNN은 메모리와 연산량 사이에서 균형을 이루며 좋은 정확도를 보였습니다.
- **자원 제약적 탐색 결과 (표 5, 그림 6)**:
  - **DNN**: 메모리 제약적이며 모델을 확장해도 정확도가 약 87%에서 포화됩니다.
  - **CNN**: DNN보다 나은 정확도를 달성하지만, 최종 완전 연결 계층의 가중치에 의해 제약을 받습니다.
  - **RNN (Basic LSTM, LSTM, GRU)**: CNN보다 나은 정확도를 달성하며, 일부 경우 더 적은 연산량으로 더 작은 모델을 만들 수 있었습니다.
  - **CRNN**: CNN과 RNN의 장점을 결합하여 더 적은 연산량으로도 CNN 및 RNN보다 나은 정확도를 달성했으며, 자원 활용에 따라 잘 확장되었습니다.
  - **DS-CNN**: 모든 자원 제약 조건(S, M, L)에서 가장 우수한 정확도 (각각 94.4%, 94.9%, 95.4%)를 달성했습니다. 깊이 분리형 합성곱 계층의 효율성 덕분에 더 깊은 아키텍처가 가능하여 높은 확장성을 보여주었습니다. 심지어 8KB 미만의 메모리 공간과 500K 미만의 연산량에서도 DNN 모델보다 우수한 정확도를 달성했습니다 (그림 7).
- **양자화 결과 (표 6)**:
  - 대표적인 8비트 양자화된 신경망 모델들은 원래의 32비트 부동소수점 네트워크와 동일하거나 약간 더 나은 정확도를 보였습니다. 이는 양자화가 더 나은 정규화 효과를 제공하기 때문일 수 있습니다.
- **MCU 배포**:
  - Cortex-M7 기반 STM32F746G-DISCO 개발 보드에 8비트 DNN 모델 KWS 애플리케이션을 배포한 결과, 추론당 약 12ms가 소요되었고 전체 애플리케이션은 약 70KB의 메모리를 차지했습니다.

## 🧠 Insights & Discussion

- 마이크로컨트롤러와 같은 자원 제약적인 환경에서는 하드웨어에 최적화된 신경망 아키텍처 설계가 효율적인 KWS 시스템 구축에 매우 중요합니다.
- 깊이 분리형 합성곱(DS-CNN)은 낮은 연산량과 메모리 사용량으로 높은 정확도를 달성하며, 다양한 MCU 자원 제약 조건에 대해 뛰어난 확장성을 보여주어 매우 효과적인 아키텍처임을 입증했습니다.
- 8비트 양자화는 재훈련 없이도 신경망 모델의 정확도 손실 없이 모델 크기를 4배 줄이고, 고정소수점 연산을 통해 실행 속도를 크게 향상시킬 수 있어 MCU 배포에 필수적인 기법입니다.
- 이 연구는 실제 MCU에 KWS를 배포하기 위한 실용적인 지침을 제공하며, DNN, CNN, RNN 등 다양한 아키텍처의 장단점을 자원 제약적 관점에서 명확히 보여줍니다.

## 📌 TL;DR

이 논문은 마이크로컨트롤러에서 키워드 스팟팅(KWS)을 효율적으로 실행하기 위한 신경망 아키텍처를 탐색합니다. 연구팀은 다양한 기존 모델(DNN, CNN, RNN, CRNN)을 평가하고, 자원 효율적인 **깊이 분리형 합성곱 신경망(DS-CNN)** 모델을 제안했습니다. DS-CNN은 일반적인 MCU 자원 제약 조건(메모리 80KB-500KB, 연산량 6-80MOps) 내에서 다른 모델들보다 최고 **95.4%**의 정확도를 달성하며 가장 우수한 성능과 확장성을 보였습니다. 또한, 8비트 양자화를 통해 모델의 정확도 손실 없이 메모리 사용량과 연산 속도를 최적화할 수 있음을 입증하여, 자원 제약적인 엣지 디바이스에 딥러닝 기반 KWS를 성공적으로 배포할 수 있는 실용적인 가이드를 제공합니다.
