# Streaming keyword spotting on mobile devices

Oleg Rybakov, Natasha Kononenko, Niranjan Subrahmanya, Mirk ́o Visontai, Stella Laurenzo

## 🧩 Problem to Solve

이 논문은 모바일 기기에서 키워드 스포팅(KWS) 모델을 효율적으로 스트리밍 모드로 실행하는 문제를 다룹니다. 특히, 다음과 같은 주요 과제를 해결하고자 합니다:

1. **모델 변환의 비효율성**: 비스트리밍 모드로 학습된 신경망(NN) 모델을 스트리밍 모드로 변환하는 과정이 수동 재작성을 필요로 하여 많은 시간과 노력이 소요됩니다.
2. **지연 시간 및 정확도 트레이드오프**: 실시간 KWS 애플리케이션에서 요구되는 낮은 지연 시간(latency)과 높은 정확도(accuracy)를 모바일 환경에서 동시에 달성하는 것이 어렵습니다.
3. **모델 배포의 복잡성**: 음성 특징 추출 과정이 모델 외부에서 처리될 경우, 모바일 기기에서의 배포 및 최적화가 복잡해집니다.

## ✨ Key Contributions

이 논문의 핵심 기여는 다음과 같습니다:

- **자동 스트리밍 모델 변환 라이브러리 개발**: Keras 기반의 `Stream` 래퍼(wrapper) 계층을 설계하여, 비스트리밍 모델을 수동 재작업 없이 내부/외부 상태를 가진 스트리밍 추론 모델로 자동 변환할 수 있도록 했습니다.
- **KWS 분류 오류율 10% 감소**: Multi-head attention을 적용한 새로운 KWS 모델인 MHAtt-RNN을 제안하여, Google speech commands dataset V2에서 최신 기술(state-of-the-art) 대비 분류 오류율을 10% 감소시켰습니다.
- **다양한 KWS 모델 벤치마킹 및 분석**: Google speech commands dataset을 사용하여 DNN, CNN, LSTM, GRU, DSCNN 등 여러 인기 KWS 모델들을 구현하고, Pixel 4 모바일 폰에서 스트리밍 및 비스트리밍 모드에서의 정확도와 지연 시간을 비교 분석하여 다양한 성능 트레이드오프를 보여주었습니다.
- **음성 특징 추출기의 모델 내 캡슐화**: 음성 특징 추출기(MFCC 기반)를 Keras 계층으로 구현하여 NN 모델의 일부로 통합함으로써, TFLite로의 변환 및 모바일 기기 배포를 간소화했습니다.
- **오픈 소스 공개**: 모든 코드, 사전 훈련된 모델 및 실험 결과는 공개되어 있습니다.

## 📎 Related Works

- **모델 최적화**: 양자화([1, 2, 3]) 및 NN 모델의 스트리밍 변환([4]).
- **음성 특징 추출**: MFCC ([6]).
- **KWS 모델 아키텍처**:
  - DNN, CNN ([5, 12, 14, 15]), RNN(LSTM [19], GRU [20]), CRNN ([17, 18]), DSCNN ([5, 21]), SVDF ([28]), TC-ResNet ([22]).
  - Attention 메커니즘 ([24, 25]) 및 Att-RNN ([27]).
  - 최신 KWS 모델: Embed+head ([13]), MatchboxNet ([23]).
- **데이터 증강**: SpecAugment ([26]).
- **데이터셋**: Google Speech Commands Dataset V1 ([29]) 및 V2 ([9]).
- **배포 도구**: TensorFlow Lite (TFLite) ([7]).

## 🛠️ Methodology

1. **Keras 스트리밍 래퍼(`Stream` Wrapper) 설계**:

   - Keras의 표준 계층(`Conv2D`, `Flatten` 등)을 `Stream` 래퍼로 감싸서, 훈련 시에는 원래 계층처럼 작동하고 추론 시에는 자동으로 스트리밍을 위한 상태(내부 또는 외부)를 관리하도록 구현했습니다.
   - RNN 계층을 위해 `streaming-aware` RNN 계층을 구축하여 스트리밍 추론 시 RNN 셀만 호출하도록 했습니다.
   - **자동 변환 과정**:
     - 입력 계층의 특징 크기를 단일 프레임으로 설정합니다.
     - Keras NN 표현을 순회하며 스트리밍이 필요한 계층에 링 버퍼를 삽입하거나 `streaming-aware` 계층의 스트리밍 함수를 호출합니다.
   - 현재 버전은 시간 차원에서 스트라이딩(striding)이나 풀링(pooling)이 $1$보다 큰 경우는 지원하지 않습니다.

2. **모델링 파이프라인**:

   - Keras 계층과 `Stream` 래퍼를 사용하여 모델을 설계하고 훈련합니다.
   - 훈련된 비스트리밍 Keras 모델을 Keras 스트리밍 모델로 자동 변환합니다.
   - 스트리밍 Keras 모델을 TFLite 모듈로 변환하고 필요시 양자화하여 모델 크기와 성능을 최적화합니다.
   - TFLite 모듈을 Pixel 4 모바일 폰에서 벤치마킹하여 정확도와 지연 시간을 측정합니다.

3. **음성 전처리 및 특징 추출**:

   - MFCC ([6]) 기반의 음성 특징 추출기를 Keras 계층으로 구현하여 NN 모델의 일부로 포함시켰습니다. 이는 모델 배포를 단순화합니다.
   - 1초 길이의 오디오 신호를 40ms 프레임(20ms 오버랩)으로 분할합니다.

4. **데이터셋 및 증강**:

   - Google Speech Commands Dataset V1 및 V2를 사용했으며, "yes", "no" 등 10개 키워드와 "silence", "unknown"을 포함한 12개 레이블로 훈련했습니다.
   - 데이터는 훈련, 검증, 테스트 세트로 80:10:10 비율로 분할했습니다.
   - 데이터 증강 기법으로 시간 이동, 신호 재샘플링, 배경 잡음, SpecAugment ([26]) 기반 주파수/시간 마스킹을 적용했습니다.

5. **모델 아키텍처**:
   - DNN, CNN, GRU, LSTM, CRNN, DSCNN, SVDF, TC-ResNet 등 기존 인기 KWS 모델들을 구현했습니다.
   - 새로운 MHAtt-RNN 모델은 Att-RNN ([27])에 4개의 헤드를 가진 Multi-head attention을 확장하고, LSTM을 GRU로 대체하여 개발했습니다.

## 📊 Results

- **정확도 향상**:
  - 제안된 MHAtt-RNN 모델은 Google Speech Commands Dataset V2에서 98.0%의 최고 정확도를 달성하며, 기존 최신 Embed+head ([13]) 모델 대비 분류 오류율을 10% 상대적으로 감소시켰습니다 (단, MHAtt-RNN은 Embed+head보다 약 2배 많은 파라미터 수를 가집니다).
  - SpecAugment와 하이퍼파라미터 최적화를 통해 기존 모델들(DNN, CNN, LSTM 등)의 V1 및 V2 데이터셋에서의 정확도도 크게 향상되었습니다 (Table 1 참조).
- **스트리밍/비스트리밍 지연 시간 및 정확도 (Pixel 4 모바일 폰)**:
  - 가장 효과적이고 정확한 스트리밍 모델은 SVDF, CRNN, GRU로 나타났습니다 (Table 2 참조).
  - CNN (스트라이드 없음) 및 DSCNN (스트라이드 없음)과 같은 합성곱 모델의 비스트리밍 대비 스트리밍 지연 시간 비율은 약 10배였으며, GRU(S) 및 CRNN(S)와 같은 RNN 모델은 약 20배였습니다. 이는 이상적인 50배(1초에 50개 프레임)에는 미치지 못하여 추가 최적화 여지가 있음을 시사합니다.
  - `stateful=True`로 훈련된 RNN 모델(GRU(S), CRNN(S))은 비상태 유지 훈련 버전 대비 정확도가 감소하는 경향을 보였습니다.
  - 음성 특징 추출 지연 시간은 약 3.7ms였으며, FFT 사용 및 모델 양자화를 통해 거의 2배 감소 가능함을 확인했습니다.

## 🧠 Insights & Discussion

- **자동 스트리밍 변환의 가치**: Keras 래퍼를 통한 자동 변환은 개발자의 수동 재작업 부담을 줄여 모델 개발 및 배포 시간을 단축하며, KWS 외 다른 시퀀스 분류 문제에도 확장 적용될 수 있는 잠재력을 가집니다.
- **성능과 복잡성의 트레이드오프**: MHAtt-RNN은 뛰어난 정확도를 제공하지만, 모델 크기가 증가합니다. 이는 특정 애플리케이션의 요구사항에 따라 정확도와 자원 효율성 사이의 균형을 신중하게 고려해야 함을 보여줍니다.
- **스트리밍 최적화의 중요성**: 현재 스트리밍 모델의 지연 시간 개선 여지가 있으며, 링 버퍼 메모리 할당 최적화 및 추론 엔진 내 내부 상태 지원을 통해 더 큰 속도 향상을 기대할 수 있습니다. 스트라이딩/풀링 지원은 TC-ResNet과 같은 모델의 스트리밍 효율성을 높일 것입니다.
- **모바일 배포 간소화**: 음성 특징 추출기를 모델 내부에 통합하고 TFLite로 변환하는 접근 방식은 모바일 기기에서의 모델 배포 복잡성을 크게 줄여줍니다.
- **RNN의 스트리밍 훈련**: RNN 모델을 스트리밍 환경에 적합하게 `stateful`하게 훈련할 때 정확도 손실이 발생할 수 있다는 점은 스트리밍 KWS를 위한 RNN 기반 모델 설계 시 중요한 고려 사항입니다.

## 📌 TL;DR

이 논문은 모바일 기기에서의 효율적인 키워드 스포팅(KWS)을 위해, Keras 기반의 **자동 스트리밍 모델 변환 라이브러리**를 제안하여 비스트리밍 모델의 수동 재작업 부담을 없앴습니다. 또한, Multi-head attention을 도입한 **MHAtt-RNN 모델**로 Google Speech Commands Dataset V2에서 98.0%의 정확도를 달성하며 기존 최신 모델 대비 분류 오류율을 10% 감소시켰습니다. Pixel 4 모바일 폰에서 다양한 KWS 모델의 스트리밍 및 비스트리밍 성능을 벤치마킹하여 SVDF, CRNN, GRU 모델의 효율성을 입증하고, 지연 시간 및 정확도 트레이드오프를 분석했습니다. 모든 실험 코드는 오픈 소스로 공개되었습니다.
