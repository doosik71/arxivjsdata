{
  "title": "BERT Meets CTC: New Formulation of End-to-End Speech Recognition with\n  Pre-trained Masked Language Model",
  "authors": "Yosuke Higuchi, Brian Yan, Siddhant Arora, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe",
  "year": 2022,
  "url": "http://arxiv.org/abs/2210.16663v2",
  "abstract": "This paper presents BERT-CTC, a novel formulation of end-to-end speech\nrecognition that adapts BERT for connectionist temporal classification (CTC).\nOur formulation relaxes the conditional independence assumptions used in\nconventional CTC and incorporates linguistic knowledge through the explicit\noutput dependency obtained by BERT contextual embedding. BERT-CTC attends to\nthe full contexts of the input and hypothesized output sequences via the\nself-attention mechanism. This mechanism encourages a model to learn\ninner/inter-dependencies between the audio and token representations while\nmaintaining CTC's training efficiency. During inference, BERT-CTC combines a\nmask-predict algorithm with CTC decoding, which iteratively refines an output\nsequence. The experimental results reveal that BERT-CTC improves over\nconventional approaches across variations in speaking styles and languages.\nFinally, we show that the semantic representations in BERT-CTC are beneficial\ntowards downstream spoken language understanding tasks.",
  "citation": 40
}