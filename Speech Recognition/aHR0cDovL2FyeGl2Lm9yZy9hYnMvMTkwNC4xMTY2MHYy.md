# Transformers with convolutional context for ASR

Abdelrahman Mohamed, Dmytro Okhonko, Luke Zettlemoyer

## 🧩 Problem to Solve

Transformer 네트워크는 신경망 기계 번역(NMT) 및 다른 자연어 처리(NLP) 태스크에서 큰 성공을 거두었지만, 음성 인식(ASR)에 적용할 때에는 몇 가지 도전 과제가 있었습니다. 주요 문제점은 다음과 같습니다:

- **위치 정보 처리**: Transformer 레이어는 본질적으로 단어 순서나 위치 정보를 보존하지 않습니다. 기존에는 사인파(sinusoidal) 위치 임베딩을 사용했지만, 이를 음성 특성과 효과적으로 결합하는 방법에 대한 연구가 필요했습니다.
- **최적화 안정성**: 대규모 Transformer 네트워크 학습 시 최적화 과정의 안정성이 문제가 될 수 있으며, 특히 하위 Transformer 레이어가 지역적 의존성을 학습하도록 강제할 경우 취약한 최적화 특성을 보입니다.

## ✨ Key Contributions

이 연구는 ASR을 위한 Transformer 모델의 이러한 문제점을 해결하기 위해 다음과 같은 핵심 기여를 제시합니다:

- **합성곱(Convolutional) 컨텍스트 기반 입력 표현 제안**: 기존의 사인파 위치 임베딩을 인코더의 2D 합성곱 레이어와 디코더의 1D 합성곱 레이어를 통해 학습된 컨텍스트 기반 입력 표현으로 대체합니다. 이는 Transformer 블록에 상대적인 위치 정보를 제공하여 장거리 관계 학습을 돕습니다.
- **최적화 특성 개선**: 제안된 시스템은 고정된 학습률 $1.0$과 웜업(warmup) 스텝 없이도 안정적인 최적화 특성을 보이며, 이는 모델이 하위 레이어에서 지역적 종속성을 학습할 필요가 없기 때문입니다.
- **경쟁력 있는 성능 달성**: 외부 언어 모델(LM) 텍스트를 제공하지 않은 Librispeech 벤치마크에서 "test clean" $4.7\%$, "test other" $12.9\%$의 경쟁력 있는 단어 오류율(WER)을 달성했습니다.
- **모델링 작업의 분할**: 지역적 관계 학습(합성곱)과 전역적 순차 구조 학습(Transformer)으로 모델링 작업을 분할하여 Transformer 최적화를 단순화하고 더 나은 결과를 얻었습니다.

## 📎 Related Works

- **시퀀스-투-시퀀스 모델**: 신경망 기계 번역(NMT) [7, 8]과 음성 인식(ASR) [9, 10] 분야에서 성공을 거두며 ASR 시스템을 단순화하는 데 기여했습니다.
- **Transformer 네트워크**: Vaswani et al. [11]이 제안한 Transformer는 셀프-어텐션 메커니즘만을 사용하여 장거리 관계를 학습하며, NLP의 많은 태스크에서 높은 성능을 보였습니다 [12].
- **ASR 분야의 Transformer 적용**: CTC 손실 [13] 또는 인코더-디코더 프레임워크 [14, 15]에서 Transformer를 사용하려는 초기 시도들은 기준 시스템에 비해 평범한 성능을 보였습니다.
- **시간 제한 셀프-어텐션**: 하이브리드 ASR 시스템에서 시간 제한 셀프-어텐션 레이어를 도입한 연구 [16]도 있었습니다.
- **합성곱 레이어 활용**: 음향 인코더에서 순환 레이어 전에 합성곱 레이어를 사용하여 계산 효율성을 높인 사례 [17]가 있었으나, 성능에 미치는 영향은 미미했습니다.

## 🛠️ Methodology

이 논문은 인코더와 디코더 모두에서 Transformer 레이어 이전에 합성곱 레이어를 도입하여 모델에 컨텍스트 정보를 제공하는 방식을 제안합니다.

1. **모델 구조 분할**: 전체 모델링 작업을 두 가지 하위 구성 요소로 나눕니다.
   - **지역적 관계 학습**: 작은 컨텍스트 내의 지역적 관계는 합성곱 레이어가 학습합니다.
   - **전역적 순차 구조 학습**: 입력의 전역적 순차 구조는 Transformer 레이어가 학습합니다.
2. **위치 임베딩 대체**: 기존의 사인파 위치 임베딩을 사용하지 않고, 합성곱 레이어에 의해 학습된 컨텍스트 입력 표현으로 대체합니다.
   - **인코더 합성곱 블록**: 입력 음성 특성($80$-D log mel-filterbank + 3 fundamental frequency features)에 대해 2D 합성곱 블록을 사용합니다. 각 블록은 K개의 합성곱 레이어와 2D Max Pooling 레이어로 구성되어 지역적 시공간 패턴을 추출합니다. (그림 2-우측 참조)
   - **디코더 합성곱 블록**: 이전에 예측된 단어 임베딩에 대해 1D 합성곱 블록을 사용합니다. 각 블록은 N개의 1D 합성곱 레이어로 구성되어 출력 시퀀스의 지역적 문맥을 학습합니다. (그림 2-좌측 참조)
3. **Transformer 블록**: 합성곱 레이어를 거친 후의 컨텍스트화된 표현은 표준 Transformer 블록(멀티 헤드 셀프 어텐션, 피드포워드 네트워크, 잔차 연결, 레이어 정규화)으로 전달되어 장거리 의존성을 모델링합니다. (그림 1-좌측 참조)
   - 디코더는 추가적으로 인코더의 컨텍스트를 통합하기 위한 별도의 멀티 헤드 어텐션 레이어를 사용합니다.
   - 디코더의 셀프 어텐션은 미래 시점의 정보를 마스킹하여 좌-우 출력 생성을 존중합니다.
4. **학습 설정**:
   - **데이터셋**: Librispeech (1000시간).
   - **출력 단위**: sentencepiece [20]로 학습된 5k "유니그램" 서브워드 단위.
   - **최적화**: AdaDelta [21] 옵티마이저를 사용하며, 고정 학습률 $1.0$, 기울기 클리핑 $10.0$을 적용합니다. 학습률 웜업 스케줄은 사용하지 않습니다.
   - **정규화**: 모든 블록에 $0.15$의 드롭아웃(dropout) 비율을 사용합니다.
   - **훈련 에포크**: $80$ 에포크 동안 훈련하며, 마지막 $30$개 체크포인트의 평균 모델을 사용하여 결과를 보고합니다.
5. **표준 모델 구성**:
   - 인코더: 두 개의 2D 합성곱 블록(각 2개의 합성곱 레이어, 커널 크기=3, Max Pooling 커널=2, 64/128 특징 맵). 10개의 인코더 Transformer 블록 (차원=$1024$, $16$ 헤드, ReLU 레이어 크기=$2048$).
   - 디코더: 입력 단어 임베딩 차원=$512$. 세 개의 1D 합성곱 레이어(커널 크기=3, Max Pooling 없음). 10개의 디코더 Transformer 블록 (인코더 측 멀티 헤드 어텐션 포함).
   - 총 파라미터 약 $223$M개.

## 📊 Results

- **위치 임베딩 방식 비교**:
  - 제안된 합성곱 컨텍스트 모델은 Librispeech "test clean" $5.3\%$, "test other" $14.0\%$ WER을 달성했습니다 (표 1).
  - 사인파 위치 임베딩을 사용한 모델은 "test clean" $5.4\%$, "test other" $14.8\%$ WER로 더 낮은 성능을 보였습니다.
  - 두 임베딩 방식을 결합해도 성능 향상은 없었으며, 이는 합성곱 컨텍스트가 충분한 상대적 위치 정보를 제공함을 시사합니다.
- **디코더 합성곱 컨텍스트 깊이 및 크기**: 더 깊은 합성곱 레이어에 걸쳐 더 넓은 컨텍스트를 사용하는 것이 가장 좋은 성능을 보였습니다 (표 2). 파라미터 예산이 동일할 경우, 단일 깊이보다 다중 깊이의 합성곱이 유리했습니다.
- **Transformer 깊이의 영향**:
  - 인코더 깊이가 성능에 매우 중요했으며, 디코더 깊이도 도움이 되었지만 인코더만큼 결정적이지는 않았습니다 (표 3).
  - 인코더/디코더 깊이가 각각 $12/12$일 때 "test clean" $5.0\%$, "test other" $13.3\%$ WER을 기록하며 좋은 성능을 보였습니다.
- **최종 모델 성능**: 가장 좋은 성능을 보이는 설정을 조합하여 (4k ReLU 레이어, 16개 인코더 Transformer 블록, 6개 디코더 Transformer 블록) "test clean" $4.7\%$, "test other" $12.9\%$ WER을 달성했습니다 (빔 크기 $20$ 사용, 표 4).
- **기존 연구 대비 개선**: 외부 LM 없이도 Librispeech "dev other" 및 "test other" 서브셋에서 기존에 발표된 모델 대비 $12\%$에서 $16\%$의 상대적 WER 감소를 보였습니다. 이는 모델이 음향적으로 어려운 데이터에서 음성의 장거리 음향 특성(화자, 환경 특성)을 잘 학습함을 시사합니다.

## 🧠 Insights & Discussion

- **합성곱 컨텍스트의 역할**: 합성곱 레이어가 지역적 의존성과 암묵적인 상대적 위치 인코딩을 학습함으로써, 후속 Transformer 레이어는 장거리 관계 학습에 집중할 수 있게 됩니다. 이는 최적화 과정을 단순화하고 모델의 안정성을 높입니다.
- **최적화 안정성**: 고정된 학습률과 웜업 스케줄의 부재에도 불구하고 모델이 안정적으로 수렴하는 것은 합성곱 계층의 도입이 최적화 특성을 개선했음을 강력히 시사합니다.
- **깊은 인코더의 중요성**: 깊은 Transformer 인코더는 데이터의 장거리 구조를 포착하고, 화자 및 환경과 같은 전역적인 발화 특정 특성을 주변화하여 음성 콘텐츠 자체에 집중할 수 있게 합니다. 이는 특히 음향적으로 복잡한 "other" 데이터셋에서 성능 향상에 기여했습니다.
- **제한 및 향후 연구**: 이 연구에서 얻은 성능 개선은 더 큰 텍스트 코퍼스로 훈련된 언어 모델을 통한 잠재적인 WER 개선과 직교합니다. 향후 연구에서는 Optimal Completion Distillation (OCD) [27]과 같은 더 나은 훈련 절차와 제안된 시스템을 결합하여 추가적인 WER 개선 가능성을 탐구할 수 있습니다.

## 📌 TL;DR

Transformer를 ASR에 적용할 때 위치 정보 처리와 최적화 안정성 문제를 해결하기 위해, 이 논문은 사인파 위치 임베딩 대신 인코더(2D 합성곱)와 디코더(1D 합성곱)에 컨텍스트를 학습하는 합성곱 레이어를 도입한 새로운 Transformer 기반 ASR 시스템을 제안합니다. 이 방법은 학습률 웜업 없이도 안정적인 최적화를 가능하게 하며, Librispeech "test clean" $4.7\%$, "test other" $12.9\%$의 WER을 달성하여 외부 LM 없이 기존 모델 대비 "other" 서브셋에서 $12-16\%$의 상대적 WER 감소를 이루었습니다. 이는 합성곱 레이어가 지역적 컨텍스트를 제공하여 Transformer가 장거리 의존성 학습에 집중할 수 있게 함으로써 성능을 향상시킨다는 점을 보여줍니다.
