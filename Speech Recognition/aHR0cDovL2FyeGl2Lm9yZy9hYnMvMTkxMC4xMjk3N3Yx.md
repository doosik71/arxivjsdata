# TRANSFORMER-TRANSDUCER: END-TO-END SPEECH RECOGNITION WITH SELF-ATTENTION

Ching-Feng Yeh, Jay Mahadeokar, Kaustubh Kalgaonkar, Yongqiang Wang, Duc Le, Mahaveer Jain, Kjell Schubert, Christian Fuegen, Michael L. Seltzer

## 🧩 Problem to Solve

기존의 음성 인식(ASR) 시스템은 음향, 발음, 언어 모델과 같은 여러 개별 구성 요소로 이루어진 하이브리드 시스템이었습니다. 이는 구축 과정에 많은 수작업이 필요하고, 높은 계산 복잡도 및 메모리 사용량으로 인해 온디바이스(on-device) ASR과 같은 리소스 제한 환경에서의 배포를 어렵게 만들었습니다. 이러한 문제점을 해결하기 위해 오디오 신호에서 직접 텍스트로 변환하는 엔드투엔드(end-to-end) 접근 방식이 대두되었으며, 특히 RNN-T(Recurrent Neural Network Transducer)가 유망한 성능을 보였습니다.

그러나 기존 RNN 기반 모델(예: LSTM)은 순환 연결로 인해 병렬 계산이 어렵고, 모든 문맥 정보가 고정된 차원의 상태 벡터에 압축되어 긴 문맥(long context) 포착에 한계가 있었습니다. 트랜스포머(Transformer) 모델은 자기 주의(self-attention) 메커니즘을 통해 이러한 RNN의 단점을 극복하고 병렬 계산 및 긴 문맥 포착에 강점을 보였지만, ASR을 위한 뉴럴 트랜스듀서 프레임워크에 적용할 때 몇 가지 문제가 있었습니다. 특히, 자기 주의 메커니즘은 순서/위치 정보를 잃을 위험이 있고, 전체 입력 시퀀스에 주의를 기울이므로 스트리밍 추론(streaming inference)이 불가능하며, 계산 복잡도가 $O(T^2)$ (여기서 $T$는 입력 시퀀스 길이)로 높아 비효율적이라는 과제가 있었습니다.

## ✨ Key Contributions

- **VGGNet과 인과 컨볼루션(Causal Convolution)을 이용한 문맥 모델링:** 트랜스포머에 위치 정보를 통합하고 효율적인 추론을 위해 프레임 속도를 줄였습니다.
- **Truncated Self-Attention 제안:** 트랜스포머의 높은 계산 복잡도와 스트리밍 불가 문제를 해결하기 위해, 자기 주의가 특정 제한된 문맥 내에서만 작동하도록 하여 스트리밍을 가능하게 하고 계산 복잡도를 $O(T)$로 감소시켰습니다.
- **RNN-T에서 LSTM/BLSTM 기반 모델보다 우수한 성능 달성:** 제안된 Transformer-Transducer는 LibriSpeech 코퍼스에서 기존 RNN 기반 모델 대비 더 낮은 WER(Word Error Rate)을 기록했습니다.
- **스트리밍 가능, 컴팩트, 효율적인 시스템:** 전체 시스템이 45.7M 파라미터로 컴팩트하고, 스트리밍을 지원하며, $O(T)$의 계산 복잡도를 유지하여 온디바이스 ASR과 같은 리소스 제한 시나리오에 적합함을 입증했습니다.

## 📎 Related Works

- **전통적인 하이브리드 ASR 시스템:** CD-DNN-HMM [1]과 같이 음향, 발음, 언어 모델이 분리된 접근 방식.
- **엔드투엔드 ASR 모델:**
  - **CTC (Connectionist Temporal Classification)** [9]: 시퀀스-투-시퀀스(sequence-to-sequence) 변환을 모델링하는 초기 엔드투엔드 접근법.
  - **RNN-T (Recurrent Neural Network Transducer)** [5, 6]: CTC를 개선하여 이전 출력 심볼에 조건부로 의존하여 모델링하며, 엔드투엔드 ASR에서 유망한 성능을 보였습니다 [7, 18].
  - **Attention 기반 ASR 모델** [11, 15]: RNN의 한계를 극복하기 위해 어텐션 메커니즘을 도입.
- **Transformer** [14]: 자기 주의 메커니즘을 기반으로 순환 연결 없이 시퀀스 모델링에서 최첨단 성능을 달성한 모델.
- **Convolutional Context for ASR** [8, 17, 21]: 트랜스포머의 위치 정보 손실을 보완하기 위해 컨볼루션 네트워크를 활용하는 접근법.
- **Time-Delayed Neural Network (TDNN)** [22, 23]: 문맥 정보를 효율적으로 모델링하는 데 사용된 네트워크로, Truncated Self-Attention 아이디어에 영향을 주었습니다.

## 🛠️ Methodology

이 논문은 뉴럴 트랜스듀서(neural transducer) 프레임워크에 트랜스포머 네트워크를 적용한 **Transformer-Transducer** 모델을 제안합니다.

1. **뉴럴 트랜스듀서 아키텍처:**

   - **인코더:** 입력 음향 시퀀스 $x = (x_1, \dots, x_T)$를 상위 수준 표현 $h = (h_1, \dots, h_{T'})$으로 인코딩합니다.
   - **예측기:** 이전 공백이 아닌 출력 심볼 $y_{u-1}$을 인코딩하여 $p_u$를 생성합니다.
   - **조이너:** 인코더 출력 $h_t$와 예측기 출력 $p_u$를 결합하여 최종 로짓 $z_{t,u}$를 생성합니다.
     $$z_{t,u} = \phi(h_t W_h + p_u W_p) W_o$$
     여기서 $W_h$, $W_p$는 각각 $h_t$와 $p_u$를 공통 특징 공간으로 투영하는 가중치 행렬이며, $\phi(\cdot)$는 ReLU 활성화 함수, $W_o$는 로짓을 생성하는 가중치 행렬입니다.

2. **인코더: VGG-Transformer**

   - **Causal VGGNet:** 트랜스포머의 위치 정보 손실 문제를 해결하고 프레임 속도를 줄이기 위해 사용됩니다. 두 개의 2차원 인과 컨볼루션 레이어(미래 정보 누출 방지)와 2차원 맥스 풀링 레이어를 연속적으로 쌓습니다. 첫 번째 VGGNet은 시간 차원에서 맥스 풀링 3, 두 번째는 맥스 풀링 2를 적용하여 총 $6$배의 프레임 속도 감소를 달성합니다.
   - **트랜스포머 인코더 레이어:** VGGNet 출력 후 차원 축소를 위한 선형 레이어와 여러 개의 트랜스포머 인코더 레이어가 뒤따릅니다. 각 레이어는 멀티 헤드 자기 주의, 피드-포워드 네트워크, 레이어 정규화 및 드롭아웃으로 구성됩니다.
   - **Truncated Self-Attention:** 자기 주의의 계산 복잡도 ($O(T^2)$)와 스트리밍 불가 문제를 해결하기 위해, 각 시간 단계 $t$의 출력 $h_t$가 $x_{t-L}$부터 $x_{t+R}$까지의 제한된 문맥에만 의존하도록 합니다. $L$은 왼쪽 문맥, $R$은 오른쪽 문맥을 나타내며, 이를 통해 스트리밍을 가능하게 하고 계산 복잡도를 $O(T)$로 줄입니다.

3. **예측기:** 실험 결과, 2개의 700개 은닉 유닛 LSTM 레이어로 구성된 `LSTM 2x700`이 트랜스포머 기반 예측기보다 더 나은 성능을 보여 채택되었습니다.

4. **학습 및 디코딩:**
   - LibriSpeech 960시간 코퍼스로 학습되었습니다.
   - 80차원 로그 멜 필터 뱅크 특징을 10ms마다 추출하고 전역 평균으로 정규화했습니다.
   - 데이터 증강을 위해 SpecAugment [25]를 적용했습니다.
   - 256개의 심볼을 가진 SentencePiece [26] 모델을 출력 심볼로 사용했습니다.
   - 디코딩에는 빔 크기 10의 표준 빔 서치(beam search)를 사용했습니다.

## 📊 Results

실험은 LibriSpeech 코퍼스에 대해 수행되었으며, WER(Word Error Rate)로 성능을 평가했습니다.

1. **Transformer/LSTM 조합 결과 (표 1):**

   - 인코더로 무제한 자기 주의를 사용하는 `Transformer 12x`는 `test-clean`에서 6.08%, `test-other`에서 13.89%의 WER을 달성하여, `BLSTM 4x640` (비-스트리밍) 및 `LSTM 5x1024` (스트리밍) 기반 인코더보다 우수한 성능을 보였습니다.
   - 예측기로는 `LSTM 2x700`이 `Transformer 6x`보다 consistently 더 나은 결과를 제공했습니다. 이는 빔 서치 디코딩 과정에서 트랜스포머 예측기의 병렬 처리 이점이 상쇄되기 때문입니다.
   - 최종적으로, 인코더로 `Transformer 12x`를, 예측기로 `LSTM 2x700`을 사용하는 조합 (`Transformer 12x + LSTM 2x700`)이 가장 유망하며, 전체 시스템은 45.7M 매개변수로 구성됩니다.

2. **Truncated Self-Attention 결과 (표 2):**
   - 오른쪽 문맥 $R$이 $0$일 경우 (순수 인과적), WER이 크게 증가했습니다 (`test-clean` 12.32%, `test-other` 23.08%).
   - $R$ 값이 증가함에 따라 WER은 점진적으로 개선되어 $R=8$일 때 무제한 자기 주의의 성능에 근접했습니다 (`test-clean` 5.99%, `test-other` 14.17%).
   - 스트리밍 가능성과 지연 시간 최소화를 위해 오른쪽 문맥 $R=4$를 고정한 후, 다양한 왼쪽 문맥 $L$ 값으로 실험했습니다.
   - $(L, R) = (16, 4)$일 때, VGG-Transformer는 LSTM/BLSTM baseline보다 더 나은 WER을 기록했습니다.
   - **제안된 최적 모델:** $(L, R) = (32, 4)$로 설정했을 때, `test-clean`에서 6.37%, `test-other`에서 15.30%의 WER을 달성했습니다. 이는 무제한 자기 주의 대비 `test-clean`에서 약 4.7%, `test-other`에서 약 10.1%의 상대적 성능 손실로, 시스템은 스트리밍 가능하고 $O(T)$의 계산 복잡도를 유지했습니다.

## 🧠 Insights & Discussion

- **트랜스포머 인코더의 우수성:** VGG-Transformer 인코더는 기존 BLSTM 인코더를 능가하는 성능을 보였습니다. 이는 VGGNet을 통한 효율적인 위치 정보 통합과 자기 주의 메커니즘이 시퀀스 전체의 문맥을 효과적으로 포착하는 능력이 ASR 인코더에 매우 적합함을 보여줍니다.
- **예측기로서 LSTM의 역할:** 예측기에서는 트랜스포머보다 LSTM이 더 나은 성능을 보였습니다. 이는 빔 서치(beam search) 디코딩 과정이 토큰을 순차적으로 생성하기 때문에 트랜스포머의 병렬 계산 이점을 충분히 활용하기 어렵다는 것을 의미합니다. 예측기 역할에서는 순환적 문맥 모델링이 여전히 효율적일 수 있습니다.
- **Truncated Self-Attention의 중요성과 성능-효율성 트레이드오프:**
  - 오른쪽 문맥 $R$을 제한하는 것은 모델을 스트리밍 가능하게 만들고, 왼쪽 문맥 $L$을 제한하는 것은 계산 복잡도를 $O(T)$로 줄여 효율성을 높이는 데 필수적입니다.
  - 그러나 문맥이 너무 제한되면 (예: $R=0$) 성능이 크게 저하되므로, 자기 주의 메커니즘이 충분한 문맥 정보를 활용할 수 있도록 적절한 $L$과 $R$ 값을 찾는 것이 중요합니다.
  - $(L, R) = (32, 4)$와 같은 적절한 문맥 범위를 통해 성능 손실을 최소화하면서 스트리밍 가능성, 낮은 지연 시간, $O(T)$의 계산 효율성을 동시에 달성할 수 있었습니다. 이는 온디바이스 ASR과 같이 리소스가 제한된 환경에서 트랜스포머를 실용적으로 활용하는 데 핵심적인 통찰을 제공합니다.
- **시스템의 실용성:** 제안된 Transformer-Transducer는 높은 정확도와 함께 스트리밍 가능성, 컴팩트한 모델 크기, 효율적인 계산 복잡도를 제공하여 온디바이스 음성 인식과 같은 실제 응용 시나리오에서 매우 유망한 솔루션임을 보여줍니다.

## 📌 TL;DR

이 논문은 엔드투엔드 음성 인식을 위해 뉴럴 트랜스듀서 프레임워크에 트랜스포머 네트워크를 적용한 **Transformer-Transducer**를 제안합니다. 기존 RNN의 병렬 처리 한계와 트랜스포머의 위치 정보 손실 및 스트리밍 불가, 높은 계산 복잡도 문제를 해결하고자 했습니다. 이를 위해, 인코더에 **VGGNet과 인과 컨볼루션**을 도입하여 위치 정보를 통합하고 프레임 속도를 줄였으며, **Truncated Self-Attention**을 사용하여 자기 주의의 문맥 범위를 제한함으로써 스트리밍 추론을 가능하게 하고 계산 복잡도를 $O(T)$로 낮췄습니다. 결과적으로 `LibriSpeech` 데이터셋에서 `test-clean` 6.37%, `test-other` 15.30%의 WER을 달성하며 기존 LSTM/BLSTM 기반 모델들을 능가했습니다. 제안된 시스템은 45.7M 매개변수로 컴팩트하고, 스트리밍 가능하며, 계산 효율적이라는 점에서 온디바이스 음성 인식과 같은 리소스 제한 환경에 매우 적합한 솔루션입니다.
