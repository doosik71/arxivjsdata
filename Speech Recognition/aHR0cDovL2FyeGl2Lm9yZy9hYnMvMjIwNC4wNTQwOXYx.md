# Unified Speech-Text Pre-training for Speech Translation and Recognition
Yun Tang, Hongyu Gong, Ning Dong, Changhan Wang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski, Xian Li, Abdelrahman Mohamed, Michael Auli, Juan Pino

## 🧩 Problem to Solve
기존의 음성 또는 텍스트 전용 사전 학습(pre-training) 방식은 대량의 비전사(untranscribed) 음성 데이터와 풍부한 텍스트 데이터를 효과적으로 통합하지 못합니다. 전사된 음성 데이터는 부족하여 견고한 언어 지식을 학습하기 어렵습니다. 이 연구는 음성-텍스트(Speech-to-Text, S2T) 번역 및 자동 음성 인식(Automatic Speech Recognition, ASR)과 같은 음성-텍스트 작업에서 다양한 양식(modality)의 데이터 통합이 표현 학습에 이득이 되는지, 그리고 이를 위한 통합된 모델링 프레임워크를 어떻게 구축할 것인지를 탐구합니다.

## ✨ Key Contributions
*   **다중 작업 학습 프레임워크 제안:** 음성 및 텍스트 데이터를 활용하는 4가지 하위 작업(subtask)을 하나의 모델에서 학습하는 다중 작업 학습(multi-task learning) 프레임워크를 제안하여, 텍스트 코퍼스의 언어 정보를 음성 사전 학습에 성공적으로 통합했습니다.
*   **하위 작업 간의 간섭 분석:** 제안된 사전 학습 방법론에 대한 상세 분석을 통해 서로 다른 하위 작업들 간의 학습 간섭 현상을 밝혀냈습니다.
*   **간섭 완화를 위한 구성 제안:** ASR 및 S2T 각각에 대해 학습 간섭을 완화하기 위한 두 가지 사전 학습 구성(완전 공유 인코더: FSE, 부분 공유 인코더: PSE)을 제안했습니다.
*   **최첨단 성능 달성:** MUST-C 음성 번역 데이터셋에서 기존 최신 시스템 대비 1.7에서 2.3 BLEU 점수 향상을 달성했으며, LIBRISPEECH ASR 작업에서 wav2vec 2.0과 유사한 WER(Word Error Rate)을 기록했습니다.

## 📎 Related Works
*   **사전 학습 (Pre-training):**
    *   **대조 손실 (Contrastive Loss):** wav2vec 2.0 (Baevski et al., 2020b) 등에서 음성 인식에 성공적으로 적용되었습니다.
    *   **마스킹 예측 손실 (Masked Prediction Loss):** BERT (Devlin et al., 2019), BART (Lewis et al., 2020b)와 같이 NLP 분야에서 시작하여 음성 처리 (Baevski et al., 2020a; Hsu et al., 2021)로 확장되었습니다. 본 연구의 자기 지도 학습 음성 하위 작업(SSL)은 마스킹 예측 손실과 유사하지만, 오류에 취약한 하드 레이블 예측 대신 KL 발산(Kullback-Leibler divergence)을 사용하여 소프트 레이블 예측을 수행합니다.
*   **자기 학습 (Self-training):**
    *   레이블이 없는 음성 데이터 활용을 위해 널리 사용되는 접근 방식 (Kahn et al., 2020; Xu et al., 2020). 소량의 지도 학습 데이터를 통해 가짜 레이블(pseudo labels)을 생성하여 모델을 개선합니다. 본 연구는 소량의 지도 학습 데이터를 사용하여 음성-텍스트 모델링 공간을 통합하지만, SSL은 하드 예측을 피하고 KL 발산을 통해 마스킹된 스팬과 관찰된 특징 프레임 간의 상호 정보를 최대화합니다.
*   **다중 작업 학습 (Multi-task learning):**
    *   데이터 부족 문제를 해결하기 위해 ST에서 병렬 텍스트 학습 데이터를 활용하는 데 널리 사용됩니다 (Weiss et al., 2017; Anastasopoulos and Chiang, 2018). 그러나 이러한 방법은 주로 지도 학습 음성 데이터를 사용하지만, 본 연구는 대량의 레이블이 없는 음성 데이터를 사전 학습 단계에서 활용합니다.
*   **동시 연구 (Concurrent works):**
    *   Ao et al. (2021)은 ASR 및 TTS(Text-to-Speech) 애플리케이션을 위한 완전 비지도 방식의 음성-텍스트 공동 사전 학습을 제안했습니다. 본 연구는 지도 학습 음성 데이터 활용에 중점을 둡니다.
    *   Bapna et al. (2021)은 음성 및 텍스트 데이터를 사용한 음성 인코더 사전 학습에 중점을 두지만, 본 연구는 인코더-디코더(encoder-decoder) 프레임워크와 사전 학습 단계에서 인코더와 디코더 모두를 학습하는 것을 강조합니다.

## 🛠️ Methodology
본 연구는 어텐션 기반 인코더-디코더(Attention-based Encoder-Decoder, AED) 프레임워크를 활용하며, 다음 네 가지 하위 작업을 포함하는 통합된 사전 학습 프레임워크인 **STPT (Speech and Text joint Pre-Training)**를 제안합니다.

1.  **(자기-)지도 학습 텍스트-텍스트 하위 작업 (T2T)**:
    *   ASR 사전 학습에서는 BART(Denoising Autoencoder)로, ST 사전 학습에서는 텍스트 기반 신경 기계 번역(NMT)으로 구현됩니다.
    *   모델의 디코더는 인코더 출력에 기반하여 텍스트를 생성하며, 풍부한 텍스트 데이터를 활용합니다.
    *   입력 텍스트를 해당 발음 형태(음소 시퀀스)로 변환하여 음성과 텍스트 인코더 출력 간의 정렬을 용이하게 합니다.

2.  **자기 지도 학습 음성 학습 하위 작업 (SSL)**:
    *   대량의 레이블 없는 음성 데이터를 활용하여 일반적인 음성 표현을 학습하는 데 사용됩니다.
    *   wav2vec 2.0과 유사한 특징 추출기(feature extractor)와 컨텍스트 인코더(context encoder)로 구성됩니다.
    *   **마스킹된 KL 발산 손실 (Masked KL Divergence Loss)**을 사용하여 최적화됩니다. 마스킹된 음성 특징 스팬에 대해 첫 번째 패스에서 예측된 음소 분포($p(o_j|e_i)$)를 소프트 레이블로 사용하여 두 번째 패스에서 마스킹된 출력($p(\hat{o}_j|e_i)$)과의 KL 발산을 최소화합니다. 이는 하드 레이블 예측 오류를 피합니다.

3.  **지도 학습 음성-음소 분류 하위 작업 (S2P)**:
    *   전사된 ASR 데이터셋을 사용하여 프레임 레벨 음소 레이블을 예측합니다.
    *   HMM-GMM 모델을 사용하여 강제 정렬(forced-alignment)을 통해 프레임 레벨 레이블을 생성합니다.
    *   자기 지도 학습된 음성 모델과 텍스트 모델을 하나의 모델링 공간으로 통합하는 역할을 합니다.

4.  **지도 학습 AED 기반 음성-텍스트 하위 작업 (S2T)**:
    *   다운스트림 ASR 또는 ST 작업과 동일한 보조 작업으로 사전 학습 단계에 포함됩니다.
    *   ASR의 경우 음성 전사 레이블을, ST의 경우 번역 레이블을 사용합니다.
    *   사전 학습에서 미세 조정(fine-tuning)으로의 전환을 원활하게 하여 하류 작업 성능을 향상시키는 것을 목표로 합니다.

**전체 손실 함수:** $L = L_{T2T} + \alpha L_{SSL} + \beta L_{S2P} + \gamma L_{S2T}$

**모델 구성:**
*   **ASR 사전 학습:** 완전 공유 인코더(FSE) 구성 (그림 1a)을 사용하여 정보 공유를 장려합니다. ASR 하위 작업은 상호 보완적이기 때문입니다.
*   **ST 사전 학습:** 부분 공유 인코더(PSE) 구성 (그림 1b)을 사용하여 인코더 전용 하위 작업(SSL, S2P)과 시퀀스-투-시퀀스 AED 하위 작업(T2T, S2T) 간의 정보 공유를 최소화하여 간섭을 줄입니다.

**학습 전략:**
1.  **T2T 사전 학습:** 먼저 T2T 하위 작업을 수렴할 때까지 학습하여 음소 임베딩에 좋은 초기화를 제공합니다.
2.  **공동 사전 학습:** 모든 네 가지 하위 작업을 동시에 최적화합니다.
3.  **미세 조정:** 사전 학습된 모델을 다운스트림 작업에 대해 미세 조정합니다 (T2T 및 S2T 작업은 유지하고 SSL 및 S2P는 제외).
*   공유 인코더 입력에 LayerNorm을 적용하여 다중 작업 학습의 안정성을 확보합니다.

## 📊 Results
*   **ASR (Librispeech):**
    *   STPT는 기존 AED 기반 시스템 (LAS, Transformer)을 능가하며, 공동 학습된 Transformer 모델 대비 평균 1.2 WER 감소를 달성했습니다.
    *   외부 언어 모델(LM) 없이 CTC 기반 wav2vec 2.0보다 2.2 WER이 낮으며, LM을 적용했을 때 wav2vec 2.0과 필적하는 WER을 보였습니다.
    *   STPT 모델은 디코딩 LM의 도움을 크게 받지 않는다는 점이 주목할 만합니다 (0.2 WER 감소). 이는 모델이 텍스트 코퍼스의 언어 정보를 효과적으로 융합했음을 시사합니다.
    *   6만 시간 Librilight 데이터로 사전 학습 시 어려운 "other" 데이터셋에서 WER을 추가로 감소시켰습니다.
*   **음성 번역 (MuST-C):**
    *   STPT는 EN-ES에서 2.3 BLEU, EN-FR에서 1.7 BLEU를 기록하여 최신 시스템 대비 크게 향상된 최첨단 성능을 달성했습니다.
*   **모델 구조의 영향 (하위 작업 간섭):**
    *   ASR 사전 학습(FSE): 공유 인코더에서 하위 작업 간의 기울기 유사성이 낮아 심각한 간섭이 관찰되지 않았고, 하위 작업들이 상호 보완적임을 나타냅니다. ASR 작업에서는 FSE가 더 낮은 WER을 달성했습니다.
    *   ST 사전 학습(PSE): 공유 인코더의 특정 계층에서 상당한 기울기 유사성(양수 및 음수 모두)이 관찰되어 간섭이 심각함을 보였습니다. ST 작업에서는 PSE가 더 높은 BLEU 점수를 달성하여 간섭 완화의 효과를 입증했습니다.
*   **학습 데이터의 영향:**
    *   사전 학습 단계에서 더 많은 지도 학습 음성 데이터는 WER 감소에 항상 도움이 되었습니다.
    *   적은 양의 레이블 데이터(10시간)로 사전 학습한 후 대량의 데이터(960시간)로 미세 조정해도 좋은 음성 표현을 얻을 수 있었습니다 (평균 4.0 WER).
    *   하지만 지도 학습 데이터가 적을 경우 성능이 급격히 저하될 수 있음을 확인했습니다 (10시간 사전 학습 + 10시간 미세 조정 시 평균 24.6 WER).
    *   사전 학습-미세 조정 조건의 불일치(clean vs. other)는 WER을 약간 증가시킬 수 있으나, 일치하는 조건에서 최상의 결과를 얻었습니다.
*   **마스킹된 KL 발산 손실 vs. 대조 손실:**
    *   SSL 하위 작업에 제안된 마스킹된 KL 발산 손실이 대조 손실보다 우수한 성능을 보였습니다 (Librispeech dev 셋에서 약 0.6 낮은 WER, MuST-C에서 0.7~1.4 높은 BLEU).
*   **어블레이션 연구 (Ablation Study):**
    *   **T2T 사전 학습 생략 시:** 평균 0.5 WER 증가, EN-ES에서 1.2 BLEU 손실. (초기화의 중요성)
    *   **S2T 하위 작업 제거 시:** ASR 테스트에서 평균 1.1 WER 증가, ST 방향에서 1.8 BLEU 감소. (AED 작업의 중요성)
    *   **S2P 하위 작업 제거 시:** 학습이 수렴하지 않고 SSL 및 S2T 하위 작업에서 진행이 거의 없음. (S2P가 음성 및 텍스트 모델 통합에 중요)
    *   **공동 사전 학습 없이 학습 시 (T2T 및 S2T만 최적화):** Librispeech 테스트 세트에서 약 1.4 WER 증가, ST 방향에서 평균 3.4 BLEU 감소. (공동 사전 학습의 필수성)

## 🧠 Insights & Discussion
본 연구는 음성-텍스트 작업을 위해 음성과 텍스트 정보를 효과적으로 융합하는 통합 사전 학습 방법론인 STPT를 성공적으로 제시했습니다. 주요 통찰은 다음과 같습니다:

*   **다양한 양식 데이터 통합의 중요성:** 대량의 레이블 없는 음성 데이터와 풍부한 텍스트 데이터를 공동 사전 학습에 활용하여 음성 번역 및 음성 인식 성능을 크게 향상시킬 수 있음을 입증했습니다.
*   **언어 정보 통합의 효과:** 텍스트 코퍼스에서 얻은 언어 정보가 음성 사전 학습에 성공적으로 통합되어, ASR 모델이 디코딩 LM 없이도 강력한 언어 능력을 발휘할 수 있게 됩니다.
*   **하위 작업 간 간섭 관리:** 다중 작업 학습에서 하위 작업 간의 간섭이 실제 문제임을 확인하고, 이를 완화하기 위해 ASR (FSE) 및 ST (PSE)에 최적화된 인코더 구성을 제안한 것이 핵심적인 기여입니다.
*   **마스킹된 KL 발산 손실의 우수성:** SSL 하위 작업에서 제안된 마스킹된 KL 발산 손실이 기존의 대조 손실보다 효과적임을 보여주었습니다.
*   **지도 학습 데이터의 역할:** 사전 학습에 사용되는 소량의 지도 학습 데이터조차도 모델의 교차 양식(cross-modality) 표현 학습에 중요하며, 최종 성능에 긍정적인 영향을 미칩니다.

**제한 사항:** 본 연구는 텍스트 데이터를 적절하게 처리하지 않을 경우 발생할 수 있는 잠재적 부정적 영향(예: 불공정하거나 부적절한 데이터 학습)에 대해 언급하며, 데이터 수집 시 공정성 평가의 중요성을 강조합니다.

## 📌 TL;DR
STPT는 음성 번역 및 인식을 위해 음성과 텍스트를 통합하여 사전 학습하는 인코더-디코더 프레임워크입니다. 네 가지 하위 작업 (T2T, SSL, S2P, S2T)을 통해 대량의 비레이블 음성 및 풍부한 텍스트 데이터를 활용합니다. 하위 작업 간 간섭을 완화하기 위해 ASR에는 완전 공유 인코더(FSE)를, ST에는 부분 공유 인코더(PSE)를 사용하여 최적의 성능을 달성합니다. 결과적으로 MUST-C 음성 번역에서 SOTA를 달성하고, LIBRISPEECH ASR에서 wav2vec 2.0에 필적하는 성능을 보여, 음성과 텍스트 정보의 효과적인 융합과 언어 정보 통합의 중요성을 입증했습니다.