# SPEECH SYNTHESIS AS AUGMENTATION FOR LOW-RESOURCE ASR
Deblin Bagchi, Shannon Wotherspoon, Zhuolin Jiang, Prasanna Muthukumar

## 🧩 Problem to Solve
현대 자동 음성 인식(ASR) 시스템은 대규모 데이터셋을 요구하지만, 이 데이터는 이그보어(Igbo)와 같은 저자원 언어에는 존재하지 않습니다. 이 논문은 현대 신경망 기반 ASR 기술을 사용하면서도 필요한 데이터의 양을 줄여 저자원 언어를 위한 실행 가능한 ASR 시스템을 구축하는 것을 목표로 합니다.

## ✨ Key Contributions
*   음성 합성을 데이터 증강 기술로 사용하여 저자원 ASR의 성능을 향상시킬 가능성을 탐구했습니다.
*   세 가지 다른 유형의 음성 합성기(통계적 파라미터, 신경망, 적대적)를 사용하여 저자원 시나리오에서 그 효용성을 평가했습니다.
*   저자원 및 다중 화자 환경에서 멜 켑스트럼 왜곡(MCD)과 같은 객관적 품질 측정의 한계를 밝혀냈습니다.
*   Tacotron2와 같은 최신 신경망 기반 합성기가 저자원 및 다중 화자 데이터셋에서 직면하는 기술적 및 연구적 어려움을 강조했습니다.
*   현재 음성 합성 기술이 저자원 ASR 데이터 증강에 충분히 발전하지 못했음을 보여주었으며, 향후 연구 방향을 제시했습니다.

## 📎 Related Works
*   **Rosenberg et al. [8]:** Tacotron 음성 합성기를 사용하여 실제 음성을 증강하여 어휘 및 음향 다양성을 높이는 접근 방식을 설명했습니다. 본 연구는 이와 유사하지만, 더 적은 양의 데이터를 사용하여 Tacotron을 넘어 다른 합성기도 탐색해야 했습니다.
*   **Xu et al. (LRSpeech) [10]:** 고자원 언어의 ASR 및 TTS 시스템을 부트스트랩하여 저자원 언어에 적용하는 방법을 제시했습니다. 본 연구는 저자원 언어 자체의 코퍼스 외에는 가용한 코퍼스가 없다고 가정하며, 상호 보완적인 접근 방식입니다.

## 🛠️ Methodology
1.  **음성 인식기 설정:**
    *   BBN Sage [11] (Kaldi [12] 확장) 음성 인식 시스템을 사용했습니다.
    *   다국어 초기화 [13]된 하이브리드 TDNN-F [14] 모델을 사용했습니다.
    *   훈련은 Lattice-Free MMI (LF-MMI) 1 에포크와 sMBR 2 에포크로 진행했습니다.
    *   언어 모델은 Librispeech 전사본 100시간 분량으로 훈련된 단어 수준의 트라이그램 모델을 사용했습니다.
    *   오디오 데이터의 양을 조절하며 음향 모델에 중점을 두었습니다.
2.  **데이터 증강을 위한 음성 합성기 실험:**
    *   **통계적 파라미터 합성 (Clustergen [15]):**
        *   텍스트를 음소 시퀀스로 변환하는 Festival 시스템 [16]을 사용하고, 랜덤 포레스트로 멜 주파수 켑스트럼 계수(MFCC)를 예측합니다.
        *   다중 화자 ASR 코퍼스에 맞추기 위해 i-벡터 [17]를 사용하여 유사한 화자들을 클러스터링하고, 각 클러스터에 대해 "평균 모델"을 구축했습니다.
    *   **신경망 TTS (Tacotron2 [6] + Waveglow [18]):**
        *   Tacotron2는 문자 임베딩을 입력으로 받아 멜 스펙트로그램을 예측하는 순환 시퀀스-투-시퀀스 신경망입니다. Waveglow [18]는 멜 스펙트로그램을 원시 음성으로 변환하는 보코더입니다.
        *   다중 화자 훈련을 지원하기 위해 화자 임베딩(원-핫 또는 i-벡터)을 문자 임베딩과 함께 입력으로 추가했습니다.
    *   **적대적 TTS (WGANSing [7]):**
        *   생성적 적대 신경망(GAN) 기반의 시스템으로, 원래 노래 합성용이지만 개방 어휘 음성 합성을 위해 사용되었습니다.
        *   정확한 음소 지속 시간과 피치 윤곽과 같은 입력 특성에 대한 요구 사항을 피하기 위해 도구 키트를 수정했습니다.

## 📊 Results
*   **통계적 파라미터 합성 (Clustergen):**
    *   합성된 음성을 추가 훈련 데이터로 사용했을 때 ASR 성능(WER)은 20시간 실제 데이터(12.7 WER)와 비교하여 거의 개선되지 않거나 오히려 악화되었습니다 (예: 20시간 실제 + 20시간 합성: 12.6 WER; 20시간 실제 + 60시간 합성: 13.0 WER).
    *   순수 합성 음성으로만 훈련했을 때의 ASR WER은 80시간 실제 데이터(11.4 WER)에 비해 현저히 나빴습니다 (예: 80시간 비클러스터링 합성: 36.2 WER, 80시간 클러스터링 합성: 47.1 WER). 이는 Clustergen의 합성 품질이 데이터 증강에 충분하지 않음을 시사합니다.
*   **신경망 TTS (Tacotron2):**
    *   24시간 LJ Speech 데이터셋으로는 고품질 합성을 생성했지만, 1시간 CMU Arctic 데이터셋에서는 유사한 결과를 얻지 못했습니다 (원래 화자와 매우 유사한 목소리로 "중얼거림"만 생성).
    *   다중 화자 확장을 시도했으나, 합성 품질이 고전적인 파라미터 합성(Clustergen)보다 현저히 나빴기 때문에 증강 실험을 진행하지 않았습니다.
*   **적대적 TTS (WGANSing):**
    *   낮은 훈련 데이터량으로 인해 다른 합성기들에 비해 합성 품질이 좋지 않았습니다. 저자원 환경이 적대적 학습의 "아킬레스건"일 수 있음을 시사합니다.

## 🧠 Insights & Discussion
*   **멜 켑스트럼 왜곡(MCD) 측정의 실패:** 표준 음성 합성에서는 신뢰할 수 있는 MCD가 시끄러운 다중 화자 데이터에서는 품질을 제대로 반영하지 못하고 오해의 소지가 많았습니다. 높은 MCD에도 불구하고 이해 가능한 음성이 있거나, 낮은 MCD에도 불구하고 알아들을 수 없는 음성이 있었습니다. 이는 단일 화자 객관적 측정 지표가 다중 화자 훈련 시나리오에 잘 일반화되지 않음을 보여줍니다.
*   **Tacotron2의 도전 과제:** 상당한 계산 자원 요구와 긴 수렴 시간, 작은 데이터셋에서의 성능 저하 등 엔지니어링 및 연구 측면에서 많은 어려움을 겪었습니다. 신경망의 불투명한 특성으로 인해 다중 화자 시스템으로 확장하기 어려웠습니다.
*   **결론적으로 현재 합성 기술의 한계:** 현대 음성 합성 기술은 저자원 음성 인식기 훈련에 충분히 발전하지 못했습니다. 하지만 합성 품질이 향상됨에 따라 이 접근 방식이 결국 실행 가능해질 것이라고 믿습니다.
*   **미래 연구 방향:** 미래의 합성기는 단순히 사람의 귀를 위한 것이 아니라, 명확하게 증강 데이터의 원천으로 작동하도록 설계되어야 할 가능성이 높습니다.

## 📌 TL;DR
저자원 ASR을 위한 데이터 부족 문제를 해결하고자, 음성 합성을 데이터 증강 방법으로 사용했습니다. 통계적(Clustergen), 신경망(Tacotron2), 적대적(WGANSing) 세 가지 합성기를 실험했지만, 소량의 데이터셋에서 충분히 높은 품질의 합성 음성을 생성하지 못하여 ASR 성능을 효과적으로 향상시키지 못했습니다. 특히 다중 화자 시나리오와 저자원 환경에서 합성 품질 저하와 기존 평가 지표(MCD)의 한계가 드러났습니다. 현재 음성 합성 기술은 저자원 ASR 증강에 아직 미흡하지만, 향후 합성 품질 개선 시 유망한 접근법이 될 수 있음을 시사합니다.