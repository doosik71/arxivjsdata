# Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition

Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu

## 🧩 Problem to Solve

이 논문은 제한된 양의 레이블이 지정된 데이터(LibriSpeech)와 방대한 양의 레이블이 지정되지 않은 오디오 데이터(Libri-Light)를 활용하여 자동 음성 인식(ASR) 모델의 성능을 향상시키는 것을 목표로 합니다. 특히, 최신 준지도 학습(SSL) 방법론을 통합하여 LibriSpeech 벤치마크에서 기존의 최신 기술(SOTA) 성능을 뛰어넘는 것을 목표로 합니다.

## ✨ Key Contributions

- **최신 SSL 방법론 결합**: wav2vec 2.0 사전 학습(pre-training)과 SpecAugment를 활용한 노이지 스튜던트 학습(noisy student training)을 결합한 새로운 준지도 학습 파이프라인을 제안했습니다.
- **거대 컨포머(Conformer) 모델 활용**: Conformer XL (6억 파라미터) 및 Conformer XXL (10억 파라미터)과 같은 대규모 컨포머 모델을 사용하여 ASR 성능을 극대화했습니다.
- **LibriSpeech SOTA 달성**: LibriSpeech 데이터셋에서 새로운 SOTA 단어 오류율(WERs)인 test-clean 1.4%, test-other 2.6%를 달성하여 기존 SOTA (1.7%/3.3%)를 크게 개선했습니다.
- **모델 규모와 사전 학습의 중요성 분석**: 모델 크기만 늘리는 것은 성능 향상에 도움이 되지 않지만, 사전 학습과 결합될 때 대규모 모델이 일관된 성능 향상을 가져옴을 보여주었습니다.
- **세부적인 기법 분석**: 사전 학습 및 미세 조정(fine-tuning) 단계의 다양한 설정(예: 입력 특징, 데이터 증강, 데이터 혼합 비율)에 대한 절삭 연구(ablation studies)를 통해 각 구성 요소의 영향을 분석했습니다.

## 📎 Related Works

- **자기 학습 (Self-training)**: 레이블이 지정되지 않은 데이터에 대해 교사(teacher) 모델이 의사(pseudo) 레이블을 생성하고, 이를 학생(student) 모델 학습에 사용하는 방법 [22-33]. ASR 분야에서 광범위하게 연구되어 왔습니다 [25-33].
- **일관성 기반 학습 (Consistency-based Learning)**: 레이블이 지정되지 않은 데이터에서 좋은 데이터 표현을 학습하기 위해 일관성 기반 작업을 도입하고 이를 통해 네트워크를 사전 학습시키는 방법 [34-43].
- **wav2vec 2.0**: 음성 표현의 자기 지도 학습을 위한 프레임워크로, NLP 연구의 발전에 영향을 받았습니다 [6, 45]. 이 논문에서는 컨포머 인코더의 사전 학습에 활용되었습니다.
- **노이지 스튜던트 학습 (Noisy Student Training, NST)**: 이미지 도메인 연구 [7, 44]에서 영감을 받은 자기 학습 방법으로, 데이터 증강을 활용하는 것이 특징입니다 [8].
- **SpecAugment**: ASR을 위한 간단하면서도 효과적인 데이터 증강 기법 [9, 10].
- **Conformer**: 컨볼루션과 트랜스포머의 장점을 결합한 ASR 아키텍처 [5].

## 🛠️ Methodology

본 논문은 wav2vec 2.0 사전 학습과 노이지 스튜던트 학습(NST)을 결합한 반복적인 준지도 학습 파이프라인을 사용합니다.

1. **모델 아키텍처**:

   - LSTM 디코더와 Conformer 인코더로 구성된 시퀀스 트랜스듀서(sequence transducer) 네트워크를 사용합니다.
   - 대규모 모델(Conformer XL, XXL, XXL+)에서는 상대 위치 임베딩(relative positional embedding)을 제거하여 학습 속도를 향상시켰습니다.
   - Conformer XL은 6억 개, XXL은 10억 개의 파라미터를 가지며, XXL+는 XXL에 Conformer 블록과 스태킹 레이어를 추가하여 약 5천만 개의 파라미터를 더 가집니다.

2. **wav2vec 2.0 사전 학습**:

   - Libri-Light 데이터셋의 "unlab-60k" 서브셋(6만 시간)을 사용하여 Conformer 인코더를 사전 학습합니다.
   - 원본 wav2vec 2.0과 달리, 원시 파형 대신 로그-멜 스펙트로그램을 입력으로 사용합니다.
   - 양자화(quantization) 레이어 대신 선형 레이어를 사용하여 목표(target) 컨텍스트 벡터를 생성하고, 마스킹된 위치의 컨텍스트 벡터와 대조 학습(contrastive loss)을 최적화합니다.
   - 사전 학습된 인코더는 이후 미세 조정 단계에서 트랜스듀서로 전달될 특징을 생성하는 투영(projection) 블록과 함께 사용됩니다.

3. **노이지 스튜던트 학습 (NST) 파이프라인**:
   - 사전 학습된 Conformer 모델에 대해 NST를 적용합니다.
   - **단계 요약**:
     1. **초기 미세 조정**: 사전 학습된 모델 $M_0$를 SpecAugment와 함께 레이블이 지정된 데이터셋 $S$ (LibriSpeech)로 미세 조정합니다. $M = M_0$로 설정합니다.
     2. **LM 융합 및 성능 측정**: 현재 모델 $M$을 언어 모델(LM)과 얕게 융합(shallow-fusing)하고 성능을 측정합니다.
     3. **의사 레이블 생성**: 융합된 모델을 사용하여 레이블이 지정되지 않은 데이터셋 $U$ (Libri-Light)에 대한 전사(transcripts)를 생성합니다 ($M(U)$).
     4. **데이터 혼합**: $M(U)$와 $S$를 혼합합니다.
     5. **학생 모델 미세 조정**: 새로운 사전 학습된 모델 $M'$을 SpecAugment와 함께 혼합된 데이터셋으로 미세 조정합니다.
     6. **모델 업데이트 및 반복**: $M = M'$로 설정하고 2단계로 돌아가서 과정을 반복합니다.
   - **SpecAugment**: 적응형 SpecAugment [9, 10]를 사용하여 학생 모델의 입력 데이터를 증강합니다 (주파수 마스크 $F=27$, 시간 마스크 비율 $p_S=0.05$).
   - **배치별 혼합**: 모든 세대에서 배치당 지도 학습 데이터 대 교사 레이블 데이터의 비율을 1:9로 고정합니다.
   - **언어 모델**: 8-레이어, 1억 3백만 파라미터 트랜스포머 LM [48]을 LibriSpeech LM 코퍼스로 학습시켰습니다. LM 융합 파라미터는 각 세대에서 개발 세트 성능을 최대화하도록 재조정됩니다.

## 📊 Results

- **SOTA WER 달성**:
  - LibriSpeech test-clean에서 1.4% (이전 SOTA 1.7%)
  - LibriSpeech test-other에서 2.6% (이전 SOTA 3.3%)
  - LibriSpeech dev-clean에서 1.3%, dev-other에서 2.6%를 달성했습니다.
- **상대적 개선**: 최종 3세대 Conformer XXL+ 모델은 사전 학습된 기준선 대비 dev/test 세트 전반에서 WER이 7-15% 상대적으로 개선되었습니다.
- **모델 크기와 사전 학습의 효과**:
  - 사전 학습 없이 모델 크기만 Conformer L (1억 파라미터)에서 XXL (10억 파라미터)로 늘릴 경우 성능 향상이 없거나 오히려 저하되었습니다.
  - 사전 학습과 결합될 때, 모델 크기가 커질수록 WER이 2.0%에서 1.6% (dev-clean)로, 4.6%에서 3.2% (dev-other)로 일관되게 개선되었습니다 (LM 융합 없음 기준).

## 🧠 Insights & Discussion

- **사전 학습의 필수성**: 대규모 ASR 모델의 잠재력을 최대한 발휘하기 위해서는 단순히 모델 크기를 키우는 것만으로는 부족하며, wav2vec 2.0과 같은 효과적인 사전 학습 기법이 필수적임을 입증했습니다. 이는 대규모 모델의 과적합 방지 및 유용한 표현 학습에 기여합니다.
- **사전 학습 구성 요소의 영향**:
  - 입력으로 로그-멜 스펙트로그램을 사용하는 것과 양자화 레이어 대신 선형 레이어를 사용하는 것이 효과적임을 확인했습니다.
  - 긴 오디오 세그먼트를 사전 학습에 사용할 때는 컨볼루션 서브샘플링 블록을 통해 입력 길이를 더 적극적으로 줄이는 것이 필요합니다.
  - 음성 활동 감지(VAD)를 사용하여 입력 세그먼트를 나눌 경우 성능이 저하될 수 있음을 발견했는데, 이는 모든 세그먼트가 사전 학습에 충분한 문맥을 제공하지 못하기 때문일 수 있습니다.
- **노이지 스튜던트 학습 미세 조정 분석**:
  - 지도 학습 데이터와 교사 생성 데이터의 배치별 혼합 비율은 1:9 또는 비배치별 무작위 혼합이 2:8보다 더 나은 성능을 보였습니다.
  - 이 연구의 특정 작업 및 모델 시리즈에서는 교사 생성 전사(transcripts)를 신뢰도 기반으로 필터링하거나 균형을 맞추는 것이 1세대 모델의 성능 향상에 도움이 되지 않았습니다. 이는 0세대 교사 모델이 이미 매우 우수하여, 의사 레이블 데이터의 양을 최대화하는 것이 품질을 미미하게 개선하는 것보다 더 중요했기 때문으로 추정됩니다.

## 📌 TL;DR

이 논문은 ASR 성능을 향상시키기 위해 Conformer 아키텍처와 대규모 레이블 없는 데이터를 활용하는 **준지도 학습의 한계**를 탐색합니다. **wav2vec 2.0 사전 학습**과 **SpecAugment를 활용한 노이지 스튜던트 학습**을 결합한 파이프라인을 제안하여, LibriSpeech 데이터셋에서 **새로운 SOTA WER (test-clean 1.4%, test-other 2.6%)**를 달성했습니다. 주요 발견은 대규모 모델이 사전 학습과 결합될 때만 성능 이점을 가져오며, 이는 대규모 ASR 모델 개발에서 사전 학습의 중요성을 강조합니다.
