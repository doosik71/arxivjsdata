# SELF-ATTENTION NETWORKS FOR CONNECTIONIST TEMPORAL CLASSIFICATION IN SPEECH RECOGNITION

Julian Salazar, Katrin Kirchhoff, Zhiheng Huang

## 🧩 Problem to Solve

기존의 음성 인식(ASR) 모델은 크게 두 가지 방식으로 발전해 왔습니다:

1. **Connectionist Temporal Classification (CTC) 기반 모델:** 단조로운 시퀀스 변환 작업에 효과적이며, 레이블 정렬 절차를 피할 수 있습니다. 전통적으로 순환 레이어(RNN, LSTM)를 사용하여 시간적 종속성을 모델링했지만, 이는 계산 비용이 높고 병렬화에 한계가 있습니다. 최근에는 컨볼루션 레이어를 사용하는 모델도 제안되었지만, 넓은 시간적 수용 영역을 얻기 위해 매우 깊은 네트워크가 필요합니다.
2. **어텐션 기반 인코더-디코더 모델:** 일반적인 시퀀스 변환에서 큰 성공을 거두었으며 ASR에도 적용되었지만, 단조성(monotonicity)이 강제되지 않아 수천 시간의 데이터, 신중한 학습률 스케줄, 사전 학습, 또는 보조 CTC 손실과 같은 추가적인 노력이 필요하여 학습이 어렵습니다. 또한, 디코더는 일반적으로 추론 시 자기회귀적(autoregressive)으로 동작하여 추론 속도를 제한합니다.

최근 자연어 처리(NLP) 분야에서 자기-어텐션(self-attention) 메커니즘이 큰 성공을 거두었으며, ASR의 인코더-디코더 모델에도 적용되기 시작했습니다. 하지만, 이 연구는 자기-어텐션 메커니즘을 순수 CTC 기반 모델에 통합하여 기존 CTC 모델의 병렬화 및 수용 영역 한계와 인코더-디코더 모델의 추론 속도 문제를 동시에 해결할 수 있는지 탐구하는 것을 목표로 합니다.

## ✨ Key Contributions

- **SAN-CTC 도입:** CTC 손실을 사용하는 깊고 완전한 자기-어텐션 네트워크인 SAN-CTC(Self-Attention Networks for Connectionist Temporal Classification)를 음성 인식 분야에 새롭게 제안했습니다.
- **경쟁력 있는 성능 달성:** SAN-CTC가 기존 CTC 모델 및 대부분의 인코더-디코더 모델보다 경쟁력이 있거나 더 우수한 성능을 보임을 입증했습니다 (예: WSJ 데이터셋에서 하루 만에 4.7% CER, LibriSpeech 데이터셋에서 일주일 만에 2.8% CER).
- **빠른 학습 및 병렬 추론:** SAN-CTC가 빠르게 학습되며, 인코더-디코더 시스템과 달리 추론 시 토큰을 병렬로 예측할 수 있어 효율적인 시스템임을 보여주었습니다.
- **자기-어텐션의 ASR 적용 가능성 입증:** 컨볼루션 프런트엔드나 중간 순환 레이어 없이도 완전한 자기-어텐션 네트워크를 대규모 ASR 데이터셋에서 성공적으로 학습시켰습니다.
- **아키텍처 설계 선택 분석:** 다운샘플링(downsampling) 및 위치 인코딩(position encoding) 방식이 모델 성능에 미치는 영향을 분석하여 ASR을 위한 자기-어텐션 모델 설계에 대한 통찰을 제공했습니다.
- **레이블 알파벳의 영향 탐구:** 문자(character), 음소(phoneme), 서브워드(subword)와 같은 다양한 레이블 알파벳이 어텐션 헤드의 동작 및 전체 성능에 어떻게 영향을 미치는지 분석했습니다.

## 📎 Related Works

- **Connectionist Temporal Classification (CTC) [1]:** 정렬(alignment) 과정 없이 시퀀스 데이터를 레이블링하는 종단 간(end-to-end) 접근 방식으로, ASR [1, 4-11]을 포함한 여러 시퀀스 변환 작업에 활용되어 왔습니다.
  - **순환 신경망(RNN) 기반 CTC [1-6]:** BLSTM [4] 및 BRDNN [5] 등을 활용했으나, 계산 비용이 높고 병렬화가 어렵습니다.
  - **컨볼루션 신경망(CNN) 기반 CTC [8-11]:** 순환 모델의 한계를 극복하기 위해 제안되었으나, 넓은 수용 영역을 위해서는 매우 깊은 네트워크가 필요합니다 [23].
- **인코더-디코더(Encoder-Decoder) 모델 [13]:** 어텐션 메커니즘 [14]과 결합되어 기계 번역에서 성공을 거두었으며, ASR [15-20]에도 적용되었습니다. 하지만 학습의 어려움과 자기회귀적 디코딩으로 인한 추론 속도 제한이 있습니다.
  - **보조 CTC 손실(Auxiliary CTC Losses) [17, 20, 21]:** 인코더-디코더 모델의 학습 안정화를 위해 CTC 손실이 종종 함께 사용됩니다.
  - **자기-어텐션 기반 인코더-디코더 [19, 28, 29]:** Transformer [22]를 ASR에 적용한 사례들이 있습니다.
- **자기-어텐션(Self-Attention) 메커니즘 [22, 24]:** Transformer [22]를 통해 제안된 것으로, 순환이나 컨볼루션 없이 시퀀스 내의 모든 위치에 대해 상호작용을 모델링할 수 있습니다.
  - **시간 제한 자기-어텐션(Time-restricted self-attention) [26]:** ASR을 위한 HMM-NN 시스템에서 특정 레이어에 적용된 사례가 있습니다.
  - **자기-어텐션 기반 어쿠스틱 모델 [27]:** Listen, Attend and Spell(LAS) 모델의 인코더에 자기-어텐션을 사용한 연구가 있습니다.
- **CTC에 어텐션 모델링 적용 [30, 31]:** CTC 기반 모델에 어텐션 메커니즘을 적용하려는 선행 연구가 있었습니다.

## 🛠️ Methodology

SAN-CTC는 CTC 프레임워크에 완전한 자기-어텐션 네트워크를 통합합니다.

1. **입력 처리:**

   - 음성 입력 $X \in \mathbb{R}^{T \times d_{\text{fr}}}$는 $T$개의 특징 벡터 시퀀스로 구성됩니다.
   - WSJ 데이터셋의 경우 40차원 Mel-scale filter bank, LibriSpeech의 경우 13차원 Mel-frequency cepstral coefficients(MFCCs)를 사용하며, 모두 시간적 1차 및 2차 차분(difference)을 연결하여 특징을 구성합니다.

2. **다운샘플링 (Downsampling) [섹션 2.4]:**

   - 자기-어텐션의 $O(T^2)$ 계산 복잡도를 완화하기 위해 입력 길이 $T$를 $k=3$만큼 줄입니다.
   - **재구성(reshaping):** $k$개의 연속된 프레임을 하나로 연결하는 방식이 가장 효과적이었습니다.
   - 다른 방식으로는 서브샘플링(subsampling, $k$번째 프레임만 취함)과 풀링(pooling, $k$개 프레임을 통계치로 집계)을 고려했습니다.

3. **임베딩 및 위치 인코딩 (Embedding and Position Encoding) [섹션 2.5]:**

   - 입력 특징을 $d_h$-차원 임베딩으로 변환합니다.
   - 자기-어텐션은 내용 기반이므로, 시퀀스 내의 위치 정보를 명시적으로 주입합니다.
   - 표준 삼각 함수 기반 위치 임베딩 $PE(t, 2i) = \sin(\frac{t}{10000^{2i/d_{\text{emb}}}})$ 및 $PE(t, 2i+1) = \cos(\frac{t}{10000^{2i/d_{\text{emb}}}})$을 사용합니다.
   - **가산 방식(additive):** 임베딩에 위치 인코딩을 더하는 방식 ($d_{\text{emb}} = d_h$).
   - **내용 기반(content-only):** 위치 인코딩을 사용하지 않는 방식.
   - **연결 방식(concatenative):** 임베딩에 위치 인코딩을 연결하는 방식 ($d_{\text{emb}} = 40$).

4. **자기-어텐션 레이어 (Self-Attention Layer) [그림 1a, 1b; 섹션 2.3]:**

   - **아키텍처:** 10개 레이어, 은닉 차원 $d_h=512$, 8개 어텐션 헤드, 피드포워드 네트워크 차원 $d_{\text{ff}}=2048$. 총 약 3천만 개의 파라미터.
   - 각 레이어는 두 개의 서브레이어로 구성되며, 각 서브레이어 후에는 잔차 연결(residual connection)과 레이어 정규화(layer normalization, LN)가 적용됩니다.
     - **멀티-헤드 스케일드 닷-프로덕트 자기-어텐션 (Multi-head, Scaled Dot-Product, Self-Attention):**
       - 각 헤드 $i$는 입력 $H$로부터 쿼리 $Q^{(i)}$, 키 $K^{(i)}$, 값 $V^{(i)}$를 생성합니다.
       - 헤드 출력은 $\sigma(Q^{(i)}K^{(i)>} / \sqrt{d_h})V^{(i)}$로 계산됩니다.
       - 모든 헤드 출력을 연결하여 $MltHdAtt$를 구성하고, 여기에 잔차 연결 및 LN을 적용합니다: $MidLyr(H) = LN(MltHdAtt(H) + H)$.
     - **위치별 피드포워드 네트워크 (Position-wise Feed-Forward Network, FFN):**
       - $\text{ReLU}(HW_1 + b_1)W_2 + b_2$ 형태로, 각 위치에 대해 독립적으로 적용됩니다.
       - 마찬가지로 잔차 연결 및 LN을 적용하여 최종 레이어 출력을 생성합니다: $SelfAttLyr(H) = LN(FFN(MidLyr(H)) + MidLyr(H))$.

5. **투사 (Projection):**

   - 자기-어텐션 스택의 마지막 레이어 출력은 중간 알파벳 $L'$에 대한 로짓(logits)으로 매핑됩니다.

6. **CTC 손실 (CTC Loss):**

   - 모델은 CTC 손실 함수 $L_{\text{CTC}}(X,y) = -\log P(y|X)$를 사용하여 학습됩니다. 이 손실은 입력 프레임과 출력 레이블 사이의 모든 가능한 정렬을 주변화(marginalizing)하여 전체 시퀀스 확률을 계산합니다.

7. **학습 및 디코딩:**
   - **최적화:** Nesterov 가속 경사 하강법, 배치 크기 20.
   - **학습률 스케줄링:** 선형 웜업(warmup) 후 역제곱근 감쇠(inverse square decay) ($\text{LR}(n) = \frac{\lambda}{\sqrt{d_h}} \min(\frac{n}{n_{\text{warmup}}^{1.5}}, \frac{1}{\sqrt{n}})$)를 사용하며, 검증 정확도가 정체되면 학습률을 10으로 나누는 과정을 두 번 반복합니다.
   - **그래디언트 클리핑:** 글로벌 노름(global norm) 1.0으로 클리핑합니다.
   - **드롭아웃:** WSJ 0.2, LibriSpeech 0.1.
   - **레이블 스무딩(Label Smoothing):** 일부 모델에 적용되었습니다 (예: $\lambda=0.1$ for WSJ, $\lambda=0.05$ for LibriSpeech).
   - **단어 오류율(WER) 계산:** Kaldi/EESEN 툴킷을 사용하여 WFST(Weighted Finite-State Transducer) 기반 디코딩과 언어 모델(LM)을 통합하여 WER을 계산합니다.

## 📊 Results

이 연구는 WSJ(80시간)와 LibriSpeech(960시간) 데이터셋에서 SAN-CTC 모델을 평가했습니다.

- **Wall Street Journal (WSJ) 데이터셋:**

  - **문자 모델 (reshape, additive, 레이블 스무딩 $\lambda=0.1$):**
    - `eval92`에서 4.7%의 문자 오류율(CER)과 5.9%의 단어 오류율(WER)을 달성했습니다.
    - 이는 기존 대부분의 CTC 및 인코더-디코더 모델을 능가하는 결과입니다.
    - 단일 Tesla V100 GPU로 약 1일 만에 학습되었습니다.
  - **음소 모델 (reshape, additive, 레이블 스무딩 $\lambda=0.1$):**
    - `eval92`에서 4.73%의 음소 오류율(PER)과 5.23%의 WER을 달성했습니다.
  - **설계 선택의 영향:**
    - **위치 인코딩:** 가산 방식(additive)이 가장 좋은 성능을 보였으나, 위치 인코딩을 사용하지 않은(content-only) 모델도 경쟁력 있는 결과를 보였습니다 (7.62% CER vs. 7.10% CER). 이는 CTC의 단조성(monotonicity) 가정 덕분에 위치 정보의 중요성이 상대적으로 덜하다는 것을 시사합니다.
    - **다운샘플링:** 재구성(reshape) 방식이 전반적으로 우수했습니다.

- **LibriSpeech 데이터셋 (대규모 실험):**
  - **문자 모델 (reshape, concatenative, 레이블 스무딩 없음):**
    - `test-clean`에서 2.8%의 CER과 4.8%의 WER을 달성했습니다.
    - 이는 OCD 학습 [41]을 제외한 모든 기존 종단 간(end-to-end) 결과보다 우수합니다.
    - 단일 Tesla V100 GPU로 약 1주(70 에포크) 만에 학습되었으며, 이는 기존 SOTA CTC 모델(4 GPU로 4-8주)보다 훨씬 효율적입니다.
  - **레이블 스무딩:** LibriSpeech와 같은 대규모 데이터셋에서는 오히려 성능에 해로웠습니다 (2.8% CER vs. 3.5% CER).
  - **서브워드(BPE) 모델:** 300개의 병합(merge) 연산으로 학습 시 CER 7.4%, 언어 모델 없이 WER 8.7%, 서브워드 WFST LM과 함께 WER 5.2%를 달성했습니다.

## 🧠 Insights & Discussion

- **효율성과 경쟁력:** SAN-CTC는 단일 GPU 환경에서 대규모 음성 인식 데이터셋에 대해 빠르고 효율적인 학습을 가능하게 하며, 기존의 최첨단 CTC 및 인코더-디코더 모델과 비교하여 경쟁력 있거나 우수한 성능을 달성할 수 있음을 보여주었습니다. 특히, 병렬 토큰 예측 능력은 추론 속도 면에서 큰 장점입니다.
- **CTC의 단조성과 자기-어텐션:** CTC의 내재된 단조성(monotonicity) 및 조건부 독립성(conditional independence) 가정 덕분에 위치 인코딩의 영향이 인코더-디코더 시스템에 비해 상대적으로 적었습니다. 또한, 손실이 있는 다운샘플링이 CER은 유지하면서 WER을 저하시키는 것은 프레임 전환에 대한 정보 손실이 문자 예측에는 덜 중요하지만, 단어 문맥 형성에는 여전히 영향을 미친다는 것을 의미합니다.
- **어텐션 헤드 분석 (그림 2):**
  - **문자 레이블:** 첫 레이어부터 최종 레이어까지 전방(forward-attending) 및 후방(backward-attending) 모두를 포착하는 어텐션 헤드가 나타났습니다. 이는 컨볼루션 CTC에서 수동으로 설계하는 컨텍스트 확장과 유사하게, 자기-어텐션이 깊이에 따라 점진적으로 컨텍스트를 확장하는 방식을 학습함을 시사합니다.
  - **음소 레이블:** 날카로운 후방 어텐션 헤드와 더 확산된 헤드들을 보였습니다. 이는 영어 문자가 음소보다 문맥 의존적이기 때문으로 해석됩니다 (예: 'tt'를 예측하려면 CTC의 공백(blank) 심볼 규칙 때문에 앞을 내다봐야 함).
  - 이러한 분석은 미래 연구에서 어텐션 헤드의 수를 줄이거나, 지향성(directed) 또는 제한된(restricted) 어텐션을 사용하여 학습 효율성을 높일 수 있음을 시사합니다.
- **레이블 알파벳의 영향:** 문자 레이블이 가장 좋은 성능을 보였지만, 서브워드 모델도 언어 모델과 통합될 경우 잠재력을 보여주었습니다. 어텐션 패턴은 사용된 레이블 알파벳에 따라 다르게 나타나, 각 알파벳 단위의 특성에 맞는 학습이 이루어짐을 암시합니다.
- **한계 및 향후 연구:** 초기 학습 단계에서 Xavier 초기화가 불안정성을 보이는 경우가 있었고, 이는 초기화 전략 개선의 여지를 남깁니다. 향후 연구로는 SAN-CTC를 다른 디코더나 목적 함수와 결합한 멀티태스킹(multitasking) 접근 방식, 그리고 지향성 또는 제한된 어텐션을 통해 네트워크 구조를 간소화하는 방안을 제안합니다.

## 📌 TL;DR

**문제:** 기존의 순환/컨볼루션 기반 CTC 음성 인식 모델은 병렬 처리 및 넓은 컨텍스트 확보에 제약이 있었고, 어텐션 기반 인코더-디코더 모델은 학습이 어렵고 추론이 느리다는 단점이 있었습니다.

**해결책:** 본 논문은 자기-어텐션 네트워크(Self-Attention Network)를 Connectionist Temporal Classification (CTC) 손실 함수와 결합한 종단 간(end-to-end) 모델인 **SAN-CTC**를 제안합니다. 이 모델은 순환/컨볼루션 레이어 없이 순수 자기-어텐션으로 구성된 깊은 인코더를 사용하며, 음성 인식에 맞게 다운샘플링과 위치 인코딩 기법을 적용합니다.

**주요 발견:** SAN-CTC는 WSJ 및 LibriSpeech 데이터셋에서 기존 CTC 및 대부분의 인코더-디코더 모델보다 뛰어난 성능(예: LibriSpeech `test-clean` 2.8% CER)을 달성했습니다. 단일 GPU로도 매우 빠른 학습 시간(WSJ 1일, LibriSpeech 1주)을 보였으며, 추론 시 토큰을 병렬로 예측할 수 있어 효율적입니다. 또한, CTC의 단조성 덕분에 위치 인코딩의 영향이 상대적으로 적고, 문자 및 음소 레이블에 따라 어텐션 헤드의 학습 패턴이 다름을 밝혀 자기-어텐션의 내부 작동에 대한 중요한 통찰을 제공했습니다.
