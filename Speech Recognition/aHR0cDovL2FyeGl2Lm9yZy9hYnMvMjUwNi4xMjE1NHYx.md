# Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding
Haoran Zhou, Xingchen Song, Brendan Fahy, Qiaochu Song, Binbin Zhang, Zhendong Peng, Anshul Wadhawan, Denglin Jiang, Apurv Verma, Vinay Ramesh, Srivas Prasad, Michele M. Franceschini

## Problem to Solve
OpenAI의 Whisper는 대규모 음성 데이터를 학습하여 강력한 음성 인식(**ASR**) 성능을 보이지만, 그 인코더-디코더 아키텍처는 비인과적(**non-causal**) 설계와 순차 대 순차(**sequence-to-sequence**) 학습 목표 때문에 스트리밍 **ASR**을 기본적으로 지원하지 않습니다. 기존의 Whisper 적응 노력들은 모델을 실제로 스트리밍 가능하게 만들지 못하거나, 불완전한 오디오에 대한 추론 시 학습-추론 불일치로 인해 낮은 지연 시간(**low-latency**) 시나리오에서 정확도 저하 및 비효율성을 초래했습니다.

## Key Contributions
*   Whisper를 **Unified Two-pass (U2)** 구조를 사용하여 스트리밍 **ASR** 모델로 성공적으로 **미세 조정**(**fine-tune**)했습니다.
*   인과적 어텐션 마스크(**causal attention masks**)로 학습된 추가 **CTC** 디코더를 도입하여 스트리밍 부분 전사(**partial transcripts**)를 생성하고, 기존 Whisper 디코더는 이 부분 출력을 **재채점**(**rescore**)하여 최종 결과를 결정하도록 했습니다.
*   **CTC** 디코더에는 더 작은 토큰 공간을 사용하고 어텐션 디코더에는 Whisper의 원래 토큰 공간을 유지하는 **하이브리드 토크나이저**(**hybrid tokenizer**) 접근 방식을 제안하여 데이터 효율성과 일반화 성능을 향상시켰습니다.
*   충분한 **미세 조정** 데이터를 통해 Whisper가 스트리밍 **ASR** 모델로 기능할 수 있음을 입증했으며, **CPU**에서 실시간(**real-time**)으로 실행될 수 있음을 보여주었습니다.

## Methodology
*   **U2 모델 적응**: Whisper의 인코더 위에 **CTC** 디코더를 추가하고, 원래 Whisper 디코더는 어텐션 디코더로 유지합니다.
*   **하이브리드 CTC-어텐션 손실**: 학습 시 $L=\alpha\cdot L_{CTC} + (1-\alpha)\cdot L_{Attention}$ 공식을 사용하여 **CTC**와 어텐션 디코더가 모두 참조 전사를 생성하도록 학습합니다.
*   **동적 어텐션 마스크**: 학습 중 동적 어텐션 마스크를 사용하여 인코더의 은닉 표현이 과거 또는 미래 컨텍스트의 작은 부분에만 의존하도록 보장하며, 0.1초에서 1.0초 사이의 임의의 청크 크기로 샘플링하여 모델이 다양한 청크 크기에 걸쳐 일반화되도록 합니다.
*   **스트리밍 추론**:
    *   인코더는 오디오 입력을 청크 단위로 처리합니다.
    *   **CTC** 디코더는 프리픽스 빔 탐색(**prefix beam search**)을 수행하여 상위-k 스트리밍 부분 전사를 생성합니다.
    *   종단점(**endpoint**)이 감지되면 (0.5초의 침묵 또는 최대 지연(**max delay**) 제약 도달 시), 어텐션 디코더는 상위-k **CTC** 가설들을 **재채점**하여 최상의 최종 전사를 결정합니다.
    *   WeNet **C++** 추론 런타임을 사용하며, 효율적인 키-값 (**KV**) 캐시와 단일 배치 디코더 통과를 통해 추론 효율성을 최적화하여 **CPU** 기반 실시간 처리를 가능하게 합니다.
*   **하이브리드 토크나이저**:
    *   **CTC** 디코더의 토큰 공간을 Whisper 토크나이저의 처음 8,000개 토큰으로 제한하여 희귀 단어 처리 능력을 향상시킵니다.
    *   어텐션 디코더는 Whisper의 원래 전체 토큰 세트(50,000개 이상)를 유지합니다.
    *   추론 시, **CTC** 가설은 문자열로 디코딩된 후 Whisper 토크나이저로 **재토큰화**되어 어텐션 디코더로 전달됩니다.

## Results
*   **데이터 스케일링**: **미세 조정** 데이터 양을 늘릴수록 단어 오류율(**WER**)이 감소했습니다. **하이브리드 토크나이저**는 데이터가 제한적일 때(예: 725시간) **WER**을 크게 낮추며 단일 토크나이저보다 일관되게 우수한 성능을 보였지만, 데이터셋 규모가 커질수록 그 이점은 감소했습니다. 사전 학습된 Whisper 가중치가 모델 성능에 중요함이 확인되었습니다.
*   **런타임 구성 및 성능**:
    *   **청크 크기**: 청크 크기가 작아질수록 정확도가 감소했습니다(예: 1500ms에서 16.65% **WER** vs. 100ms에서 25.54% **WER**). 이는 부분 디코딩 시 올바른 포맷팅에 필요한 미래 컨텍스트가 부족하기 때문입니다. **재채점**은 정확도를 약간 개선했지만, **CTC** 가설들이 세부 사항에서만 차이가 나 큰 효과는 없었습니다.
    *   **최대 지연**: 최대 지연 시간이 길어질수록 **WER**이 향상되지만(**12s**에서 **17.30% WER** vs. **20s**에서 **16.96% WER**), 계산 비용과 최종화 지연 시간도 증가합니다.
    *   **실시간 성능**: **U2** Whisper 모델은 **12초** 최대 지연 설정에서 허용 가능한 스트리밍 지연 시간으로 실행될 수 있으며, **CPU**에서 실시간(**Real-Time Factor, RTF** < 1) 추론이 가능합니다.
*   **다른 모델과의 비교**:
    *   **UFAL** Whisper와 비교했을 때, **U2** Whisper는 작은 청크 크기에서 (수익 보고서 데이터셋 및 LibriSpeech `test-clean`에서) 더 나은 성능을 보였습니다. 청크 크기가 커질수록 **UFAL** Whisper가 유리했습니다.
    *   **계산 효율성**: **U2** Whisper는 **CPU**에서 효율적으로 실행되는 반면, **UFAL** Whisper는 **GPU** 자원이 더 많이 필요하며 작은 청크 크기에서는 **GPU**에서도 실시간을 달성하기 어렵습니다.
    *   **최종화 지연 시간**: **U2** Whisper는 종단점 감지 시 부분 전사를 최종화하며 최대 지연 시간을 보장하는 반면, **UFAL**은 두 개의 연속 예측이 일치할 때 최종화되어 상한이 엄격하게 제한되지 않습니다.
*   결론적으로, 충분한 도메인 내(**in-domain**) 데이터가 있을 경우 **U2** Whisper는 낮은 지연 시간(**low-latency**)의 부분 전사와 높은 계산 효율성이 중요한 실시간 응용 프로그램에 더 적합합니다.