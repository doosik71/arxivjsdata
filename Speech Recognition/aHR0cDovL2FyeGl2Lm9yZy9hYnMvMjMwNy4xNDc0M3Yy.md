# Turning Whisper into Real-Time Transcription System
Dominik Macháček, Raj Dabre, Ondřej Bojar

## 🧩 Problem to Solve
최근 가장 발전된 다국어 음성 인식(ASR) 및 번역 모델 중 하나인 Whisper는 실시간 전사(transcription)를 위해 설계되지 않았습니다. 현재 Whisper의 공개 구현은 오디오 문서 전체가 준비되어야만 오프라인 처리가 가능하며, 실시간 처리 시 발생하는 지연 시간(latency) 및 세그먼트 경계에서의 품질 저하 문제가 있습니다. 이 논문은 이러한 Whisper의 한계를 극복하고 실시간 음성 전사 시스템을 구축하는 것을 목표로 합니다.

## ✨ Key Contributions
*   **Whisper-Streaming 구현**: Whisper와 유사한 모델을 위한 실시간 음성 전사 및 번역 시스템인 Whisper-Streaming을 구현했습니다.
*   **LocalAgreement 정책 적용**: LocalAgreement-2 정책과 자기 적응형 지연 시간을 사용하여 스트리밍 전사를 가능하게 했습니다.
*   **성능 평가**: 분할되지 않은 장문의 음성 전사 테스트 세트에서 Whisper-Streaming이 높은 품질과 평균 3.3초의 낮은 지연 시간을 달성함을 입증했습니다.
*   **강건성 및 실용성 입증**: 다국어 콘퍼런스 라이브 전사 서비스의 구성 요소로서 Whisper-Streaming의 강건성(robustness)과 실용성을 시연했습니다.
*   **공개 배포**: Whisper-Streaming을 공개적으로 사용할 수 있도록 배포했습니다.

## 📎 Related Works
*   **Whisper (Radford et al., 2022)**: 본 시스템의 기반이 되는 최신 다국어 음성 인식 및 번역 Transformer 모델.
*   **faster-whisper (guillaumekln)**: CTranslate2를 활용하여 Whisper 추론 속도를 향상시킨 재구현체.
*   **LocalAgreement (Liu et al., 2020)**: 임의의 전체 시퀀스-대-전체 시퀀스 모델을 동시 스트리밍 모드로 작동시킬 수 있는 스트리밍 정책. IWSLT 2022 동시 음성 번역 태스크에서 CUNI-KIT 시스템이 가장 효과적인 정책으로 LocalAgreement-2를 사용했습니다 (Polák et al., 2022).
*   **ESIC corpus (Macháček et al., 2021)**: 평가 데이터로 사용된 유럽 의회 연설 데이터셋.
*   **ELITR (European Live Translator, Bojar et al., 2020)**: 복잡한 분산 라이브 음성 전사 및 번역 시스템 프레임워크.

## 🛠️ Methodology
Whisper-Streaming은 다음의 핵심 구성 요소와 작동 방식을 가집니다:

1.  **업데이트 루프 (Update Loop)**: `MinChunkSize` 파라미터가 각 반복에서 처리되는 최소 오디오 지속 시간을 제어합니다. 새로운 오디오 청크가 도착할 때마다 스트리밍 정책 업데이트를 트리거합니다.
2.  **오디오 버퍼 (Audio Buffer)**: Whisper는 최대 30초 길이의 시퀀스를 처리하도록 훈련되었으며, 구두점과 단어 수준 타임스탬프를 제공합니다. 들어오는 오디오를 버퍼에 저장하고 전체 버퍼를 Whisper로 처리합니다. 버퍼는 항상 새 문장으로 시작하도록 유지됩니다.
3.  **LocalAgreement-2 적용**: 현재 및 이전 Whisper 출력에 LocalAgreement-2를 적용하여 안정화된 타겟 세그먼트를 식별합니다. 두 개의 연속적인 소스 청크에서 모델의 가장 긴 공통 접두사를 출력합니다.
4.  **확인된 출력 건너뛰기**: 이전 업데이트에서 마지막으로 확인된 단어와 관련된 전사된 단어의 위치를 결정할 때, Whisper 타임스탬프의 잠재적 부정확성을 고려합니다. 일치하는 경우 해당 단어를 건너뛰지만, 이는 향후 개선될 수 있습니다.
5.  **오디오 버퍼 트리밍**: 버퍼 길이가 30초를 초과하는 것을 방지하기 위해, 문장 종료 구두점 뒤에 새 문장이 시작되면 해당 구두점 타임스탬프에서 버퍼를 트리밍합니다.
6.  **문장 간 컨텍스트 연결 (Prompt)**: `prompt` 파라미터를 사용하여 문서 내 일관성(스타일, 용어, 문장 간 참조)을 유지합니다. 이전 오디오 버퍼의 확인된 출력에서 마지막 200 단어를 `prompt`로 사용합니다.
7.  **음성 활동 감지 (VAD)**: Whisper의 기본 VAD 필터를 활성화/비활성화하는 옵션이 있으며, 이는 품질과 지연 시간 모두에 영향을 미칩니다.

평가 환경:
*   **데이터셋**: ESIC 코퍼스의 개발 세트 (영어, 독일어, 체코어 ASR, 179개 문서, 5시간 분량).
*   **평가 지표**: WER (Word Error Rate), ASR 지연 시간 (금본 전사 타임스탬프와 ASR 출력의 정렬 기반).
*   **하드웨어**: NVIDIA A40 GPU 사용. 재현성 확보를 위해 동일한 설정을 10회 반복 실행하여 평균 및 표준 편차를 보고했습니다.

## 📊 Results
*   **VAD 영향**:
    *   **영어 (원문 연설)**: VAD를 비활성화했을 때 품질은 거의 동일하지만 평균 지연 시간이 0.23~0.41초 감소했습니다.
    *   **독일어/체코어 (동시 통역)**: VAD를 활성화했을 때 지연 시간은 약간 증가하지만 (약 0.1초) 품질(WER)이 2~3% 향상되었습니다. 이는 통역 연설에 빈번한 멈춤이 있기 때문입니다.
*   **전반적인 성능**:
    *   `MinChunkSize` 1초 설정 시, 평균 지연 시간은 영어 3.3초, 독일어 4.4초, 체코어 4.8초였습니다.
    *   WER은 오프라인 모드 대비 영어와 독일어는 약 2%, 체코어는 약 6% 더 높았습니다.
    *   영어에서 WER과 지연 시간이 가장 낮았고, 독일어, 체코어 순이었습니다. 이는 Whisper 훈련에 사용된 각 언어의 데이터 양 및 언어의 형태론적 복잡성과 관련이 있습니다.
    *   `MinChunkSize`가 클수록 지연 시간이 길어지지만, 시스템이 충분한 컨텍스트를 가질 수 있어 품질은 더 높아집니다.
*   **오프라인 모드 및 계산 비인식 지연 시간 비교**:
    *   오프라인 모드 (전체 오디오 처리)는 스트리밍 모드보다 낮은 WER을 보였습니다. 이는 오프라인 모드에서 더 넓은 컨텍스트 (미래 컨텍스트 포함)를 활용할 수 있기 때문입니다.
    *   계산 비인식(computationally unaware) 지연 시간 (계산 시간 제외)은 청크 크기의 약 두 배였습니다. 이는 LocalAgreement-2 정책의 특성상 두 번의 연속 업데이트에 대한 합의가 필요하기 때문입니다.

## 🧠 Insights & Discussion
*   **VAD의 중요성**: VAD 필터의 활성화 여부는 음성 특성(원문 연설 vs. 동시 통역)에 따라 지연 시간과 품질에 상이한 영향을 미치므로, 실제 적용 시 시나리오에 맞춰 최적화해야 합니다. 특히 통역 연설과 같이 멈춤이 잦은 경우에는 VAD를 켜는 것이 품질 향상에 도움이 됩니다.
*   **지연 시간 요소**: 지연 시간은 주로 두 가지 원인, 즉 언어적 불확실성(모델이 확정된 출력을 내놓기까지 걸리는 시간)과 계산 시간 자체에서 발생합니다. 모델이나 스트리밍 정책 개선을 통해 언어적 불확실성으로 인한 지연 시간을 줄일 수 있고, 하드웨어 또는 추론 알고리즘 최적화를 통해 계산 시간으로 인한 지연 시간을 줄일 수 있습니다.
*   **실제 적용 가능성**: Whisper-Streaming은 ELITR 프레임워크와 통합되어 실제 다국어 콘퍼런스에서 라이브 음성 번역 서비스의 구성 요소로 성공적으로 시연되었습니다. 이는 시스템의 강건성과 실용성을 입증합니다.
*   **제한 사항**:
    *   ESIC 코퍼스의 데이터가 상대적으로 오래되어 Whisper 훈련 데이터셋에 잠재적 누출 가능성이 있습니다.
    *   더 저렴한 하드웨어에서의 성능 테스트와 계산 비용 측면에서의 추가 평가가 필요합니다.
    *   보고된 지연 시간 및 품질 지표가 다른 언어 또는 언어 변형에 완전히 일반화될 수 없을 수 있습니다.
    *   알고리즘이나 구현 최적화보다는 Whisper의 온라인 기능을 시연하는 데 중점을 두었으므로, 실제 지연 시간은 변동될 수 있습니다. 최대 지연 시간을 보장하기 위해서는 추가적인 정책 수정이 필요할 수 있습니다.
    *   다른 최신 시스템(예: IWSLT)과의 직접적인 비교 테스트는 공통 평가 프레임워크 및 장문 음성 테스트셋의 부재로 수행되지 못했습니다.

## 📌 TL;DR
Whisper가 실시간 전사에 적합하지 않다는 문제를 해결하기 위해, 이 논문은 LocalAgreement-2 정책과 자기 적응형 지연 시간을 활용하여 Whisper 모델 기반의 실시간 음성 전사 시스템 **Whisper-Streaming**을 개발했습니다. 이 시스템은 ESIC 데이터셋에서 평균 3.3초의 낮은 지연 시간과 높은 전사 품질을 달성했으며, 실제 다국어 콘퍼런스 라이브 전사 서비스에 통합되어 그 강건성과 실용성을 성공적으로 입증했습니다.