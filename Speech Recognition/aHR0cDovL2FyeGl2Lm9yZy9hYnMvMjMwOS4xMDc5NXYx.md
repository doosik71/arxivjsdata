# 저자원 음성 합성을 위한 음성 향상 탐구
Zhaoheng Ni, Sravya Popuri, Ning Dong, Kohei Saijo, Xiaohui Zhang, Gael Le Lan, Yangyang Shi, Vikas Chandra, Changhan Wang

## 🧩 해결할 문제
고품질의 음성 합성을 위해서는 깨끗하고 명료한 음성 데이터가 필수적이지만, 저자원 언어의 경우 이러한 데이터를 확보하는 것이 어렵고 비용이 많이 듭니다. 자동 음성 인식(ASR) 코퍼스는 저자원 언어에 대해 풍부하게 존재하지만, 대부분 잡음이 많아 음성 합성(TTS) 모델 학습에 직접 사용하기 어렵습니다. 음성 향상(Speech Enhancement, SE) 기술을 ASR 코퍼스에 적용하여 학습 데이터를 보강할 수 있지만, SE 모델에 의해 발생하는 비선형 음성 왜곡이 TTS 학습에 어떤 영향을 미치는지에 대한 체계적인 연구는 부족했습니다. 본 논문은 특히 단일 채널 음성 향상 모델이 저자원 TTS 모델 학습에 미치는 영향을 탐구하고자 합니다.

## ✨ 주요 기여
*   저자원 음성 합성을 위한 새로운 파이프라인을 제안합니다: TF-GridNet 음성 향상 모델을 잡음이 있는 ASR 데이터에 적용한 후, 이 향상된 음성으로 이산 단위 기반 TTS 모델을 학습시키는 방식.
*   제안된 파이프라인이 ASR WER(단어 오류율) 지표에서 다른 기준선 방법들보다 저자원 TTS 시스템(아랍어 데이터셋 예시)의 성능을 크게 향상시킴을 입증했습니다.
*   음성 향상 성능과 TTS 성능 간의 상관관계에 대한 경험적 분석을 수행하여, 높은 음성 향상 성능이 반드시 TTS 모델 성능 향상으로 이어지는 것은 아니라는 점(데이터 불일치 등의 요인으로 인해)을 발견했습니다.
*   특히 TF-GridNet 모델이 사용된 단일 채널 음성 향상 모델 중 최고의 T2U(Text-to-Unit) 성능을 달성함을 보여주었습니다.

## 📎 관련 연구
*   **심층 학습 기반 TTS 시스템**: Tacotron, Transformer TTS와 같은 자기회귀(autoregressive) 방식과 FastSpeech, Glow-TTS와 같은 비자기회귀(non-autoregressive) 방식이 발전했습니다. 최근에는 VQ-VAE 및 FFTNet [12] 또는 자기 지도 학습(SSL) 모델에서 추출한 이산 단위를 사용하는 HiFiGAN [13]과 같은 이산 단위 기반 TTS 시스템이 연구되고 있습니다.
*   **저자원 TTS의 도전 과제**: 고품질 데이터 부족은 저자원 언어의 TTS 연구에서 주요 난제로, 잡음이 있는 ASR 코퍼스를 활용하려는 시도 [15, 16]가 이루어져 왔습니다.
*   **음성 향상**: 배경 잡음 및 방해 스피커를 줄여 음성 품질과 명료도를 개선하는 기술입니다. 음성 번역 [17] 및 멜-스펙트로그램 기반 TTS [18]와 같은 다운스트림 작업에 음성 향상이 적용되었습니다.
*   **음성 향상의 잠재적 부작용**: 단일 채널 음성 향상 모델이 ASR 성능을 저하시킬 수 있는 비선형 음성 왜곡을 유발할 수 있다는 연구 [19, 20]가 존재합니다.
*   **주요 음성 향상 모델**: DEMUCS [23], FullSubNet [24], FRCRN [25], D2Former [26], TF-GridNet [21] 등 다양한 최신 모델들이 있습니다.

## 🛠️ 방법론
*   **문제 정의**: 잡음이 있는 음성 신호 $x \in \mathbb{R}^L$는 깨끗한 음성 $s_i$와 잡음 $n_i$의 합으로 $x_i = s_i + n_i$로 모델링됩니다. 본 연구의 목표는 텍스트 입력 $t$를 받아 깨끗하고 명료한 음성을 생성하는 TTS 시스템 $G$를 구축하는 것입니다.
*   **제안된 파이프라인**:
    1.  **음성 향상**: 잡음이 있는 음성 $x$에 TF-GridNet 모델을 적용하여 잡음을 제거하고 향상된 음성 $\hat{s}$를 얻습니다. TF-GridNet은 STFT(Short-Time Fourier Transform)를 인코더로, iSTFT(Inverse STFT)를 디코더로 사용하며, 복소수 스펙트럼 매핑을 통해 대상 음성의 실제 및 허수 부분을 추정합니다.
        *   **TF-GridNet 아키텍처**: 입력 음성을 시간-주파수 도메인 특징으로 변환한 후, 2D 컨볼루션 및 gLN(global layer normalization)을 거쳐 D차원 특징 공간으로 매핑합니다. 이 특징은 B개의 TF-GridNet 블록에 입력됩니다. 각 블록은 인트라-프레임 풀밴드 모듈(프레임 내 주파수 정보 모델링), 서브밴드 시간 모듈(시간 정보 모델링), 교차-프레임 자기-어텐션 모듈(장기적 맥락 정보 캡처)로 구성됩니다. 마지막으로 2D 디컨볼루션을 통해 시간-도메인 향상 음성을 얻습니다.
        *   **모델 개선**: 복소수 스펙트럼 매핑 시 발생하는 발화 끝의 예기치 않은 잡음을 완화하기 위해, 모델을 복소수 비율 마스크(complex ratio mask) [29]를 추정하도록 학습시켰습니다.
    2.  **음성 활동 감지 (VAD)**: 향상된 음성 $\hat{s}$에 `webrtcvad` 모델을 적용하여 무음 구간을 제거합니다. 이는 TTS 학습 데이터의 불필요한 부분을 줄여 모델의 수렴 속도를 높입니다.
    3.  **이산 단위 추출**: 향상된 음성에서 XLS-R-1B [27] 모델(128개 언어로 사전 학습된 10억 매개변수 SSL 모델)의 35번째 트랜스포머 레이어 출력을 SSL 특징으로 사용합니다. 이 특징들은 k-means 클러스터링 알고리즘을 통해 이산 단위 시퀀스로 변환됩니다.
    4.  **Text-to-Unit (T2U) 모델 학습**: Transformer 기반의 인코더-디코더 모델을 정렬된 텍스트와 추출된 이산 단위 시퀀스 쌍으로 학습시킵니다 (SeamlessM4T [22]의 T2U 모델 아키텍처 활용). 자기회귀 모델에서 반복 토큰 예측 문제를 완화하기 위해 연속적으로 반복되는 단위는 줄였습니다.
    5.  **파형 생성**: T2U 모델이 예측한 이산 단위 시퀀스를 이산 단위 기반 HiFiGAN 보코더 [14]에 입력하여 최종 음성 파형을 생성합니다.

## 📊 결과
*   **음성 향상 및 VAD의 효과**: TF-GridNet 대신 DEMUCS와 VAD를 적용한 경우에도, 처리되지 않은 음성으로 학습한 모델(WER 57.6%)과 비교하여 ASR WER이 46.6%로 크게 개선되었습니다. 이는 음성 향상 및 VAD가 저자원 TTS 모델 학습에 유의미한 이점을 제공함을 확인시켜 줍니다.
*   **음성 향상 모델 성능 비교**: DNS 2020 테스트셋에서 TF-GridNet은 잔향(reverb)이 없는 조건에서 WB-PESQ 3.2, Si-SDR 20.6 dB, STOI 97.7%를 기록하며 모든 지표에서 최고의 성능을 달성했습니다. FullSubNet은 훈련 데이터에 잔향을 포함했기 때문에 잔향이 있는 조건에서 가장 우수했지만, TF-GridNet 역시 잔향 조건에서 다른 모델들(FullSubNet 제외)보다 뛰어난 성능을 보였습니다. D2Former는 훈련 데이터와 테스트 데이터의 불일치로 인해 성능이 저조했습니다.
*   **T2U 모델 성능 (ASR WER) 비교**:
    *   원래 FLEURS 데이터셋의 잡음 특성으로 인해 "Reference"(원음)의 WER은 18.6%였고, 원음의 이산 단위로 재합성한 "Re-synthesis" 및 처리되지 않은 음성으로 학습한 "Unprocessed" 모델의 WER은 약 57%대였습니다.
    *   음성 향상된 음성으로 학습된 모든 T2U 모델은 "Re-synthesis"와 "Unprocessed" 모델을 크게 능가하여, 저자원 TTS 파이프라인에서 음성 향상의 중요성을 강조합니다.
    *   TF-GridNet으로 향상된 음성으로 학습된 T2U 모델이 가장 낮은 WER(44.7%)을 달성하여, 처리되지 않은 음성으로 학습한 모델 대비 12.9%p의 절대적인 WER 감소를 보였습니다.
    *   FullSubNet은 음성 향상 자체는 우수했지만, T2U 성능에서는 TF-GridNet보다 낮았습니다. 이는 아랍어 훈련 데이터에 잔향이 거의 없었기 때문에, 음성 향상 모델의 훈련 데이터 특성과 T2U 훈련 데이터 간의 불일치가 전체 파이프라인 성능에 영향을 미쳤음을 시사합니다.

## 🧠 통찰 및 논의
*   저자원 음성 합성 환경에서 잡음이 많은 ASR 코퍼스를 활용할 때, 음성 향상 기술은 TTS 성능을 크게 개선하는 데 필수적인 요소임을 확인했습니다.
*   TF-GridNet은 본 연구에서 제안하는 파이프라인 내에서 음성 향상을 위해 매우 효과적인 모델임이 입증되었습니다.
*   중요한 통찰은 음성 향상 모델의 성능 지표(예: PESQ, Si-SDR, STOI)가 항상 다운스트림 TTS(T2U) 모델의 최종 성능과 직접적으로 비례하지는 않는다는 점입니다. 이는 음성 향상 모델의 훈련 데이터 특성(예: 잔향 포함 여부)과 실제 TTS 훈련에 사용되는 데이터의 특성 간의 불일치가 전체 파이프라인 성능에 영향을 미칠 수 있음을 의미합니다.
*   현재 음성 향상 평가 지표가 TTS에 중요한 음성 특성을 완전히 포착하지 못할 수 있다는 한계점을 발견했습니다.
*   향후 연구에서는 저자원 음성 합성 성능을 더 잘 반영할 수 있는 새로운 음성 향상 평가 지표를 설계하는 데 초점을 맞출 것입니다.

## 📌 TL;DR
저자원 음성 합성은 고품질 데이터 부족 문제에 직면하며, 이를 해결하기 위해 잡음이 많은 ASR 데이터를 활용하려면 음성 향상이 필수적입니다. 본 연구는 TF-GridNet 음성 향상 모델을 잡음 데이터에 적용한 후, 이 향상된 음성으로 이산 단위 기반 TTS 모델을 학습하는 파이프라인을 제안합니다. 이 방식은 아랍어 데이터셋에서 ASR WER을 12.9%p 감소시키며 저자원 TTS 성능을 크게 향상시켰습니다. 그러나, 음성 향상 성능이 높다고 해서 반드시 TTS 성능이 최적화되는 것은 아니며, 음성 향상 모델의 훈련 데이터와 TTS 학습 데이터 간의 특성 불일치(예: 잔향)가 중요한 영향을 미칠 수 있음을 발견했습니다.