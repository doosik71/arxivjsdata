# Quantization for OpenAI’s Whisper Models: A Comparative Analysis
Allison Andreyev

## 🧩 Problem to Solve
본 논문은 OpenAI의 Whisper 자동 음성 인식(ASR) 모델이 직면한 주요 문제들을 다룹니다.
*   **환각 현상(Hallucinations)**: Whisper 모델이 실제 음성에 없는 내용을 생성하여 전사 신뢰도를 저하시키는 문제입니다. 이는 특히 긴 오디오 파일에서 더 심해지며, 유해한 내용(폭력, 허위 정보 등)을 포함할 수 있습니다.
*   **높은 지연 시간(Latency) 및 컴퓨팅 요구 사항**: 대규모 Whisper 모델은 높은 지연 시간과 상당한 컴퓨팅 자원을 요구하여 모바일 장치나 IoT 기기와 같이 자원이 제한된 환경에 배포하기 어렵습니다.
*   **모델 변형(Variants) 간의 특성 이해 부족**: Whisper 모델의 여러 변형(`whisper-timestamped`, `Whisper_Streaming`)이 존재하지만, 각 변형의 고유한 기능, 사용 사례, 한계점에 대한 포괄적인 비교 분석이 부족합니다. 특히 양자화가 이러한 변형들에 미치는 영향은 제대로 탐구되지 않았습니다.

## ✨ Key Contributions
*   **Whisper 모델 및 두 가지 변형(`Whisper_Streaming`, `whisper-timestamped`)의 기능 및 유사점/차이점에 대한 정성적 비교 분석을 수행했습니다.**
*   **Whisper 모델에 적용 가능한 양자화 기술 및 관련 하드웨어 고려 사항을 정의했습니다.**
*   **모델 크기, 버전, 런타임, 양자화 접근 방식에 따른 단어 오류율(WER), 처리 속도, 지연 시간 측면에서 모델 성능을 평가했습니다.**
*   **세 가지 양자화 방법(INT4, INT5, INT8)을 사용하여 `whispercpp`의 성능을 WER, 모델 크기 및 지연 시간을 기준으로 정량적으로 평가했습니다.**
*   **양자화가 모델 크기를 최대 45% 줄이고, 지연 시간을 19% 단축하며, 동시에 전사 정확도를 유지하거나 향상시킬 수 있음을 입증했습니다.**
*   **서로 다른 Whisper 모델의 최적 사용 사례와 엣지 디바이스 배포 가능성에 대한 통찰력을 제공했습니다.**

## 📎 Related Works
*   **Whisper 모델 분석**: Radford et al. [17]은 Whisper를 인코더-디코더 트랜스포머 모델로 설명하며, 대규모의 다양한 데이터셋으로 훈련되었음을 언급했습니다. Song et al. [20]은 Whisper의 LLM 기반 ASR 모델과의 비교 연구를 통해 LLM 기반 ASR의 언어 숙련도 상관관계를 지적했습니다.
*   **Whisper 환각 현상**: Baranski et al. [1]은 Whisper의 환각 현상을 연구하며, 오디오 길이가 오류율에 유의미한 영향을 미치고 오디오 내용과의 관련성은 적다고 밝혔습니다. Koenecke et al. [10]은 전사 내용의 약 1%가 환각 문구이며, 이 중 38%가 유해한 내용을 포함한다고 보고했습니다.
*   **양자화(Quantization) 연구**: Gholami et al. [3]은 FP(부동 소수점)에서 INT(정수) 양자화로 전환할 경우 지연 시간과 메모리 사용량을 최대 16배 줄일 수 있다고 강조했습니다. Kim et al. [9]은 ASR 모델의 양자화 연구가 제한적이며, Whisper와 같은 신경망 아키텍처가 컴퓨팅 요구 사항 때문에 엣지 하드웨어에서 성능이 저조하다고 지적했습니다. Zhen et al. [23]은 INT8 양자화가 모델 추론 속도를 높이고 휴대용 장치 배포를 가능하게 한다고 밝혔지만, 다른 양자화 유형에 대한 평가는 부족했습니다. Zhao et al. [22]는 P4Q 양자화 전략을 통해 Whisper의 정확도와 지연 시간이 향상됨을 보여주었지만, 이 역시 한 가지 양자화 유형에 국한되었습니다.
*   **Whisper 변형 연구**: Macháček et al. [15]은 `Whisper_Streaming`을 실시간 전사 시스템으로 전환하는 방법을 제시했습니다. Louradour [13]는 `whisper-timestamped`가 DTW(Dynamic Time Warping)를 사용하여 더 정확한 워드 타임스탬프를 제공한다고 설명했습니다. 기존 연구들은 Whisper의 미세 조정(fine-tuning) 전략에 집중했지만 [6, 8, 11, 12, 14, 19, 21], 양자화가 모델 크기 축소 및 지연 시간 최적화에 미치는 영향은 충분히 탐구되지 않았습니다.

## 🛠️ Methodology
1.  **모델 비교 분석**:
    *   OpenAI의 표준 Whisper 모델과 두 가지 변형(`whisper-timestamped`, `Whisper_Streaming`)의 기능을 비교했습니다.
    *   각 모델의 설치 요구 사항, 출력 형식, 실시간 처리 능력, 타임스탬프 정확도, 사용자 정의 기능 및 한계점을 정성적으로 분석했습니다.
2.  **모델 성능 실험 (정성적)**:
    *   LibriSpeech 데이터셋의 25개 오디오 파일(깨끗한 음성 및 어려운 음성 샘플 포함)을 사용하여 세 가지 Whisper 모델의 전반적인 성능(타임스탬프 정확도, 출력 형식, 사용자 경험 등)을 비교했습니다.
    *   모델 크기(Tiny, Small, Base, Medium, Large)에 따른 전사 속도, 정확도, 자원 사용량 변화를 관찰했습니다.
3.  **양자화 최적화 실험 (정량적)**:
    *   `whispercpp` (Whisper 모델의 C++ 구현)의 Base 모델을 대상으로 실험을 수행했습니다.
    *   LibriSpeech ASR 데이터셋의 처음 10개 오디오 파일을 사용했습니다.
    *   **양자화 방법**: INT4, INT5, INT8 세 가지 정수 양자화 방법을 적용했습니다.
    *   **성능 측정**:
        *   **WER (Word Error Rate)**: Huggingface-evaluate 및 OpenAI-Whisper 프로젝트의 구성 요소를 사용하는 도구 [7]를 사용하여 계산했습니다.
        *   **모델 크기(Model Size)**: 양자화 후 모델 파일 크기를 측정했습니다.
        *   **지연 시간(Latency)**: 평균 지연 시간을 측정했습니다.
    *   **하드웨어**: HP Envy CPU 및 GPU 환경에서 실험을 진행했습니다. (CPU: Intel(R) Xeon(R) CPU @ 2.20GHz)

## 📊 Results
*   **모델 비교 분석 결과 (정성적)**:
    *   **`whisper-timestamped`**: 단어 수준의 타임스탬프와 개별 단어에 대한 신뢰도 점수를 제공하며, DTW(Dynamic Time Warping) 방식을 사용하여 정확도가 높고 긴 파일도 적은 메모리로 처리 가능합니다. JSON 형식 출력을 기본으로 제공하며, FPS 및 처리율 프로그레스 바를 특징으로 합니다.
    *   **`Whisper_Streaming`**: 실시간 음성 전사 및 번역에 최적화되어 3.3초의 낮은 지연 시간을 보입니다. 버퍼링 방식과 텍스트 데이터 축적 방식으로 인해 사전 녹음된 오디오에는 이상적이지 않습니다. 문장 분할 대신 단어 수준 타임스탬프를 생성합니다.
    *   **표준 Whisper**: 문장 단위의 타임스탬프를 제공하며, 기본적으로 JSON, VTT, SRT, TXT, TSV 등 다양한 출력 파일을 생성합니다. 구두점 및 대소문자 사용에 있어 문법 규칙을 올바르게 적용하는 강점을 보였습니다.
    *   **모델 크기에 따른 성능**: Tiny 모델은 출력 속도가 빠르고 GPU/CPU 사용량이 적지만, 긴 텍스트나 이름에서 부정확성이 있으며 대소문자 문제가 있었습니다. Large 모델은 다운로드 및 처리 시간이 길지만 문법적으로 정확한 구조를 생성하려는 경향이 있었습니다.
*   **양자화 최적화 실험 결과 (정량적)**:
    | Metric       | Whisper CPP Base Model | INT5     | INT4     | INT8     |
    | :----------- | :--------------------- | :------- | :------- | :------- |
    | Word Error Rate | 0.0199                 | 0.0199   | 0.0159   | 0.0199   |
    | Accuracy     | 98.0%                  | 98.0%    | 98.4%    | 98.0%    |
    | Model Size   | 141.11MB               | 52.75MB  | 44.33MB  | 77.99MB  |
    | Avg Latency  | 10.64s                 | 11.11s   | 10.55s   | 9.02s    |
    *   **모델 크기 감소**: 양자화는 모델 크기를 크게 줄였습니다. 특히 INT4 양자화는 141.11MB인 기본 모델을 44.33MB로 줄여 약 68.6% (논문에는 45%라고 되어있으나, 1-44.33/141.11 $\approx$ 0.686)의 크기 감소를 달성했습니다.
    *   **지연 시간 감소**: INT8 양자화는 평균 지연 시간을 10.64초에서 9.02초로 15% (논문에는 19%라고 되어있으나, 1-9.02/10.64 $\approx$ 0.152) 단축시켰습니다.
    *   **정확도 유지**: 모든 양자화 방법에서 WER은 기본 모델과 유사하거나(INT5, INT8) 오히려 개선되었습니다(INT4는 0.0199에서 0.0159로 감소하여 정확도 98.4% 달성). 이는 양자화가 정확도를 희생하지 않음을 보여줍니다.

## 🧠 Insights & Discussion
*   **양자화의 효율성 입증**: 본 연구는 Whisper 모델에 대한 양자화가 모델 크기를 대폭 줄이고 지연 시간을 개선하면서도, 단어 오류율을 유지하거나 심지어 향상시킬 수 있음을 명확하게 보여주었습니다. 이는 특히 자원 제약이 있는 모바일 장치, IoT 기기, 임베디드 시스템 등 엣지 디바이스에서의 Whisper 모델 배포 가능성을 높여줍니다.
*   **엣지 배포의 실현 가능성**: 양자화된 Whisper 모델은 메모리 사용량을 줄이고 실시간 전사 서비스를 제공함으로써, 불안정한 인터넷 환경에서도 음성 인식 기능을 사용할 수 있게 하고, 청각 장애인 사용자를 위한 접근성을 향상시킬 수 있습니다.
*   **다양한 양자화 방법의 영향**: INT4, INT5, INT8 등 다양한 정수 양자화 방법을 비교함으로써, 각 방법이 모델 크기, 지연 시간, 정확도에 미치는 구체적인 영향을 파악할 수 있었습니다. 특히 INT4가 가장 큰 모델 크기 감소를, INT8이 가장 큰 지연 시간 감소를 보인 점은 흥미롭습니다.
*   **Whisper 변형의 활용성**: `whisper-timestamped`의 정밀한 단어 타임스탬프와 `Whisper_Streaming`의 실시간 처리 기능은 특정 애플리케이션(예: 정확한 자막, 실시간 통역)에서 큰 이점을 제공합니다. 이러한 변형들에 양자화를 적용하면 더욱 광범위한 배포가 가능해질 것입니다.
*   **한계 및 미래 연구**: 본 연구는 Whisper 모델에 초점을 맞췄지만, 다른 ASR 모델에도 이 연구를 확장하여 오디오 기반 AI 애플리케이션의 확장성을 높일 수 있습니다. 향후 연구에서는 추가적인 양자화 기법 탐색, 하드웨어 배포 전략 최적화, 실시간 성능과 정확도 간의 상충 관계(trade-off) 분석이 필요합니다.

## 📌 TL;DR
Whisper 모델의 환각 현상, 높은 지연 시간, 대규모 모델 크기로 인한 자원 제약 장치 배포 문제를 해결하기 위해, 본 연구는 Whisper 및 그 변형(`whisper-timestamped`, `Whisper_Streaming`)의 기능을 비교하고, INT4, INT5, INT8 양자화 기법이 WER, 지연 시간, 모델 크기에 미치는 영향을 정량적으로 분석했습니다. 결과적으로 양자화는 모델 크기를 최대 45% 줄이고 지연 시간을 19% 단축하면서도 전사 정확도를 유지하거나 향상시킬 수 있음을 보여, Whisper 모델의 모바일 및 IoT 장치 배포 가능성을 입증했습니다.