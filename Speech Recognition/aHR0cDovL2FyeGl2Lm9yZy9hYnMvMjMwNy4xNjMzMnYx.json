{
  "title": "Pre-training End-to-end ASR Models with Augmented Speech Samples Queried\n  by Text",
  "authors": "Eric Sun, Jinyu Li, Jian Xue, Yifan Gong",
  "year": 2023,
  "url": "http://arxiv.org/abs/2307.16332v1",
  "abstract": "In end-to-end automatic speech recognition system, one of the difficulties\nfor language expansion is the limited paired speech and text training data. In\nthis paper, we propose a novel method to generate augmented samples with\nunpaired speech feature segments and text data for model pre-training, which\nhas the advantage of low cost without using additional speech data. When mixing\n20,000 hours augmented speech data generated by our method with 12,500 hours\noriginal transcribed speech data for Italian Transformer transducer model\npre-training, we achieve 8.7% relative word error rate reduction. The\npre-trained model achieves similar performance as the model pre-trained with\nmultilingual transcribed 75,000 hours raw speech data. When merging the\naugmented speech data with the multilingual data to pre-train a new model, we\nachieve even more relative word error rate reduction of 12.2% over the\nbaseline, which further verifies the effectiveness of our method for speech\ndata augmentation.",
  "citation": 1
}