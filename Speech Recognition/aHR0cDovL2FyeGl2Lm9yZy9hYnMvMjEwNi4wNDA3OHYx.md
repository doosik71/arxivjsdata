# 화자 활동 및 중첩 감지에 조건부 종단간 화자 분할 (END-TO-END SPEAKER DIARIZATION CONDITIONED ON SPEECH ACTIVITY AND OVERLAP DETECTION)
Yuki Takashima, Yusuke Fujita, Shinji Watanabe, Shota Horiguchi, Paola García, Kenji Nagamatsu

## 🧩 Problem to Solve
기존의 클러스터링 기반 화자 분할(Speaker Diarization) 방법은 중첩 발화(overlapping speech) 처리와 직접적인 분할 오류 최소화에 한계가 있었습니다. 종단간 신경망 화자 분할(End-to-End Neural Diarization, EEND)은 중첩 발화를 잘 처리하지만, 화자별 조건부 EEND (Speaker-wise Conditional EEND, SC-EEND)의 경우 음성 활동 감지(Speech Activity Detection, SAD) 성능의 한계로 인해 실제 대화 녹음에서의 성능이 충분하지 않았습니다. 본 연구는 이러한 EEND 시스템의 성능을 향상시키기 위해, 화자 분할의 하위 태스크(subtask)인 SAD와 중첩 감지(Overlap Detection, OD)를 활용하고 태스크 간의 의존성을 명시적으로 고려하는 방법을 제안합니다.

## ✨ Key Contributions
*   EEND를 위한 새로운 조건부 다중 작업 학습(conditional multitask learning) 프레임워크를 제안했습니다.
*   확률적 연쇄 법칙(probabilistic chain rule)을 기반으로 화자 활동 및 중첩 감지에 조건부로 화자 분할을 최적화하여 태스크 의존성을 명시적으로 모델링했습니다.
*   하위 태스크를 먼저 실행한 후 화자 분할을 수행하는 "하위 태스크 우선(subtask-first)" 접근 방식을 도입했습니다.
*   기존 EEND 시스템 대비 분할 오류율(Diarization Error Rate, DER)을 크게 개선했으며, 특히 누락(Misses, MI) 오류 및 오경보(False Alarms, FA) 오류를 효과적으로 감소시켰습니다.
*   여러 하위 태스크(SAD 및 OD)를 결합했을 때 추가적인 성능 향상을 입증했습니다.
*   모델 파라미터 증가가 미미하여 효율적인 학습 전략임을 보여주었습니다.

## 📎 Related Works
*   **기존 화자 분할:** SAD, 음성 분할, 특징 추출 및 클러스터링(x-벡터, d-벡터, AHC, k-평균, 스펙트럼 클러스터링 등) 기반 방법.
*   **종단간 신경망 화자 분할 (EEND):** 중첩 발화를 처리할 수 있는 유망한 방향으로 [14, 15]에서 제시되었으며, 다양한 확장 연구가 진행되었습니다.
*   **EEND 확장:** 유동적인 화자 수를 처리하기 위한 화자별 조건부 EEND (SC-EEND) [16, 17] 및 온라인 EEND [30]가 있습니다. 본 논문은 SC-EEND [16]를 확장합니다.
*   **다중 작업 학습:** 일반적인 개념 [18, 19]과 태스크 간의 계층적 관계를 명시적으로 활용하는 접근 방식 [31, 32, 33] (예: 의미론적 태스크) 및 ASR에서의 적용 [35, 36] (예: 음소-음소 문자 모델)이 있습니다.
*   **조건부 연쇄 매핑 (Conditional Chain Mapping):** [21]에서 여러 출력 시퀀스 간의 연관성을 확률적 연쇄 법칙을 통해 모델링하는 기술로, 음성 분리 및 다중 화자 음성 인식에 적용되었습니다. 본 논문에서 핵심적으로 채택한 방법입니다.
*   **기타 화자 분할 연구:** 비교 대상으로 Zhang et al. [13], McCree et al. [41]의 시스템이 언급됩니다.

## 🛠️ Methodology
*   **조건부 다중 작업 학습 공식화:** 화자 분할과 하위 태스크(SAD, OD)를 입력 음향 특징 $X$에 대한 결합 확률 $P(y_1,...,y_S, u_1,...,u_K|X)$ 문제로 공식화합니다.
*   **확률적 연쇄 법칙 적용:** 결합 확률을 하위 태스크와 화자 활동 간의 의존성을 고려하여 다음과 같이 분해합니다:
    $$P(y_1,...,y_S,u_1,...,u_K|X) = \prod_{k=1}^{K} P(u_k|u_1,...,u_{k-1},X) \times \prod_{s=1}^{S} P(y_s|y_1,...,y_{s-1},u_1,...,u_K,X)$$
    이는 "하위 태스크 우선" 접근 방식을 가능하게 합니다.
*   **모델 아키텍처:**
    *   **공유 인코더 (Shared Encoder):** 4개의 Transformer 인코더 블록 [37]을 사용하여 음향 특징 $X$를 임베딩 $E$로 변환합니다. 이는 하위 태스크와 화자 분할 네트워크 모두에 공유됩니다.
    *   **조건부 연쇄 모듈 (Conditional Chain Module):** 단방향 LSTM을 사용하여, 공유된 임베딩 $E$와 *이전* 추정된 하위 태스크/화자 활동을 입력으로 받아 은닉 표현 $H_l$을 계산합니다. 각 스텝에서 $H_l$은 이전 태스크의 출력에 조건부로 의존합니다. 예를 들어, 첫 번째 화자의 추정치는 마지막 하위 태스크 출력($\tilde{y}_0 := \tilde{u}_K$)에 조건부로 연결됩니다.
        $$H_l = \begin{cases} \text{LSTM}([E, f(\tilde{u}_{l-1})], H_{l-1}) & (1 \le l \le K) \\ \text{LSTM}([E, f(\tilde{y}_{l-K-1})], H_{l-1}) & (K+1 \le l \le K+S) \end{cases}$$
        여기서 $f(\cdot)$는 선형 투영이고, $\tilde{u}$와 $\tilde{y}$는 임계값 처리된 출력입니다.
    *   **태스크별 디코더 (Task-Specific Decoders):** 선형 투영($g^{(k)}_{Sub}$, $g_{Diar}$)과 시그모이드 활성화 함수($\sigma$)를 사용하여 LSTM 은닉 상태 $H_k$, $H_{s+K}$를 확률 $v_k$ (하위 태스크) 및 $z_s$ (화자)로 변환합니다.
        $$v_k = \sigma(g^{(k)}_{Sub}(H_k))$$
        $$z_s = \sigma(g_{Diar}(H_{s+K}))$$
*   **하위 태스크 정의:**
    *   SAD ($u^*_{SAD}$): 각 프레임에서 모든 화자의 최대 활동( $\max(y^*_{1,t},...,y^*_{S,t})$).
    *   OD ($u^*_{OD}$): 두 명 이상의 화자가 동시에 활성 상태인 프레임($I(\sum_{s=1}^{S} y^*_{s,t} > 1)$).
    *   별도의 라벨링 없이 기존 분할 라벨에서 파생 가능합니다.
*   **손실 함수:** 총 손실은 하위 태스크 손실($L_{Sub}$)과 PIT(Permutation-Invariant Training) 손실($L_{PIT}$)의 합 $L = L_{Sub} + L_{PIT}$입니다.
    *   **하위 태스크 손실($L_{Sub}$):** 각 하위 태스크에 대한 요소별 이진 교차 엔트로피(BCE) 합산.
        $$L_{Sub} = \frac{1}{T} \sum_{k=1}^{K} \text{BCE}(v_k, u^*_k)$$
    *   **분할 손실($L_{PIT}$):** 화자 활동에 대한 PIT 손실 (BCE 사용).
        $$L_{PIT} = \frac{1}{ST} \min_{\phi \in \text{perm}(S)} \sum_{s=1}^{S} \text{BCE}(z_s, y^*_{\phi_s})$$
*   **학습 전략:** 성능 향상을 위해 ground-truth 라벨을 조건부 입력으로 사용하는 Teacher-forcing [39]과 화자 순열 처리를 위한 2단계 PIT 손실 계산 [16]을 사용합니다. 가변적인 화자 수는 정지 시퀀스 조건 [16]으로 처리됩니다.

## 📊 Results
*   **데이터셋:** 시뮬레이션된 2화자 및 가변 화자 학습 데이터셋, CALLHOME 2화자 및 가변 화자 적응/테스트 데이터셋.
*   **평가 지표:** 중첩 구간 및 SAD 관련 오류(누락, 오경보, 혼동 오류)를 포함하는 분할 오류율(DER).
*   **고정된 2화자 모델 (CALLHOME-2spk):**
    *   제안된 SAD-first SC-EEND는 SC-EEND 대비 DER 6.2% 상대적 개선(9.39% $\rightarrow$ 8.81%)을 달성했으며, 기존 다중 작업 SC-EEND 대비 4.2% 개선되었습니다. SAD 누락 오류가 크게 감소했습니다.
    *   OD-first SC-EEND는 SC-EEND 대비 성능 향상을 보였고, DER 오경보 오류를 유의미하게 감소시켰습니다.
    *   SAD-OD-first SC-EEND는 가장 낮은 DER(8.53%)을 달성하여, SAD-first 및 OD-first에 비해 각각 3.18%, 6.16% 상대적 개선을 보였습니다. 이는 효율적인 파라미터 증가로 달성되었습니다(<0.01%).
*   **가변 화자 수 모델 (CALLHOME-vspk):**
    *   SAD-first SC-EEND는 SC-EEND 대비 1.35% (15.57% $\rightarrow$ 15.36%) 상대적 DER 개선을 보였고, x-벡터 클러스터링 대비 19.2% 향상되었습니다.
    *   SAD-OD-first SC-EEND는 15.32% DER로 모든 평가 방법 중 최고의 성능을 달성했습니다.
    *   OD-first SC-EEND는 기존 SC-EEND보다 우수하지 않았는데, 이는 OD가 더 어려운 태스크이며 보다 신중한 학습 전략이 필요함을 시사합니다.
    *   제안된 방법은 대부분의 화자 수에서 SC-EEND보다 우수한 성능을 보여 화자 수 증가에 강건함을 입증했습니다.
*   **화자 수 세기 정확도:** 제안된 방법(75.6%)은 x-vector+AHC(54.6%)보다 개선되었으나, SC-EEND(77.6%)보다는 약간 낮아, 적은 수의 화자에 대한 분할 정확도에 집중함을 나타냅니다.
*   **다른 시스템과의 비교 (Oracle SAD 조건, 중첩 제외):** 제안된 SAD-OD-first SC-EEND (7.4% DER)는 Zhang et al. [13] (7.6%)을 능가했으며, McCree et al. [41] (7.1%)과 경쟁적인 성능을 보였습니다.

## 🧠 Insights & Discussion
*   **조건부 다중 작업 학습의 효과:** 확률적 연쇄 법칙을 통한 태스크 의존성 모델링과 쉬운 하위 태스크(SAD, OD)의 활용은 EEND의 성능을 크게 향상시킵니다.
*   **SAD의 중요성:** SAD를 하위 태스크로 사용하면 누락된 발화 구간(MI 오류)을 효과적으로 줄여 전체 분할 정확도에 결정적인 기여를 합니다.
*   **OD의 역할:** OD는 단일 화자 구간에서 발화의 과도한 감지를 방지하여 오경보(FA 오류)를 줄이는 데 도움을 줍니다.
*   **효율성:** 제안된 방법은 모델 파라미터의 미미한 증가로 상당한 성능 향상을 달성합니다.
*   **한계 및 향후 연구:**
    *   OD는 더 어려운 태스크이므로, 스케줄 학습(scheduled learning)과 같은 더 정교한 학습 전략이 필요할 수 있습니다.
    *   데이터셋 불균형으로 인해 4명 이상의 화자에 대한 화자 수 세기 정확도는 여전히 어려운 문제입니다. 향후 시뮬레이션 데이터 생성 및 손실 가중치 적용을 고려할 것입니다.
    *   향후에는 다른 태스크와의 결합 및 더 어려운 시나리오에서의 적용을 탐구할 예정입니다.

## 📌 TL;DR
*   **문제:** 종단간 화자 분할(EEND), 특히 SC-EEND는 음성 활동 감지(SAD) 성능 및 중첩 발화 처리에서 한계를 보여 실제 대화 녹음에서 분할 오류가 발생합니다.
*   **방법:** 본 논문은 SC-EEND를 위한 "하위 태스크 우선" 조건부 다중 작업 학습 프레임워크를 제안합니다. 이는 확률적 연쇄 법칙을 사용하여 음성 활동 감지(SAD) 및 중첩 감지(OD)와 같은 하위 태스크의 화자 분할에 대한 의존성을 명시적으로 모델링합니다. 모델은 공유 트랜스포머 인코더, 조건부 연쇄 LSTM 모듈 및 태스크별 디코더로 구성되며, teacher-forcing과 PIT 손실을 결합하여 학습됩니다.
*   **주요 결과:** 제안된 방법은 기존 EEND 및 전통적인 다중 작업 접근 방식에 비해 분할 오류율(DER)을 크게 줄였습니다. SAD는 주로 누락 오류를 줄이는 반면, OD는 오경보를 줄입니다. SAD와 OD를 하위 태스크로 함께 사용하면 최소한의 파라미터 오버헤드로 최고의 성능을 달성하며, 다양한 화자 수에 대한 강건성을 보여주었습니다.