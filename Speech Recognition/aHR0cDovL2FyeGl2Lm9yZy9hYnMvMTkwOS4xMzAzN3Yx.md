# Self-Attention Transducers for End-to-End Speech Recognition

Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengqi Wen

## 🧩 Problem to Solve

순환 신경망 트랜스듀서(Recurrent Neural Network Transducers, RNN-T)는 end-to-end 음성 인식 분야에서 성공적으로 활용되었지만, 순환 구조로 인해 병렬화가 어렵다는 본질적인 한계가 있습니다. 특히 발화(utterance) 길이가 길어질수록 이러한 병렬화 문제는 더욱 중요해지며, 계산 효율성을 저해합니다. 따라서 이 논문은 RNN의 병렬화 문제를 해결하면서도 긴 범위의 의존성(long-term dependencies)을 효과적으로 모델링할 수 있는 새로운 end-to-end 음성 인식 모델을 제안하는 것을 목표로 합니다.

## ✨ Key Contributions

- **자기-어텐션 트랜스듀서 (Self-Attention Transducer, SA-T) 제안:** RNN을 강력한 자기-어텐션 블록으로 대체하여 시퀀스 내의 긴 범위 의존성을 효과적으로 모델링하고, 효율적인 병렬화를 가능하게 하는 end-to-end 음성 인식 모델을 구축했습니다.
- **경로 인식 정규화 (Path-Aware Regularization, PAR) 도입:** SA-T 모델이 정렬(alignment)을 더 잘 학습하고 전반적인 성능을 향상시키도록 돕기 위해 새로운 정규화 기법을 제안했습니다. 이는 CTC-CE 훈련에서 영감을 받았습니다.
- **청크-플로우 메커니즘 (Chunk-Flow Mechanism) 활용:** SA-T의 온라인 디코딩(streaming speech recognition)을 가능하게 하기 위해 자기-어텐션의 범위를 제한하는 메커니즘을 도입했습니다.

## 📎 Related Works

- 이전 연구에서는 CTC, 어텐션 기반 시퀀스-투-시퀀스, lattice-free MMI, RNN-T 등 다양한 end-to-end 음성 인식 모델들이 제시되었습니다.
- 자기-어텐션 메커니즘은 RNN을 대체하기 위해 다양한 태스크에서 성공적으로 적용되었으며, 음성 인식 분야에서도 경쟁력 있는 결과를 보였습니다 [6, 17, 21].
- 본 연구는 자기-어텐션 메커니즘을 트랜스듀서 기반 모델에 도입했다는 점에서 기존 작업과 차별화됩니다.
- 스트리밍 음성 인식을 위한 청크-플로우 메커니즘은 [21]의 청크-호핑 메커니즘과는 다르게, 각 레이어에서 슬라이딩 윈도우를 사용하여 자기-어텐션의 범위를 제한합니다. 이는 [16]의 시간-제한 자기-어텐션 레이어와 더 유사합니다.

## 🛠️ Methodology

SA-T (Self-Attention Transducer)는 RNN-T의 핵심 구성 요소인 RNN을 자기-어텐션 블록으로 대체하여 모델의 효율성과 성능을 향상시킵니다.

- **SA-T 모델 아키텍처:**
  - **인코더 및 예측 네트워크:** RNN-T에서 RNN이 사용되던 부분에 Figure 1(b)에 묘사된 자기-어텐션 블록 [5]을 사용합니다.
  - **자기-어텐션 블록:** 일반적으로 멀티-헤드 자기-어텐션(Multi-head Self-Attention, MHA) 레이어, 포지션-와이즈 피드 포워드 네트워크(Position-wise Feed Forward Network, FFN), 잔차 연결(residual connections), 그리고 레이어 정규화(layer normalization)로 구성됩니다.
    - **멀티-헤드 어텐션 (MHA):** 모델이 여러 위치의 정보에 동시에 집중할 수 있도록 합니다. 각 헤드 $h_i$는 독립적인 자기-어텐션 컴포넌트로 정의됩니다:
      $$ \text{SelfAttn}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d*k}})V $$
            $$ \text{MultiHead}(Q,K,V) = \text{Concat}(h_1, h_2, ..., h*{n*h})W_O $$
      여기서 $h_i = \text{SelfAttn}(QW*{Q*i},KW*{K*i},V W*{V_i})$ 입니다.
    - **포지션-와이즈 피드 포워드 네트워크 (FFN):** 두 개의 선형 변환과 ReLU 활성화 함수로 구성됩니다:
      $$ \text{FFN}(x) = \text{max}(0,xW_1+b_1)W_2+b_2 $$
  - **위치 인코딩 (Position Encoding):** 자기-어텐션 블록에는 순환 또는 컨볼루션 구조가 없으므로, [5]에서 제안된 사인 및 코사인 위치 임베딩을 모든 실험에 적용합니다.
  - **잔차 연결 및 레이어 정규화:** 각 서브-레이어의 출력은 $\text{LayerNorm}(x+\text{Sublayer}(x))$ 형태로 구성됩니다.
- **경로 인식 정규화 (Path-Aware Regularization, PAR):**
  - **목적:** SA-T 훈련을 안정화하고 정렬 학습을 돕기 위해 CTC-CE 훈련 [18, 19]에서 영감을 받았습니다.
  - **손실 함수:** 전체 손실 $L_{\text{joint}}(x)$는 트랜스듀서 손실 $L_{\text{transducer}}(x)$와 경로 인식 정규화 손실 $L_{\text{par}}(x)$의 가중합으로 정의됩니다.
    $$ L*{\text{joint}}(x) = L*{\text{transducer}}(x) + \beta \cdot L*{\text{par}}(x) $$
    여기서 $L*{\text{par}}(x) = - \sum*{t=0}^{T-1} \sum*{u=0}^{U-1} \sum*{k=0}^{K-1} w*{t,u} c*{t,u,k} \text{log}p(k|t,u)$ 이고, $w*{t,u} = 1-p(\phi|t,u)$ ($p(\phi|t,u)$는 'blank' 레이블의 확률)입니다.
  - **프레임 레벨 대상 레이블($c_{t,u,k}$) 생성:** Kaldi Toolkit [22]을 사용하여 각 발화에 대한 문자 레벨 정렬 시퀀스를 생성한 후, 이를 기반으로 프레임 레벨 대상 레이블을 얻습니다. 훈련 중에는 Figure 1(c)의 녹색 원으로 표시된 정렬 위치에 대해서만 교차 엔트로피 손실을 계산하여, 모델이 특정 정렬 경로 최적화에 집중하도록 유도합니다.
- **청크-플로우 메커니즘 (Chunk-Flow Mechanism):**
  - **목적:** 자기-어텐션 메커니즘의 특성상 전체 시퀀스를 입력으로 받아야 하므로 스트리밍(온라인) 음성 인식에 직접 적용하기 어려운 문제를 해결합니다.
  - **동작 방식:** 자기-어텐션 블록의 처리 범위를 고정 길이 청크로 제한합니다. 이 청크는 특징 시퀀스의 시간 축을 따라 슬라이딩하며, 여러 자기-어텐션 블록을 쌓아 더 넓은 시간적 컨텍스트를 모델링합니다.
  - **수식:** 각 시간 $t$에서의 $i$번째 헤드 출력 $h_{i,t}$는 다음과 같이 정의됩니다:
    $$ h*{i,t} = \sum*{\tau=t-N*l}^{t+N_r} \alpha*{i,\tau} s\_{\tau} $$
        여기서 $N_l$과 $N_r$은 현재 시간 $t$의 왼쪽과 오른쪽 프레임 수를 나타내며, 각 블록 내 청크의 길이는 $N_l + N_r + 1$입니다.

## 📊 Results

- **데이터셋:** 모든 실험은 공개 중국어 음성 코퍼스인 AISHELL-1 [25]을 사용하여 수행되었습니다.
- **SA-T의 기본 성능:** 제안된 SA-T는 baseline RNN-T 대비 개발 세트에서 8.2%, 테스트 세트에서 9.5%의 상대적 CER(Character Error Rate) 감소를 달성했습니다. 또한, 어텐션 기반 LAS 모델 [20]보다 우수한 성능을 보였습니다.
- **경로 인식 정규화 (PAR)의 효과:**
  - 다양한 $\beta$ 가중치로 SA-T를 훈련한 결과, $\beta=10$일 때 일반 SA-T 대비 테스트 세트에서 11.1%의 상대적 CER 감소를 달성하며 최적의 성능을 보였습니다.
  - 정규화는 SA-T의 수렴을 가속화하고 성능을 향상시켰으며, $\beta$ 값이 5보다 클 때 더 나은 성능을 나타냈습니다.
  - PAR은 모델이 더 정확한 정렬과 더 선명한 확률 분포를 학습하도록 돕는 것이 시각적으로 확인되었습니다 (Figure 4 참조).
- **청크-플로우 메커니즘의 효과:**
  - 청크 길이에 따른 실험에서, 모델링되는 컨텍스트가 길수록 성능이 향상됨을 확인했습니다 (예: 왼쪽 20프레임, 오른쪽 10프레임 컨텍스트에서 최상).
  - 청크-플로우 SA-T는 baseline RNN-T와 필적하는 성능을 보이면서도 온라인 디코딩을 가능하게 했으며, 성능 저하가 거의 없었습니다.
  - 청크-플로우 SA-T에 PAR ($\beta=10$)을 적용한 결과, 정규화를 적용하지 않은 모델 대비 개발 세트에서 12.5%, 테스트 세트에서 11.4%의 추가적인 상대적 CER 감소를 달성하여 PAR의 효과를 다시 한번 입증했습니다.
- **종합 결과:** 경로 인식 정규화가 적용된 SA-T는 baseline RNN-T 대비 총 21.3%의 상대적 CER 감소를 달성하며 우수한 성능을 입증했습니다.

## 🧠 Insights & Discussion

- **자기-어텐션의 우수성:** SA-T는 RNN의 순환 구조를 자기-어텐션 블록으로 대체하여 병렬 계산 능력을 크게 향상시키고, 더 깊은 네트워크 구조와 효율적인 긴 범위 의존성 모델링을 통해 기존 RNN-T보다 우수한 성능을 달성했습니다.
- **정규화의 중요성:** 경로 인식 정규화(PAR)는 SA-T가 정렬을 더 효과적으로 학습하고 훈련 과정을 안정화하는 데 결정적인 역할을 했습니다. 이는 모델이 특정 정렬 경로에 집중하도록 유도함으로써 이루어지며, 모델의 일반화 성능을 높였습니다.
- **온라인 디코딩의 실현:** 청크-플로우 메커니즘은 자기-어텐션 기반 모델의 고질적인 문제인 전체 시퀀스 입력 요구 사항을 해결하여, 성능 저하를 최소화하면서도 스트리밍 음성 인식을 위한 온라인 디코딩을 가능하게 했습니다.
- **향후 과제:** 실험 결과, 모델이 발음이 유사한 문자를 쉽게 예측하는 경향이 있다는 점이 관찰되었습니다. 이는 모델의 언어 모델링 능력 개선이 필요함을 시사하며, 향후 연구 방향으로 제시되었습니다.

## 📌 TL;DR

- **문제:** 기존 RNN-T는 강력하지만 순환 구조로 인해 병렬화가 어렵고 긴 발화 처리에서 비효율적이다.
- **제안:** RNN을 자기-어텐션 블록으로 대체한 **자기-어텐션 트랜스듀서(SA-T)**를 제안하여 병렬 처리 및 긴 범위 의존성 모델링을 개선했다.
- **핵심 기여:**
  - 모델의 정렬 학습 및 성능 향상을 위한 **경로 인식 정규화(Path-Aware Regularization, PAR)**를 도입했다.
  - 온라인 디코딩을 가능하게 하는 **청크-플로우 메커니즘**을 구현했다.
- **결과:** 중국어 AISHELL-1 데이터셋에서 제안된 SA-T (PAR 포함)는 baseline RNN-T 대비 **21.3%**의 상대적 CER 감소를 달성했으며, 청크-플로우 메커니즘을 통해 성능 저하 없이 온라인 디코딩이 가능함을 입증했다.
