{
  "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
  "authors": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang",
  "year": 2020,
  "url": "http://arxiv.org/abs/2005.08100v1",
  "abstract": "Recently Transformer and Convolution neural network (CNN) based models have\nshown promising results in Automatic Speech Recognition (ASR), outperforming\nRecurrent neural networks (RNNs). Transformer models are good at capturing\ncontent-based global interactions, while CNNs exploit local features\neffectively. In this work, we achieve the best of both worlds by studying how\nto combine convolution neural networks and transformers to model both local and\nglobal dependencies of an audio sequence in a parameter-efficient way. To this\nregard, we propose the convolution-augmented transformer for speech\nrecognition, named Conformer. Conformer significantly outperforms the previous\nTransformer and CNN based models achieving state-of-the-art accuracies. On the\nwidely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without\nusing a language model and 1.9%/3.9% with an external language model on\ntest/testother. We also observe competitive performance of 2.7%/6.3% with a\nsmall model of only 10M parameters.",
  "citation": 4302
}