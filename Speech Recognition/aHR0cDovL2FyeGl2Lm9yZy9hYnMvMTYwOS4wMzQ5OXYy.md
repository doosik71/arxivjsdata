# WAVENET: A GENERATIVE MODEL FOR RAW AUDIO

Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu

## 🧩 Problem to Solve

이 논문은 높은 시간 해상도(초당 수만 개의 샘플)를 가진 신호인 광대역 원본 오디오 파형(raw audio waveforms)을 생성하는 문제를 다룹니다. 기존의 텍스트-음성 변환(TTS) 시스템은 합성 음성의 자연스러움과 유연성 측면에서 한계가 있었으며, 원본 오디오 파형의 복잡한 분포를 효율적으로 모델링하는 것이 주요 도전 과제였습니다. 특히, 각 오디오 샘플이 이전 모든 샘플에 조건화되는(autoregressive) 확률론적 모델을 구축하면서도 이를 대규모 오디오 데이터에 효율적으로 훈련하는 방법을 찾는 것이 목표입니다.

## ✨ Key Contributions

- 인간 평가자들이 주관적으로 전례 없는 자연스러움을 가진 원본 음성 신호를 생성할 수 있음을 입증하여 텍스트-음성 변환(TTS) 분야에서 최첨단 성능을 달성했습니다.
- 원본 오디오 생성에 필요한 장거리 시간 종속성을 처리하기 위해 매우 넓은 수용 필드(receptive field)를 가진 확장형 인과 컨볼루션(dilated causal convolutions) 기반의 새로운 아키텍처를 개발했습니다.
- 화자 ID에 따라 모델을 조건화함으로써, 단일 WaveNet 모델이 여러 다른 화자의 음성을 동일한 충실도로 생성할 수 있음을 보였습니다.
- 동일한 아키텍처가 소규모 음성 인식 데이터셋에서 강력한 결과를 보였으며, 음악과 같은 다른 오디오 모달리티를 생성하는 데에도 유망함을 입증했습니다.

## 📎 Related Works

- **신경망 자기회귀 생성 모델 (Neural Autoregressive Generative Models):** 이미지(PixelRNN, PixelCNN) 및 텍스트와 같은 복잡한 분포를 모델링하는 최근 발전에서 영감을 받았습니다.
- **확장형 컨볼루션 (Dilated Convolutions):** 신호 처리 및 이미지 분할 분야에서 이전에 사용된 바 있습니다.
- **게이트 활성화 단위 (Gated Activation Units):** 게이트가 있는 PixelCNN(van den Oord et al., 2016b)에서 사용된 비선형성을 활용했습니다.
- **잔여 학습 (Residual Learning):** 깊은 모델 훈련을 가속화하기 위해 He et al. (2015)의 잔여 연결 기법을 사용했습니다.
- **텍스트-음성 변환 (TTS) 베이스라인:**
  - **LSTM-RNN 기반 통계적 파라메트릭 합성 (Statistical Parametric Synthesis):** (Zen et al., 2016)
  - **HMM 기반 단위 선택 연결 합성 (HMM-driven Unit Selection Concatenative Synthesis):** (Gonzalvo et al., 2016)
- **음성 인식 (Speech Recognition):** 기존의 멜 필터뱅크 에너지 또는 MFCC 대신 원본 오디오를 직접 사용하는 최근 접근 방식(Palaz et al., 2013; Tüske et al., 2014)과 LSTM-RNN(Hochreiter & Schmidhuber, 1997)을 언급했습니다.

## 🛠️ Methodology

WaveNet은 원본 오디오 파형 $x = \{x_1, ..., x_T\}$에 직접 작동하는 심층 생성 모델입니다. 모델은 자기회귀적이며, 각 오디오 샘플 $x_t$의 예측 분포는 이전 모든 샘플에 조건화됩니다.
$$ p(x) = \prod*{t=1}^{T} p(x_t | x_1, ..., x*{t-1}) $$

주요 구성 요소는 다음과 같습니다:

- **인과 컨볼루션 (Causal Convolutions):**

  - 모델이 데이터를 모델링하는 시간 순서(미래 데이터에 의존하지 않음)를 지키도록 보장합니다.
  - 일반 컨볼루션의 출력을 몇 개의 시점만큼 시프트하여 구현할 수 있습니다.
  - 훈련 시에는 모든 시점의 조건부 예측이 병렬로 이루어지지만, 생성 시에는 각 샘플이 예측된 후 다음 샘플 예측을 위해 네트워크에 다시 입력됩니다.

- **확장형 인과 컨볼루션 (Dilated Causal Convolutions):**

  - 일반 인과 컨볼루션의 제한된 수용 필드 문제를 해결하기 위해 도입되었습니다. 필터가 특정 단계(dilation)로 입력 값을 건너뛰며 적용되어 계산 비용을 크게 늘리지 않으면서 수용 필드를 몇 배로 확장합니다.
  - 각 레이어마다 확장(dilation) 인수를 2배로 늘리고(예: $1, 2, 4, ..., 512$) 반복하는 방식으로 스택됩니다. 이를 통해 깊이에 따라 수용 필드가 기하급수적으로 증가합니다.

- **소프트맥스 분포 및 $\mu$-law 양자화 (Softmax Distributions and $\mu$-law Quantization):**

  - 각 개별 오디오 샘플 $x_t$에 대한 조건부 분포 $p(x_t|x_1,...,x_{t-1})$를 모델링하기 위해 소프트맥스 분포를 사용합니다. 이는 데이터의 모양에 대한 가정을 하지 않아 더 유연합니다.
  - 원본 오디오는 일반적으로 16비트 정수 값으로 저장되므로, 65,536가지 가능한 값을 모델링해야 합니다. 이를 처리하기 위해 데이터를 먼저 $\mu$-law компа딩 변환(companding transformation)하고 256가지 값으로 양자화합니다.
    $$ f(x_t) = \text{sign}(x_t) \frac{\ln(1 + \mu|x_t|)}{\ln(1 + \mu)}, \quad \text{where } -1 < x_t < 1 \text{ and } \mu = 255 $$
  - 이 비선형 양자화는 단순 선형 양자화보다 훨씬 더 나은 재구성을 제공합니다.

- **게이트 활성화 단위 (Gated Activation Units):**

  - PixelCNN에서 사용된 게이트 활성화 단위를 사용합니다: $z = \tanh(W_{f,k} * x) \odot \sigma(W_{g,k} * x)$. 여기서 $\odot$는 요소별 곱셈, $\sigma(\cdot)$는 시그모이드 함수입니다. 오디오 신호 모델링에 rectified linear activation function(ReLU)보다 훨씬 더 효과적이었습니다.

- **잔여 및 스킵 연결 (Residual and Skip Connections):**

  - 수렴 속도를 높이고 훨씬 더 깊은 모델을 훈련할 수 있도록 네트워크 전반에 걸쳐 사용됩니다.

- **조건부 WaveNets (Conditional WaveNets):**
  - 추가 입력 $h$가 주어질 때 오디오의 조건부 분포 $p(x|h)$를 모델링할 수 있습니다.
    $$ p(x|h) = \prod*{t=1}^{T} p(x_t | x_1, ..., x*{t-1}, h) $$
  - **글로벌 조건화 (Global Conditioning):** 전체 시점에 영향을 미치는 단일 잠재 표현 $h$ (예: 화자 임베딩, 스피커 ID 원-핫 벡터). 활성화 함수는 $z = \tanh(W_{f,k} * x + V_{f,k}^T h) \odot \sigma(W_{g,k} * x + V_{g,k}^T h)$로 수정됩니다.
  - **로컬 조건화 (Local Conditioning):** 오디오 신호보다 낮은 샘플링 주파수를 가질 수 있는 두 번째 시계열 $h_t$ (예: TTS의 언어학적 특징). 전치 컨볼루션 네트워크로 오디오 신호와 동일한 해상도의 $y=f(h)$로 변환한 다음 활성화 단위에 사용합니다: $z = \tanh(W_{f,k} * x + V_{f,k} * y) \odot \sigma(W_{g,k} * x + V_{g,k} * y)$.

## 📊 Results

WaveNet은 세 가지 주요 오디오 모델링 작업에서 뛰어난 성능을 보였습니다.

- **다중 화자 음성 생성 (Multi-Speaker Speech Generation):**

  - 텍스트에 조건화되지 않았을 때, 비실존적이지만 인간 언어와 유사한 단어를 자연스러운 억양으로 생성했습니다.
  - 단일 WaveNet 모델이 109명의 다른 화자 특성을 성공적으로 학습하고, 스피커 ID를 조건으로 하여 음성을 모델링할 수 있음을 입증했습니다.
  - 화자 음성 외에도 음향, 녹음 품질, 화자의 숨소리 및 입술 움직임과 같은 다른 특성도 모방했습니다.

- **텍스트-음성 변환 (Text-to-Speech, TTS):**

  - 북미 영어 및 중국어 만다린어 데이터셋에서 기존 최첨단 파라메트릭 및 연결형 TTS 시스템을 능가하는 주관적 자연스러움을 달성했습니다.
  - 언어학적 특징에만 조건화된 WaveNet($L$)은 자연스러운 세그먼트 품질을 가졌지만, 때로는 부자연스러운 운율을 보였습니다.
  - 언어학적 특징과 로그 기본 주파수($\text{log}F_0$) 값 모두에 조건화된 WaveNet($L+F$)은 운율 문제를 해결하고 MOS(Mean Opinion Score) 테스트에서 4.0 이상의 점수를 달성했습니다. 이는 해당 훈련 데이터셋 및 테스트 문장에서 보고된 MOS 값 중 가장 높았습니다.
  - 가장 좋은 합성 음성과 자연 음성 간의 MOS 격차를 북미 영어에서 51%, 만다린어에서 69% 감소시켰습니다.

- **음악 모델링 (Music Modelling):**

  - MagnaTagATune 및 YouTube 피아노 데이터셋에서 훈련되었습니다.
  - 수용 필드를 크게 늘리는 것이 음악적인 샘플을 얻는 데 중요했습니다. 모델은 종종 조화롭고 미학적으로 만족스러운 음악적 단편을 생성했습니다.
  - 장르나 악기와 같은 태그에 조건화된 조건부 음악 모델도 효과적으로 작동했습니다.

- **음성 인식 (Speech Recognition):**
  - 생성 모델로 설계되었음에도 불구하고, TIMIT 데이터셋에서 음성 인식과 같은 판별 작업에 적용되었습니다.
  - 확장형 컨볼루션 뒤에 10밀리초 프레임으로 활성화 값을 집계하는 평균 풀링 레이어를 추가했습니다.
  - 다음 샘플 예측과 프레임 분류라는 두 가지 손실 항으로 훈련되어 TIMIT 테스트 세트에서 18.8 PER(Phone Error Rate)을 달성했으며, 이는 원본 오디오에 직접 훈련된 모델 중 최고의 결과였습니다.

## 🧠 Insights & Discussion

- WaveNet은 오디오 데이터의 장거리 시간 종속성을 모델링하는 데 효과적인 강력하고 유연한 프레임워크를 제공합니다. 확장형 인과 컨볼루션 덕분에 Recurrent Neural Network (RNN)보다 훈련 속도가 빠르면서도 넓은 수용 필드를 가질 수 있습니다.
- 기존 TTS 시스템의 주요 한계점이었던 음질 및 자연스러움의 격차를 크게 줄였습니다. 특히, $\mu$-law 양자화와 소프트맥스 분포의 사용은 원본 오디오의 미묘한 세부 사항을 포착하는 데 중요했습니다.
- 화자 ID 및 언어학적 특징과 같은 추가 입력에 모델을 조건화함으로써 다양한 특성을 가진 오디오를 생성할 수 있는 뛰어난 제어력을 보여주었습니다.
- 자유 형식 음성 생성에서는 장거리 일관성 부족으로 "비실존적" 콘텐츠가 생성되는 한계가 있었습니다. 이는 모델의 수용 필드 크기(약 300밀리초)가 언어의 장기적 의미론적 구조를 포착하기에는 여전히 제한적이기 때문입니다. 그러나 $\text{log}F_0$ 값을 조건으로 사용하면 TTS에서 운율 문제를 해결할 수 있었습니다.
- 생성 모델로서의 잠재력 외에도 음성 인식과 같은 판별 작업에서도 유망한 결과를 보여 다재다능함을 입증했습니다. 이는 TTS, 음악 합성, 음성 향상, 음성 변환, 음원 분리 등 오디오 생성에 의존하는 다양한 애플리케이션에 대한 일반적인 해결책이 될 수 있음을 시사합니다.

## 📌 TL;DR

WaveNet은 원본 오디오 파형을 생성하기 위한 심층 자기회귀 신경망 모델입니다. 이 모델은 각 오디오 샘플이 이전 모든 샘플에 조건화되도록 설계되었으며, **확장형 인과 컨볼루션**을 사용하여 계산 효율성을 유지하면서 **매우 넓은 수용 필드**를 달성합니다. $\mu$-law 양자화 및 소프트맥스 출력을 통해 오디오의 미세한 특징을 정확히 모델링합니다. 이 방법은 텍스트-음성 변환(TTS)에서 인간이 느끼는 자연스러움 측면에서 **기존 최첨단 시스템을 훨씬 뛰어넘는 성능**을 보여주었으며, 다중 화자 음성 생성, 음악 모델링, 심지어 음성 인식에서도 유망한 결과를 얻었습니다. WaveNet은 오디오 생성에 대한 유연하고 강력한 프레임워크를 제공하여 인공 음성과 자연 음성 간의 격차를 크게 줄였습니다.
