{
  "title": "XLSR-Transducer: Streaming ASR for Self-Supervised Pretrained Models",
  "authors": "Shashi Kumar, Srikanth Madikeri, Juan Zuluaga-Gomez, Esa√∫ Villatoro-Tello, Iuliia Thorbecke, Petr Motlicek, Manjunath K E, Aravind Ganapathiraju",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.04439v2",
  "abstract": "Self-supervised pretrained models exhibit competitive performance in\nautomatic speech recognition on finetuning, even with limited in-domain\nsupervised data. However, popular pretrained models are not suitable for\nstreaming ASR because they are trained with full attention context. In this\npaper, we introduce XLSR-Transducer, where the XLSR-53 model is used as encoder\nin transducer setup. Our experiments on the AMI dataset reveal that the\nXLSR-Transducer achieves 4% absolute WER improvement over Whisper large-v2 and\n8% over a Zipformer transducer model trained from scratch. To enable streaming\ncapabilities, we investigate different attention masking patterns in the\nself-attention computation of transformer layers within the XLSR-53 model. We\nvalidate XLSR-Transducer on AMI and 5 languages from CommonVoice under\nlow-resource scenarios. Finally, with the introduction of attention sinks, we\nreduce the left context by half while achieving a relative 12% improvement in\nWER.",
  "citation": 7
}