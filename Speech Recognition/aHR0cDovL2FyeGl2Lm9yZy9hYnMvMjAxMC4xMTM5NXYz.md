# DEVELOPING REAL-TIME STREAMING TRANSFORMER TRANSDUCER FOR SPEECH RECOGNITION ON LARGE-SCALE DATASET

Xie Chen, Yu Wu, Zhenghao Wang, Shujie Liu, Jinyu Li

## 🧩 Problem to Solve

최근 트랜스포머 기반의 종단간(End-to-End, E2E) 모델이 음성 인식 분야에서 큰 성공을 거두었지만, 추론 시 발생하는 높은 계산 비용은 실제 애플리케이션에 적용하는 데 핵심적인 장애물입니다. 특히 트랜스포머 트랜스듀서(Transformer Transducer, T-T)의 계산 비용은 입력 시퀀스 길이에 비례하여 크게 증가하며, 이는 실시간 스트리밍 시나리오에 부적합합니다. 기존 스트리밍 방법론들은 긴 지연 시간, 정확도 저하, 또는 훈련 병렬성 손상과 같은 단점을 가지고 있습니다. 이 논문은 대규모 데이터셋에서 낮은 지연 시간과 빠른 속도로 동작하는 실시간 스트리밍 T-T 및 컨포머 트랜스듀서(Conformer Transducer, C-T) 모델을 개발하고, 훈련 비용, 런타임 비용, 정확도 사이의 균형을 맞추는 것을 목표로 합니다.

## ✨ Key Contributions

- 트랜스포머-XL(Transformer-XL)과 청크 단위 스트리밍 처리 아이디어를 결합하여, 낮은 지연 시간과 빠른 속도를 제공하는 스트리밍 트랜스포머 트랜스듀서 모델을 설계했습니다.
- 트랜스포머 레이어 수에 따라 지연 시간이 선형적으로 증가하는 것을 방지하기 위해, 과거 정보를 유지하면서 미래 정보를 제한하는 간단하고 효과적인 어텐션 마스크(attention mask) 전략을 제안했습니다.
- 스트리밍 시나리오에서 제안된 T-T 및 C-T 모델이 하이브리드 모델, RNN 트랜스듀서(RNN-T), 그리고 스트리밍 트랜스포머 어텐션 기반 인코더-디코더(AED) 모델보다 우수한 성능을 보임을 입증했습니다.
- 상대적으로 작은 선행 탐색(look-ahead)을 허용할 경우 런타임 비용과 지연 시간을 최적화할 수 있음을 보여주었으며, 산업 요구 사항을 충족하는 실시간 처리 속도를 달성했습니다.
- 캐싱(caching) 및 청크 단위 계산(chunk-wise compute)과 같은 효율적인 추론 최적화 기법을 도입하여 실제 배포 가능성을 높였습니다.
- 대규모 데이터셋(65,000시간)에서 32개의 V100 GPU를 사용하여 이틀 만에 효율적인 훈련을 완료했습니다.

## 📎 Related Works

- **종단간 음성 인식 (E2E ASR)**: RNN 트랜스듀서(RNN-T) [10]와 어텐션 기반 인코더-디코더(AED) [1, 11, 12]가 주로 사용됩니다.
- **스트리밍 AED 모델**: 단조로운 청크 단위 어텐션(monotonic chunk-wise attention) [13] 및 트리거드 어텐션(triggered attention) [14, 15] 등이 연구되었습니다.
- **트랜스포머 기반 ASR**: 트랜스포머 [18]는 RNN을 대체하며 E2E 모델의 핵심 구성 요소가 되었고 [19, 20, 21], 트랜스포머 기반 트랜스듀서(T-T) [22, 33]가 제안되었습니다.
- **컨포머 (Conformer)**: CNN과 트랜스포머를 결합하여 SR 성능을 향상시킨 모델 [23]입니다.
- **기존 스트리밍 트랜스포머 접근 방식**:
  - **시간 제한 방식 (Time-restricted)** [22, 24, 25, 26]: 어텐션에서 좌우 컨텍스트를 마스킹하지만, 트랜스포머 레이어 수에 따라 수용 필드가 선형적으로 증가하여 큰 지연 시간을 초래합니다.
  - **청크 단위 방식 (Chunk-wise)** [27, 15]: 입력을 작은 청크로 분할하여 처리하지만, 청크 간의 관계를 무시하여 정확도 손실이 발생합니다.
  - **메모리 기반 방식 (Memory-based)** [28, 29]: 이력 정보를 인코딩하는 컨텍스트 벡터를 사용하지만, 훈련 시 트랜스포머의 병렬성을 저해합니다.
- **상대 위치 임베딩 (Relative Position Embedding)**: 절대 위치 임베딩 [34, 31, 32]보다 우수한 성능을 보여주며, 이 논문에서는 이를 효율적으로 구현했습니다.

## 🛠️ Methodology

### 1. 트랜스듀서 아키텍처

트랜스듀서 모델은 음향 인코더(encoder), 레이블 예측기(predictor), 조인트 네트워크(joint network)의 세 가지 구성 요소로 이루어집니다.

- **인코더(Encoder)**: 입력 음향 특징 시퀀스 $x_{1}^{t}$를 받아 $f_t$를 출력합니다. 본 논문에서는 트랜스포머 또는 컨포머를 사용합니다.
- **예측기(Predictor)**: 이전 레이블 시퀀스 $y_{1}^{u-1}$를 받아 $g_{u-1}$를 계산합니다. 본 논문에서는 속도 및 메모리 효율성을 위해 LSTM을 사용합니다.
- **조인트 네트워크(Joint Network)**: $f_t$와 $g_{u-1}$를 결합하여 소프트맥스 함수를 통해 어휘 분포에 대한 확률 $P(y_u | x_{1}^{t}, y_{1}^{u-1})$를 계산합니다.
  $$
  \begin{aligned}
  f_t &= \text{encoder}(x_{1}^{t}) \\
  g_{u-1} &= \text{predictor}(y_{1}^{u-1}) \\
  h_{t,u-1} &= \text{relu}(f_t + g_{u-1}) \\
  P(y_u | x_{1}^{t}, y_{1}^{u-1}) &= \text{softmax}(W_o \cdot h_{t,u-1})
  \end{aligned}
  $$

### 2. 스트리밍 모델을 위한 어텐션 마스크 설계

스트리밍 모델 훈련을 위해 이력(history)을 잘라내고 제한된 미래 정보만 허용하는 마스크 전략을 사용합니다.

1. **청크 분할**: 입력 음향 특징 시퀀스를 지정된 청크 크기로 분할하며, 효율적인 훈련을 위해 청크 간의 겹침은 없습니다.
2. **동일 청크 내 상호 참조**: 동일한 청크 내의 프레임들은 서로를 참조할 수 있습니다. 예를 들어, 청크 $[x_{10}, x_{11}, x_{12}]$ 내에서 $x_{10}$은 $x_{10}, x_{11}, x_{12}$를 모두 볼 수 있습니다. 각 프레임에 대한 평균 선행 탐색은 청크 크기의 절반입니다.
3. **청크 간 미래 제한**: 서로 다른 청크에 있는 두 프레임의 경우, 왼쪽 프레임은 오른쪽 프레임을 볼 수 없습니다. 이는 청크 경계가 수용 필드를 엄격하게 제한하여 모델이 깊어짐에 따라 수용 필드가 선형적으로 증가하는 것을 방지합니다.
4. **청크 간 과거 허용**: 서로 다른 청크에 있는 두 프레임의 경우, 오른쪽 프레임은 과거 이력 윈도우(history window) 크기 내에 있으면 왼쪽 프레임을 볼 수 있습니다. 이 방법을 통해 왼쪽 수용 필드는 모델이 깊어짐에 따라 선형적으로 증가할 수 있습니다 (이력 윈도우 크기만큼).
   이 어텐션 마스크는 모든 트랜스포머 레이어에 걸쳐 공유됩니다.

### 3. 추론 최적화 (Inference Optimizations)

- **캐싱 (Caching)**: 각 트랜스포머 레이어에서 키($k_{\tau} = W_k x_{\tau}$)와 값($v_{\tau} = W_v x_{\tau}$) 벡터를 한 번 계산 후 캐싱하여 반복 계산을 피합니다. 현재 프레임 $x_t$에 대해서는 쿼리($q_t = W_q x_t$)만 새로 계산합니다. 잘린 이력(truncated history) 덕분에 캐싱 메모리 소비는 $t$의 증가에 따라 늘어나지 않습니다.
- **청크 단위 계산 (Chunk-wise compute)**: 여러 프레임(예: $[x_{10}, x_{11}, x_{12}]$)을 작은 미니배치로 그룹화하여 트랜스포머 인코더에 동시에 입력합니다. 이는 효율적인 행렬 연산을 가능하게 하여 상당한 속도 향상을 가져옵니다. 이 방법은 약간의 지연 시간을 도입하지만, 프레임별 계산의 비효율성을 크게 개선합니다.

## 📊 Results

- **실험 설정**: 65,000시간의 Microsoft 데이터로 훈련하고, 13개 애플리케이션 시나리오(총 180만 단어)의 테스트 세트에서 WER을 측정했습니다. 모델 크기는 약 80M 파라미터입니다. 런타임 속도는 16코어 Intel Xeon CPU (e5-2620, 2.10 GHz)에서 측정했습니다.

- **제로 선행 탐색 (Zero Look-ahead) 설정**:

  - T-T 및 C-T는 RNN-T보다 WER 측면에서 현저히 우수했으며, C-T가 T-T보다 약간 더 나은 성능을 보였습니다.
  - 전체 컨텍스트를 사용하는 T-T는 RTF가 1보다 훨씬 높았지만, 제안된 이력 잘라내기(예: 60프레임)를 적용하면 WER 손실 없이 RTF를 크게 줄였습니다 (T-T 1.75 → 0.69).
  - 트랜스포머 인코더는 전체 추론 시간의 약 90%를 차지하는 병목 현상으로 나타났습니다.
  - **배치 계산의 효과**: 여러 프레임을 배치로 묶어 계산하면 RTF가 크게 향상됩니다. 2프레임 배치(60ms 지연) 시 T-T의 RTF는 0.69로 감소하며, 15프레임 배치(450ms 지연) 시 RTF는 0.19까지 감소합니다. 트랜스포머는 RNN-T보다 배치 연산에 더 적합했습니다.

- **작은 선행 탐색 (Small Look-ahead) 설정 (평균 360ms 지연)**:

  - T-T (이력 60프레임, 선행 탐색 24프레임)와 C-T (이력 60프레임, 선행 탐색 24프레임)는 각각 8.28% 및 8.19%의 WER을 달성하여, 스트리밍 RNN S2S (9.61%) 및 스트리밍 트랜스포머 S2S (9.16%) 모델을 능가했습니다.
  - RNN-T (9.11%)와 비교하여 더 낮은 WER을 보였으며, RTF도 0.16~0.22로 매우 효율적이었습니다 (4스레드 사용 시).
  - 작은 선행 탐색을 사용하는 T-T의 성능(8.28%)은 전체 발화를 사용하는 오프라인 T-T (7.78%)와 매우 유사하여, 제안된 전략이 오프라인 모델 대비 성능 저하를 최소화함을 시사합니다.

- **8비트 양자화 (INT8 Optimization)**:
  - RNN-T의 경우 INT8 양자화를 통해 WER 저하 없이 3.6배의 속도 향상을 얻었습니다.
  - T-T 및 C-T의 경우 약 2배의 속도 향상을 보였으나, 약간의 WER 저하(0.2~0.22%)가 있었습니다. 이는 트랜스포머 레이어 내 소프트맥스(softmax) 연산이 여전히 부동 소수점 정밀도로 수행되어 CPU에서 계산 비용이 높기 때문으로 추정됩니다.

## 🧠 Insights & Discussion

이 논문은 스트리밍 트랜스포머 트랜스듀서(T-T) 및 컨포머 트랜스듀서(C-T) 모델이 대규모 음성 인식 환경에서 실시간 애플리케이션의 요구 사항을 충족할 수 있음을 성공적으로 보여줍니다. 제안된 어텐션 마스크 전략은 트랜스포머 레이어가 깊어짐에 따라 지연 시간이 선형적으로 증가하는 문제를 효과적으로 해결하며, 긴 과거 정보를 활용하면서도 미래 정보를 제한할 수 있게 합니다.

추론 최적화(캐싱, 청크 단위 계산)는 T-T 모델의 높은 계산 비용을 효과적으로 줄여 실시간 처리를 가능하게 합니다. 특히, 수백 밀리초 수준의 작은 선행 탐색(예: 360ms)을 허용함으로써 T-T 및 C-T는 오프라인 모델에 거의 근접하는 정확도를 유지하면서도 매우 낮은 실시간 계수(RTF)를 달성할 수 있습니다. 이는 스트리밍 시나리오에서 RNN-T 및 기타 어텐션 기반 E2E 모델보다 T-T가 더 강력하고 효율적인 선택임을 의미합니다.

INT8 양자화는 런타임 효율성을 더욱 높이지만, 트랜스포머 모델의 특정 연산(예: 소프트맥스)이 여전히 부동 소수점 정밀도를 사용하는 경우 성능 이득이 제한될 수 있다는 점은 향후 연구 방향을 제시합니다.

## 📌 TL;DR

- **문제**: 트랜스포머 트랜스듀서(T-T)는 음성 인식 정확도가 높지만, 추론 시 발생하는 높은 계산 비용과 지연 시간 때문에 실시간 스트리밍 애플리케이션에 적용하기 어렵습니다.
- **방법**: 트랜스포머-XL과 청크 단위 처리를 결합하고, 과거 정보는 충분히 활용하되 미래 정보의 수용 필드 증가를 제한하는 새로운 어텐션 마스크를 설계했습니다. 또한 캐싱 및 청크 단위 계산과 같은 추론 최적화를 통해 효율성을 높였습니다.
- **결과**: 제안된 스트리밍 T-T 및 컨포머 트랜스듀서(C-T)는 RNN-T 및 다른 스트리밍 모델보다 우수한 정확도를 달성했습니다. 360ms 정도의 작은 선행 탐색을 허용하고 추론 최적화를 적용하면, 오프라인 모델에 버금가는 정확도를 유지하면서 실시간 요구사항을 충족하는 낮은 런타임 계수를 달성하여 대규모 실시간 음성 인식 시스템에 실용적임을 입증했습니다.
