{
  "url": "http://arxiv.org/abs/2408.02622v1",
  "title": "Language Model Can Listen While Speaking",
  "authors": "Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen",
  "year": 2024,
  "abstract": "Dialogue serves as the most natural manner of human-computer interaction\n(HCI). Recent advancements in speech language models (SLM) have significantly\nenhanced speech-based conversational AI. However, these models are limited to\nturn-based conversation, lacking the ability to interact with humans in\nreal-time spoken scenarios, for example, being interrupted when the generated\ncontent is not satisfactory. To address these limitations, we explore full\nduplex modeling (FDM) in interactive speech language models (iSLM), focusing on\nenhancing real-time interaction and, more explicitly, exploring the\nquintessential ability of interruption. We introduce a novel model design,\nnamely listening-while-speaking language model (LSLM), an end-to-end system\nequipped with both listening and speaking channels. Our LSLM employs a\ntoken-based decoder-only TTS for speech generation and a streaming\nself-supervised learning (SSL) encoder for real-time audio input. LSLM fuses\nboth channels for autoregressive generation and detects turn-taking in real\ntime. Three fusion strategies -- early fusion, middle fusion, and late fusion\n-- are explored, with middle fusion achieving an optimal balance between speech\ngeneration and real-time interaction. Two experimental settings, command-based\nFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity\nto diverse instructions. Our results highlight LSLM's capability to achieve\nduplex communication with minimal impact on existing systems. This study aims\nto advance the development of interactive speech dialogue systems, enhancing\ntheir applicability in real-world contexts."
}