# LARGE-SCALE VISUAL SPEECH RECOGNITION

Brendan Shillingford, Yannis Assael, Matthew W. Hoffman, Thomas Paine, Cían Hughes, Utsav Prabhu, Hank Liao, Hasim Sak, Kanishka Rao, Lorrayne Bennett, Marie Mulville, Ben Coppin, Ben Laurie, Andrew Senior, Nando de Freitas

## 🧩 Problem to Solve

기존의 딥러닝 기반 독순술(lipreading) 연구들은 주로 제한된 어휘와 상대적으로 작은 데이터셋에 국한되어 있었으며, 단일 단어 분류에 초점을 맞춰 개방형 어휘(open-vocabulary)의 연속적인 시각적 음성 인식(Visual Speech Recognition, VSR) 문제를 해결하지 못했습니다. 이는 언어 장애 환자를 돕는 것과 같은 실제 응용 분야에서 VSR 시스템의 확장을 어렵게 만드는 주요 제약이었습니다.

## ✨ Key Contributions

- **최대 규모의 VSR 데이터셋 구축**: 3,886시간 분량의 비디오와 텍스트 쌍으로 구성된 현존하는 가장 큰 개방형 어휘 VSR 데이터셋인 LSVSR(Large-Scale Visual Speech Recognition)을 구축했습니다. 이는 이전 데이터셋보다 어휘(127,055개 단어) 및 예제 시퀀스 면에서 훨씬 큰 규모를 자랑합니다.
- **통합 독순술 시스템 개발**: 원시 비디오에서 단어 시퀀스를 출력하는 통합 시스템을 설계하고 훈련했습니다. 이 시스템은 비디오 처리 파이프라인, 확장 가능한 딥 신경망(V2P), 그리고 프로덕션 수준의 음성 디코더로 구성됩니다.
- **V2P(Vision to Phoneme) 신경망 아키텍처 도입**: 원시 비디오 프레임 시퀀스를 음소(phoneme) 분포 시퀀스로 매핑하는 새로운 신경망 아키텍처인 V2P를 제안했습니다. 이 모델은 시공간 컨볼루션과 양방향 LSTM을 사용하여 대규모 데이터셋에서도 효율적인 연산 및 메모리 사용을 위해 최적화되었습니다.
- **음소 기반 CTC 손실 및 FST 디코더 활용**: 음소 예측과 단어 디코딩을 분리함으로써, 신경망을 재훈련하지 않고도 어휘를 자유롭게 확장할 수 있는 유연한 설계를 도입했습니다. 이는 Connectionist Temporal Classification (CTC) 손실 함수를 사용하여 음소를 예측하고, 유한 상태 변환기(FST) 기반의 언어 모델(LM)을 통해 단어 시퀀스를 생성하는 방식입니다.
- **최고 수준의 WER 달성**: LSVSR 데이터셋의 held-out 세트에서 40.9%의 단어 오류율(WER)을 달성하여, 전문 독순가(86.4% 또는 92.9% WER) 및 이전 최고 성능 모델(LipNet 89.8%, Watch, Attend, and Spell (WAS) 76.8% WER) 대비 압도적인 성능 향상을 보여주었습니다. 또한, 훈련되지 않은 LRS3-TED 데이터셋에서도 이전 최고 성능을 능가하며 뛰어난 일반화 능력을 입증했습니다.

## 📎 Related Works

- **초기 독순술 연구**: 대부분 단일 단어 분류에 초점을 맞췄으며, 상당한 사전 지식과 Hidden Markov Model (HMM)을 활용했습니다 (Goldschen et al., 1997; Potamianos et al., 1997; Chu & Huang, 2000; Neti et al., 2000).
- **딥러닝 기반 단어 분류**: 시각 전용 또는 멀티모달 오디오-시각 표현 학습을 통해 단어 분류 성능을 향상시켰습니다 (Hinton et al., 2012; Ngiam et al., 2011; Chung & Zisserman, 2016a; Stafylakis & Tzimiropoulos, 2017).
- **문장 수준 독순술**:
  - **LipNet (Assael et al., 2017)**: 최초의 종단 간(end-to-end) 문장 수준 독순 모델로, 시공간 컨볼루션과 GRU를 결합하고 CTC 손실로 훈련되어 문자 시퀀스를 예측했습니다. GRID corpus에서 4.8%-11.4% WER을 달성했습니다.
  - **Watch, Attend, and Spell (WAS) (Chung et al., 2017)**: 주의 메커니즘을 포함한 시퀀스-투-시퀀스 모델을 사용하여 실제 데이터셋(LRS)에서 오디오-시각 음성 인식을 다루었으며, 50.2% WER을 기록했습니다. 사전 학습된 특징을 사용하여 메모리 요구 사항을 해결했습니다.
  - **LRS3-TED 데이터셋 및 Transformer 모델 (Afouras et al., 2018a, 2018c)**: 본 연구와 동시 진행된 연구로, LRS3-TED 데이터셋과 문자 기반 Transformer 모델을 사용하여 57.9%-61.8% WER을 달성했습니다.
- **본 연구와의 차별점**: LipNet과 달리, 본 연구의 V2P 모델은 음소 시퀀스를 예측하고 디코더를 통해 단어 시퀀스를 생성하는 유연한 설계를 채택하여 대규모 어휘를 쉽게 수용합니다. 또한, 이전 작업과 달리 사전 학습된 특징 없이도 메모리 및 연산 효율성을 달성합니다.

## 🛠️ Methodology

본 논문은 원시 비디오를 단어 시퀀스로 변환하는 포괄적인 시각적 음성 인식 시스템을 제안합니다.

1. **데이터 처리 파이프라인 (LSVSR 데이터셋 생성)**
   - **데이터 소스**: 공개 YouTube 비디오에서 140,000시간의 오디오 세그먼트를 추출한 후, 약 2%의 클립만이 필터링 기준을 만족하여 최종적으로 3,886시간 분량의 정렬된 음소 및 입술 프레임 시퀀스 데이터셋을 생성합니다.
   - **주요 구성 요소**:
     - **길이 및 언어 필터**: 1~12초 길이의 클립을 유지하고, 영어 화자를 분류하여 비영어 발언을 제거합니다.
     - **샷 경계 감지 및 얼굴 감지**: 비디오에서 샷 경계를 제거하고, FaceNet을 사용하여 얼굴을 감지 및 추적합니다.
     - **클립 품질 필터**: 흐릿한 클립, 눈-눈 너비가 80픽셀 미만인 얼굴, 23fps 미만의 프레임 레이트 클립을 제거합니다.
     - **얼굴 랜드마크 스무딩**: 얼굴 랜드마크 추적 결과를 시간적 가우시안 커널로 스무딩하고, 얼굴의 요(yaw) 및 피치(pitch) 각도가 $\pm 30^\circ$ 이내인 세그먼트만 유지합니다.
     - **뷰 정규화 (View Canonicalization)**: 랜드마크에 아핀 변환을 적용하여 정규화된 얼굴을 얻고, 입술 주변 영역을 크롭합니다.
     - **발화 필터 (Speaking Filter)**: 입술 움직임이 적거나 발화하지 않는 얼굴을 제외합니다.
     - **발화 분류기 (Speaking Classifier - V2P-Sync)**: 오디오 및 비디오 채널 정렬을 검증하는 신경망으로, `logmel-spectrogram`과 9개의 그레이스케일 비디오 프레임을 입력으로 받아 오디오와 비디오 임베딩 간의 유클리드 거리를 기반으로 동기화 여부를 판단합니다. 대조 손실(contrastive loss)로 훈련됩니다.
2. **V2P(Vision to Phoneme) 모델 아키텍처**
   - **3D 컨볼루션 모듈**: 입력 비디오 클립(T×128×128×3)에서 시공간 특징을 추출합니다.
     - 5개의 컨볼루션 레이어([64, 128, 256, 512, 512] 필터)로 구성되며, VGG 아키텍처를 볼륨형(volumetric)으로 변형했습니다.
     - GPU 메모리 제약을 고려하여 첫 두 레이어의 필터 수를 제한하고, 입술 중심이므로 공간 패딩을 생략했습니다. 시간 차원에서는 패딩을 유지하고 단위 스트라이드(unit stride)로 컨볼루션합니다.
     - 배치 정규화(Batch Normalization) 대신 그룹 정규화(Group Normalization)를 사용하여 작은 배치 크기에서도 안정적인 학습을 보장합니다.
   - **시간 모듈**: 추출된 특징을 시간적으로 집계하여 음소 분포 시퀀스를 출력합니다.
     - 3개의 양방향 LSTM(hidden state 768) 스택과 그룹 정규화를 교대로 사용합니다.
     - 최종 MLP 레이어를 통해 T개의 조건부 독립적인 음소 분포 $p(u_t|x)$를 생성합니다.
   - **손실 함수**: CTC(Connectionist Temporal Classification) 손실 함수를 사용하여 모델을 훈련합니다. CTC는 입력 시퀀스와 레이블 토큰 간의 정렬이 필요 없이 레이블 토큰 시퀀스에 대한 분포를 파라미터화합니다.
   - **음소 및 CTC 선택 이유**:
     - 문자 대신 음소를 사용함으로써 동음이의어(homophone) 문제(예: "fare"와 "fair")로 인해 CTC가 불필요한 모드(spurious modes)를 생성하는 것을 방지합니다. 음소는 단어에 대한 일대다 매핑을 가지므로, 시간 모델은 소리 불확실성만 모델링하고, 단어 불확실성은 디코더가 처리할 수 있습니다.
     - 어휘와 언어 모델을 디코더에서 분리함으로써, 음소 인식 모델을 재훈련하지 않고도 어휘나 언어 모델을 쉽게 변경할 수 있는 유연성을 제공합니다.
3. **디코딩**
   - **유한 상태 변환기 (FST)**: 모델이 생성한 음소 분포 시퀀스를 단어 시퀀스로 변환하기 위해 업계 표준인 FST 기반 디코딩 방법을 사용합니다.
   - **WFST 구성**:
     - **CTC 후처리 FST**: 중복 문자 및 CTC 블랭크 기호를 제거합니다.
     - **어휘 FST (Lexicon FST)**: 입력 음소를 출력 단어에 매핑합니다.
     - **N-그램 언어 모델 (LM) FST**: 5천만 개의 n-그램과 백만 개의 어휘를 가진 5-그램 언어 모델을 WFST로 표현합니다.
   - 이 세 가지 FST의 조합은 음소 시퀀스를 단어 시퀀스로 변환하는 WFST를 생성하며, 빔 탐색(beam search) 절차를 통해 가장 가능성 있는 단어 시퀀스를 찾습니다.

## 📊 Results

- **LSVSR 데이터셋 성능**:
  - **V2P**: WER 40.9% (음소 오류율(PER): 33.6%, 문자 오류율(CER): 28.3%).
  - **전문 독순가**: 문맥 없는 경우 92.9% WER, 문맥 있는 경우 86.4% WER.
  - **Baseline-LipNet-Ph**: 89.8% WER.
  - **Baseline-Seq2seq-Ch (WAS 변형)**: 76.8% WER.
  - **V2P-FullyConv (LSTM 대신 컨볼루션 사용)**: 51.6% WER.
  - **V2P-NoLM (언어 모델 제거)**: 53.6% WER (언어 모델이 WER을 약 13%p 감소시킴).
- **LRS3-TED 데이터셋 일반화 성능**:
  - V2P는 훈련되지 않은 LRS3-TED 데이터셋에서도 강력한 일반화 능력을 보여주었습니다.
  - LSVSR과 동일한 프로토콜로 필터링된 테스트 세트에서 47.0% WER을 달성했습니다.
  - 필터링되지 않은 전체 테스트 세트에서 55.1% WER을 달성하여, 해당 데이터셋에서 훈련된 TM-seq2seq 모델(57.9% WER)보다 우수한 성능을 보였습니다.
- **오류 분석**: 가장 흔한 오류는 치아에 의해 가려지는 음소(/d/, /n/, /t/)와 가장 흔한 영어 모음(/ə/)에서 발생했습니다. 살리언시 맵(saliency map)은 모델이 입술 주변 영역에 주로 집중함을 보여줍니다.

## 🧠 Insights & Discussion

- **독순술 성능의 비약적 발전**: 본 연구의 V2P 시스템은 이전 최고 성능 방법론들의 오류율을 거의 절반으로 줄여 독순술 분야에서 상당한 발전을 이루었습니다. 이는 대규모 데이터셋과 효율적인 모델 아키텍처의 결합이 가져온 결과입니다.
- **유연한 아키텍처 설계의 중요성**: 음소 예측과 단어 디코딩을 분리한 설계는 신경망을 재훈련할 필요 없이 어휘와 언어 모델을 변경할 수 있게 하여, 특정 도메인에 대한 적용이나 새로운 단어 추가에 매우 유용합니다.
- **의료 분야에의 잠재적 영향**: 본 시스템은 무성증(aphonia) 또는 발성 장애(dysphonia)를 겪는 환자들에게 대체 통신 수단을 제공하여 삶의 질을 향상시킬 수 있는 엄청난 잠재력을 가지고 있습니다. 특히, 발화가 어렵거나 불가능한 상황에서 전화 통신 능력 향상 등 실질적인 도움을 줄 수 있습니다.
- **모델의 한계**: V2P 모델은 특정 촬영 조건(예: 얼굴 요 및 피치 각도 $\pm 30^\circ$ 이내, 특정 거리, 고품질 비디오)에서 잘 작동하도록 설계 및 훈련되었으므로, 이러한 범위를 벗어나는 컨텍스트에서는 성능이 저하될 수 있습니다. 이는 실제 환경에서 시스템을 배치할 때 고려해야 할 중요한 제약 사항입니다.
- **추가 연구 방향**: 보다 강건한 VSR 시스템을 위해 다양한 촬영 조건과 얼굴 각도에 대한 모델의 일반화 능력을 향상시키는 연구가 필요합니다.

## 📌 TL;DR

본 논문은 개방형 어휘 시각적 음성 인식(VSR)을 위한 확장 가능한 솔루션을 제시합니다. 이를 위해 3,886시간 분량의 비디오로 구성된 현존하는 가장 큰 VSR 데이터셋인 LSVSR을 구축하고, 비디오 처리 파이프라인, 음소 분포를 예측하는 새로운 딥 신경망(V2P), 그리고 유한 상태 변환기(FST) 기반의 단어 디코더를 통합한 시스템을 개발했습니다. V2P는 시공간 컨볼루션과 양방향 LSTM을 사용하여 효율적으로 음소를 예측하며, 음소와 단어 디코딩을 분리하여 어휘 확장의 유연성을 확보했습니다. 이 시스템은 LSVSR 데이터셋에서 40.9%의 단어 오류율(WER)을 달성하여 전문 독순가 및 이전 최고 성능 모델들을 훨씬 능가하는 혁신적인 성능을 보여주었으며, 언어 장애 환자를 돕는 데 큰 잠재력을 가집니다.
