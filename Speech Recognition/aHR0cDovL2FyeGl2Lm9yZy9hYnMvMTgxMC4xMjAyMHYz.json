{
  "title": "An improved hybrid CTC-Attention model for speech recognition",
  "authors": "Zhe Yuan, Zhuoran Lyu, Jiwei Li, Xi Zhou",
  "year": 2018,
  "url": "http://arxiv.org/abs/1810.12020v3",
  "abstract": "Recently, end-to-end speech recognition with a hybrid model consisting of the\nconnectionist temporal classification(CTC) and the attention encoder-decoder\nachieved state-of-the-art results. In this paper, we propose a novel CTC\ndecoder structure based on the experiments we conducted and explore the\nrelation between decoding performance and the depth of encoder. We also apply\nattention smoothing mechanism to acquire more context information for\nsubword-based decoding. Taken together, these strategies allow us to achieve a\nword error rate(WER) of 4.43% without LM and 3.34% with RNN-LM on the\ntest-clean subset of the LibriSpeech corpora, which by far are the best\nreported WERs for end-to-end ASR systems on this dataset."
}