{
  "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment\n  of Continuation Writing",
  "authors": "Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, Jiajun Zhang",
  "year": 2023,
  "url": "http://arxiv.org/abs/2309.00916v2",
  "abstract": "The emergence of large language models (LLMs) has sparked significant\ninterest in extending their remarkable language capabilities to speech.\nHowever, modality alignment between speech and text still remains an open\nproblem. Current solutions can be categorized into two strategies. One is a\ncascaded approach where outputs (tokens or states) of a separately trained\nspeech recognition system are used as inputs for LLMs, which limits their\npotential in modeling alignment between speech and text. The other is an\nend-to-end approach that relies on speech instruction data, which is very\ndifficult to collect in large quantities. In this paper, we address these\nissues and propose the BLSP approach that Bootstraps Language-Speech\nPre-training via behavior alignment of continuation writing. We achieve this by\nlearning a lightweight modality adapter between a frozen speech encoder and an\nLLM, ensuring that the LLM exhibits the same generation behavior regardless of\nthe modality of input: a speech segment or its transcript. The training process\ncan be divided into two steps. The first step prompts an LLM to generate texts\nwith speech transcripts as prefixes, obtaining text continuations. In the\nsecond step, these continuations are used as supervised signals to train the\nmodality adapter in an end-to-end manner. We demonstrate that this\nstraightforward process can extend the capabilities of LLMs to speech, enabling\nspeech recognition, speech translation, spoken language understanding, and\nspeech conversation, even in zero-shot cross-lingual scenarios.",
  "citation": 60
}