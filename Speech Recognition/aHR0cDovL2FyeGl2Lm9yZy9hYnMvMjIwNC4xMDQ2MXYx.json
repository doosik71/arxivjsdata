{
  "title": "WaBERT: A Low-resource End-to-end Model for Spoken Language\n  Understanding and Speech-to-BERT Alignment",
  "authors": "Lin Yao, Jianfei Song, Ruizhuo Xu, Yingfang Yang, Zijian Chen, Yafeng Deng",
  "year": 2022,
  "url": "http://arxiv.org/abs/2204.10461v1",
  "abstract": "Historically lower-level tasks such as automatic speech recognition (ASR) and\nspeaker identification are the main focus in the speech field. Interest has\nbeen growing in higher-level spoken language understanding (SLU) tasks\nrecently, like sentiment analysis (SA). However, improving performances on SLU\ntasks remains a big challenge. Basically, there are two main methods for SLU\ntasks: (1) Two-stage method, which uses a speech model to transfer speech to\ntext, then uses a language model to get the results of downstream tasks; (2)\nOne-stage method, which just fine-tunes a pre-trained speech model to fit in\nthe downstream tasks. The first method loses emotional cues such as intonation,\nand causes recognition errors during ASR process, and the second one lacks\nnecessary language knowledge. In this paper, we propose the Wave BERT (WaBERT),\na novel end-to-end model combining the speech model and the language model for\nSLU tasks. WaBERT is based on the pre-trained speech and language model, hence\ntraining from scratch is not needed. We also set most parameters of WaBERT\nfrozen during training. By introducing WaBERT, audio-specific information and\nlanguage knowledge are integrated in the short-time and low-resource training\nprocess to improve results on the dev dataset of SLUE SA tasks by 1.15% of\nrecall score and 0.82% of F1 score. Additionally, we modify the serial\nContinuous Integrate-and-Fire (CIF) mechanism to achieve the monotonic\nalignment between the speech and text modalities.",
  "citation": 2
}