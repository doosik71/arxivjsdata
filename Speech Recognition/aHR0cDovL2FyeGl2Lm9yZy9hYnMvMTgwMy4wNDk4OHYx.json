{
  "title": "LCANet: End-to-End Lipreading with Cascaded Attention-CTC",
  "authors": "Kai Xu, Dawei Li, Nick Cassimatis, Xiaolong Wang",
  "year": 2018,
  "url": "http://arxiv.org/abs/1803.04988v1",
  "abstract": "Machine lipreading is a special type of automatic speech recognition (ASR)\nwhich transcribes human speech by visually interpreting the movement of related\nface regions including lips, face, and tongue. Recently, deep neural network\nbased lipreading methods show great potential and have exceeded the accuracy of\nexperienced human lipreaders in some benchmark datasets. However, lipreading is\nstill far from being solved, and existing methods tend to have high error rates\non the wild data. In this paper, we propose LCANet, an end-to-end deep neural\nnetwork based lipreading system. LCANet encodes input video frames using a\nstacked 3D convolutional neural network (CNN), highway network and\nbidirectional GRU network. The encoder effectively captures both short-term and\nlong-term spatio-temporal information. More importantly, LCANet incorporates a\ncascaded attention-CTC decoder to generate output texts. By cascading CTC with\nattention, it partially eliminates the defect of the conditional independence\nassumption of CTC within the hidden neural layers, and this yields notably\nperformance improvement as well as faster convergence. The experimental results\nshow the proposed system achieves a 1.3% CER and 3.0% WER on the GRID corpus\ndatabase, leading to a 12.3% improvement compared to the state-of-the-art\nmethods."
}