{
  "title": "Advanced Long-context End-to-end Speech Recognition Using\n  Context-expanded Transformers",
  "authors": "Takaaki Hori, Niko Moritz, Chiori Hori, Jonathan Le Roux",
  "year": 2021,
  "url": "http://arxiv.org/abs/2104.09426v1",
  "abstract": "This paper addresses end-to-end automatic speech recognition (ASR) for long\naudio recordings such as lecture and conversational speeches. Most end-to-end\nASR models are designed to recognize independent utterances, but contextual\ninformation (e.g., speaker or topic) over multiple utterances is known to be\nuseful for ASR. In our prior work, we proposed a context-expanded Transformer\nthat accepts multiple consecutive utterances at the same time and predicts an\noutput sequence for the last utterance, achieving 5-15% relative error\nreduction from utterance-based baselines in lecture and conversational ASR\nbenchmarks. Although the results have shown remarkable performance gain, there\nis still potential to further improve the model architecture and the decoding\nprocess. In this paper, we extend our prior work by (1) introducing the\nConformer architecture to further improve the accuracy, (2) accelerating the\ndecoding process with a novel activation recycling technique, and (3) enabling\nstreaming decoding with triggered attention. We demonstrate that the extended\nTransformer provides state-of-the-art end-to-end ASR performance, obtaining a\n17.3% character error rate for the HKUST dataset and 12.0%/6.3% word error\nrates for the Switchboard-300 Eval2000 CallHome/Switchboard test sets. The new\ndecoding method reduces decoding time by more than 50% and further enables\nstreaming ASR with limited accuracy degradation."
}