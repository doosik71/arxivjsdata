# End-to-End Whisper to Natural Speech Conversion using Modified Transformer Network
Abhishek Niranjan, Mukesh C. Sharma, Sai Bharath Chandra Gutha, M. Ali Basha Shaik

## 🧩 Problem to Solve
속삭이는 음성(whispered speech)과 같은 비정형 음성(atypical speech)은 시간적, 스펙트럼적, 운율적 패턴에서 높은 변동성을 보이며, 성대 진동이 없어 기본 주파수($F_0$)가 존재하지 않아 자동 음성 인식(ASR) 시스템에 큰 어려움을 초래합니다. 또한, 이러한 비정형 음성에 대한 충분한 학습 데이터가 부족한 것이 주요 문제입니다. 본 연구는 속삭이는 음성의 기계 판독성을 향상시키고, 궁극적으로 종단 간(end-to-end) ASR 시스템의 인식 정확도를 높이는 것을 목표로 합니다.

## ✨ Key Contributions
*   **향상된 트랜스포머 아키텍처 제안**: 속삭이는 음성을 자연 음성으로(또는 그 반대로) 변환하기 위한 종단 간 시퀀스-투-시퀀스(sequence-to-sequence) 접근 방식에 기반한 변형된 트랜스포머 네트워크를 제안했습니다.
*   **보조 디코더의 효과 입증**: 인코더의 특정 계층 이후에 삽입된 보조 디코더가 프레임 단위 음소 레이블을 식별하는 목적 함수와 함께 훈련될 때 음성 변환 품질을 향상시킴을 입증했습니다.
*   **다양한 데이터 활용**: 병렬 및 비병렬 데이터를 모두 사용하여 데이터 희소성 문제를 해결했습니다.
*   **특징 분석**: MFCC(Mel Frequency Cepstral Coefficients)와 스무딩된 스펙트럼 특징(smoothed spectral features) 등 다양한 음향 특징의 효과를 탐구했습니다.
*   **새로운 평가 지표 제안**: 생성된 음성의 스펙트럼 형태를 참조 음성과의 포먼트 분포 유사성을 측정하는 포먼트 발산 지표(Formant Divergence Metric, FDM)를 제안하고 사용했습니다. FDM은 쿨백-라이블러 발산(KL-Divergence)을 기반으로 계산됩니다.
*   **ASR 성능 향상**: 속삭이는 음성을 자연 음성으로 변환함으로써 ASR 시스템의 WER(Word Error Rate)을 약 65% 감소시키는 것을 보여주었습니다.
*   **포먼트 분포 유사성 확인**: 변환된 음성의 포먼트 확률 분포가 원본 자연 음성의 분포와 유사함을 입증했습니다.

## 📎 Related Works
*   **비정형 음성 인식**: [2, 3, 4, 5] 등은 비정형 음성 인식이 ASR의 도전적인 과제임을 언급합니다.
*   **속삭이는 음성 인식**: 스펙트럼 희소 기반 접근 방식 [10] 및 딥러닝(DNN)을 사용한 강건한 켑스트럼 특징 생성 [11] 연구가 있었습니다.
*   **음성 변환**:
    *   자연 음성을 속삭이는 음성으로 변환하여 제한된 속삭이는 음성 데이터셋을 증강하는 연구 [12, 13, 14]가 있었습니다.
    *   규칙 기반 변환 [15]은 자연스러움이 부족하며, GMM [20] 및 신경망 [21, 22]을 사용한 학습 모델도 탐구되었습니다.
    *   프레임 정렬 병렬 데이터로 훈련된 DBLSTM(Deep Bidirectional Long Short Term Memory) 네트워크 [23]가 더 자연스러운 결과를 생성했습니다.
    *   [15]는 LSTM 유닛을 사용한 시퀀스-투-시퀀스 프레임워크를 제안했습니다.
*   **트랜스포머 네트워크**: 본 연구는 [24]의 기존 트랜스포머 아키텍처를 기반으로 합니다.
*   **보조 디코더**: [25]에서는 오프라인 음성 번역 작업에서 완전한 발화에 대한 음소 시퀀스 예측을 위해 두 개의 LSTM 기반 보조 디코더를 통합했습니다. 본 연구는 단일 보조 디코더를 사용한다는 점에서 차이가 있습니다.

## 🛠️ Methodology
1.  **네트워크 아키텍처 (향상된 트랜스포머)**:
    *   기존 트랜스포머 아키텍처를 수정하여 임베딩 및 위치 인코딩 계층을 생략하고 보조 디코더를 추가했습니다.
    *   **인코더**: $N=6$개의 동일한 계층으로 구성되며, 각 계층은 멀티 헤드 셀프 어텐션(Multi-head self-attention)과 완전 연결 피드 포워드 네트워크로 이루어집니다.
    *   **디코더**: $N=6$개의 동일한 계층으로 구성되며, 각 계층은 마스크된 멀티 헤드 셀프 어텐션, 인코더-디코더 멀티 헤드 어텐션, 피드 포워드 계층으로 이루어집니다. 선형 계층이 최종 출력 계층으로 사용됩니다.
    *   **보조 디코더**: 인코더의 첫 $n$개 계층 이후에 입력됩니다. Kaldi (s5 recipe) ASR 시스템을 통해 생성된 원본 음성 신호의 강제 정렬된 삼중음소(triphone) 단위를 프레임 단위로 예측합니다. 교차 엔트로피 손실 함수와 스케일드 닷-프로덕트 어텐션(scaled dot-product attention) 메커니즘을 사용합니다.
2.  **데이터셋 및 특징 추출**:
    *   **훈련 데이터**: SpeechOcean (King-ASR-066 American English ASR Corpus)과 공개된 wTIMIT(Whispered TIMIT) 코퍼스를 결합한 SWPC-066 (Speech-Whisper parallel corpus-066)을 사용했습니다. SWPC-066은 100시간의 음성 및 100시간의 속삭이는 음성(병렬 데이터)을 포함합니다. Librispeech의 일부(30시간)를 사용하여 인코더와 보조 디코더 파라미터를 사전 학습시키는 데 비병렬 데이터를 활용했습니다.
    *   **테스트 데이터**: wTIMIT 테스트 코퍼스(70개 파일)와 CHAINS 코퍼스(1332개 파일)를 사용했습니다.
    *   **특징**: Librosa 툴킷을 사용하여 80차원 MFCC 특징을, WORLD Vocoder 툴킷을 사용하여 24차원 스무딩된 스펙트럼 특징을 추출했습니다. 프레임 길이는 25ms, 오버랩은 10ms입니다.
3.  **모델 훈련 변형**:
    *   **W1/V1 (베이스라인)**: MFCC/스펙트럼 특징을 사용하여 속삭이는 음성 -> 자연 음성 변환을 수행하며 보조 디코더는 사용하지 않습니다. 목적 함수는 RMSE(Root Mean Square Error)입니다:
        $$L_1 = \left( \sum_{i=1}^{k} \right) \left( \sqrt{\frac{1}{n} \sum_{j=1}^{n} (t_{ji} - y_{ji})^2} \right)$$
        여기서 $k$는 프레임 수, $n$은 출력 벡터 차원입니다.
    *   **W2/V2 (향상된 모델)**: 보조 디코더를 사용하여 속삭이는 음성 -> 자연 음성 변환을 수행합니다. RMSE 손실과 음소 분류를 위한 교차 엔트로피 손실($L_2$)을 결합한 총 손실 함수($L$)를 사용합니다:
        $$L_2 = \left( \sum_{i=1}^{k} \right) \left( -\sum_{j=1}^{P} (d_{ji} \log(p_{ji})) \right)$$
        $$L = L_1 + L_2$$
        여기서 $d_i$는 $P$차원의 원-핫 벡터로 프레임 $i$의 삼중음소를 나타내고, $p_i$는 보조 디코더가 생성한 소프트맥스 벡터입니다.
    *   **W3/V3**: 자연 음성 -> 속삭이는 음성 변환을 위한 모델입니다.
4.  **평가 지표**:
    *   **WER**: 생성된 자연 음성 및 원본 속삭이는 음성에 대해 RETURNN E2E ASR 시스템을 사용하여 측정합니다.
    *   **BLEU 점수**: 번역 품질 지표로 사용됩니다.
    *   **포먼트 발산 지표(FDM)**: 참조 음성과 생성된 음성의 포먼트(F1-F4) 사이의 KL-Divergence를 측정합니다. $k$-컴포넌트 GMM(Gaussian Mixture Model)을 사용합니다.

## 📊 Results
*   **ASR WER 결과**:
    *   원본 속삭이는 음성에 대한 ASR WER은 매우 높았지만 (예: wTIMIT에서 S3 시스템으로 104.76%), 속삭이는 음성에서 자연 음성으로 변환된 음성은 ASR 시스템에 의해 훨씬 더 잘 인식되었습니다.
    *   특히 보조 디코더를 사용한 모델(W2, V2)이 가장 낮은 WER을 달성했습니다. WTIMIT에서 W2 모델은 WER을 104.76%에서 13.33%로 크게 낮추는 효과를 보였습니다 (약 65% 감소).
    *   MFCC 특징이 스무딩된 스펙트럼 특징보다 더 나은 성능을 보였습니다.
*   **BLEU 점수**: W2 모델(MFCC, 보조 디코더)이 wTIMIT에서 85.36, CHAINS에서 62.68의 가장 높은 BLEU 점수를 달성했습니다. 이는 생성된 음성의 번역 품질이 우수함을 나타냅니다.
*   **포먼트 분석 (FDM)**:
    *   W2 모델의 포먼트 발산 지표(FDM)가 W1 모델보다 낮았습니다 (Table 5 참조). 이는 W2 모델이 생성한 음성의 포먼트 분포가 참조 자연 음성의 분포와 더 유사함을 의미합니다.
    *   포먼트 GMM 비교 그래프(Fig. 3)에서도 W2 모델의 GMM이 참조 GMM에 더 가깝게 나타났습니다.

## 🧠 Insights & Discussion
*   **변환의 중요성**: 속삭이는 음성을 자연 음성으로 변환하는 것은 ASR 시스템의 인식 정확도를 획기적으로 향상시킬 수 있는 효과적인 방법입니다.
*   **보조 디코더의 기여**: 보조 디코더는 프레임 수준의 음소 분류 목표를 통해 모델이 더 풍부한 은닉 표현을 학습하도록 돕고, 이는 변환된 음성의 품질(낮은 WER, 향상된 포먼트 매칭) 개선으로 이어집니다.
*   **데이터 활용 효율성**: 병렬 데이터와 비병렬 데이터를 모두 활용하는 접근 방식은 비정형 음성 인식에서 흔히 발생하는 데이터 부족 문제를 효과적으로 해결합니다.
*   **특징 선택**: MFCC가 본 작업에서 스무딩된 스펙트럼 특징보다 더 효과적인 음향 특징임을 확인했습니다.
*   **아키텍처 효율성**: 멀티 헤드 및 셀프 어텐션 원리를 사용하는 제안된 아키텍처는 기존 순차적(예: LSTM) 시스템에 비해 훈련 및 디코딩 지연 시간 측면에서 효율적입니다.
*   **새로운 평가 지표의 유용성**: FDM은 생성된 음성의 스펙트럼 형태 품질을 객관적으로 평가하는 데 유용한 새로운 지표로, 명시적으로 포먼트 최적화 없이도 모델이 잠재된 포먼트 분포를 학습하는 능력을 보여줍니다.
*   **한계**: 본 연구에서는 억양 및 운율 특징에 대한 분석은 다루지 않았습니다. CHAINS 코퍼스에서 ASR의 전체 정확도가 wTIMIT 데이터셋에 비해 높은 삽입 오류로 인해 감소하는 경향을 보였습니다.

## 📌 TL;DR
**문제**: 속삭이는 음성은 ASR 성능을 저해하며, 관련 데이터가 부족합니다.
**제안 방법**: 본 논문은 속삭이는 음성을 자연 음성으로 변환하기 위한 향상된 트랜스포머 네트워크를 제안합니다. 이 아키텍처는 인코더에 보조 디코더를 포함하며, 병렬 및 비병렬 데이터를 모두 활용하여 종단 간 학습을 수행합니다.
**주요 결과**: 제안된 변환 모델은 속삭이는 음성을 ASR이 훨씬 더 잘 인식할 수 있는 자연 음성으로 성공적으로 변환합니다 (WER 약 65% 감소). 특히, 보조 디코더를 사용한 모델(W2)이 가장 우수한 성능을 보였으며, 생성된 음성의 포먼트 분포가 원본 자연 음성과 매우 유사함을 새로운 포먼트 발산 지표(FDM)를 통해 입증했습니다.