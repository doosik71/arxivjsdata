{
  "title": "Alignment Knowledge Distillation for Online Streaming Attention-based\n  Speech Recognition",
  "authors": "Hirofumi Inaguma, Tatsuya Kawahara",
  "year": 2021,
  "url": "http://arxiv.org/abs/2103.00422v2",
  "abstract": "This article describes an efficient training method for online streaming\nattention-based encoder-decoder (AED) automatic speech recognition (ASR)\nsystems. AED models have achieved competitive performance in offline scenarios\nby jointly optimizing all components. They have recently been extended to an\nonline streaming framework via models such as monotonic chunkwise attention\n(MoChA). However, the elaborate attention calculation process is not robust for\nlong-form speech utterances. Moreover, the sequence-level training objective\nand time-restricted streaming encoder cause a nonnegligible delay in token\nemission during inference. To address these problems, we propose CTC\nsynchronous training (CTC-ST), in which CTC alignments are leveraged as a\nreference for token boundaries to enable a MoChA model to learn optimal\nmonotonic input-output alignments. We formulate a purely end-to-end training\nobjective to synchronize the boundaries of MoChA to those of CTC. The CTC model\nshares an encoder with the MoChA model to enhance the encoder representation.\nMoreover, the proposed method provides alignment information learned in the CTC\nbranch to the attention-based decoder. Therefore, CTC-ST can be regarded as\nself-distillation of alignment knowledge from CTC to MoChA. Experimental\nevaluations on a variety of benchmark datasets show that the proposed method\nsignificantly reduces recognition errors and emission latency simultaneously.\nThe robustness to long-form and noisy speech is also demonstrated. We compare\nCTC-ST with several methods that distill alignment knowledge from a hybrid ASR\nsystem and show that the CTC-ST can achieve a comparable tradeoff of accuracy\nand latency without relying on external alignment information. The best MoChA\nsystem shows recognition accuracy comparable to that of RNN-transducer (RNN-T)\nwhile achieving lower emission latency.",
  "citation": 27
}