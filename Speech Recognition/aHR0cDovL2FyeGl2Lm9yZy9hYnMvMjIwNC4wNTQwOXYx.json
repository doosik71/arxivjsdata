{
  "url": "http://arxiv.org/abs/2204.05409v1",
  "title": "Unified Speech-Text Pre-training for Speech Translation and Recognition",
  "authors": "Yun Tang, Hongyu Gong, Ning Dong, Changhan Wang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski, Xian Li, Abdelrahman Mohamed, Michael Auli, Juan Pino",
  "year": 2022,
  "abstract": "We describe a method to jointly pre-train speech and text in an\nencoder-decoder modeling framework for speech translation and recognition. The\nproposed method incorporates four self-supervised and supervised subtasks for\ncross modality learning. A self-supervised speech subtask leverages unlabelled\nspeech data, and a (self-)supervised text to text subtask makes use of abundant\ntext training data. Two auxiliary supervised speech tasks are included to unify\nspeech and text modeling space. Our contribution lies in integrating linguistic\ninformation from the text corpus into the speech pre-training. Detailed\nanalysis reveals learning interference among subtasks. Two pre-training\nconfigurations for speech translation and recognition, respectively, are\npresented to alleviate subtask interference. Our experiments show the proposed\nmethod can effectively fuse speech and text information into one model. It\nachieves between 1.7 and 2.3 BLEU improvement above the state of the art on the\nMuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the\nLibrispeech speech recognition task.",
  "citation": 102
}