# LCANet: End-to-End Lipreading with Cascaded Attention-CTC

Kai Xu, Dawei Li, Nick Cassimatis, Xiaolong Wang

## 🧩 Problem to Solve

자동 독순술(Machine lipreading)은 사람의 입 움직임을 시각적으로 해석하여 음성을 전사하는 작업입니다. 최근 딥러닝 기반 독순술 방법이 많은 발전을 이루었지만, 여전히 실제 환경 데이터(wild data)에서는 높은 오류율을 보입니다. 주요 문제는 다음과 같습니다:

- **미묘한 구별 문제:** "p"와 "b"와 같이 시각적으로 매우 유사한 비셈(viseme, 시각적 음소)들 간의 미묘한 차이를 구분하기 어렵습니다. 이는 인간 독순자도 약 20%의 정확도만 보이는 난제입니다.
- **CTC (Connectionist Temporal Classification)의 한계:** CTC는 레이블 간의 조건부 독립성을 가정하며, 개별 문자의 확률에 중점을 두어 단기 정보에만 집중하는 경향이 있습니다. 이는 장기 컨텍스트 정보를 통해 비셈을 구별해야 하는 독순술에는 적합하지 않을 수 있습니다.
- **어텐션(Attention) 기반 Seq2Seq 모델의 한계:** 조건부 독립성 가정을 완화하지만, 너무 유연하여 음성 인식에서 적절한 정렬을 예측하기 어렵고, 긴 입력 시퀀스에 대한 정렬 오류로 인해 처음부터 학습하기 어렵습니다.
- 효과적으로 시공간 정보를 포착하고, CTC 및 어텐션 모델의 단점을 보완하는 엔드-투-엔드 독순술 시스템이 필요합니다.

## ✨ Key Contributions

- 순수하게 시각 정보에만 의존하는 엔드-투-엔드 독순술 딥 뉴럴 네트워크 아키텍처인 **LCANet**을 제안합니다.
- 인코딩된 시공간 특징으로부터 텍스트를 생성하기 위해 **계층적 어텐션-CTC(cascaded attention-CTC) 디코더**를 도입했습니다. 이 설계는 기존 CTC 기반 방법의 조건부 독립성 가정 결함을 부분적으로 해결합니다.
- 제안된 시스템이 기존 최신 방법 대비 12.3% 향상된 성능(1.3% CER, 2.9% WER)과 더 빠른 수렴 속도를 달성함을 실험적으로 입증했습니다.

## 📎 Related Works

- **딥러닝 기반 음성 인식 (ASR):**
  - **CTC 기반 모델 [18, 19]:** 각 오디오 프레임에 대한 레이블을 예측하고 예측과 실제 레이블 간의 정렬을 최적화합니다.
  - **어텐션 기반 Seq2Seq 모델 [21, 32, 33]:** 정보 흐름을 개선하여 CTC 기반 모델을 능가하지만, 시끄러운 음성 데이터에 취약하고 긴 시퀀스에서 정렬 문제가 발생할 수 있습니다.
  - **하이브리드 CTC/어텐션 아키텍처 [43]:** CTC 손실과 어텐션 손실의 가중 합으로 공동 손실을 최소화하여 두 방법의 장점을 활용합니다. LCANet은 손실 함수의 가중치 매개변수 튜닝 없이 출력값을 직접 생성하는 계층적 방식을 사용합니다.
- **기계 독순술:**
  - **전통적인 방법:** 입 영역에서 시각 특징 추출 (DCT, DWT [44-47], 기하학적 특징, 모션 기반 특징 [48], 모델 기반 특징 [49-51]) 후 HMM과 같은 동적 분류기 [44, 52]를 사용했습니다. 그러나 조건부 독립성 가정으로 인해 장기 의존성 모델링에 한계가 있습니다.
  - **딥러닝 기반 방법:**
    - CNN을 이용한 시각 특징 추출 및 GMM [6].
    - **LipNet [3]:** 3D CNN과 GRU를 사용하여 엔드-투-엔드 문장 레벨 독순술을 수행하며 CTC 손실을 사용합니다. GRID 데이터셋에서 인간 독순자보다 뛰어난 성능을 보였습니다.
    - LSTM을 사용한 엔드-투-엔드 시각 음성 인식 [2].
    - 멀티모달 음성 인식을 위한 결합된 시각/오디오 특징 활용 [5, 53].
    - **LRSW [5], LRW [62]:** 각각 "Lip Reading Sentences" 및 "Lip Reading in the Wild" 데이터셋에서 최신 성능을 달성했습니다.

## 🛠️ Methodology

LCANet은 시공간 비디오 인코더와 계층적 어텐션-CTC 디코더로 구성된 엔드-투-엔드 딥 뉴럴 네트워크입니다 (그림 2).

1. **입력 전처리:**

   - 얼굴 정렬을 통해 입 영역($100 \times 50$ 해상도)을 잘라냅니다.
   - RGB 입 이미지에 대해 평균 감소 및 정규화를 수행합니다.
   - 과적합 방지를 위해 데이터 증강을 적용합니다.

2. **시공간 비디오 인코더 (Spatio-temporal Video Encoder):** 입력 비디오 프레임 시퀀스 $x = (x_1, ..., x_n)$를 연속적인 잠재 표현 $h = (h_1, ..., h_n)$ 시퀀스로 매핑합니다.

   - **3D-CNN:** 입력 비디오 프레임을 처리하여 시각적 정보와 단기 시간 정보를 인코딩합니다. 2D-CNN과 달리 시간 단계에 걸쳐 합성곱을 수행하여 짧은 시간 컨텍스트 (예: 3~5프레임)를 통해 개별 또는 인접한 비셈 수준의 특징을 포착합니다.
   - **하이웨이 네트워크 (Highway Network):** 3D-CNN 위에 두 층의 하이웨이 네트워크를 쌓습니다. 변환 게이트 $t$와 전달 게이트 $(1-t)$를 사용하여 일부 입력 정보를 직접 출력으로 전달하여, 깊은 네트워크에서 그라디언트 흐름을 안정화하고 더 풍부한 의미론적 특징을 인코딩합니다.
     $$ t = \sigma(W_T x + b_T) $$
        $$ g = t \odot \sigma(W_H x + b_H) + (1-t) \odot x $$
        여기서 $x$는 입력, $g$는 네트워크 출력, $\odot$는 원소별 곱셈입니다.
   - **양방향 GRU (Bi-GRU):** 장기 시간 정보를 인코딩합니다. GRU는 재설정 게이트 $r_t$와 업데이트 게이트 $z_t$를 사용하여 이전 기억 상태와 현재 입력 간의 혼합을 제어합니다. Bi-GRU는 순방향 및 역방향 정보 흐름을 모두 포착하여 시각적으로 유사한 비셈을 구별하는 데 필요한 장기 컨텍스트를 활용합니다.

3. **계층적 어텐션-CTC 디코더 (Cascaded Attention-CTC Decoder):** 인코딩된 시공간 특징을 기반으로 문자 기호 출력 시퀀스 $y = (y_1, ..., y_m)$를 생성합니다.
   - **어텐션 메커니즘:** 디코더의 각 출력 시퀀스 요소 $s_t$와 인코더의 은닉 상태 $h_j$ 사이의 정렬을 찾습니다.
     - 어텐션 점수: $e_{j,t} = V_a \tanh(W_a s_{t-1} + U_a h_j)$
     - 정규화된 어텐션 가중치: $\alpha_{j,t} = \frac{\exp(e_j)}{\sum_{k=1}^T \exp(e_k)}$
     - 컨텍스트 벡터: $c_t = \sum_{k=1}^T \alpha_{k,t} h_k$
     - 예측 결과: $y_t = \sigma(W_o E y_{t-1} + U_o h_{t-1} + C_o c_t)$ (여기서 $E$는 문자 수준 임베딩 행렬).
   - **CTC 손실:** 어텐션 모듈의 출력 예측은 CTC 손실 함수($L_{CTC} = -\ln p(l|x)$)에 입력되어 모델 훈련 과정을 안내합니다.
   - **계층적 설계의 이점:** 어텐션 메커니즘은 CTC의 조건부 독립성 가정을 완화하여 장기적인 시간적 상관관계를 갖는 출력 시퀀스를 생성합니다. 동시에 CTC는 단조로운 정렬을 강제하여 어텐션 모듈이 입력 비디오 프레임의 중요한 순차 부분에 집중하도록 유도함으로써, 훈련 중 불규칙한 정렬을 크게 줄이고 더 빠른 수렴을 가능하게 합니다.

## 📊 Results

- **데이터셋:** GRID 코퍼스 데이터셋에서 평가되었습니다.
- **평가 지표:** 문자 오류율(CER, Character Error Rate), 단어 오류율(WER, Word Error Rate), BLEU 점수.
- **성능:**
  - LCANet은 **1.3% CER**과 **2.9% WER**을 달성했습니다.
  - 이는 최신 기술 대비 **12.3%의 성능 향상**을 의미합니다.
  - LipNet [3] 대비 CER 31.6%, WER 37.5% 개선을 보였습니다 (LipNet: CER 1.9%, WER 4.8%).
  - 더 큰 데이터셋으로 사전 훈련된 LRSW [5] (WER 3.0%)보다 뛰어난 성능을 보였습니다.
- **모듈별 효과 (Ablation Study):**
  - LCANet (AH-CTC)이 멀티태스크 AH-CTC-CE (CER 2.1%, WER 4.9%), H-CTC (CER 1.8%, WER 4.3%), A-CTC (CER 1.7%, WER 4.1%) 모델보다 우수했습니다.
  - 하이웨이 네트워크는 A-CTC 모델(하이웨이 없음) 대비 CER 0.4%, WER 1.1% 개선을 가져왔습니다.
  - 어텐션 메커니즘을 제거했을 때(H-CTC) 성능 저하가 컸습니다.
- **수렴 속도:** LCANet은 훈련 손실 4에 도달하는 데 **14 epoch** (검증 손실 4에는 20 epoch)가 소요된 반면, LipNet은 훈련 손실 4에 도달하는 데 38 epoch (검증 손실 4에는 40 epoch)가 소요되어, LCANet의 수렴 속도가 훨씬 빠름을 입증했습니다.
- **혼동 행렬:** 비셈 클래스에 대한 혼동 행렬 분석 결과, LCANet이 시각적으로 유사한 비셈들 간의 효과적인 구별 능력을 보여주며, 독순술 작업에 효과적임을 나타냅니다.

## 🧠 Insights & Discussion

- **계층적 어텐션-CTC 디코더의 효과:** 어텐션 메커니즘은 CTC의 조건부 독립성 가정을 완화하여 장기 컨텍스트 정보를 명시적으로 포착합니다. 동시에 CTC는 단조로운 정렬을 강제하여 어텐션 모듈이 입력 비디오의 핵심 순차 부분에 집중하도록 안내함으로써, 훈련 중 불필요한 정렬 오류를 줄이고 모델링 능력을 향상시킵니다. 이러한 시너지는 더 나은 성능과 빠른 수렴으로 이어집니다.
- **하이웨이 네트워크의 이점:** 3D-CNN 위에 하이웨이 네트워크를 쌓음으로써, CNN 개별 필터가 감지한 국부 특징을 적응적으로 결합하여 인코더의 모델링 능력을 강화합니다. 이는 그라디언트가 여러 층을 감쇠 없이 흐르도록 하여 ResNet과 유사한 효과를 제공합니다.
- **멀티태스크 학습과의 비교:** 제안된 계층적 아키텍처는 CTC와 어텐션 손실을 공동으로 최적화하는 멀티태스크 모델보다 효과적입니다. 이는 계층적 방식이 단일 CTC 손실로 인코더와 디코더를 모두 업데이트하며, 모듈 간의 조율 문제를 피할 수 있기 때문입니다.
- **한계점 및 향후 연구:** 현재 LCANet은 어텐션 메커니즘으로 CTC의 조건부 독립성 가정을 처리하지만, 손실 함수 자체를 직접 수정하지는 않습니다. 향후 연구에서는 손실 함수에 직접 적용될 수 있는 해결책을 탐색할 계획입니다. 또한, [5]와 같이 오디오 및 시각 입력을 결합하여 추가적인 성능 향상을 탐색할 예정입니다.

## 📌 TL;DR

LCANet은 시각 정보만을 사용하여 엔드-투-엔드 독순술(lipreading)을 수행하는 딥 뉴럴 네트워크입니다. 기존 CTC 모델의 조건부 독립성 가정과 어텐션 모델의 정렬 문제를 해결하기 위해, 3D-CNN, Highway Network, Bi-GRU로 구성된 인코더와 **계층적 어텐션-CTC(cascaded attention-CTC) 디코더**를 제안합니다. 이 디코더는 어텐션 메커니즘으로 장기 컨텍스트 정보를 활용하면서 CTC를 통해 단조로운 정렬을 유도하여 모델 성능을 크게 향상시키고 수렴 속도를 가속화합니다. GRID 데이터셋에서 1.3%의 CER과 2.9%의 WER을 달성하며 최신 기술 대비 12.3% 향상된 성능과 빠른 수렴을 입증했습니다.
