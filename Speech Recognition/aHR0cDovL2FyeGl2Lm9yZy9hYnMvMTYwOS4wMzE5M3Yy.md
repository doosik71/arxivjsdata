# Wav2Letter: an End-to-End ConvNet-based Speech Recognition System

Ronan Collobert, Christian Puhrsch, Gabriel Synnaeve

## 🧩 Problem to Solve

기존의 음성 인식 시스템은 일반적으로 HMM/GMM 기반의 음향 모델과 음소(phoneme) 수준의 강제 정렬(force alignment)이 필요했으며, 이는 복잡하고 중간 단계의 수동 또는 자동 음성학적 전사(transcription)를 요구했습니다. 또한, CTC와 같은 시퀀스 기준(sequence criterion)을 사용하는 최신 엔드투엔드 RNN 기반 모델은 높은 계산 비용과 긴 학습 시간을 필요로 했습니다. 이 연구는 이러한 복잡성과 계산 부담을 줄이면서도 경쟁력 있는 성능을 달성할 수 있는 간단하고 효율적인 엔드투엔드 음성 인식 시스템을 개발하는 것을 목표로 합니다.

## ✨ Key Contributions

- **간단한 ConvNet 기반 엔드투엔드 시스템 제시**: 음성 신호(MFCC, 파워 스펙트럼, 원시 파형)에서 전사(글자)로 바로 가는 ConvNet 기반 음향 모델과 그래프 디코딩을 결합한 시스템을 제안합니다.
- **새로운 자동 분할 기준(Auto Segmentation Criterion, ASG) 도입**: 기존 CTC [6]와 유사한 성능을 보이면서도 더 간단한 형태의 ASG를 제안합니다. ASG는 "blank" 레이블이 없고, 비정규화된 점수를 사용하며, 전역 정규화를 적용합니다.
- **강제 정렬 및 음성학적 전사 불필요**: 음소 수준의 강제 정렬이나 중간 음성학적 전사 없이 글자(graphemes)를 직접 출력하도록 모델을 훈련시킵니다.
- **경쟁력 있는 결과 달성**: Librispeech 코퍼스 [18]에서 MFCC 특징을 사용하여 7.2%의 단어 오류율(WER)을 달성하며 경쟁력 있는 성능을 보였고, 원시 파형(raw waveform)에서도 유망한 결과를 얻었습니다.
- **효율성 입증**: RNN 기반 접근 방식 [1]에 비해 훨씬 적은 파라미터(최고 모델 2,300만 개 vs. 1억 개)를 사용하며, 계산 비용이 적어 학습 시간이 더 짧습니다.

## 📎 Related Works

- **고전적인 HMM/GMM 기반 시스템**: 음성 인식 시스템 구축의 전통적인 파이프라인으로, 강제 정렬을 통해 컨텍스트 의존적인 음소 상태를 맞추는 방식 [27]을 사용했습니다.
- **DNN 및 CNN 도입**: HMM/GMM 파이프라인에 딥 뉴럴 네트워크(DNN) [14, 10]와 컨볼루션 뉴럴 네트워크(CNN) [24, 25]를 음향 모델링에 적용하여 성능을 개선했습니다.
- **HMM/GMM-프리 훈련**: 일부 연구 [23]에서는 GMM-프리 훈련을 제안했지만, 여전히 강제 정렬이 필요했습니다.
- **RNN 기반 엔드투엔드 시스템**: HMM/GMM 파이프라인 및 강제 정렬에서 벗어나 RNN [7]을 사용하여 음소 전사를 수행하거나, CTC [6]와 같은 시퀀스 기준을 통해 음성 신호에서 글자로 직접 훈련하는 경쟁력 있는 엔드투엔드 접근 방식 [8, 13, 21, 1]이 제안되었습니다 (예: Baidu의 Deep Speech [1]).
- **원시 파형 입력**: 원시 음성 파형을 입력으로 사용하는 ConvNet 기반 시스템에 대한 연구 [15, 16]가 있었습니다.

## 🛠️ Methodology

Wav2Letter 시스템은 ConvNet 기반 음향 모델, 자동 분할 기준(ASG), 그리고 빔 서치(beam-search) 디코더로 구성됩니다.

1. **입력 특징 (Features)**:

   - **MFCC (Mel-Frequency Cepstral Coefficients)**: 전통적인 음성 특정 특징.
   - **파워 스펙트럼 (Power-spectrum)**: 최근 딥러닝 음향 모델링에서 주로 사용.
   - **원시 파형 (Raw wave)**: 실험적으로 탐색된 특징.
   - ConvNet은 이 모든 입력 특징 유형에 유연하게 적용 가능합니다.

2. **ConvNet 음향 모델 (ConvNet Acoustic Model)**:

   - 표준 1D 컨볼루션 뉴럴 네트워크를 기반으로 합니다.
   - 풀링 계층(pooling layers) 대신 스트라이딩 컨볼루션(striding convolutions)을 활용하여 더 넓은 컨텍스트를 학습하고 파라미터 수를 줄입니다.
   - 컨볼루션 연산과 비선형 활성화 함수(HardTanh, ReLU, 하이퍼볼릭 탄젠트)를 교차로 사용합니다.
   - 네트워크의 마지막 계층은 글자 사전($|L|$)의 각 글자에 대한 점수를 출력합니다.
   - 원시 파형을 위한 아키텍처는 [16]에서 영감을 받았으며, Figure 1에 자세히 설명되어 있습니다. MFCC 및 파워 스펙트럼 기반 네트워크는 첫 번째 계층을 포함하지 않습니다.

3. **자동 분할 기준 (AutoSegCriterion, ASG)**:

   - CTC [6]의 대안으로 제안된 시퀀스 기준입니다.
   - **주요 차이점**:
     - **"Blank" 레이블 없음**: CTC와 달리 "blank" 상태를 사용하지 않아 그래프 구조가 더 간단합니다 (Figure 3a, 3b). 글자 반복(예: "caterpillar"의 "ll")은 특수 반복 문자(예: "2" for repetition of the previous letter)로 모델링됩니다.
     - **비정규화된 노드/엣지 점수**: 네트워크 출력(노드 점수 $f_t(\cdot)$)과 전이 점수($g_{i,j}(\cdot)$)를 비정규화된 상태로 사용할 수 있어 외부 언어 모델을 쉽게 결합할 수 있습니다. 이는 "레이블 편향" [3, 11] 문제를 완화하는 데 중요합니다.
     - **전역 정규화**: 비정규화된 점수를 사용할 때 잘못된 전사가 낮은 신뢰도를 가지도록 전역 정규화($logadd_{\pi \in G_{full}(\theta,T)}$)를 적용합니다.
   - ASG는 다음 목표 함수를 최소화합니다:
     $$
     ASG(\theta,T) = -\logadd_{\pi \in G_{asg}(\theta,T)} \sum_{t=1}^{T} (f_{\pi_t}(x) + g_{\pi_{t-1},\pi_t}(x)) + \logadd_{\pi \in G_{full}(\theta,T)} \sum_{t=1}^{T} (f_{\pi_t}(x) + g_{\pi_{t-1},\pi_t}(x))
     $$
     여기서 첫 번째 항은 올바른 전사로 이어지는 시퀀스를 장려하고, 두 번째 항은 모든 가능한 시퀀스를 억제하여 전역 정규화 역할을 합니다. 두 부분 모두 Forward 알고리즘으로 효율적으로 계산됩니다.

4. **빔 서치 디코더 (Beam-Search Decoder)**:
   - 원패스(one-pass) 디코더로, 빔 임계값(beam thresholding), 히스토그램 가지치기(histogram pruning), 언어 모델 스미어링(smearing) [26]을 사용하여 간단한 빔 서치를 수행합니다.
   - 언어 모델링을 위해 KenLM [9]을 사용합니다.
   - 비정규화된 음향 점수(전이 및 방출)를 입력으로 받습니다.
   - 다음 값을 최대화하여 전사 $\theta$를 찾습니다:
     $$
     L(\theta) = \logadd_{\pi \in G_{asg}(\theta,T)} \sum_{t=1}^{T} (f_{\pi_t}(x) + g_{\pi_{t-1},\pi_t}(x)) + \alpha \log P_{lm}(\theta) + \beta |\theta|
     $$
     여기서 $P_{lm}(\theta)$는 전사 $\theta$에 대한 언어 모델 확률이고, $\alpha$와 $\beta$는 언어 모델 가중치와 단어 삽입 페널티를 제어하는 하이퍼파라미터입니다.

## 📊 Results

- **ASG vs. CTC**:

  - LER (Letter Error Rate) 측면에서는 ASG와 CTC가 유사한 성능을 보였습니다 (예: `dev-clean` 10.4% vs 10.7%).
  - 처리 속도 측면에서, ASG는 긴 시퀀스(입력 프레임 700)에서 CTC의 CPU 구현 및 GPU 구현보다 빠름을 보였습니다 (CPU 16.0-19.2ms vs. CTC GPU 97.9-100.3ms). ASG는 CPU에서만 실행되었음에도 불구하고 긴 시퀀스에서 더 효율적이었습니다.

- **훈련 데이터 크기 및 데이터 증강 효과**:

  - 데이터 증강(shift, stretching)은 작은 훈련 세트(10h)에서 LER 성능 향상에 도움을 주었으나, 훈련 데이터가 충분할수록(1000h) 그 효과가 줄어들었습니다 (Figure 4a).
  - MFCC와 파워 스펙트럼 특징은 충분한 훈련 데이터가 주어지면 비슷한 성능을 보였습니다.
  - WER 측면에서, Wav2Letter는 960시간의 Librispeech 데이터로 훈련되었음에도 불구하고, 훨씬 더 많은 데이터를 사용해 훈련된 Deep Speech 1 & 2 [8, 1]에 비해 경쟁력 있는 결과를 보였습니다 (Figure 4b).

- **LibriSpeech에서의 최고 성능**:
  - 1000시간의 음성 데이터를 훈련하여 얻은 결과입니다.
  - **MFCC**: LER 6.9%, WER 7.2% (`test-clean`)
  - **파워 스펙트럼 (PS)**: LER 9.1%, WER 9.4% (`test-clean`)
  - **원시 파형 (Raw)**: LER 10.6%, WER 10.1% (`test-clean`)
  - 출력 정밀도(output precision)를 높이기 위해 입력 시퀀스를 10ms씩 이동하여 여러 번 네트워크에 입력하는 방법으로 약 1%의 성능 향상을 얻었습니다. MFCC가 가장 좋은 성능을 보였지만, 파워 스펙트럼과 원시 파형도 충분한 데이터가 있다면 격차가 줄어들 것으로 예상됩니다.

## 🧠 Insights & Discussion

- **단순성과 효율성**: 이 연구의 핵심 강점은 복잡한 HMM/GMM 파이프라인이나 계산 집약적인 RNN 모델 없이도 경쟁력 있는 엔드투엔드 음성 인식 시스템을 구축할 수 있다는 것을 보여주었다는 점입니다. ConvNet 기반 아키텍처와 ASG의 조합은 효율적인 학습과 추론을 가능하게 합니다.
- **전통적인 제약으로부터의 해방**: 음소 정렬, HMM/GMM 사전 훈련, 복잡한 음성학적 지식 없이 글자를 직접 예측하는 방식은 시스템 설계와 구현을 크게 단순화합니다.
- **ASG의 유효성**: ASG는 CTC와 유사한 정확도를 유지하면서도 더 간단한 구조(blank 레이블 없음)와 유연한 비정규화 점수 사용을 통해 새로운 가능성을 제시합니다. 특히 긴 시퀀스에서 CTC보다 빠른 처리 속도를 보인 점은 주목할 만합니다.
- **원시 파형의 잠재력**: MFCC나 파워 스펙트럼에 비해 아직 성능 격차가 존재하지만, 원시 파형으로 직접 훈련하는 모델의 유망성을 보여주었습니다. 이는 특징 추출 단계의 필요성을 없애고 순수하게 데이터 기반의 학습을 가능하게 한다는 점에서 중요합니다.
- **제한 사항**: 현재 시스템은 언어 모델의 가중치($\alpha$)나 단어 삽입 페널티($\beta$)와 같은 디코더 하이퍼파라미터 튜닝이 필요하며, 더 복잡한 언어 모델이나 도메인 적응(adaptation)을 적용하면 추가적인 성능 향상을 기대할 수 있습니다.

## 📌 TL;DR

Wav2Letter는 HMM/GMM 사전 훈련이나 강제 정렬 없이 음성 신호에서 글자로 직접 변환하는 간단한 ConvNet 기반의 엔드투엔드 음성 인식 시스템입니다. 이 시스템은 "blank" 레이블 없이 비정규화된 점수와 전역 정규화를 사용하는 새로운 자동 분할 기준(ASG)을 도입하여 CTC와 유사한 정확도를 보이며 긴 시퀀스에서 더 빠른 처리 속도를 제공합니다. Librispeech 데이터셋에서 MFCC로 7.2%의 WER을 달성하는 등 경쟁력 있는 성능을 입증했으며, 원시 파형 입력에 대한 가능성도 보여주면서 기존 RNN 기반 모델보다 계산 효율적입니다.
