# Sequence Transduction with Recurrent Neural Networks

Alex Graves

## 🧩 Problem to Solve

많은 기계 학습 작업은 입력 시퀀스를 출력 시퀀스로 변환하는 시퀀스 변환(transduction)으로 표현될 수 있습니다. 음성 인식, 기계 번역, 단백질 2차 구조 예측 등이 그 예입니다. 시퀀스 변환의 주요 과제 중 하나는 시퀀스 확장, 축소, 이동과 같은 **시퀀스 왜곡에 불변한 방식으로 입력 및 출력 시퀀스를 표현하는 방법을 학습하는 것**입니다.

기존 순환 신경망(RNN)은 이러한 표현 학습에 강력한 능력을 보여주었지만, **입력 및 출력 시퀀스 간의 정렬(alignment)이 미리 정의되어 있어야만 변환을 수행할 수 있다**는 심각한 제약이 있었습니다. 많은 시퀀스 변환 문제에서 이 정렬을 찾는 것이 가장 어려운 부분이며, 심지어 출력 시퀀스의 길이를 결정하는 것조차 어려운 경우가 많습니다.

또한, Connectionist Temporal Classification (CTC)는 정렬 문제를 다루지만, 출력 간의 상호 의존성을 모델링하지 않으며 출력 시퀀스가 입력보다 길 수 있는 텍스트-음성 변환과 같은 작업에는 부적합합니다.

## ✨ Key Contributions

- RNN에 전적으로 기반한 종단 간(end-to-end), 확률적 시퀀스 변환 시스템을 제안합니다.
- **어떤 입력 시퀀스든 어떤 유한하고 이산적인 출력 시퀀스로 변환할 수 있는 원칙적 능력**을 갖춥니다.
- CTC를 확장하여 **모든 길이의 출력 시퀀스에 대한 분포를 정의**하고, **입력-출력 및 출력-출력 의존성을 공동으로 모델링**합니다.
- 음성 인식(TIMIT 데이터셋) 작업에서 당시 RNN 기반 모델 중 가장 낮은 오류율을 달성하며 이 방법론의 효과를 입증했습니다.

## 📎 Related Works

- **순환 신경망(RNNs):** 시퀀스 학습에 강력한 아키텍처로, 특히 Long Short-Term Memory (LSTM) 변형은 장기 의존성 학습에 뛰어나다는 것이 입증되었습니다 (Hochreiter & Schmidhuber, 1997; Gers, 2001; Graves et al., 2008).
- **Connectionist Temporal Classification (CTC):** 미리 분할되지 않은 시퀀스 데이터에 레이블을 지정하기 위한 RNN 출력 계층으로, 입력-출력 정렬에 대한 분포를 정의합니다 (Graves et al., 2006). 본 논문의 변환기는 CTC의 개념을 확장합니다.
- **Conditional Random Fields (CRFs):** 'chain-graph' 조건부 무작위 필드와 같은 판별적 시퀀스 모델과 유사점을 가집니다 (Lafferty et al., 2001). 그러나 본 논문의 RNN 기반 변환기는 원시 데이터에서 특징을 추출하고 잠재적으로 무한한 범위의 의존성을 다룬다는 점에서 차이가 있습니다.
- **Graph Transformer Network:** 미분 가능한 모듈(종종 신경망)을 사용하여 전역적으로 훈련될 수 있는 그래프 변환 패러다임입니다 (Bottou et al., 1997).
- **Bidirectional RNNs (Bi-RNNs):** 입력 시퀀스 전체에 의존하는 출력을 계산하기 위해 순방향 및 역방향으로 스캔하는 RNN 아키텍처 (Schuster & Paliwal, 1997). 본 논문의 전사 네트워크에서 사용됩니다.

## 🛠️ Methodology

본 논문에서 제안하는 RNN 변환기는 두 개의 순환 신경망으로 구성되며, **$\bar{Y} = Y \cup \emptyset$**로 정의되는 확장된 출력 공간(null 출력을 포함)을 통해 입력 시퀀스 $x = (x_1, ..., x_T)$와 출력 시퀀스 $y = (y_1, ..., y_U)$ 간의 조건부 확률 **$\text{Pr}(y \in Y^*|x)$**를 모델링합니다.

1. **전사 네트워크($F$):**

   - 입력 시퀀스 $x$를 스캔하는 양방향 RNN(Bi-RNN)입니다.
   - 두 개의 독립적인 은닉 계층(순방향 및 역방향)이 단일 출력 계층으로 피드 포워드됩니다.
   - 각 입력 시간 단계 $t$에 대해 전사 벡터 $f_t$를 출력하며, 이는 입력-출력 의존성을 인코딩합니다.
   - Long Short-Term Memory (LSTM) 셀을 사용하여 장기 의존성을 포착합니다.

2. **예측 네트워크($G$):**

   - $\emptyset$가 앞에 붙은 출력 시퀀스 $\hat{y} = (\emptyset, y_1, ..., y_U)$를 스캔하는 단방향 RNN입니다.
   - 각 출력 단계 $u$에 대해 예측 벡터 $g_u$를 출력하며, 이는 이전 출력에 기반한 다음 출력에 대한 의존성을 모델링합니다(언어 모델과 유사).
   - 마찬가지로 LSTM 셀을 사용하여 장기 의존성을 포착합니다.

3. **출력 분포:**

   - 전사 벡터 $f_t$와 예측 벡터 $g_u$가 주어졌을 때, 확장된 출력 공간 $\bar{Y}$의 레이블 $k$에 대한 출력 밀도 함수를 다음과 같이 정의합니다:
     $$h(k,t,u) = \exp(f_k^t + g_k^u)$$
   - 이를 정규화하여 조건부 출력 분포를 얻습니다:
     $$\text{Pr}(k \in \bar{Y}|t,u) = \frac{h(k,t,u)}{\sum_{k' \in \bar{Y}} h(k',t,u)}$$
   - 이 확률은 입력 $x$와 출력 $y$ 사이의 모든 가능한 정렬을 나타내는 격자(lattice)에서 전이 확률을 결정합니다.

4. **학습 (Forward-Backward 알고리즘):**

   - 목표 시퀀스 $y^*$의 로그 손실 $L = -\ln \text{Pr}(y^*|x)$를 최소화하여 모델을 훈련합니다.
   - $\text{Pr}(y^*|x)$는 격자를 통해 확률이 확산되는 것을 고려하여 순방향 변수 $\alpha(t,u)$와 역방향 변수 $\beta(t,u)$를 사용하여 효율적으로 계산됩니다.
     - $\alpha(t,u)$: 입력 $f_{[1:t]}$ 동안 $y_{[1:u]}$를 출력할 확률.
     - $\beta(t,u)$: 입력 $f_{[t:T]}$ 동안 $y_{[u+1:U]}$를 출력할 확률.
   - 네트워크 가중치에 대한 $L$의 기울기는 Backpropagation Through Time (BPTT)을 사용하여 각 네트워크에 독립적으로 계산됩니다.
   - $O(TU)$의 `exp` 함수 호출을 $O(T+U)$로 줄이는 계산 효율성 최적화가 적용됩니다.

5. **테스트 (빔 검색):**
   - 입력 시퀀스에 의해 유도된 출력 시퀀스 분포의 최빈값(가장 확률이 높은 시퀀스)을 찾습니다.
   - 예측 함수 $g(y_{[1:u]})$가 이전 모든 출력에 의존할 수 있으므로, 고정 너비 빔 검색(Algorithm 1)을 사용하여 근사치를 찾습니다. 이는 계산 비용과 검색 정확도 사이의 균형을 제공합니다.
   - 더 짧은 출력 시퀀스가 과도하게 선호되는 것을 방지하기 위해 길이 정규화(length-normalization)가 적용됩니다.

## 📊 Results

- **실험 작업:** TIMIT 음성 코퍼스를 사용한 음소 인식.
- **비교 대상:** 단독 예측 RNN, 단독 Connectionist Temporal Classification (CTC) RNN, 그리고 제안된 변환기.
- **결과 요약 (당시 기준):**
  - **변환기(Transducer) 음소 오류율 (PER):** 23.2%
  - **CTC 음소 오류율:** 25.5%
  - **단독 예측 RNN 오분류율:** 72.9%
  - **변환기 로그 손실:** 1.0 bits/음소
  - **CTC 로그 손실:** 1.3 bits/음소
  - **단독 예측 RNN 로그 손실:** 4.0 bits/음소
- 변환기는 TIMIT에서 기록된 음소 오류율 중 가장 낮은 수준(당시 벤치마크 20.5%) 중 하나를 달성했으며, RNN 기반 모델 중 가장 좋은 성능을 보였습니다.
- 단독 CTC 네트워크에 비해 변환기의 이점은 상대적으로 미미했지만, 이는 TIMIT 데이터셋의 크기가 예측 네트워크(언어 모델) 학습에 충분하지 않았을 가능성이 제시됩니다.

## 🧠 Insights & Discussion

- 변환기가 CTC 네트워크 단독 모델에 비해 약간의 우위만 보인 것은 TIMIT 전사 데이터셋의 크기가 예측 네트워크가 출력-출력 의존성을 완전히 학습하기에 너무 작았기 때문일 수 있습니다 (약 150K 레이블 vs. 일반적인 언어 모델 학습에 사용되는 수백만 단어).
- 더 큰 데이터셋에서는 변환기의 성능 향상이 더 클 것으로 예상됩니다. 또는 예측 네트워크를 대규모 '타겟 전용' 데이터셋으로 사전 학습시킨 후 변환기의 일부로 더 작은 데이터셋에서 공동으로 재훈련하는 방안이 제안됩니다. 이는 HMM 기반 음성 인식기에서 대규모 텍스트 코퍼스에서 추출한 언어 모델과 작은 음성 코퍼스에서 훈련한 음향 모델을 결합하는 것과 유사합니다.
- 미분 가능한 시스템의 장점은 각 구성 요소가 다른 구성 요소에 미치는 민감도를 쉽게 계산할 수 있다는 것입니다. 이를 통해 출력 확률 격자가 입력 시퀀스와 이전 출력에 얼마나 의존하는지 분석할 수 있습니다. 시각화는 두 네트워크 모두 장거리 의존성을 학습하며 입력 및 출력 시퀀스 길이에 걸쳐 가시적인 효과를 보여줍니다.

## 📌 TL;DR

미리 정의된 정렬이나 출력 길이 정보 없이 입력 시퀀스를 출력 시퀀스로 변환하는 문제를 해결하기 위해, 본 논문은 **RNN 기반의 종단 간 확률적 시퀀스 변환기**를 제안합니다. 이 변환기는 입력-출력 의존성을 모델링하는 **양방향 전사 네트워크**와 출력-출력 의존성을 모델링하는 **단방향 예측 네트워크**로 구성됩니다. Null 출력을 포함하는 확장된 출력 공간과 동적 프로그래밍 기반의 학습 알고리즘(Forward-Backward)을 사용하여 모든 길이의 출력 시퀀스에 대한 분포를 정의합니다. TIMIT 음소 인식 작업에서 23.2%의 오류율로 당시 RNN 기반 모델 중 가장 좋은 성능을 달성했으며, 음향 정보와 언어 정보를 효과적으로 통합할 수 있음을 입증하여 다양한 시퀀스-투-시퀀스 문제에 대한 가능성을 제시합니다.
