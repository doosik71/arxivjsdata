# Listen, Attend and Spell

William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals

## 🧩 Problem to Solve

전통적인 DNN-HMM 기반 음성 인식 시스템은 음향 모델링, 발음 모델링, 언어 모델과 같은 여러 구성 요소를 개별적으로 훈련시키며, 이는 복잡한 파이프라인과 여러 목적 함수를 야기합니다. 최근 시도된 종단 간(end-to-end) 접근 방식들에도 한계가 있습니다:

- **CTC (Connectionist Temporal Classification)**: 레이블 출력이 서로 조건부 독립적이라는 가정을 합니다.
- **어텐션 기반 시퀀스-투-시퀀스 모델**: 주로 음소(phoneme) 시퀀스에 적용되었으며, 음성 인식의 최종 출력(문자)까지 완전히 종단 간으로 훈련되지 않았습니다.

이 논문은 이러한 한계를 극복하고 음성 신호를 문자로 직접 전사하는 종단 간 신경망 모델을 제안하여, HMM이나 음소 개념에 의존하지 않고 문자 간 독립성 가정을 배제하는 것을 목표로 합니다.

## ✨ Key Contributions

- **LAS (Listen, Attend and Spell) 모델 제안**: 음성 발화를 문자로 직접 전사하는 종단 간 신경망 모델을 제시했습니다.
- **두 가지 핵심 구성 요소**: Listener (피라미드형 RNN 인코더)와 Speller (어텐션 기반 RNN 디코더)를 사용합니다.
- **문자 간 독립성 가정 제거**: 기존 CTC 모델과 달리 출력 문자 시퀀스 간의 독립성 가정을 하지 않아, 더 유연한 문자 시퀀스 생성이 가능합니다.
- **피라미드 BLSTM (pBLSTM) 사용**: Listener에 pBLSTM을 적용하여 입력 시퀀스 길이를 줄이고, 어텐션 모델이 더 적은 시간 단계에서 정보를 추출하게 하여 학습 속도와 성능을 향상시켰습니다.
- **종단 간 공동 훈련**: Listener와 Speller를 HMM이나 음소 개념 없이 완전히 공동 훈련합니다.
- **훈련-추론 불일치 완화**: 훈련 시 이전 예측 문자 분포에서 샘플링하는 기법(Scheduled Sampling)을 적용하여 훈련 시와 추론 시의 불일치를 줄였습니다.
- **다양한 철자 및 OOV(Out-Of-Vocabulary) 단어 처리**: 문자 단위 출력을 통해 사전이나 언어 모델 없이도 희귀 단어 및 OOV 단어를 자동으로 처리하고, 동일 음향에 대해 여러 철자 변형(예: "triple a"와 "aaa")을 자연스럽게 생성할 수 있음을 보였습니다.

## 📎 Related Works

- **하이브리드 DNN-HMM 시스템**: DNN을 음향 모델링, 발음 모델링, 언어 모델링 등 음성 인식의 여러 구성 요소에 활용했으나, 각 구성 요소가 개별적으로 훈련되는 방식이었습니다.
- **종단 간(End-to-End) 음성 인식**:
  - **CTC (Connectionist Temporal Classification)**: RNN으로 레이블 시퀀스를 직접 예측하는 초기 종단 간 접근 방식입니다.
  - **어텐션 기반 시퀀스-투-시퀀스 모델**: 기계 번역에서 큰 성공을 거두었으며, 음성 인식 분야에서는 주로 음소 시퀀스 예측에 적용되었습니다.
- **Scheduled Sampling**: RNN 기반 시퀀스 예측 모델의 훈련-추론 불일치를 완화하기 위한 기법입니다.

## 🛠️ Methodology

LAS 모델은 음향 특징을 입력으로 받아 영어 문자를 출력하는 두 가지 주요 모듈로 구성됩니다.

1. **Listener (인코더)**:
   - **입력**: 필터 뱅크 스펙트럼 특징으로 구성된 입력 시퀀스 $x = (x_1, \dots, x_T)$.
   - **구조**: 피라미드형 양방향 LSTM (pBLSTM)을 사용합니다.
   - **역할**: 긴 입력 음성 신호 $x$를 더 짧은 고수준 특징 표현 $h = (h_1, \dots, h_U)$으로 변환합니다 ($U \leq T$).
   - **pBLSTM의 작동 방식**: 각 연속적인 pBLSTM 계층에서 시간 해상도를 2배씩 줄입니다. $j$번째 계층의 $i$번째 시간 단계 출력을 계산할 때, 이전 계층의 연속적인 두 출력 $[h^{j-1}_{2i}, h^{j-1}_{2i+1}]$을 결합하여 다음 계층의 입력으로 사용합니다.
     $$h^j_i = \text{pBLSTM}(h^j_{i-1}, [h^{j-1}_{2i}, h^{j-1}_{2i+1}])$$
     이 모델에서는 최하단 BLSTM 계층 위에 3개의 pBLSTM 계층을 쌓아 시간 해상도를 $2^3 = 8$배 감소시킵니다.
2. **Speller (디코더)**:

   - **입력**: Listener에서 생성된 고수준 특징 $h$.
   - **구조**: 어텐션 기반 2계층 LSTM 변환기(transducer)입니다.
   - **역할**: $h$를 소비하고 출력 문자 시퀀스 $y = (\langle\text{sos}\rangle, y_1, \dots, y_S, \langle\text{eos}\rangle)$에 대한 확률 분포를 생성합니다.
   - **어텐션 메커니즘**: 각 출력 단계 $i$에서 디코더 상태 $s_i$와 인코더 특징 $h$를 사용하여 컨텍스트 벡터 $c_i$를 생성합니다.
     - 에너지 $e_{i,u} = \langle\phi(s_i), \psi(h_u)\rangle$를 계산합니다.
     - 소프트맥스 함수를 통해 어텐션 분포 $\alpha_{i,u} = \frac{\exp(e_{i,u})}{\sum_u \exp(e_{i,u})}$를 생성합니다.
     - 컨텍스트 벡터 $c_i = \sum_u \alpha_{i,u} h_u$는 Listener 특징 $h_u$의 선형 혼합으로 생성됩니다.
     - 다음 디코더 상태 $s_i = \text{RNN}(s_{i-1}, y_{i-1}, c_{i-1})$를 계산하고, 최종적으로 문자 분포 $P(y_i|x, y_{\lt i}) = \text{CharacterDistribution}(s_i, c_i)$를 통해 다음 문자를 예측합니다.

3. **학습 (Learning)**:
   - Listen과 AttendAndSpell 함수는 음성 인식을 위해 종단 간으로 공동 훈련되며, 로그 확률 $\max_\theta \sum_i \log P(y_i|x, y^*_{\lt i}; \theta)$를 최대화합니다.
   - **Scheduled Sampling**: 훈련 중 $y^*_{\lt i}$ (정답) 대신 이전 예측 문자 분포에서 10%의 확률로 샘플링한 $\tilde{y}_{\lt i}$를 다음 입력으로 사용하여 훈련 시와 추론 시의 불일치(exposure bias)를 줄입니다.
   - 사전 훈련은 필요하지 않음을 발견했습니다.
4. **디코딩 및 재평가 (Decoding and Rescoring)**:
   - 가장 가능성 있는 문자 시퀀스 $\hat{y} = \arg \max_y \log P(y|x)$를 찾기 위해 빔 탐색(beam search) 알고리즘을 사용합니다.
   - 외부 언어 모델(LM)을 사용하여 빔 탐색 결과의 점수를 재평가할 수 있습니다. 언어 모델 확률 $P_{LM}(y)$와 결합하여 최종 점수 $s(y|x) = \frac{\log P(y|x)}{|y|_c} + \lambda \log P_{LM}(y)$를 계산합니다 ($|y|_c$는 문자 수로 정규화).

## 📊 Results

- **데이터셋**: Google 음성 검색 발화 3백만 개(2000시간), 20배 데이터 증강을 사용했습니다.
- **WER (Word Error Rate) 비교**:

  | Model                         | Clean WER | Noisy WER |
  | :---------------------------- | :-------- | :-------- |
  | CLDNN-HMM [20] (SOTA)         | 8.0%      | 8.9%      |
  | LAS (Sampling)                | 14.1%     | 16.5%     |
  | LAS (Sampling) + LM Rescoring | **10.3%** | **12.0%** |

  - LAS는 사전이나 언어 모델 없이도 경쟁력 있는 성능을 보였고, 언어 모델 재평가를 통해 크게 개선되었습니다.

- **어텐션 시각화**: 문자 출력과 음향 신호 간의 명시적인 단조 정렬(monotonic alignment)을 학습하는 것을 확인했습니다.
- **발화 길이 효과**: 매우 짧은 발화(2단어 이하)에서는 대체 및 삽입 오류가, 매우 긴 발화에서는 삭제 오류가 지배적이었습니다.
- **단어 빈도**: 희귀 단어는 일반적으로 회상률(recall)이 낮지만, 음향적으로 고유한 단어는 높은 회상률을 보일 수 있습니다. 자주 등장하는 단어는 다른 단어로 잘못 전사되는 경우가 있었습니다.
- **흥미로운 디코딩 예시**: "triple a" 발화에 대해 "call aaa roadside assistance"와 "call triple a roadside assistance" 두 가지를 모두 상위 빔에서 생성하는 등 다중 철자 변형을 처리했습니다. 또한, "cancel cancel cancel"과 같이 단어가 반복되는 발화도 어텐션 메커니즘이 잘 처리했습니다.

## 🧠 Insights & Discussion

- LAS는 HMM이나 명시적인 음소 표현 없이도 음성 신호를 문자 시퀀스로 직접 전사하는 종단 간 어텐션 기반 모델의 강력한 잠재력을 입증했습니다.
- Listener의 피라미드형 구조와 Speller의 어텐션 메커니즘은 모델의 효율성과 성능에 필수적입니다. 이들이 없으면 모델이 과적합되거나 수렴이 매우 느려집니다.
- 훈련 중 Scheduled Sampling을 사용하면 훈련 및 추론 간의 불일치를 효과적으로 줄여 WER을 개선할 수 있습니다.
- 이 모델은 출력 문자를 통해 내재적인(implicit) 언어 모델을 학습하며, 이를 통해 OOV 단어를 자동으로 처리하고 동일 음향에 대한 다양한 철자 변형을 자연스럽게 생성할 수 있습니다.
- 성능은 SOTA CLDNN-HMM 시스템에 비해 여전히 약간 높지만, 이는 모델이 명시적인 음향, 발음, 언어 모델 없이도 이러한 수준에 도달했음을 감안하면 주목할 만합니다. CNN 필터를 통합하면 추가적인 성능 향상을 기대할 수 있습니다.
- 내용 기반 어텐션 메커니즘이 반복되는 단어도 잘 처리하는 능력은 위치 기반 선행 지식 없이도 강력한 시퀀스 정렬을 수행할 수 있음을 보여줍니다.

## 📌 TL;DR

이 논문은 음성 발화를 문자로 직접 전사하는 종단 간 신경망 모델인 **Listen, Attend and Spell (LAS)**을 제안합니다. LAS는 음성 신호를 고수준 특징으로 인코딩하는 **Listener (피라미드 BLSTM 인코더)**와 이 특징을 사용하여 문자를 디코딩하는 **Speller (어텐션 기반 LSTM 디코더)**로 구성됩니다. 이 모델은 전통적인 DNN-HMM 시스템이나 CTC의 레이블 독립성 가정 없이 HMM 및 음소 개념을 우회하여 음성 인식의 모든 구성 요소를 공동으로 학습합니다. Scheduled Sampling 기법과 외부 언어 모델 재평가와 결합하여, Google 음성 검색 태스크에서 SOTA CLDNN-HMM 모델에 근접한 경쟁력 있는 WER (언어 모델 재평가 시 10.3%)을 달성했으며, 희귀 단어 및 다중 철자 변형을 처리하는 강력한 능력을 보여주었습니다.
