# Listening while Speaking: Speech Chain by Deep Learning

Andros Tjandra, Sakriani Sakti, Satoshi Nakamura

## 🧩 Problem to Solve

음성 인식(ASR)과 음성 합성(TTS) 연구는 인간의 음성 지각(perception)과 생성(production) 간의 밀접한 관계에도 불구하고 오랫동안 독립적으로 발전해 왔습니다. 인간의 의사소통에서는 화자의 입에서 귀로 전달되는 청각 피드백을 포함하는 폐쇄 루프(closed-loop) 음성 사슬(speech chain) 메커니즘이 핵심적인 역할을 합니다. 본 논문은 이러한 인간의 언어 처리 방식을 모방하여, 딥러닝 기반으로 ASR과 TTS를 통합하는 폐쇄 루프 모델을 개발하고, 이를 통해 레이블링된 데이터뿐만 아니라 레이블링되지 않은 데이터를 효과적으로 활용하는 학습 전략을 모색하고자 합니다.

## ✨ Key Contributions

- **딥러닝 기반 폐쇄 루프 음성 사슬 모델 제안:** 인간의 음성 지각 및 생성 행동을 통합하는 최초의 딥러닝 모델로, ASR과 TTS 시스템을 단일 아키텍처 내에서 상호 연결합니다.
- **레이블링되지 않은 데이터 활용:** 폐쇄 루프 아키텍처를 통해 레이블링된(음성-텍스트 쌍) 데이터와 레이블링되지 않은(음성만 또는 텍스트만) 데이터를 모두 사용하여 모델을 훈련할 수 있음을 보였습니다.
- **자기 지도 학습 메커니즘:** ASR이 레이블링되지 않은 음성을 텍스트로 변환하고 TTS가 그 텍스트를 음성으로 재구성하며, 반대로 TTS가 텍스트를 음성으로 생성하고 ASR이 그 음성을 텍스트로 재구성하는 방식으로 두 모델이 서로를 '가르치는' 자동 인코더(autoencoder)와 유사한 학습 방식을 구현했습니다.
- **성능 향상 입증:** 제안하는 접근 방식이 레이블링된 데이터만으로 훈련된 분리된 시스템보다 ASR 및 TTS 성능을 유의미하게 향상시킴을 단일 화자 및 다중 화자 실험을 통해 입증했습니다.

## 📎 Related Works

- **음성 사슬(Speech Chain) 개념:** Denes et al. [1]이 처음 제시한 개념으로, 화자의 생각에서 청자의 생각으로 음성 메시지가 전달되는 과정을 설명합니다.
- **기존 ASR 및 TTS 기술:** 초기 음향-음성 지식, DTW, HMM-GMM 기반 ASR [2-6]과 규칙 기반, 파형 단위 연결, HSMM-GMM 기반 TTS [7-12] 기술의 발전을 언급합니다.
- **딥러닝 기반 ASR 및 TTS:** 딥러닝의 부흥 이후 ASR [13-15] 및 TTS [16-18] 분야에서 최첨단 성능을 달성한 신경망 프레임워크들을 참조합니다.
- **신경 기계 번역(NMT)의 이중 학습(Dual Learning):** He et al. [33] 및 Cheng et al. [34]은 NMT에서 소스-타겟 및 타겟-소스 번역 모델이 폐쇄 루프를 형성하여 레이블링되지 않은 단일 언어 데이터를 활용해 모델 성능을 향상시키는 방식을 제안했습니다. 본 논문은 음성과 텍스트라는 이질적인 도메인에서 이와 유사한 개념을 음성 처리 과제에 적용한 최초의 연구입니다.

## 🛠️ Methodology

본 논문은 딥러닝 기반의 폐쇄 루프 음성 사슬 모델을 제안하며, 이는 시퀀스-투-시퀀스(sequence-to-sequence) ASR, 시퀀스-투-시퀀스 TTS, 그리고 이 둘을 연결하는 루프(loop)로 구성됩니다. 핵심 아이디어는 ASR과 TTS 모델을 공동으로 훈련하는 것입니다.

- **모델 아키텍처:**

  - **ASR 모델:** 어텐션(attention) 기반 인코더-디코더 구조를 사용합니다 (Fig. 3). 인코더는 로그 멜 스펙트로그램(log Mel-scale spectrogram)을 입력받아 BiLSTM 레이어로 처리하고, 디코더는 문자 임베딩을 입력받아 LSTM 레이어로 처리하며, MLP 스코어 함수를 통해 어텐션 컨텍스트를 계산합니다.
  - **TTS 모델:** Tacotron [23]을 기반으로 수정된 아키텍처를 사용합니다 (Fig. 4). 인코더는 문자 임베딩을 CBHG 블록(1-D Convolution Bank + Highway + bidirectional GRU)에 통과시킵니다. 디코더는 이전 단계의 로그 멜 스펙트로그램을 입력받아 스택형 LSTM과 단조 어텐션(monotonic attention) 메커니즘을 통해 컨텍스트를 추출합니다. 로그 멜 스펙트로그램과 로그 크기 스펙트로그램(log magnitude spectrogram)을 출력하며, 음성 종료를 예측하는 이진 예측 레이어를 추가하여 모델의 강건성을 높였습니다.

- **학습 알고리즘 (Algorithm 1):**
  1. **지도 학습 (Supervised Training):**
     - 레이블링된 음성-텍스트 쌍 ($x_P, y_P$) 데이터셋 $D_P$를 사용하여 ASR 및 TTS 모델을 독립적으로 훈련합니다.
     - ASR 손실 $L_{ASR}^P$은 예측된 텍스트 확률 벡터 $p_y$와 실제 텍스트 $y_P$ 간의 교차 엔트로피로 계산됩니다.
     - TTS 손실 $L_{TTS}^P$은 생성된 음성 $\hat{x}_P$와 실제 음성 $x_P$ 간의 MSE(Mean Squared Error)와 음성 종료 이진 예측 손실의 합으로 계산됩니다.
  2. **비지도 학습 (Unsupervised Training):**
     - **TTS-to-ASR 언롤링:** 레이블링되지 않은 텍스트 데이터 $Y_U$에서 텍스트 $y_U$를 샘플링합니다.
       - TTS 모델이 $y_U$로부터 음성 $\hat{x}_U$를 생성합니다.
       - ASR 모델이 $\hat{x}_U$로부터 텍스트 확률 벡터 $p_y$를 재구성합니다.
       - 재구성된 $p_y$와 원본 텍스트 $y_U$ 간의 ASR 손실 $L_{ASR}^U$을 계산합니다.
     - **ASR-to-TTS 언롤링:** 레이블링되지 않은 음성 데이터 $X_U$에서 음성 $x_U$를 샘플링합니다.
       - ASR 모델이 $x_U$로부터 텍스트 $\hat{y}_U$를 생성합니다.
       - TTS 모델이 $\hat{y}_U$로부터 음성 $\hat{x}_U$를 재구성합니다.
       - 재구성된 음성 $\hat{x}_U$와 원본 음성 $x_U$ 간의 TTS 손실 $L_{TTS}^U$을 계산합니다.
  3. **손실 결합:** 모든 손실을 가중합하여 최종 손실 $L$을 계산합니다.
     $$ L = \alpha \cdot (L*{TTS}^P + L*{ASR}^P) + \beta \cdot (L*{TTS}^U + L*{ASR}^U) $$
        여기서 $\alpha$는 지도 손실 계수, $\beta$는 비지도 손실 계수입니다. 이 최종 손실을 통해 ASR 및 TTS 모델의 파라미터를 업데이트합니다.

## 📊 Results

- **단일 화자 실험:**
  - Google TTS로 생성된 BTEC(Basic Travel Expression Corpus) 영어 문장 데이터셋을 사용했습니다.
  - ASR 성능은 CER(Character Error Rate)로 측정했으며, TTS 성능은 로그 멜 스펙트로그램 및 로그 크기 스펙트로그램의 MSE와 음성 프레임 종료 예측 정확도(Acc)로 측정했습니다.
  - 레이블링된 데이터(10k 쌍)만으로 훈련한 시스템 대비, 비지도 데이터(40k 쌍)를 추가하여 훈련한 결과 ASR의 CER이 최대 4.6% (10.06%에서 5.44%) 개선되었습니다. TTS 또한 MSE가 감소하고 음성 종료 예측 정확도가 향상되었습니다.
- **다중 화자 실험:**
  - BTEC ATR-EDB 실제 음성 코퍼스(미국 북동부 방언, 약 50명 화자)를 사용했습니다.
  - ASR CER은 26.47%에서 19.99%로 크게 개선되었습니다. TTS MSE도 감소하고 Acc는 유지되었습니다.
  - 단일 화자 실험과 유사하게 ASR과 TTS 모델 모두 비지도 데이터셋을 추가 훈련하여 성능이 향상됨을 보였습니다. 특히, 기준 모델의 성능이 낮을수록 지도 학습 손실 계수 $\alpha$를 크게 설정하는 것이 더 효과적임을 발견했습니다 ($\alpha=0.5$일 때 더 큰 개선).

## 🧠 Insights & Discussion

본 연구는 인간의 의사소통에서 영감을 받은 '음성 사슬' 개념을 딥러닝 모델에 성공적으로 통합하여 ASR과 TTS 시스템의 성능을 동시에 향상시킬 수 있음을 입증했습니다. 특히, 레이블링된 데이터가 제한적일 때 레이블링되지 않은 대규모 데이터를 활용하여 모델을 효과적으로 훈련할 수 있는 새로운 자기 지도 학습 패러다임을 제시합니다. ASR과 TTS가 서로의 '선생님' 역할을 하며 피드백 루프를 통해 지속적으로 개선되는 방식은 두 분야의 상호 의존성을 강조하며, 향후 음성 처리 연구에 중요한 방향을 제시합니다.

한계점으로는 현재 실험이 특정 언어(영어)와 비교적 깨끗한 환경의 데이터에 한정되어 있다는 점입니다. 향후 연구에서는 다양한 언어, 자연스러운 발화, 잡음이 많거나 감정적인 음성 등 더 복잡한 조건에서 제안된 접근 방식의 효과를 검증할 필요가 있습니다.

## 📌 TL;DR

본 논문은 인간의 음성 지각과 생성 메커니즘인 '음성 사슬'에서 영감을 받아, ASR(음성 인식)과 TTS(음성 합성) 모델을 딥러닝 기반의 폐쇄 루프 아키텍처로 통합하는 방법을 제안합니다. 이 모델은 ASR과 TTS가 서로를 재구성하는 자기 지도 학습 방식을 통해 레이블링되지 않은 대규모 데이터를 활용하여 훈련될 수 있습니다. 실험 결과, 제안된 접근 방식은 레이블링된 데이터만으로 훈련된 분리된 시스템보다 ASR의 CER을 최대 4.6% 감소시키고 TTS 성능을 향상시키는 등 두 과제 모두에서 유의미한 성능 개선을 달성했습니다. 이는 레이블링된 데이터의 부족 문제를 해결하고, 음성 이해 및 생성 시스템을 통합적으로 개선할 수 있는 새로운 가능성을 제시합니다.
