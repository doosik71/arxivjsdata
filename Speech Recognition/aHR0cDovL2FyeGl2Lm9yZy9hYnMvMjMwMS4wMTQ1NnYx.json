{
  "title": "Audio-Visual Efficient Conformer for Robust Speech Recognition",
  "authors": "Maxime Burchi, Radu Timofte",
  "year": 2023,
  "url": "http://arxiv.org/abs/2301.01456v1",
  "abstract": "End-to-end Automatic Speech Recognition (ASR) systems based on neural\nnetworks have seen large improvements in recent years. The availability of\nlarge scale hand-labeled datasets and sufficient computing resources made it\npossible to train powerful deep neural networks, reaching very low Word Error\nRate (WER) on academic benchmarks. However, despite impressive performance on\nclean audio samples, a drop of performance is often observed on noisy speech.\nIn this work, we propose to improve the noise robustness of the recently\nproposed Efficient Conformer Connectionist Temporal Classification (CTC)-based\narchitecture by processing both audio and visual modalities. We improve\nprevious lip reading methods using an Efficient Conformer back-end on top of a\nResNet-18 visual front-end and by adding intermediate CTC losses between\nblocks. We condition intermediate block features on early predictions using\nInter CTC residual modules to relax the conditional independence assumption of\nCTC-based models. We also replace the Efficient Conformer grouped attention by\na more efficient and simpler attention mechanism that we call patch attention.\nWe experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip\nReading Sentences 3 (LRS3) datasets. Our experiments show that using audio and\nvisual modalities allows to better recognize speech in the presence of\nenvironmental noise and significantly accelerate training, reaching lower WER\nwith 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)\nmodel achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on\nLRS2 and LRS3 test sets. Code and pretrained models are available at\nhttps://github.com/burchim/AVEC.",
  "citation": 65
}