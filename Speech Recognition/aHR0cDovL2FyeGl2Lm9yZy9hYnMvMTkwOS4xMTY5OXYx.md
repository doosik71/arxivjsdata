# SPEECH RECOGNITION WITH AUGMENTED SYNTHESIZED SPEECH
Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhadran, Ye Jia, Pedro Moreno, Yonghui Wu, Zelin Wu

## 🧩 Problem to Solve
이 연구는 최근 크게 발전한 음성 합성(TTS) 기술, 특히 Tacotron 아키텍처를 활용하여 음성 인식(ASR) 시스템 학습에 사용되는 값비싼 수동 전사(transcribed) 인간 음성 데이터를 대체하거나 보강할 수 있는지 평가하는 것을 목표로 합니다. 구체적으로, 합성 음성을 훈련 데이터로 추가하여 ASR 성능을 향상시킬 수 있는지, 그리고 견고한 ASR을 위해 필요한 음향적 및 어휘적 다양성을 합성 음성으로 제공할 수 있는지를 탐구합니다. 또한, 한 도메인에서 학습된 합성기를 사용하여 다른 도메인으로 ASR을 도메인 적응시키는 데이터 증강 전략으로서의 실현 가능성을 검증합니다.

## ✨ Key Contributions
*   TTS 발화를 사용한 데이터 증강이 ASR 성능 향상을 가져옴을 입증했습니다.
*   ASR 데이터 증강을 위한 TTS의 효과적이고 다양한 화자 표현의 중요성을 처음으로 보여주었습니다.
*   ASR 데이터 증강을 위해 어휘적으로 다양한 TTS 발화 사용의 영향을 처음으로 보여주었습니다.
*   소량의 다중 화자 훈련 데이터로 TTS 훈련에 크게 기여하는 새로운 TTS 기술인 계층적 VAE를 설명했습니다.

## 📎 Related Works
*   **Li et al. [3]**: 이 연구와 가장 유사하며, 음성 합성을 데이터 증강의 소스로 사용했습니다. 그들은 Global Style Token (GST)을 통해 운율 변화를 확장하여 음향 다양성을 높였고, 인식 네트워크의 깊이를 늘리는 하이퍼파라미터 튜닝을 통해 성능을 향상시켰습니다. 본 연구는 이들의 발견을 확증하면서 어휘 다양성에 대한 탐구를 추가했습니다.
*   **Speech Chain (Denes et al. [8, 9] 및 Tjandra et al. [10, 11])**: ASR과 TTS 시스템을 공동으로 훈련하는 접근 방식입니다. DeepChain [10]은 폐쇄 루프 아키텍처에서 시퀀스-투-시퀀스 모델을 사용하여 레이블이 지정된 데이터와 레이블이 지정되지 않은 데이터 모두로 훈련할 수 있게 했습니다. 이 접근 방식은 TTS가 ASR로부터 배우고 ASR이 합성된 음성을 사용하여 레이블이 지정된 텍스트로부터 배우도록 합니다. [11]에서는 스피커 검증 모듈을 추가하여 미등록 화자를 처리할 수 있게 확장했습니다.

## 🛠️ Methodology
*   **음성 합성 모델**:
    *   Tacotron 2 [1] 기반의 다중 화자 음성 합성 모델과 WaveRNN 보코더 [4]를 사용했습니다.
    *   `ISOLATED-SENTENCES`와 같은 비정형 데이터 훈련의 견고성을 위해 GMM 기반 주의 메커니즘 [13]과 계층적 VAE (Variational Auto Encoder)를 도입했습니다 (그림 1 참조). 계층적 VAE는 로컬 인코더 (고정된 2초 청크)와 글로벌 인코더 (전체 발화)를 포함하여 스타일 정보를 인코딩합니다.
*   **음성 인식 모델**:
    *   Listen-Attend-Spell (LAS) [15, 16] 기반의 엔드-투-엔드 인코더-디코더 모델을 사용했습니다.
    *   입력 음성은 80-멜 특징과 델타, 더블 델타로 표현됩니다. 인코더는 컨볼루션 계층과 양방향 LSTM 계층으로 구성되며, 디코더는 단방향 LSTM 계층으로 구성됩니다.
    *   훈련 중 언어 모델은 포함되지 않았습니다.
*   **데이터 증강 전략**:
    *   **음향 다양성 증강**: 훈련 텍스트의 합성 복사본을 생성하며, 화자 임베딩 (d-벡터) 생성 방식에 따라 세 가지 접근법을 사용했습니다.
        *   **Original**: 훈련 발화 자체에서 파생된 d-벡터 사용.
        *   **Sampled**: 훈련 중 사용된 다른 발화에서 d-벡터를 무작위로 샘플링.
        *   **Random**: 무작위 256차원 벡터를 생성하고 L2-정규화를 통해 단위 초구에 투영하여 d-벡터로 사용.
    *   **어휘 다양성 증강**: 훈련 데이터에서 학습된 MaxEnt 언어 모델 [20]을 사용하여 새로운 발화 시퀀스를 생성하고 이를 합성하여 훈련 데이터에 추가했습니다.
*   **데이터 필터링**: 잘못 합성된 LIBRISPEECH 발화는 WER이 20%를 초과하는 경우 훈련에서 제외했습니다.
*   **코퍼스**: LIBRISPEECH (960시간)와 ISOLATED-SENTENCES (76시간) 두 가지 코퍼스를 사용했습니다.

## 📊 Results
*   **음향 다양성 증강 (LIBRISPEECH)**:
    *   `Sampled` d-벡터 접근 방식이 `test-clean`에서 4.58%, `test-other`에서 13.78%의 WER을 기록하며, 증강을 하지 않은 경우 (4.77%, 13.89%) 대비 약 4%의 상대적 WER 감소를 보였습니다.
    *   `Original` 및 `Random` d-벡터는 성능 개선에 효과적이지 않았습니다.
    *   단일 화자 TTS로 증강한 경우 WER이 극적으로 나빠져, 다중 화자 TTS가 효과적인 데이터 증강에 필수적임을 보여주었습니다.
*   **소스 데이터 감소 시 성능**:
    *   합성 데이터 증강은 소스 데이터가 적을 때 더 큰 성능 향상(예: 100시간 훈련 시 WER 12.46%에서 9.25%로 감소)을 가져왔으나, 합성 데이터만으로는 인간 음성 데이터에 필적하는 성능을 유지할 수 없었습니다.
*   **어휘 다양성 증강 (LIBRISPEECH)**:
    *   언어 모델을 통해 생성된 어휘적으로 다양한 합성 발화를 추가했을 때, 최대 600k 발화까지는 WER이 5% 상대적으로 감소했습니다 (`test-clean` 4.55%, `test-other` 13.64%).
    *   증강 발화 수가 1.1M에 달하면 추가적인 큰 성능 향상은 보이지 않았습니다.
*   **도메인 적응 (ISOLATED-SENTENCES)**:
    *   `ISOLATED-SENTENCES` 데이터에 `LIBRISPEECH` TTS 모델로 합성된 데이터를 증강했을 때도 성능 향상 (예: 32.9%에서 29.6% WER로 감소)이 관찰되었습니다.
    *   인-도메인 (ISOLATED-SENTENCES) TTS로 증강하는 것이 아웃-오브-도메인 (LIBRISPEECH) TTS로 증강하는 것보다 더 도움이 되었습니다.
    *   인-도메인 TTS 기반 데이터 증강 (32.9%에서 30.74% WER)은 960시간의 아웃-오브-도메인 인간 음성 추가 (32.9%에서 30.94% WER)와 비슷한 효과를 보였습니다.
    *   아웃-오브-도메인 인간 음성(`LIBRISPEECH`)과 인-도메인 TTS를 모두 사용했을 때 29.51%로 추가적인 성능 향상을 달성했습니다.

## 🧠 Insights & Discussion
*   음성 합성을 통한 데이터 증강은 ASR 성능 향상에 효과적입니다. 특히 화자 특성을 다양하게 변화시키거나 언어 모델을 통해 새로운 발화를 생성하여 음향적, 어휘적 다양성을 높이는 것이 유효합니다.
*   다중 화자 TTS 모델은 효과적인 데이터 증강에 필수적이며, 단일 화자 TTS는 충분한 성능 향상을 제공하지 못했습니다. 이는 ASR이 다양한 화자 특성에 견고해야 하기 때문입니다.
*   합성 데이터 증강은 특히 훈련 데이터의 양이 적을 때 큰 이점을 제공하지만, 여전히 실제 인간 음성으로 훈련된 시스템과 합성 음성으로만 훈련된 시스템 간에는 상당한 성능 격차가 존재합니다. 합성 음성이 실제 음성만큼의 가치를 ASR 훈련에 제공하기 위해서는 더 많은 연구가 필요합니다.
*   한 도메인(LIBRISPEECH)에서 훈련된 TTS 모델을 사용하여 다른 도메인(ISOLATED-SENTENCES)의 ASR 성능을 향상시키는 도메인 적응 전략으로서의 가능성도 확인되었습니다. 이는 일반 목적의 합성기가 여러 도메인에서 데이터 증강에 사용될 수 있음을 시사합니다.

## 📌 TL;DR
이 논문은 Tacotron 기반의 다중 화자 음성 합성(TTS)을 사용하여 음성 인식(ASR) 훈련 데이터를 증강하는 방법을 탐구합니다. 연구 결과, 다양한 화자 특성 (`Sampled` d-벡터) 및 언어 모델을 통한 어휘적 다양성을 가진 합성 음성으로 훈련 데이터를 보강하면 ASR 성능이 향상됨을 보였습니다. 특히 소량의 인간 음성 데이터에 대한 증강 효과가 컸으며, 한 도메인에서 훈련된 TTS 모델이 다른 도메인의 ASR 성능 향상에도 기여할 수 있음을 입증했습니다. 그러나 합성 음성이 여전히 실제 인간 음성만큼의 훈련 가치를 가지지는 못하며, 둘 사이에는 상당한 성능 격차가 남아있습니다.