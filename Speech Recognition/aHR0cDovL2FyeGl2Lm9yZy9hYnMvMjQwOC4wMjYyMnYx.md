# Language Model Can Listen While Speaking

Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen

## 🧩 해결할 문제

최근 음성 언어 모델(SLM)의 발전에도 불구하고, 대부분의 모델은 턴 기반(turn-based) 대화에 국한되어 실시간 상호작용, 특히 사용자가 발화 중 모델을 중단시키는 '가로채기(interruption)' 능력 없다는 한계가 있습니다. 기존의 전이중(Full Duplex) 모델링 시도들은 텍스트 기반 LLM에 ASR 및 TTS 모듈을 계단식으로 연결하여 지연이 발생하고 비언어적 특징 표현 능력이 부족했습니다. 이 논문은 이러한 한계를 해결하고 SLM 자체에 실시간 가로채기 기능을 내재화하는 것을 목표로 합니다.

## ✨ 주요 기여

- 대화형 음성 언어 모델(iSLM)에서 중요한 과제인 전이중 모델링(FDM)을 명확히 정의했습니다.
- 실시간 턴-테이킹(turn-taking) 문제를 해결하기 위한 종단 간(end-to-end) 단일 모델인 LSLM(Listening-while-Speaking Language Model)을 제안했습니다. 이 모델은 발화 중 외부 음성 신호를 듣고 실시간으로 피드백을 제공할 수 있습니다.
- 전이중 신호를 융합하는 세 가지 방법(Early Fusion, Middle Fusion, Late Fusion)을 탐색했으며, Middle Fusion이 음성 생성과 실시간 상호작용 간 최적의 균형을 제공함을 입증했습니다.
- 제안된 LSLM의 FDM 능력을 명령어 기반(Command-based) 및 음성 기반(Voice-based) FDM의 두 가지 시나리오에서 테스트하여, 기존 시스템에 미치는 영향을 최소화하면서 전이중 통신이 가능함을 보였습니다.

## 📎 관련 연구

- **단방향(Simplex) 및 반이중(Half Duplex) 음성 언어 모델:**
  - **단방향 SLM:** 듣기 또는 말하기 중 한 가지 채널만 지원합니다. LLM 기반 ASR, 음성 번역, 음성 감정 이해, TTS 등이 있습니다. (예: [46, 24, 45, 32], [15, 25, 18, 31]) 한 방향 통신에 국한됩니다.
  - **반이중 SLM (턴 기반):** 듣기와 말하기 모두 가능하지만 동시에 수행할 수 없습니다. 인간의 턴-테이킹 문제를 해결하지 못합니다. (예: SpeechGPT [48], LauraGPT [5], VioLA [42])
- **전이중(Full Duplex) 음성 언어 모델:**
  - 동시에 듣고 말할 수 있어 인간의 가로채기에 대응할 수 있습니다.
  - 최근의 노력은 텍스트 중심의 LLM에 계단식 ASR 및 TTS 모듈을 사용하여 전이중 기능을 구축하려 했습니다. (예: [49, 41])
  - GPT-4o 및 Moshi와 같은 최첨단 제품이 전이중 기능을 보여주지만, 공개 소스 모델이나 자세한 분석은 부족한 실정입니다. 이 연구는 이 격차를 메우기 위해 종단 간 SLM 기반의 전이중 모델을 제안합니다.

## 🛠️ 방법론

- **LSLM (Listening-while-Speaking Language Model) 아키텍처:**
  - 말하기 채널과 듣기 채널을 모두 갖춘 종단 간 단일 모델입니다.
  - **말하기 능력 구현:** 실시간 상호작용을 위해 단일 레이어의 이산 오디오 토큰을 사용하는 토큰 기반 디코더-온리 TTS(Text-to-Speech) 모델을 활용합니다.
    - 입력 음성 $X_R$은 SSL 인코더(Enc)를 통해 임베딩 $R$로 변환되고, 양자화(Qnt)를 통해 이산 토큰 $R_q$로 변환됩니다.
    - 학습 손실 함수는 다음과 같습니다:
      $$L(\theta_S) = -\sum_{t=1}^{t_{EOS}} \log P(r_q_t | R_{q_{1:t-1}}, C; \theta_S)$$
    - 추론 시 모델은 이전 토큰과 문맥 $C$에 기반하여 다음 토큰 $\hat{r}_q_t$를 예측합니다.
    - 이산 토큰 $\hat{R}_q$는 보코더(Dec)를 통해 음성 신호 $\hat{X}_R$로 복구됩니다.
  - **듣기 능력 구현:** 발화 채널과 동일한 스트리밍 SSL 인코더(Enc)를 사용하여 실시간 오디오 입력 $X_S$를 처리하고, 이를 AR 모델이 처리할 수 있는 공간으로 투영(Proj)합니다.
    - 오디오 입력 $X_S$는 SSL 인코더를 통해 임베딩 $S$로 변환되고, 프로젝션 모듈(Proj)을 통해 $S_p$로 변환됩니다.
- **FDM 능력 (채널 융합):** LSLM은 발화 채널의 이전 정보($R_{q_{1:t-1}}$)와 듣기 채널의 처리된 정보($S_{p_{1:t-1}}$)를 동시에 고려하여 다음 토큰 $r_t$를 예측합니다. 턴-테이킹 발생 시 조기 종료를 위한 '인터럽션 토큰(IRQ)'을 추가합니다.
  - 학습 손실 함수는 다음과 같이 수정됩니다:
    $$L(\theta_{LS}) = \begin{cases} -\sum_{t=1}^{t_{IRQ}} \log P(r_q_t | R_{q_{1:t-1}}, S_{p_{1:t-1}}, C; \theta_{LS}) & \text{if turn-taking,} \\ -\sum_{t=1}^{t_{EOS}} \log P(r_q_t | R_{q_{1:t-1}}, S_{p_{1:t-1}}, C; \theta_{LS}) & \text{otherwise.} \end{cases}$$
  - 추론 시 모델은 $\hat{r}_q_t \sim P(r_q_t | \hat{R}_{q_{1:t-1}}, S_{p_{1:t-1}}, C; \theta_{LS})$에 따라 샘플링합니다.
  - **세 가지 융합 전략 탐색:**
    - **Early Fusion:** 입력 임베딩 단계에서 듣기/말하기 채널을 통합합니다.
    - **Middle Fusion:** 각 Transformer 블록에서 듣기 채널을 추가로 입력에 통합합니다.
    - **Late Fusion:** 소프트맥스 연산 전 출력 로짓 단계에서 채널을 결합합니다.
- **모델 및 학습 상세:**
  - **모델 구조:** 106M 파라미터의 디코더-온리 Transformer (12개 블록, 12개 어텐션 헤드, 768 임베딩 차원).
  - **SSL 인코더:** vq-wav2vec (34M 파라미터), 스트리밍 오디오 특성 추출에 적합.
  - **보코더:** GAN 기반 토큰-음성 파형 보코더.
  - **데이터:**
    - TTS: LibriTTS 데이터셋.
    - 노이즈: MUSAN 데이터셋의 Freesound 부분(고주파/저주파 노이즈 포함).
    - 가로채기: 명령어 기반 FDM은 "Honey" 명령어, 음성 기반 FDM은 Speech Commands 데이터셋(다양한 단어, 미학습 화자 포함).
  - **학습:** 20 에포크 동안 TTS, 가로채기, 노이즈 데이터셋을 사용. 듣기 토큰에 노이즈(50%)와 가로채기(50%)를 무작위로 추가. 가로채기 시작 후 0.5초 이내에 IRQ 토큰을 출력하도록 문장을 수정. AdamW 옵티마이저 사용.

## 📊 실험 결과

- **평가 지표:**
  - **TTS 능력:** 생성된 음성과 원본 텍스트를 비교하는 WER (Word Error Rate).
  - **상호작용 능력:** Precision, Recall, F1 점수. 성공적인 턴-테이킹은 가로채기 시작 후 $[0, 2\mu]$ (1초) 이내에 모델이 발화를 중단하는 경우로 정의.
- **명령어 기반 FDM (표 2):**
  - **LSLM_MF (Middle Fusion):** 가장 좋은 성능을 보였습니다.
    - 클린 환경: WER 4.05%, F1 98.00%.
    - 노이즈 환경: WER 4.51%, F1 97.38%.
  - **LSLM_EF (Early Fusion):** WER이 33.56%로 크게 저하되어 입력 임베딩 융합이 듣기/말하기 채널 구분을 어렵게 함을 시사합니다.
  - **LSLM_LF (Late Fusion):** 노이즈 환경에서 WER이 6.87%로 증가하고 F1이 94.89%로 저하되어 노이즈와 인간 음성을 구분하는 능력이 감소했습니다.
- **음성 기반 FDM (표 3):** (Middle Fusion 아키텍처 사용)
  - 다양한 가로채기 명령어와 미학습 화자로 인해 명령어 기반 FDM보다 더 어려운 환경이었습니다.
  - 클린 환경: WER 5.33%, F1 95.50%.
  - 노이즈 환경: WER 8.50%, F1 85.15%. 노이즈 환경에서 성능 저하가 두드러졌습니다.
- **가로채기 시각화 (그림 4):**
  - LSLM의 IRQ 토큰 확률 분포 시각화 결과, 모델이 가로채기 신호를 감지하면 IRQ 토큰 확률이 급격히 증가하여 실시간 턴-테이킹 감지 능력을 보여주었습니다.
- **어블레이션 스터디 (표 4):**
  - TTS 백본과 스트리밍 SSL 인코더 모두 사전 학습 모델을 로드하고 **계속 학습(✚&✚)** 시켰을 때 가장 좋은 성능(WER 4.05%, F1 98.00%)을 보였습니다.
  - SSL 인코더 파라미터를 고정(✓)한 경우 성능이 저하되었는데, 이는 SSL 인코더가 다양한 노이즈에 대한 추가 학습이 필요함을 시사합니다.

## 🧠 고찰 및 논의

- LSLM은 음성 생성 능력에 미치는 영향을 최소화하면서 동시 청취 및 발화(전이중 통신)를 성공적으로 구현했습니다.
- **Middle Fusion**은 음성 생성 품질과 실시간 상호작용 능력 사이에서 최적의 균형을 제공하는 효과적인 융합 전략으로 입증되었습니다.
- 이 모델은 노이즈 환경에 대한 강건성과 미학습 화자의 다양한 명령어에 대한 민감성을 동시에 보여주었습니다.
- 특히, 듣기 채널의 SSL 인코더를 지속적으로 학습시키는 것이 다양한 노이즈 환경에서 모델의 견고한 성능을 유지하는 데 중요합니다.
- **한계 및 향후 연구:** 본 연구는 전이중 대화형 음성 언어 모델에 대한 초기 탐색이며, 인간-컴퓨터 음성 상호작용의 자연스러움을 향상시키기 위한 추가적인 연구가 필요합니다. 향후 연구 방향으로는 전이중 모델링 기능을 갖춘 Speech-in Speech-out 대화 시스템 개발, 가로채기 화자를 식별하는 화자 추적(speaker-following) 기능 통합, 그리고 턴-테이킹 개선을 위한 시청각 공동 유도(audiovisual co-guidance) 탐색 등이 있습니다.

## 📌 TL;DR

이 논문은 대화형 음성 언어 모델에서 전이중 통신(발화 중 청취)을 가능하게 하는 종단 간 모델 LSLM을 제안합니다. LSLM은 토큰 기반 TTS와 스트리밍 SSL 인코더를 사용하여 실시간 턴-테이킹을 처리하며, 이들을 Middle Fusion 방식으로 융합할 때 음성 생성과 상호작용 능력 간 최적의 균형을 이룹니다. 명령어 기반 및 음성 기반 FDM 환경에서의 실험을 통해 LSLM이 노이즈에 강건하고 다양한 지시에 민감하게 반응하여 실시간 인간-컴퓨터 음성 상호작용을 크게 발전시킴을 입증했습니다.
