{
  "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition",
  "authors": "Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus Müller, Sebastian Stüker, Alexander Waibel",
  "year": 2019,
  "url": "http://arxiv.org/abs/1904.13377v2",
  "abstract": "Recently, end-to-end sequence-to-sequence models for speech recognition have\ngained significant interest in the research community. While previous\narchitecture choices revolve around time-delay neural networks (TDNN) and long\nshort-term memory (LSTM) recurrent neural networks, we propose to use\nself-attention via the Transformer architecture as an alternative. Our analysis\nshows that deep Transformer networks with high learning capacity are able to\nexceed performance from previous end-to-end approaches and even match the\nconventional hybrid systems. Moreover, we trained very deep models with up to\n48 Transformer layers for both encoder and decoders combined with stochastic\nresidual connections, which greatly improve generalizability and training\nefficiency. The resulting models outperform all previous end-to-end ASR\napproaches on the Switchboard benchmark. An ensemble of these models achieve\n9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This\nfinding brings our end-to-end models to competitive levels with previous hybrid\nsystems. Further, with model ensembling the Transformers can outperform certain\nhybrid systems, which are more complicated in terms of both structure and\ntraining procedure.",
  "citation": 217
}