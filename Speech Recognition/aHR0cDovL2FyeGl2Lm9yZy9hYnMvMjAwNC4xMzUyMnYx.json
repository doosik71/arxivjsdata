{
  "title": "Research on Modeling Units of Transformer Transducer for Mandarin Speech\n  Recognition",
  "authors": "Li Fu, Xiaoxiao Li, Libo Zi",
  "year": 2020,
  "url": "http://arxiv.org/abs/2004.13522v1",
  "abstract": "Modeling unit and model architecture are two key factors of Recurrent Neural\nNetwork Transducer (RNN-T) in end-to-end speech recognition. To improve the\nperformance of RNN-T for Mandarin speech recognition task, a novel transformer\ntransducer with the combination architecture of self-attention transformer and\nRNN is proposed. And then the choice of different modeling units for\ntransformer transducer is explored. In addition, we present a new mix-bandwidth\ntraining method to obtain a general model that is able to accurately recognize\nMandarin speech with different sampling rates simultaneously. All of our\nexperiments are conducted on about 12,000 hours of Mandarin speech with\nsampling rate in 8kHz and 16kHz. Experimental results show that Mandarin\ntransformer transducer using syllable with tone achieves the best performance.\nIt yields an average of 14.4% and 44.1% relative Word Error Rate (WER)\nreduction when compared with the models using syllable initial/final with tone\nand Chinese character, respectively. Also, it outperforms the model based on\nsyllable initial/final with tone with an average of 13.5% relative Character\nError Rate (CER) reduction.",
  "citation": 7
}