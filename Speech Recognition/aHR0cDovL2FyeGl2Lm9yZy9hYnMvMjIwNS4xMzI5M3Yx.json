{
  "title": "Joint Training of Speech Enhancement and Self-supervised Model for\n  Noise-robust ASR",
  "authors": "Qiu-Shi Zhu, Jie Zhang, Zi-Qiang Zhang, Li-Rong Dai",
  "year": 2022,
  "url": "http://arxiv.org/abs/2205.13293v1",
  "abstract": "Speech enhancement (SE) is usually required as a front end to improve the\nspeech quality in noisy environments, while the enhanced speech might not be\noptimal for automatic speech recognition (ASR) systems due to speech\ndistortion. On the other hand, it was shown that self-supervised pre-training\nenables the utilization of a large amount of unlabeled noisy data, which is\nrather beneficial for the noise robustness of ASR. However, the potential of\nthe (optimal) integration of SE and self-supervised pre-training still remains\nunclear. In order to find an appropriate combination and reduce the impact of\nspeech distortion caused by SE, in this paper we therefore propose a joint\npre-training approach for the SE module and the self-supervised model. First,\nin the pre-training phase the original noisy waveform or the waveform obtained\nby SE is fed into the self-supervised model to learn the contextual\nrepresentation, where the quantified clean speech acts as the target. Second,\nwe propose a dual-attention fusion method to fuse the features of noisy and\nenhanced speeches, which can compensate the information loss caused by\nseparately using individual modules. Due to the flexible exploitation of\nclean/noisy/enhanced branches, the proposed method turns out to be a\ngeneralization of some existing noise-robust ASR models, e.g., enhanced\nwav2vec2.0. Finally, experimental results on both synthetic and real noisy\ndatasets show that the proposed joint training approach can improve the ASR\nperformance under various noisy settings, leading to a stronger noise\nrobustness.",
  "citation": 22
}