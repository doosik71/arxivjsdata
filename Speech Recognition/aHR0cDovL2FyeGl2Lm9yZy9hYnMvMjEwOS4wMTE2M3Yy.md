# EFFICIENT CONFORMER: PROGRESSIVE DOWNSAMPLING AND GROUPED ATTENTION FOR AUTOMATIC SPEECH RECOGNITION

Maxime Burchi, Valentin Vielzeuf

## 🧩 Problem to Solve

Conformer 아키텍처는 음성 인식(ASR) 분야에서 최첨단(state-of-the-art) 성능을 보여주었지만, 그 높은 계산 복잡성은 제한된 컴퓨팅 자원 환경(예: 단일 CPU에서의 추론)에서의 실제 적용을 어렵게 만듭니다. 대규모 GPU를 필요로 하는 학습 비용과 긴 추론 시간은 Conformer 모델의 주요 제약 사항입니다. 따라서 본 논문은 제한된 컴퓨팅 예산(예: 4개의 Nvidia RTX 2080 Ti GPU) 내에서 Conformer의 복잡성을 줄이고 효율성을 높여, 학습 및 추론 속도를 개선하면서도 인식 성능을 유지하거나 향상시키는 효율적인 아키텍처 설계에 중점을 둡니다.

## ✨ Key Contributions

- **프로그레시브 다운샘플링 도입:** Conformer 인코더에 프로그레시브 다운샘플링을 도입하여 더 적은 곱셈-덧셈(multiply-adds) 연산으로 더 나은 인식 성능과 빠른 디코딩 속도를 달성하는 효율적인 아키텍처를 제안했습니다.
- **새로운 그룹화된 어텐션(Grouped Attention) 메커니즘 제안:** 어텐션 복잡도를 시퀀스 길이 $n$, 은닉 차원 $d$, 그룹 크기 $g$에 대해 $O(n^2d)$에서 $O(n^2d/g)$로 줄여, Efficient Conformer 모델의 학습 및 디코딩 시간을 단축하면서 유사한 인식 성능을 유지했습니다.
- **CTC 및 RNN-T 기준 비교 연구:** 제한된 학습 예산 환경에서 Conformer를 오리지널 RNN-T 방식과 CTC 방식으로 학습했을 때의 이점을 비교 분석했습니다.
- **소형 Efficient Conformer 모델의 경쟁력 있는 성능 입증:** 경쟁력 있는 인식 성능을 가진 작고 효율적인 Conformer 모델을 제시했습니다.

## 📎 Related Works

- **ASR 아키텍처:** RNN (LSTM, GRU), CNN (Jasper, QuartzNet, CitriNet), Transformer 아키텍처가 ASR에 사용되었습니다.
- **로컬/글로벌 의존성 모델링:**
  - CNN의 글로벌 컨텍스트 강화를 위한 Squeeze-and-excitation (SE) 메커니즘이 연구되었습니다.
  - 트랜스포머 네트워크에 컨볼루션을 추가하여 로컬 및 글로벌 의존성을 모두 모델링하는 Conformer가 최첨단 결과를 달성했습니다.
- **모델 복잡도 감소 기법:** 가중치 공유(weights sharing), 가지치기(pruning), 양자화(quantization), 지식 증류(knowledge distillation), 저랭크 분해(low-rank decomposition) 및 효율적인 아키텍처 설계 등이 있으며, 본 연구는 효율적인 아키텍처 설계에 초점을 맞춥니다.
- **효율적인 어텐션 메커니즘:** 효율적인 어텐션(Efficient attention), Prob-sparse 메커니즘, 로컬 어텐션(Local attention) (쿼리 위치 주변의 로컬 이웃으로 어텐션 범위를 제한) 등의 연구가 진행되었습니다.
- **프로그레시브 다운샘플링:** ASR에서 CNN의 계산 비용을 줄이기 위해 모델의 하단에서 상단으로 점진적으로 다운샘플링을 적용하는 방식이 제안되었습니다.

## 🛠️ Methodology

본 논문은 Conformer의 복잡도를 줄이기 위해 프로그레시브 다운샘플링, 효율적인 셀프-어텐션 메커니즘, 그리고 학습 기준(CTC/RNN-T) 비교를 활용합니다.

1. **프로그레시브 다운샘플링 (Progressive Downsampling)**

   - **인코더 구조:** Efficient Conformer 인코더는 $3 \times 3$ 컨볼루션 스템(stride 2)으로 초기 오디오 특징을 다운샘플링한 후, 세 가지 인코더 스테이지로 구성됩니다. 각 스테이지는 여러 Conformer 블록으로 이루어집니다.
   - **다운샘플링 모듈:**
     - **컨볼루션 다운샘플링 모듈:** 첫 번째 및 두 번째 인코더 스테이지의 마지막 블록에서 시퀀스 다운샘플링이 수행됩니다. 오리지널 컨볼루션 모듈을 strided depthwise convolution을 포함하는 컨볼루션 다운샘플링 모듈로 대체하여 시퀀스 길이를 줄입니다. 채널 수는 gated linear unit (GLU)과 함께 투영됩니다.
     - **어텐션 다운샘플링 모듈:** strided attention을 사용하여 멀티 헤드 셀프 어텐션(MHSA)을 전역 다운샘플링 연산으로 활용하는 실험도 진행됩니다. 쿼리(query)를 시간 차원을 따라 스트라이딩하여 시퀀스 길이를 줄입니다.
   - **특징 차원 확장:** $8 \times$ 프로그레시브 다운샘플링이 시간 차원을 따라 수행되며, 인코딩된 시퀀스는 각 인코더 스테이지의 은닉 계층 복잡도를 일정하게 유지하기 위해 점진적으로 더 넓은 특징 차원으로 투영됩니다.

2. **효율적인 셀프-어텐션 (Efficient Self-Attention)**

   - **그룹화된 멀티 헤드 셀프 어텐션 (Grouped Multi-Head Self-Attention):**
     - 어텐션의 $O(n^2d)$ 복잡도 문제를 해결하기 위해 제안된 메커니즘입니다.
     - 쿼리, 키, 값, 상대 위치 임베딩을 특징 차원을 따라 인접한 요소들을 그룹화하여 재구성합니다: $Q, K, V \in \mathbb{R}^{n \times d}$에서 $Q_{grp}, K_{grp}, V_{grp} \in \mathbb{R}^{n' \times d'}$로 변환되며, 여기서 $n' = n/g$이고 $d' = d \times g$입니다 ($g$는 그룹 크기).
     - 이를 통해 어텐션 복잡도가 $O(n^2d/g)$로 감소합니다.
     - 주로 시퀀스 길이가 가장 긴 인코더의 첫 번째 스테이지의 어텐션 계층에 적용되어 계산 비대칭성을 줄입니다.
   - **로컬 멀티 헤드 셀프 어텐션 (Local Multi-Head Self-Attention):** 비교 목적으로 사용되며, 어텐션 윈도우 $w_{att}$를 정의하고, 은닉 시퀀스를 해당 크기의 블록으로 분할하여 각 블록 내에서만 어텐션을 수행합니다.

3. **학습 기준 (Loss Criteria)**
   - **RNN-T (Recurrent Neural Network Transducer):** 오디오 인코더, 레이블 디코더, 조인트 네트워크를 결합하여 입출력 및 출력 간의 의존성을 모두 모델링합니다.
   - **CTC (Connectionist Temporal Classification):** 인코더 출력에 최종 소프트맥스 계층을 추가하여 직접 확률로 변환하며, 디코딩 시간이 빠릅니다.

## 📊 Results

- **경쟁력 있는 성능:** 13M 파라미터 Efficient Conformer CTC 모델은 LibriSpeech test-clean/test-other 세트에서 외부 LM 없이 3.57%/8.99% WER을 달성했으며, 외부 6-gram LM 사용 시 2.72%/6.66% WER을 달성했습니다. 이는 21M 파라미터 Citrinet-384 모델을 능가하는 결과입니다.
- **상당한 속도 개선:**
  - Conformer CTC 베이스라인 대비 **추론 시간 29% 단축** (단일 CPU 스레드 기준).
  - Conformer CTC 베이스라인 대비 **학습 시간 36% 단축** (4개의 Nvidia RTX 2080 Ti GPU 기준).
  - 특히, 첫 번째 스테이지에 그룹화된 어텐션($g=3$)을 도입하여 21% 추론 속도 향상과 35% 학습 속도 향상을 이루면서도 유사한 인식 성능을 유지했습니다.
- **프로그레시브 다운샘플링의 효과:** 프로그레시브 다운샘플링은 더 적은 MAdds로 더 나은 정확도를 달성하며 학습 및 디코딩 시간을 단축했습니다 (특히 dev-other 세트에서 WER 개선).
- **다운샘플링 방법 비교:** strided attention을 사용한 어텐션 다운샘플링은 컨볼루션 다운샘플링과 유사한 성능을 보였고, Efficient Conformer 모델의 디코딩 시간을 약간 줄이는 효과가 있었습니다.
- **그룹화된 어텐션의 효과:** 그룹화된 어텐션은 특히 초기 어텐션 계층에서 모델 복잡도와 메모리 비용을 효과적으로 줄이면서 인식 성능에는 큰 영향을 미치지 않았습니다.
- **로컬 어텐션과의 비교:** 로컬 어텐션은 특정 윈도우 크기($w_{att}=175$)에서 일반 MHSA와 유사한 성능을 보였지만, 윈도우 크기를 더 제한하면 성능이 저하되었고, 시퀀스 패딩 오버헤드 때문에 그룹화된 어텐션보다 디코딩 속도가 느렸습니다.
- **장기 시퀀스에서의 메모리 사용량:** 그룹화된 멀티 헤드 어텐션을 모든 스테이지에 적용하면 긴 시퀀스에 대한 메모리 소비를 크게 줄일 수 있었고, 일반 MHSA를 사용하는 Conformer 모델보다 우수했습니다.
- **CTC vs. RNN-T:** RNN-T 모델이 더 빠르게 수렴하고 더 낮은 greedy WER을 달성했지만, 외부 LM을 사용한 빔 서치 시 CTC 모델이 WER 격차를 성공적으로 줄일 수 있음을 확인했습니다.

## 🧠 Insights & Discussion

- **효율성과 성능의 성공적인 균형:** 본 연구는 프로그레시브 다운샘플링과 그룹화된 어텐션의 결합을 통해 컴퓨팅 자원이 제한된 환경에서도 Conformer의 높은 ASR 성능을 유지하면서 학습 및 추론 효율성을 크게 개선할 수 있음을 강력하게 시사합니다.
- **어텐션 계산 비대칭성 해소:** 그룹화된 어텐션은 어텐션의 제곱 복잡도로 인해 발생하는 초기 계층의 높은 계산 부담 문제를 효과적으로 해결하는 혁신적인 접근 방식임을 입증했습니다. 이는 특히 긴 시퀀스 처리에 유용합니다.
- **다운샘플링 역할 확장:** strided multi-head self-attention이 컨볼루션 다운샘플링에 필적하는 정확도를 달성하며 더 빠르게 동작할 수 있다는 발견은 어텐션 메커니즘의 활용 범위를 단순히 특징 학습을 넘어 시퀀스 길이 감소 연산으로 확장할 수 있음을 보여줍니다.
- **CTC 모델의 경쟁력:** 제한된 자원 환경에서 CTC 손실 함수가 RNN-T에 필적하는, 또는 외부 LM과 결합 시 더 나은 WER을 달성할 수 있음을 보여줌으로써, 디코딩 속도가 중요한 시나리오에서 CTC 모델의 잠재력을 재확인했습니다.
- **향후 연구 방향:** 그룹화된 어텐션의 최적 그룹 크기 탐색, 선형 어텐션(linear attention) 등 다른 효율적인 어텐션 변형과의 비교 연구, 그리고 가중치 가지치기(weights pruning) 및 양자화(quantization)와 같은 상호 보완적인 복잡도 감소 기법의 적용을 통해 효율성을 더욱 극대화할 수 있을 것입니다.

## 📌 TL;DR

Conformer 모델의 높은 계산 복잡성을 해결하기 위해 본 논문은 **Efficient Conformer**를 제안합니다. 주요 방법론은 Conformer 인코더에 **프로그레시브 다운샘플링**을 적용하여 시퀀스 길이를 점진적으로 줄이고, 새로운 **그룹화된 어텐션** 메커니즘을 통해 어텐션 복잡도를 $O(n^2d)$에서 $O(n^2d/g)$로 효율적으로 감소시키는 것입니다. 또한, strided multi-head self-attention이 글로벌 다운샘플링 연산으로 효과적으로 사용될 수 있음을 보여주었습니다. LibriSpeech 데이터셋 실험 결과, 13M 파라미터 Efficient Conformer CTC 모델은 Conformer 베이스라인 대비 **29% 빠른 추론**과 **36% 빠른 학습 속도**를 달성하면서도, 외부 LM 사용 시 test-clean/test-other에서 **2.7%/6.7%의 경쟁력 있는 WER**을 기록했습니다. 이는 제한된 컴퓨팅 자원 환경에서 고성능 ASR 모델의 효율성을 크게 향상시키는 중요한 기여입니다.
