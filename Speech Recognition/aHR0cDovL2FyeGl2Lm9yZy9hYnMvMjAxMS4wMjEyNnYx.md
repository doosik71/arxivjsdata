# Incremental Machine Speech Chain: Towards Enabling Listening while Speaking in Real-time

Sashi Novitasari, Andros Tjandra, Tomoya Yanagita, Sakriani Sakti, Satoshi Nakamura

## 🧩 Problem to Solve

기존의 딥러닝 기반 기계 음성 사슬(Machine Speech Chain) 프레임워크는 사람의 "말하면서 듣기" 메커니즘을 모방하여 ASR(자동 음성 인식) 및 TTS(텍스트 음성 변환) 시스템을 동시에 개발할 수 있도록 하지만, 전체 입력 시퀀스를 받은 후에만 동작합니다. 이는 긴 발화(utterance)에서 상당한 지연을 발생시켜, 사람이 실시간으로 자신의 말을 듣고 다음 발화를 계획하는 방식과 달리 기계가 실시간으로 피드백을 처리할 수 없게 만듭니다. 이러한 지연은 시스템의 실용성을 저해하며, 사람에게 지연된 청각 피드백(DAF)이 스트레스를 유발하는 것처럼 기계에게도 비효율을 초래합니다. 따라서, 이 연구는 기존 기계 음성 사슬의 지연 문제를 해결하고, 기계가 사람처럼 실시간으로 "말하면서 들을" 수 있도록 하는 것을 목표로 합니다.

## ✨ Key Contributions

- **증분형 기계 음성 사슬(Incremental Machine Speech Chain) 프레임워크 제안**: ISR(증분형 ASR)과 ITTS(증분형 TTS)가 단기 루프(short-term loop)를 통해 함께 개선되도록 함으로써, 기계가 실시간으로 "말하면서 듣는" 능력을 갖추게 합니다.
- **지연 시간 대폭 감소**: 긴 발화 처리 시 발생하는 지연을 현저히 줄였습니다. 예를 들어, ISR은 비증분형 ASR에 비해 지연 시간을 $7.88$초에서 $0.84$초로, ITTS는 평균 $103$자에서 $30$자로 단축시켰습니다.
- **성능 유지**: 지연 시간을 대폭 줄였음에도 불구하고, 비증분형 기본 기계 음성 사슬과 비교하여 유사한 성능을 유지했습니다.
- **실시간 피드백 생성 가능성 입증**: 제안된 프레임워크가 추론 과정에서 실시간 피드백 생성을 가능하게 함으로써, 사람이 주변 환경에 비지도 방식으로 적응하는 것과 유사하게 ASR 또는 TTS가 동시에 적응할 수 있는 시스템에 한 걸음 더 다가섰음을 보여주었습니다.

## 📎 Related Works

- **ISR 및 ITTS 연구**: 동시 통역 시스템 개발을 위해 많은 연구가 진행되었습니다.
  - **기존 ASR 접근 방식**: HMM(은닉 마르코프 모델) 기반 ASR은 실시간 음성 인식이 가능했지만, 딥러닝 기반의 최신 종단 간(end-to-end) 인식은 불가능했습니다.
  - **신경망 기반 ISR**: Jaitly et al. [21]은 고정된 윈도우로 음성을 세그먼트별로 인식하는 신경 변환기 프레임워크를 제안했습니다. Novitasari et al. [22]의 AT-ISR(Attention-Transfer ISR)은 비증분형 ASR로부터 어텐션 전이(attention transfer)를 통해 낮은 지연으로 종단 간 음성 인식을 학습합니다.
  - **ITTS 연구**: 주로 HMM 기반 모델 [23, 24, 25, 26]에 집중되었습니다. Yanagita et al. [27]은 종단 간 신경망 TTS 프레임워크를 사용하여 실시간 음성 합성을 시도한 첫 연구입니다. 최근에는 prefix-to-prefix 프레임워크 [28] 기반의 ITTS가 제안되었습니다.
- **기계 음성 사슬**: Tjandra et al. [7, 8, 9, 10]은 인간의 음성 사슬 메커니즘에서 영감을 받아, ASR과 TTS 시스템의 동시적, 반지도 학습을 위한 기계 음성 사슬 프레임워크를 제안했습니다. 본 연구는 이 기본 기계 음성 사슬 [8]과 어텐션 전이 [22]를 기반으로 합니다.

## 🛠️ Methodology

이 연구는 인간의 음성 사슬 메커니즘을 밀접하게 모방하기 위해 증분형 기계 음성 사슬을 제안하며, ISR과 ITTS를 단기(short-term) 폐쇄 루프를 통해 개선합니다.

1. **기본 원리**:

   - 기존 기계 음성 사슬과 유사하게 ISR과 ITTS를 연결하는 루프를 설정합니다.
   - 주요 차이점은 데이터 전달이 완전한 발화를 기다리지 않고 짧은 시간 내에 이루어진다는 것입니다.

2. **구성 요소**:

   - **Seq2seq 증분형 음성 인식 시스템 (ISR)**:
     - AT-ISR [22]을 사용하며, 비증분형 ASR(교사 모델)로부터 어텐션 정렬(attention alignment)을 모방하도록 학습됩니다.
     - 음성 발화 $X$를 $N$개의 인식 단계로 나누어 처리합니다. 각 단계 $n$에서 $W$개의 음성 프레임으로 구성된 $X_{n}$을 인코딩하고, $K_{n}$개의 텍스트 토큰으로 구성된 $\hat{Y}_{n}$을 예측합니다.
     - 입력 윈도우를 $W$ 프레임만큼 이동하고 모델 상태를 유지합니다.
   - **Seq2seq 증분형 텍스트 음성 변환 시스템 (ITTS)**:
     - 비증분형 ASR로부터의 어텐션 전이를 사용하여 구성됩니다.
     - 각 단계 $n$에서 $K_{n}$개의 토큰으로 구성된 텍스트 세그먼트 $Y_{n}$을 인코딩하여 $W_{n}$개의 음성 프레임으로 구성된 $\hat{X}_{n}$을 생성합니다.
     - 입력 윈도우를 $K_{n}$ 토큰만큼 이동하고 모델 상태를 유지합니다.

3. **ISR 및 ITTS 학습 메커니즘 (두 단계)**:

   - **1단계: ISR 및 ITTS 독립적 지도 학습**:
     - 음성-텍스트 쌍 데이터($\text{SI-84}$ 및 $\text{SI-284}$ 세트)를 사용하여 ISR과 ITTS를 독립적으로 훈련합니다.
     - 교사 모델인 비증분형 ASR로부터 어텐션 전이를 적용하여 증분형 시스템을 학습시킵니다.
   - **2단계: 단기 폐쇄 루프를 통한 ISR-ITTS 공동 학습**:
     - $\text{SI-200}$ 세트와 같이 레이블이 없는 대규모 데이터를 사용하여 ISR과 ITTS가 서로를 지원하여 공동으로 개선되도록 합니다.
     - **ISR-to-ITTS**: 각 인식 단계 $n$에서 ISR은 음성 세그먼트 $X_{n}$을 처리하여 출력 세그먼트 $\hat{Y}_{n}$을 생성하고, ITTS는 $\hat{Y}_{n}$을 인코딩하여 음성 세그먼트 $\hat{X}_{n}$을 재구성합니다. 손실은 각 증분 단계의 ITTS 손실을 평균하여 계산됩니다:
       $$LossITTS = \frac{1}{N} \sum_{n=1}^{N} LossITTS_{n}(X_{n}, \hat{X}_{n})$$
     - **ITTS-to-ISR**: 각 단계 $n$에서 ITTS는 텍스트 세그먼트 $Y_{n}$을 받아 음성 세그먼트 $\hat{X}_{n}$을 합성하고, ISR은 이 ITTS 음성 세그먼트 $\hat{X}_{n}$을 텍스트 $\hat{Y}_{n}$으로 변환합니다. 손실은 각 $Y_{n}$과 $\hat{Y}_{n}$ 쌍을 기반으로 계산되며, 평균됩니다:
       $$LossISR = \frac{1}{N} \sum_{n=1}^{N} LossISR_{n}(Y_{n}, \hat{Y}_{n})$$
   - **중간 출력 생성 접근 방식**:
     - **교사 강제(Teacher-forcing) 접근 방식**: 레이블이 있는 데이터를 사용하여, 루프의 첫 번째 시스템이 교사 강제 디코딩 메커니즘을 통해 출력 시퀀스를 생성합니다.
     - **탐욕적(Greedy) 접근 방식**: 레이블이 없는 데이터를 사용하여, 루프의 첫 번째 시스템이 탐욕적 디코딩 메커니즘을 통해 출력 시퀀스를 생성합니다.

4. **문맥적 입력 활용**: ISR과 ITTS는 주 입력 세그먼트 전후의 블록(look-back 및 look-ahead blocks)을 활용하여 정보의 풍부함을 더합니다. ISR의 입력 세그먼트는 4개의 주요 음성 블록과 2개의 look-back, 4개의 look-ahead 블록으로 구성됩니다.

## 📊 Results

실험 결과는 제안된 증분형 프레임워크가 긴 발화에서 발생하는 지연을 크게 줄이면서도 비증분형 기계 음성 사슬과 유사한 성능을 유지함을 보여줍니다.

- **지연 시간**:
  - **ISR**: 평균 지연이 $0.84$초로, 비증분형 ASR의 $7.88$초에 비해 대폭 감소했습니다.
  - **ITTS**: 평균 $30$자를 기다려 음성 합성을 시작하여, 비증분형 TTS가 평균 $103$자를 기다리는 것에 비해 훨씬 빠릅니다.
- **성능 비교 (CER for ASR, L2-norm for TTS)**:
  - **기준선 (독립적으로 훈련된 ISR/ITTS - SI-84)**:
    - ISR CER: $17.81\%$
    - ITTS L2-norm: $1.04$
  - **비증분형 최상위 시스템 (SI-284)**:
    - ASR CER: $7.16\%$
    - TTS L2-norm: $0.75$
  - **제안된 증분형 시스템 (교사 강제 공동 학습 - SI-84 + SI-200)**:
    - **ISR CER**: $9.43\%$ (천연 입력), $12.78\%$ (합성 입력)
    - **ITTS L2-norm**: $0.79$ (천연 입력), $1.26$ (합성 입력)
  - **비증분형 기계 음성 사슬 (교사 강제 공동 학습 - SI-84 + SI-200)**:
    - **ASR CER**: $7.27\%$ (천연 입력), $6.30\%$ (합성 입력)
    - **TTS L2-norm**: $0.77$ (천연 입력), $0.80$ (합성 입력)
- **결론**: 제안된 증분형 시스템은 비증분형 시스템에 비해 약간 높은 CER과 L2-norm 값을 보이지만, 지연 시간의 엄청난 감소를 고려할 때 경쟁력 있는 성능을 제공합니다. 특히, 기준선인 독립적으로 훈련된 ISR과 ITTS보다 훨씬 우수한 성능을 달성했습니다. ISR과 ITTS의 성능 향상은 자연 입력과 합성 입력 처리 모두에서 발생하여, 증분 시스템 간의 단기 피드백 루프가 학습 품질을 효과적으로 향상시킴을 입증했습니다.

## 🧠 Insights & Discussion

- **인간 음성 사슬 메커니즘 모방 성공**: 본 연구는 ISR과 ITTS를 단기 루프를 통해 함께 개선하도록 함으로써, 인간의 "말하면서 듣는" 과정을 기계에서 성공적으로 모방했습니다. 이는 기계가 실시간으로 자신의 출력을 평가하고 다음 동작을 계획하는 데 중요한 기반을 제공합니다.
- **실시간 피드백의 중요성**: 제안된 프레임워크는 실시간 피드백 생성을 가능하게 함으로써, ASR 또는 TTS 시스템이 외부 환경에 비지도 방식으로 동시에 적응할 수 있는 가능성을 열었습니다. 이는 시스템이 동적으로 변화하는 환경에 반응하고 자체적으로 개선될 수 있음을 시사합니다.
- **성능과 지연의 균형**: 비록 최상위 비증분형 시스템에 비해 성능이 약간 뒤처질 수 있지만, $7.88$초에서 $0.84$초로의 지연 시간 감소는 실시간 응용 분야에서 엄청난 이점입니다. 이는 성능의 미미한 감소를 감수하더라도 실시간 상호작용이 훨씬 더 중요할 수 있는 시나리오(예: 동시 통역, 대화형 AI)에 적합함을 의미합니다.
- **단기 피드백 루프의 효과**: ISR과 ITTS 간의 단기 피드백 루프가 시스템의 학습 품질을 효과적으로 향상시킨다는 점은, 각 구성 요소가 독립적으로 학습하는 것보다 상호 보완적인 학습이 더 강력하다는 것을 보여줍니다.

## 📌 TL;DR

이 논문은 기존 기계 음성 사슬의 긴 지연 문제를 해결하기 위해 **증분형 기계 음성 사슬** 프레임워크를 제안합니다. 이 프레임워크는 **증분형 ASR (ISR)**과 **증분형 TTS (ITTS)**를 단기 피드백 루프를 통해 함께 학습시켜, 기계가 사람처럼 **실시간으로 "말하면서 듣는" 능력**을 갖추도록 합니다. 결과적으로, 제안된 방법은 긴 발화에 대한 지연 시간을 (예: $7.88$초에서 $0.84$초로) **대폭 줄이면서도** 비증분형 시스템에 **비견할 만한 성능**을 유지하여, 실시간 상호작용이 가능한 음성 처리 시스템 개발에 중요한 진전을 이루었습니다.
