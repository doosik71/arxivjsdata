{
  "title": "InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss",
  "authors": "Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe",
  "year": 2022,
  "url": "http://arxiv.org/abs/2211.00795v2",
  "abstract": "This paper presents InterMPL, a semi-supervised learning method of end-to-end\nautomatic speech recognition (ASR) that performs pseudo-labeling (PL) with\nintermediate supervision. Momentum PL (MPL) trains a connectionist temporal\nclassification (CTC)-based model on unlabeled data by continuously generating\npseudo-labels on the fly and improving their quality. In contrast to\nautoregressive formulations, such as the attention-based encoder-decoder and\ntransducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in\ngeneral, owing to its simple/fast inference algorithm and robustness against\ngenerating collapsed labels. However, CTC generally yields inferior performance\nthan the autoregressive models due to the conditional independence assumption,\nthereby limiting the performance of MPL. We propose to enhance MPL by\nintroducing intermediate loss, inspired by the recent advances in CTC-based\nmodeling. Specifically, we focus on self-conditional and hierarchical\nconditional CTC, that apply auxiliary CTC losses to intermediate layers such\nthat the conditional independence assumption is explicitly relaxed. We also\nexplore how pseudo-labels should be generated and used as supervision for\nintermediate losses. Experimental results in different semi-supervised settings\ndemonstrate that the proposed approach outperforms MPL and improves an ASR\nmodel by up to a 12.1% absolute performance gain. In addition, our detailed\nanalysis validates the importance of the intermediate loss.",
  "citation": 3
}