{
  "url": "http://arxiv.org/abs/2103.08393v2",
  "title": "Wav2vec-C: A Self-supervised Model for Speech Representation Learning",
  "authors": "Samik Sadhu, Di He, Che-Wei Huang, Sri Harish Mallidi, Minhua Wu, Ariya Rastrow, Andreas Stolcke, Jasha Droppo, Roland Maas",
  "year": 2021,
  "abstract": "Wav2vec-C introduces a novel representation learning technique combining\nelements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized\nrepresentations from partially masked speech encoding using a contrastive loss\nin a way similar to Wav2vec 2.0. However, the quantization process is\nregularized by an additional consistency network that learns to reconstruct the\ninput features to the wav2vec 2.0 network from the quantized representations in\na way similar to a VQ-VAE model. The proposed self-supervised model is trained\non 10k hours of unlabeled data and subsequently used as the speech encoder in a\nRNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one\nof only a few studies of self-supervised learning on speech tasks with a large\nvolume of real far-field labeled data. The Wav2vec-C encoded representations\nachieves, on average, twice the error reduction over baseline and a higher\ncodebook utilization in comparison to wav2vec 2.0",
  "citation": 66
}