{
  "title": "Self-Attention Transducers for End-to-End Speech Recognition",
  "authors": "Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengqi Wen",
  "year": 2019,
  "url": "http://arxiv.org/abs/1909.13037v1",
  "abstract": "Recurrent neural network transducers (RNN-T) have been successfully applied\nin end-to-end speech recognition. However, the recurrent structure makes it\ndifficult for parallelization . In this paper, we propose a self-attention\ntransducer (SA-T) for speech recognition. RNNs are replaced with self-attention\nblocks, which are powerful to model long-term dependencies inside sequences and\nable to be efficiently parallelized. Furthermore, a path-aware regularization\nis proposed to assist SA-T to learn alignments and improve the performance.\nAdditionally, a chunk-flow mechanism is utilized to achieve online decoding.\nAll experiments are conducted on a Mandarin Chinese dataset AISHELL-1. The\nresults demonstrate that our proposed approach achieves a 21.3% relative\nreduction in character error rate compared with the baseline RNN-T. In\naddition, the SA-T with chunk-flow mechanism can perform online decoding with\nonly a little degradation of the performance."
}