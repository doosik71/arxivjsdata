# End-to-End Speech Recognition: A Survey

Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, Ralf Schlütter, and Shinji Watanabe

## 🧩 Problem to Solve

이 서베이 논문은 지난 10년간 딥러닝 도입 이후 자동 음성 인식(ASR) 분야에서 워드 에러율(WER)이 50% 이상 감소하는 상당한 발전을 이룬 "엔드-투-엔드(E2E) ASR" 모델의 등장과 확산에 대한 포괄적인 이해를 제공하는 것을 목표로 합니다. 고전적인 은닉 마르코프 모델(HMM) 기반 ASR 시스템은 음향 모델링, 언어 모델링 등 여러 독립적인 구성 요소로 분해되어 개발 및 통합에 많은 도메인 전문 지식이 필요했습니다. E2E 모델은 이러한 구성 요소들을 단일 신경망 아키텍처로 통합하여 모델링을 단순화하고 데이터로부터 더 일관되게 학습할 수 있도록 합니다.

이 논문은 다음을 포함하여 E2E ASR 모델과 관련된 주요 질문들을 다룹니다:

1. E2E ASR의 정확한 정의와 속성 분류.
2. 다양한 E2E 모델 아키텍처(예: CTC, RNN-T, Attention-based)의 작동 방식 및 상호 관계.
3. E2E 모델의 학습, 디코딩, 외부 언어 모델(LM) 통합 방법.
4. 고전적인 HMM 기반 ASR 시스템과의 비교 및 E2E 모델의 성능과 배포 기회.
5. 현재 한계점과 미래 연구 방향 제시.

## ✨ Key Contributions

이 서베이 논문은 E2E ASR 분야의 발전을 체계적으로 정리하고 분석하여 다음과 같은 핵심 기여를 합니다:

- **E2E ASR 모델 분류**: E2E ASR의 개념을 명확히 정의하고, 정렬(alignment) 모델링 방식(명시적 vs. 암묵적)에 따라 CTC, RNN-T, Attention-based Encoder-Decoder (AED) 모델, 그리고 이들을 결합한 모델들로 분류했습니다.
- **포괄적인 리뷰**: E2E ASR의 모델링, 학습(정렬, 외부 LM, 최소 오류 훈련, 사전 학습, 훈련 스케줄, 최적화/정규화, 데이터 증강), 디코딩(그리디, 빔 서치, 모델 퓨전, 다중 통과), 언어 모델 통합 등 모든 관련 측면을 심층적으로 다루었습니다.
- **고전 ASR과의 관계 분석**: E2E 모델과 고전적인 HMM 기반 ASR 아키텍처 간의 유사점, 차이점 및 상호 관계를 자세히 논의하여 각 접근 방식의 장단점을 명확히 했습니다.
- **성능 트렌드 및 배포 사례**: Librispeech 및 Switchboard와 같은 주요 벤치마크에서의 E2E ASR 성능 개선 추이를 제시하고, Google Pixel 스마트폰 시리즈의 온디바이스(on-device) ASR 시스템 배포 사례를 통해 실제 적용 가능성을 보여주었습니다.
- **미래 연구 방향 제시**: E2E ASR의 현재 한계점(예: 낮은 자원 환경, 효율적인 훈련, 모듈성 부족, 희귀어 처리, 텍스트 전용 데이터 활용, 길이 편향)을 식별하고, 멀티채널 ASR, 화자 적응 등 잠재적인 미래 연구 영역을 제시했습니다.

## 📎 Related Works

E2E ASR은 고전적인 ASR 시스템의 발전을 기반으로 하며, 이 논문은 여러 선행 연구들을 참조합니다.

- **고전적 ASR 시스템**:
  - **통계적 ASR**: Bayes 결정 규칙 [1], [2]에 기반하여 음향 특징 추출, 음향 모델링, 언어 모델링, 탐색의 4가지 주요 구성 요소로 분해됩니다.
  - **HMM 기반 음향 모델링**: 음성 속도 변화를 설명하며, 딥러닝 도입 이전에는 가우시안 혼합 모델(GMM)과 결합되었습니다 (하이브리드 HMM [3], [4]).
  - **딥러닝 도입**: 음향 모델링에서 GMM을 대체하거나(하이브리드 HMM), 음향 특징을 강화하고 [5], [6], 언어 모델링에서 카운트 기반 접근 방식을 대체했습니다 [7], [8], [9].
- **초기 E2E 모델**:
  - **Connectionist Temporal Classification (CTC)**: 입력 시퀀스를 출력 시퀀스로 직접 매핑하기 위한 기술로 제안되었으며 [12], 명시적 정렬 모델의 기초가 됩니다.
  - **Recurrent Neural Network Transducer (RNN-T)**: CTC 모델의 조건부 독립성 가정을 완화하여 개선된 모델로 제안되었습니다 [13], [42].
  - **Recurrent Neural Aligner (RNA)**: RNN-T 모델의 조건부 독립성 가정을 추가로 제거하여 일반화된 모델입니다 [33].
  - **Attention-based Encoder-Decoder (AED) / Listen-Attend-and-Spell (LAS)**: 기계 번역 [35]에서 대중화된 어텐션 메커니즘 [36]을 사용하여 음성 시퀀스와 출력 레이블 간의 대응 관계를 암묵적으로 학습합니다 [14], [15], [46].
- **결합 및 개선 모델**:
  - **CTC-Attention 결합**: 어텐션 모델의 긴 발화 처리 취약점을 보완하기 위해 CTC와 어텐션 모델을 멀티태스크 학습으로 결합하는 연구 [59].
  - **RNN-T 및 AED 결합**: RNN-T의 스트리밍 능력과 AED의 전체 문맥 활용 능력을 결합하는 2단계 디코딩 방식 [60].
  - **스트리밍 AED 모델**: 음성의 '지역적' 관계를 활용하여 컨텍스트 벡터를 제한된 프레임 범위에서 계산하는 Neural Transducer (NT) [37], Monotonic Alignment [38], Monotonic Chunkwise Attention (MoChA) [39], Monotonic Infinite Lookback (MILK) [40] 등의 연구.
  - **컨텍스트 바이어싱**: 외부 FST [65] 또는 신경망 기반 [66], [67], [68]으로 도메인 특정 용어 또는 대화 컨텍스트를 통합하는 연구.

## 🛠️ Methodology

이 서베이 논문은 E2E ASR 모델들을 체계적으로 분류하고 각 구성 요소별로 상세한 방법론을 설명합니다.

1. **E2E 모델 분류**:

   - **표기법**: 음성 입력 $X=(x_1, \dots, x_{T'})$ 및 해당 텍스트 시퀀스 $C=(c_1, \dots, c_L)$를 사용합니다. 모든 E2E 모델은 입력 음향 프레임을 고수준 표현 $H(X)=(h_1, \dots, h_T)$로 매핑하는 인코더 모듈 $H(X)$를 포함합니다. 목표는 조건부 확률 분포 $P(C|X)$를 추정하는 것입니다.
   - **정렬(Alignment) 모델링 방식에 따른 분류**:
     - **명시적 정렬 E2E 접근 방식**: 음향 프레임 시퀀스와 레이블 시퀀스 간의 정렬을 명시적 잠재 변수로 모델링합니다.
       - **Connectionist Temporal Classification (CTC)** [12]: 특수 "blank" 심볼 $\langle b \rangle$을 도입하여 인코더 출력과 레이블 시퀀스 간의 정렬을 모델링합니다. 현재 시점 $t$의 출력 $a_t$가 다른 시점의 출력과 독립적이라는 강한 가정을 합니다 ($P(a_t|a_{t-1}, \dots, a_1, H(X)) = P(a_t|h_t)$).
       - **Recurrent Neural Network Transducer (RNN-T)** [13], [42]: CTC의 독립성 가정을 일부 완화합니다. 각 인코더 프레임 $h_t$에 대해 0개 이상의 출력 심볼을 생성하고 하나의 blank 심볼로 종료합니다. 이전 비blank 예측 시퀀스 $c_1, \dots, c_{i_{\tau}}$에 조건화합니다 ($P(a_\tau|c_{i_\tau}, \dots, c_0, h_{\tau-i_\tau})$).
       - **Recurrent Neural Aligner (RNA)** [33]: RNN-T 모델을 더욱 일반화하여 조건부 독립성 가정을 더 줄입니다. 각 프레임에서 단일 레이블(blank 또는 비blank)만 출력하며, 이전 비blank 레이블 시퀀스뿐만 아니라 이들이 방출된 특정 프레임에도 조건화합니다 ($P(a_t|q_{t-1}, h_t)$).
     - **암묵적 정렬 E2E 접근 방식**: 어텐션 메커니즘을 사용하여 입력 음성 시퀀스와 개별 출력 단위 간의 대응 관계를 암묵적으로 학습합니다.
       - **Attention-based Encoder-Decoder (AED)** [14], [15], [46]: 전체 음향 시퀀스를 한 번에 처리하며 $\langle eos \rangle$ 심볼을 사용하여 출력 종료를 나타냅니다. 디코더 상태 $s_i$와 컨텍스트 벡터 $v_i$에 기반하여 각 출력 심볼 $c_i$를 예측합니다. 모델 출력과 입력 음향 간에 어떠한 독립성 가정도 하지 않습니다.
     - **정렬 모델링이 있는 Attention-based E2E 접근 방식**: AED 모델의 비스트리밍(non-streaming) 특성을 개선하기 위해 정렬 제약을 도입합니다.
       - **Neural Transducer (NT)** [37]: 인코더 프레임을 고정된 길이의 덩어리(chunk)로 분할하고, 한 번에 하나의 덩어리만 처리합니다.
       - **Monotonic Attention Variants** [38], [39], [40]: 왼쪽에서 오른쪽으로 인코더 프레임을 스캔하여 출력을 생성할 프레임을 식별하고, 국소적인 프레임 창에 대해 어텐션을 계산합니다.
       - **Triggered Attention** [52]: CTC 네트워크를 사용하여 AED 모델의 활성화를 제어합니다.

2. **아키텍처 개선**:

   - **모델 조합**: CTC와 어텐션 모델 [59], RNN-T와 AED [60] 등 여러 E2E 모델의 장점을 결합하여 성능을 향상시킵니다.
   - **컨텍스트 통합**: shallow-fusion contextual biasing [65] 또는 신경망 기반의 딥 컨텍스트 접근 방식 [66]을 사용하여 특정 도메인의 희귀 단어에 대한 편향(biasing)을 도입합니다.
   - **인코더/디코더 구조**: LSTM [30]에서 트랜스포머 [31], Conformer [73]와 같은 컨볼루션 및 셀프 어텐션 기반 아키텍처로 진화하여 하드웨어 병렬 처리 능력을 활용합니다.
   - **통합 엔드포인팅**: 스트리밍 시스템에서 빠른 마이크 종료를 위해 $\langle eos \rangle$ 토큰 예측을 모델에 통합합니다 [81], [82].

3. **E2E 모델 훈련**:

   - **정렬 처리**: CTC, RNN-T, RNA와 같은 명시적 정렬 모델은 순방향-역방향(forward-backward) 알고리즘 [87], [88]을 사용하여 모든 정렬에 걸쳐 주변화(marginalizing)하여 훈련합니다. AED 모델은 명시적 정렬을 위한 잠재 변수가 없습니다.
   - **계층적 훈련 스케줄**: CTC로 초기 정렬을 생성하고 RNN-T 훈련으로 이어지는 다단계 접근 방식 [94]과 같이 효율적인 수렴을 위한 훈련 스케줄을 사용합니다.
   - **외부 LM 훈련**: 훈련 단계에서 외부 LM을 통합하기 위해 최대 상호 정보(MMI) 시퀀스 차별적 훈련 [97]을 활용합니다 [98], [99], [100].
   - **최소 오류 훈련 (MET)**: 워드 에러율(WER) 최소화를 직접 목표로 하는 MWER [107], OCD [112] 등의 훈련 기준을 사용합니다.
   - **사전 훈련**: 지도 학습 기반 계층별 사전 훈련 [4], [116] 및 비전사 오디오 데이터를 활용한 자기 지도 학습 [22], [308]을 포함한 다양한 사전 훈련 전략을 사용합니다.
   - **훈련 스케줄 및 커리큘럼 학습**: 학습률 제어 [132], [133], 커리큘럼 학습 [142], [143] 등을 통해 최적화 프로세스를 안내합니다.
   - **최적화 및 정규화**: SGD [147], Adam [150] 등의 최적화 알고리즘과 L2 정규화, 드롭아웃 [157], SpecAugment [176] 등의 정규화 기법을 적용합니다.
   - **데이터 증강**: 속도 교란(speed perturbation) [172], SpecAugment [176], 텍스트-음성(TTS) 생성 데이터 [178], [179] 등을 활용하여 훈련 데이터의 양과 다양성을 늘립니다.

4. **E2E 모델 디코딩**:

   - **그리디 탐색**: CTC에서 주로 사용되며, 출력 레이블 간의 종속성을 무시하고 각 시점 $t$에서 가장 확률이 높은 레이블 $a_t$를 선택하여 빠르게 디코딩합니다 [12].
   - **빔 탐색**: 모든 가능한 가설 $U^*$ 중에서 유망한 부분 집합 $\tilde{C}$을 고려하여 디코딩 폭발을 방지합니다 [6].
     - **레이블 동기식 빔 탐색**: 모든 출력 토큰 $i$마다 가설을 가지치기합니다. AED에서 주로 사용되며, 출력 길이 편향을 완화하기 위해 휴리스틱이 필요합니다 [190].
     - **프레임 동기식 빔 탐색**: 모든 입력 프레임 $t$마다 가설을 가지치기합니다. CTC, RNN-T 모델에서 사용되며, WFST [41] 또는 어휘 접두사 트리 [195]를 기반으로 합니다.
   - **블록별 디코딩**: 고정 길이 블록 단위로 입력 특징을 처리하여 지연 시간과 정확도 간의 절충을 가능하게 합니다 [203], [206].
   - **디코딩 중 모델 퓨전**:
     - **동기식 점수 퓨전**: 여러 모듈의 점수가 동기화될 때, 메인 E2E ASR 점수에 LM 점수 $P_{lm}(C)$를 로그 선형적으로 추가하는 shallow fusion [208], [209], [210]을 사용합니다.
     - **비동기식 점수 퓨전**: 프레임 종속 점수 함수와 레이블 종속 점수 함수를 결합할 때, 언어 모델 점수를 레이블 전환 상태에 통합합니다 [211].
   - **어휘 제약**: WFST 기반 TLG 변환기 [194] 또는 단어 구분자(공백) 심볼에 의해 트리거되는 단어 기반 LM 점수를 삽입하여 어휘 제약을 통합합니다 [216].
   - **다중 통과 퓨전**: N-best 재채점 [15] 또는 RNN-T $\rightarrow$ 어텐션 전환 [221]과 같은 다중 통과 알고리즘을 사용하여 디코딩 품질을 향상시킵니다.
   - **가설 및 발화 전반의 벡터화**: 빔 탐색 중 여러 가설을 벡터화하여 디코딩 프로세스의 속도를 높이고 병렬 컴퓨팅을 활용합니다 [212], [213], [225].

5. **LM 통합**:
   - **LM 유형**: N-gram LM [2], FNN-LM [8], RNN-LM [229], ConvLM [232], Transformer LM [234] 등 다양한 LM이 E2E ASR의 성능 향상을 위해 사용됩니다.
   - **퓨전 접근 방식**:
     - **Shallow fusion** [208]: E2E 모델 점수와 LM 점수를 선형적으로 결합합니다.
     - **Deep fusion** [238]: E2E 모델과 LM을 조인트 네트워크로 결합하고 모든 매개변수를 함께 훈련합니다.
     - **Cold fusion** [239]: 사전 훈련된 LM의 매개변수를 고정하고 E2E 모델을 함께 학습하여 언어 편향을 줄입니다.
     - **Internal LM estimation** [241], [34]: 훈련 데이터의 언어 편향을 줄이기 위해 E2E 모델 내부에 언어 모델을 명시적으로 추정하여 ASR 점수에서 빼는 방식입니다.
   - **대규모 사전 훈련 LM 활용**: BERT [244] 및 GPT-2 [245]와 같은 대규모 사전 훈련 LM을 N-best 재채점 [246] 또는 대화 컨텍스트 임베딩 [247] 등에 활용합니다.

## 📊 Results

E2E ASR 모델은 지난 몇 년간 Librispeech [301] 및 Switchboard [302]와 같은 널리 사용되는 벤치마크에서 상당한 성능 향상을 보였습니다.

- **초기 E2E 모델의 상당한 개선**: 초기 E2E 모델의 워드 에러율(WER)은 시간이 지남에 따라 크게 감소하여 절반 이하로 줄었습니다.
- **주요 성능 향상 동인**:
  - **데이터 증강**: 2019년 중반에 SpecAugment [176], [177]와 같은 데이터 증강 방법이 도입되면서 WER이 크게 개선되었습니다.
  - **새로운 아키텍처**: SpecAugment 이후에는 Transformer [75], Conformer [73], ContextNet [70]과 같은 새로운 인코더/디코더 아키텍처의 탐색이 성능 향상에 기여했습니다. 이러한 아키텍처는 병렬 컴퓨팅에 더 적합합니다.
  - **언어 모델링 개선**: Transformer 기반 LM [235], [75]과 같은 언어 모델링의 발전도 ASR 성능 향상에 중요한 역할을 했습니다.
  - **자기 지도 학습(Self-Supervised Learning)**: 2021년 Librispeech 벤치마크에서 자기 지도 학습 [22], [308] 및 준지도 학습 [304], [309] 기반 기술이 대량의 레이블 없는 오디오 데이터를 활용하여 추가적인 성능 향상을 달성했습니다.
- **현재 최고 성능**:
  - **Librispeech**: 현재 E2E 시스템은 Test-Clean 1.8%, Test-Other 3.7% [305]의 WER을 달성하며 고전적인 하이브리드 시스템(Test-Clean 약 2.3%, Test-Other 약 4.9%)을 능가하고 있습니다 [73], [70].
  - **Switchboard (300h)**: E2E 시스템 [151]이 5.4% WER로, 최고의 하이브리드 시스템 [306]의 6.6% WER보다 우수한 성능을 보였습니다.
- **상업적 배포**: Google은 Pixel 스마트폰 시리즈에 E2E ASR 모델을 성공적으로 배포했습니다.
  - **Pixel 4 (2019)** [212], [316]: 스트리밍 RNN-T 1단계 시스템과 AED 2단계 재채점 시스템을 사용했습니다. FST 기반 컨텍스트 바이어싱이 중요했습니다.
  - **Pixel 5 (2020)** [317]: E2E 엔드포인팅 [82] 및 FastEmit [64]와 같은 기술을 통해 사용자 인지 지연 시간을 줄였습니다.
  - **Pixel 6 (2021)** [318]: 온디바이스 TPU [319]를 활용하기 위해 Conformer 인코더 [73], 소형 임베딩 예측 네트워크 디코더 [77], 2단계 캐스케이드 인코더 [62], 신경망 LM 재채점기 등을 통합하여 품질 및 지연 시간 측면에서 가장 우수한 시스템을 출시했습니다.

## 🧠 Insights & Discussion

E2E ASR 모델은 ASR 연구의 주류가 되었지만, 상업적 배포에는 여전히 많은 연구가 필요합니다. 주요 통찰과 논의할 점은 다음과 같습니다:

- **데이터 의존성**: E2E 모델은 훈련 데이터가 풍부할 때 매우 우수한 성능을 보이지만, 저자원(low-resource) 환경에서는 확장성이 떨어지는 경향이 있습니다 [294], [295].
- **언어 모델 통합의 중요성**: E2E 모델은 음향 모델과 언어 모델의 명확한 분리가 없으므로, 텍스트 전용 데이터를 활용하기 위한 외부 언어 모델 통합 및 내부 언어 모델 추정/보정 [241], [34] 연구가 필수적입니다.
- **훈련 효율성**: 최고 성능의 E2E ASR 시스템은 고전적인 ASR 시스템보다 훨씬 더 많은 훈련 에폭(epochs)을 필요로 합니다. 효율적이고 견고한 최적화 및 훈련 스케줄에 대한 추가 연구가 필요합니다.
- **모듈성 및 설명 가능성**: E2E 모델의 높은 통합성은 모듈성의 손실로 이어져 모델의 설명 가능성과 재사용성을 저해할 수 있습니다. 모듈성을 활용한 효율적인 훈련 스케줄에 대한 탐색이 필요합니다.
- **희귀어 처리**: 음성 인식에서 희귀어와 같은 희귀 이벤트는 여전히 E2E 모델의 주요 과제이며, 이를 위한 추가 연구가 필요합니다.
- **텍스트 전용 데이터 활용**: TTS를 사용한 훈련 데이터 생성 외에 텍스트 전용 리소스를 E2E 모델 훈련에 활용할 수 있는 새로운 해결책이 필요합니다 [320], [321]. 자기 지도 학습과 준지도 학습을 위해 음성-텍스트 공동 사전 훈련 [130] 및 TTS와 ASR을 공동으로 활용하는 방안 [20], [327]이 모색되고 있습니다.
- **AED의 길이 편향**: 어텐션 기반 인코더-디코더(AED) 모델은 길이 편향(length bias) 문제를 겪습니다 [256], [257]. 이를 해결하기 위한 많은 휴리스틱이 있지만 [188], 근본적인 원인 규명과 모델 개선이 요구됩니다.
- **강건성 및 적응**: 화자 적응(speaker adaptation) [268], [269] 및 불일치(mismatch) 상황에서의 녹음 조건에 대한 강건성(robustness)은 여전히 미해결 연구 문제입니다.
- **다채널 ASR**: E2E 원칙은 음원 분리, 화자 분할(diarization), 음성 인식을 공동으로 해결하는 다채널 ASR 솔루션에 유망한 후보입니다 [328], [329].
- **ASR의 미래**: E2E가 적합한 지침 원리인지, 그리고 다양한 E2E ASR 모델들이 고전적인 ASR 접근 방식과 어떻게 관련되어 있는지에 대한 추가적인 조사가 필요합니다. 성능이 ASR 연구의 가장 중요한 지침 원리인 만큼, 미래의 ASR 연구는 계속해서 새로운 벤치마크와 평가 캠페인을 통해 발전할 것입니다.

## 📌 TL;DR

이 서베이 논문은 딥러닝 시대의 **엔드-투-엔드(E2E) 자동 음성 인식(ASR)** 모델을 종합적으로 정리합니다. 기존 HMM 기반 ASR의 복잡한 모듈식 구조를 벗어나 음성을 텍스트로 직접 변환하는 **단일 신경망 모델**의 개념, 발전 과정, 현재 상태를 설명합니다. **CTC, RNN-T, Attention-based 모델** 등 주요 E2E 아키텍처들을 정렬(alignment) 모델링 방식에 따라 분류하고, **학습(데이터 증강, 최소 오류 훈련 등) 및 디코딩(빔 탐색, 모델 퓨전 등) 방법론, 외부 언어 모델(LM) 통합 전략**을 상세히 분석합니다. Librispeech, Switchboard 벤치마크에서 **SpecAugment, Transformer, Conformer, 자기 지도 학습** 등의 도입으로 WER이 크게 감소했음을 보여주며, **Google Pixel 시리즈의 온디바이스 ASR 배포** 사례를 통해 실제 적용 성공을 강조합니다. 마지막으로, **저자원 환경 취약성, 훈련 효율성, LM 통합 문제, 길이 편향, 희귀어 처리** 등의 한계점과 **멀티채널 ASR** 같은 미래 연구 방향을 제시하여 E2E ASR의 현재와 미래를 조망합니다.
