# Robust Speech Recognition via Large-Scale Weak Supervision

Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever

## 🧩 Problem to Solve

기존의 음성 인식(ASR) 시스템은 주로 비지도 사전 학습(unsupervised pre-training) 후 특정 데이터셋에 대한 미세 조정(fine-tuning)을 통해 발전해왔습니다. 그러나 이러한 방식은 다음과 같은 문제점을 가집니다:

- **취약한 강건성 및 일반화:** 미세 조정은 종종 모델이 학습 데이터셋의 특정 특성(quirks)을 과도하게 학습하게 하여, 동일한 분포 내의 데이터에서는 초인적인 성능을 보이지만 약간만 다른 외부 데이터(out-of-distribution, OOD)에서는 성능이 급격히 저하되는 '취약성'을 보입니다.
- **미세 조정의 복잡성:** 미세 조정 과정은 복잡하고 숙련된 전문가를 필요로 하며, 배포될 모든 환경에 맞춰 디코더를 지도 학습으로 미세 조정해야 하는 비효율성이 존재합니다.
- **지도 학습 데이터의 한계:** 고품질의 지도 학습 데이터셋은 비지도 학습 데이터셋에 비해 규모가 현저히 작습니다 (예: SpeechStew 5,140시간 vs. 비지도 학습 1,000,000시간). 약한 지도(weakly supervised) 데이터셋이 이 간극을 메우고 있지만, 여전히 규모가 부족합니다.
  이 논문은 미세 조정 없이 다양한 환경에서 '즉시 사용 가능한(out-of-the-box)' 신뢰할 수 있는 음성 인식 시스템을 구축하는 것을 목표로 합니다.

## ✨ Key Contributions

- **대규모 약한 지도 학습의 효과 입증:** 680,000시간에 달하는 방대한 다국어 및 다중 작업(multitask) 약한 지도 오디오 데이터를 활용하여 음성 인식 시스템을 학습했을 때, 놀라운 강건성과 일반화 성능을 달성할 수 있음을 보여주었습니다. 이는 기존 지도 ASR 데이터셋 규모를 훨씬 뛰어넘는 수준입니다.
- **제로샷 전이 학습(Zero-shot Transfer) 성능:** 학습된 모델(Whisper)은 어떠한 미세 조정 없이(zero-shot setting) 표준 벤치마크에서 기존의 완전 지도 학습(fully supervised) 모델들과 경쟁하거나 능가하는 성능을 보였습니다.
- **다국어 및 다중 작업 학습:** 96개 언어에 대한 117,000시간의 다국어 데이터와 125,000시간의 X→영어 번역 데이터를 공동 학습하여, 하나의 모델로 음성 인식, 음성 번역, 음성 활동 감지(VAD), 언어 식별 등의 여러 작업을 수행할 수 있게 했습니다. 충분히 큰 모델에서는 다국어 및 다중 작업 학습이 오히려 성능 향상에 기여했습니다.
- **인간 수준에 근접한 강건성:** 다양한 영어 음성 인식 데이터셋에서 인간의 정확도와 강건성에 근접하는 성능을 달성했습니다. 특히, 잡음이 있는 환경(예: 펍 소음) 및 긴 형태의 오디오 전사에서 SOTA 상용 ASR 시스템에 필적하는 강건성을 입증했습니다.
- **모델 및 추론 코드 공개:** 견고한 음성 처리 연구를 위한 기반을 제공하기 위해 학습된 모델과 추론 코드를 공개했습니다.
- **단순한 접근 방식:** 최신 대규모 음성 인식 연구의 주요 흐름이었던 자기 지도(self-supervision) 또는 자기 학습(self-training) 기법 없이도 이러한 결과를 달성했습니다.

## 📎 Related Works

- **비지도 사전 학습:** Wav2Vec 2.0 (Baevski et al., 2020) 및 1,000,000시간 규모로 확장된 후속 연구(Zhang et al., 2021). 완전 비지도 음성 인식(Baevski et al., 2021).
- **다중 도메인 지도 학습:** 여러 데이터셋/도메인에서 사전 학습된 ASR 시스템의 강건성 향상 연구 (Narayanan et al., 2018; Likhomanenko et al., 2020; SpeechStew, Chan et al., 2021).
- **약한 지도 학습 확장:** 자동화된 파이프라인을 통해 10,000~30,000시간의 약한 지도 데이터셋을 구축한 연구 (Chen et al., 2021; Galvez et al., 2021). 컴퓨터 비전 분야의 약한 지도 사전 학습 연구 (Mahajan et al., 2018; Kolesnikov et al., 2020).
- **ASR 스케일링 연구:** 딥러닝 도입 초기부터 컴퓨팅 자원, 모델, 데이터셋 규모 확장이 ASR 성능에 미치는 영향을 다룬 연구 (Mohamed et al., 2009; Seide et al., 2011; Deep Speech 2, Amodei et al., 2015; Zhang et al., 2020, 2021).
- **다중 작업 학습:** NLP 분야에서 단일 모델 다중 작업 학습의 선구적인 연구 (Collobert et al., 2011), sequence-to-sequence 프레임워크에서의 다중 작업 학습 (Luong et al., 2015), 언어 코드를 활용한 다국어 기계 번역 (Johnson et al., 2017), "text-to-text" 프레임워크 (McCann et al., 2018; Radford et al., 2019; Raffel et al., 2020). 다국어 ASR (Toshniwal et al., 2018; Pratap et al., 2020a). 음성-텍스트 공동 사전 학습 (MUTE, Wang et al., 2020c; mSLAM, Bapna et al., 2022).
- **강건성 연구:** 모델의 OOD 일반화 및 강건성에 대한 장기적인 연구 (Torralba & Efros, 2011; Lake et al., 2017). 자연 분포 변화에 대한 이미지 분류 (Taori et al., 2020) 및 질의응답 모델 (Miller et al., 2020)의 강건성 연구. 다중 도메인 학습이 강건성 및 일반화를 향상시킨다는 결과 (Hendrycks et al., 2020; Radford et al., 2021).

## 🛠️ Methodology

- **데이터 처리:**
  - 인터넷에서 수집한 오디오-전사 쌍으로 구성된 방대한 데이터셋을 구축했습니다.
  - 데이터 전처리는 최소화하여, 모델이 음성 표현과 전사된 텍스트 간의 매핑을 학습하도록 했습니다. 이는 별도의 역 텍스트 정규화(inverse text normalization) 단계 없이 자연스러운 전사를 생성할 수 있게 합니다.
  - 자동화된 필터링 방법을 사용하여 전사 품질을 개선했습니다. 여기에는 기존 ASR 시스템의 출력을 제거하기 위한 휴리스틱(예: 복잡한 구두점 누락, 모든 대문자/소문자 전사 등), 오디오 언어 감지기를 통한 음성-전사 언어 일치 확인, 퍼지 중복 제거(fuzzy de-duping) 등이 포함됩니다.
  - 오디오는 30초 단위로 분할되었으며, 무음(no-speech) 구간도 음성 활동 감지(VAD) 학습을 위해 포함되었습니다.
  - 평가 데이터셋 오염을 방지하기 위해 훈련 데이터셋과 평가 데이터셋 간의 전사 수준 중복 제거를 수행했습니다.
- **모델 아키텍처 (Whisper):**
  - 견고한 스케일링 특성을 가진 표준 인코더-디코더 트랜스포머(Transformer) (Vaswani et al., 2017) 아키텍처를 사용했습니다.
  - 모든 오디오는 16,000Hz로 재샘플링되고, 25밀리초 윈도우와 10밀리초 스트라이드(stride)로 80채널 로그-멜 스펙트로그램(log-magnitude Mel spectrogram)으로 변환됩니다.
  - 인코더는 두 개의 컨볼루션 레이어와 GELU 활성화 함수로 구성된 스템(stem)으로 입력 스펙트로그램을 처리한 후, 사인형 위치 임베딩(sinusoidal position embeddings)과 트랜스포머 블록을 적용합니다.
  - 디코더는 학습된 위치 임베딩과 바이트 수준 BPE 텍스트 토크나이저(multilingual 모델의 경우 재학습된 GPT-2 토크나이저)를 사용합니다.
- **다중 작업 형식:**
  - 단일 모델이 다양한 음성 처리 작업을 수행할 수 있도록 디코더에 입력 토큰 시퀀스로 작업 사양(task specification)을 제공합니다.
  - 특수 토큰(예: `<|startoftranscript|>`, `<|nospeech|>`, `<|transcribe|>`, `<|translate|>`, `<|notimestamps|>`, `<|endoftranscript|>`)을 사용하여 언어 식별, 작업 지정(전사/번역), 타임스탬프 예측 여부 등을 지시합니다.
  - 타임스탬프는 20밀리초 단위로 양자화되며, 캡션 토큰과 교차되어 예측됩니다.
  - 디코더는 현재 오디오 세그먼트 이전의 전사 텍스트 기록에 조건화되어(audio-conditional language model) 학습됩니다.
- **훈련 세부 사항:**
  - 다양한 크기의 모델 스위트(Tiny, Base, Small, Medium, Large, Large V2)를 학습했습니다.
  - 데이터 병렬 처리, FP16, 동적 손실 스케일링(dynamic loss scaling), 활성화 체크포인트(activation checkpointing)를 사용했습니다.
  - AdamW 옵티마이저, 그래디언트 노름 클리핑(gradient norm clipping), 선형 학습률 감소 스케줄을 적용했습니다.
  - 배치 크기는 256개 세그먼트였으며, 데이터셋을 2~3회 반복하는 약 $2^{20}$ 업데이트 동안 학습했습니다.
  - 데이터셋의 다양성에 의존하여 일반화 및 강건성을 유도했으며, 데이터 증강이나 정규화는 사용하지 않았습니다(단, Large V2 모델에는 SpecAugment, Stochastic Depth, BPE Dropout 추가).
  - 스피커 이름 환각(hallucination)을 피하기 위해, 스피커 주석이 포함되지 않은 전사 데이터 서브셋에서 Whisper 모델을 간략하게 미세 조정했습니다.

## 📊 Results

- **제로샷 영어 음성 인식 강건성:**
  - 최상위 제로샷 Whisper 모델(Large V2)은 LibriSpeech test-clean에서 2.5%의 WER(Word Error Rate)을 달성하여, 2019년 중반의 지도 학습 SOTA 모델과 유사한 성능을 보였습니다.
  - OOD 데이터셋(Artie, Common Voice, CHiME-6, TED-LIUM 등 12개)에서는 LibriSpeech 학습 모델들보다 훨씬 우수했으며, 평균 55.2%의 상대적 오류 감소율을 보였습니다.
  - 그림 2에서 볼 수 있듯이, Whisper 모델은 다양한 영어 ASR 데이터셋에서 인간의 정확도 및 강건성에 근접했습니다.
- **다국어 음성 인식:**
  - Multilingual LibriSpeech (MLS) 벤치마크에서 XLS-R, mSLAM, Maestro를 제로샷으로 능가했습니다.
  - VoxPopuli에서는 다소 낮은 성능을 보였는데, 이는 다른 모델들이 이 분포를 비지도 사전 학습에 활용했거나 미세 조정을 위한 지도 데이터가 더 많았기 때문으로 추정됩니다.
  - Fleurs 데이터셋(75개 언어) 분석 결과, 언어별 학습 데이터 양의 로그값과 WER의 로그값 사이에 $r^2=0.83$의 강한 상관관계가 있었으며, 학습 데이터가 16배 증가할 때마다 WER이 절반으로 감소하는 경향을 보였습니다 (그림 3).
- **번역 (X→영어 음성 번역):**
  - CoVoST2 벤치마크에서 29.1 BLEU 점수를 달성하며 새로운 SOTA를 기록했으며, 특히 저자원 언어에서 mSLAM보다 6.7 BLEU 높은 성능을 보였습니다. 이는 훈련 데이터셋에 68,000시간의 X→영어 번역 데이터가 포함된 덕분입니다.
  - Fleurs 데이터셋을 번역 벤치마크로 활용한 결과, 학습 데이터 양과 BLEU 점수 사이에 긍정적인 상관관계가 있었으나 ($r^2=0.24$) 음성 인식에 비해 약했습니다. 이는 언어 식별 오류로 인한 노이즈가 많은 데이터 때문으로 분석되었습니다 (그림 4).
- **언어 식별:**
  - Fleurs 데이터셋에서 제로샷 Whisper의 언어 식별 정확도는 64.5%로, 기존 지도 학습 SOTA (77.7%)에 미치지 못했습니다. Fleurs의 102개 언어 중 20개에 대한 훈련 데이터가 없었기 때문에 정확도가 80.4%로 상한선이 제한되었습니다.
- **가산 잡음(Additive Noise)에 대한 강건성:**
  - LibriSpeech-학습 모델 14개 및 NVIDIA STT 모델과 비교했을 때, Whisper는 낮은 잡음 환경(40 dB SNR)에서는 다른 모델이 우수했지만, 높은 잡음 환경(10 dB 미만 SNR, 특히 펍 소음)에서는 모든 모델보다 우수한 강건성을 보였습니다 (그림 5).
- **긴 오디오 전사 (Long-form Transcription):**
  - 7개 긴 오디오 데이터셋에서 SOTA 상용 및 오픈소스 ASR 시스템들과 경쟁력 있는 성능을 보였으며, 대부분의 데이터셋에서 우수했습니다 (그림 6).
  - 긴 오디오를 안정적으로 전사하기 위한 디코딩 전략(빔 탐색, 온도 스케줄링, VAD, 이전 텍스트 조건화, 초기 타임스탬프 제약)이 점진적으로 WER을 감소시켰습니다 (표 7).
- **인간 성능과의 비교:**
  - Kincaid46 데이터셋의 일부 샘플에서 Whisper의 통합 WER은 컴퓨터 보조 전사 서비스보다 1.15% 포인트 높았고, 순수 인간 전사 서비스와는 소수점 이하의 차이로 매우 근접한 성능을 보였습니다 (그림 7).

## 🧠 Insights & Discussion

- **약한 지도 학습 스케일링의 잠재력:** 이 연구는 방대한 약한 지도 데이터를 활용한 단순한 스케일링이 음성 인식 분야에서 간과되어 왔으며, 복잡한 자기 지도 학습 기법 없이도 탁월한 강건성과 일반화 성능을 달성할 수 있음을 강력히 시사합니다.
- **제로샷 평가의 중요성:** 기존 ASR 벤치마크의 미세 조정 기반 평가는 모델의 진정한 강건성을 과대평가할 수 있습니다. 모델의 실제 활용 가능성과 인간 성능과의 비교를 위해서는 제로샷 및 OOD(Out-of-Distribution) 평가에 초점을 맞춰야 합니다.
- **모델 및 데이터셋 스케일링 법칙:**
  - 모델 크기 증가는 작업 전반에 걸쳐 성능 향상으로 이어지며, 영어 음성 인식에서는 인간 수준에 근접하면서 점진적 수익 감소(diminishing returns) 경향을 보입니다.
  - 데이터셋 크기 증가 역시 모든 작업에서 성능을 향상시키며, 다국어 ASR에서는 멱법칙(power-law), X→영어 번역에서는 로그-선형(log-linear) 추세를 따르다가 최대 규모에서 점진적 수익 감소를 보입니다. 이는 더 긴 훈련 시간, 더 큰 모델, 또는 음성 인식의 스케일링 한계에 도달하고 있음을 시사합니다.
- **다중 작업/다국어 전이 학습:**
  - 소형 모델에서는 작업/언어 간의 부정적인 전이(negative transfer)가 관찰되었으나, 규모가 커지면서 다중 작업 및 다국어 모델이 영어 전용 모델을 능가하여 긍정적인 전이 효과를 입증했습니다.
- **텍스트 정규화의 영향:** Whisper와 함께 개발된 맞춤형 텍스트 정규화기는 사소한 오류를 줄이는 데 도움이 되지만, WSJ, CallHome, Switchboard와 같은 일부 데이터셋에서는 Whisper 모델의 특이점에 과적합될 위험이 있을 수 있습니다.
- **한계점 및 향후 연구:**
  - **디코딩 전략 개선:** 반복 루프, 오디오 세그먼트의 첫/마지막 단어 누락, 환각 등 비인간적인 오류는 여전히 존재하며, 고품질 지도 데이터에 대한 미세 조정 또는 강화 학습을 통해 디코딩 성능을 개선할 필요가 있습니다.
  - **저자원 언어 데이터 증가:** 많은 언어에서 여전히 낮은 성능을 보이는데, 이는 학습 데이터 부족과 직접적인 상관관계가 있습니다. 저자원 언어에 대한 데이터 수집 노력이 크게 필요합니다.
  - **미세 조정 연구:** 본 연구는 제로샷 전이 성능에 초점을 맞췄지만, 특정 도메인에서는 미세 조정을 통해 추가적인 성능 향상을 기대할 수 있습니다.
  - **언어 모델의 영향 연구:** Whisper의 강건성에서 오디오 조건부 언어 모델(디코더)의 기여도를 명확히 파악하기 위한 추가적인 절제 연구(ablation studies)가 필요합니다.
  - **보조 학습 목표 추가:** 비지도 사전 학습 또는 자기 학습 기법을 통합하여 추가적인 성능 개선 가능성을 탐색할 수 있습니다.

## 📌 TL;DR

- **문제:** 기존 ASR 시스템은 미세 조정에 의존하여 OOD(Out-of-Distribution) 데이터에 대한 강건성과 일반화가 부족했으며, 미세 조정 과정이 복잡하고 지도 학습 데이터 규모에 한계가 있었습니다.
- **방법:** OpenAI는 680,000시간의 방대한 다국어 및 다중 작업(multitask) 약한 지도(weakly supervised) 오디오 데이터를 사용하여 Transformer 기반의 인코더-디코더 모델인 Whisper를 학습했습니다. Whisper는 특수 토큰을 통해 전사, 번역, VAD, 언어 식별 등 다양한 작업을 단일 모델로 수행합니다.
- **주요 결과:** Whisper는 미세 조정 없이(zero-shot) 영어 ASR 벤치마크에서 인간에 필적하는 강건성과 정확도를 보였고, OOD 데이터셋에서 기존 모델보다 평균 55.2% 낮은 오류율을 달성했습니다. 다국어 음성 인식 및 음성 번역에서 SOTA 성능을 달성하거나 경쟁력을 보였으며, 잡음 환경 및 긴 오디오 전사에서도 뛰어난 강건성을 입증했습니다. 본 연구는 약한 지도 학습의 대규모 스케일링이 음성 인식 분야에서 과소평가되었으며, 복잡한 자기 지도 학습 기법 없이도 높은 성능을 달성할 수 있음을 보여주었습니다.
