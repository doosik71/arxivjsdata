# daVinciNet: Joint Prediction of Motion and Surgical State in Robot-Assisted Surgery
Yidan Qin, Seyedshams Feyzabadi, Max Allan, Joel W. Burdick, Mahdi Azizian

## 🧩 Problem to Solve
이 논문은 로봇 보조 수술(RAS)에서 여러 입력 소스를 활용하여 수술 도구의 미래 궤적과 수술 하위 작업의 미래 상태를 동시에 예측하는 기술을 제시합니다. 이러한 예측은 수술 하위 작업의 공유 제어(shared control) 및 감독 자율성(supervised autonomy)을 위한 필수적인 첫 단계입니다. 몇 분 길이의 수술 하위 작업(예: 봉합 또는 초음파 스캔)은 종종 구별 가능한 도구 운동학 및 시각적 특징을 가지며, 일련의 세분화된 상태와 전환 도식으로 설명될 수 있습니다. 기존 연구들은 종종 운동 예측을 보조 정보로 사용하거나, 장기적인 수치적 궤적 예측에 초점을 맞추지 않았고, 상태 추정 모델은 시간적 정보나 관심 영역(RoI)을 충분히 활용하지 못했습니다.

## ✨ Key Contributions
*   **`daVinciNet` 제안**: 로봇 운동(말단 효과기 궤적)과 수술 상태를 동시에 예측하는 종단 간(end-to-end) 이중 작업(dual-task) 모델인 `daVinciNet`을 제안했습니다.
*   **다중 소스 특징 통합**: 로봇 운동학(robot kinematics), 내시경 영상(endoscopic vision), 시스템 이벤트(system events)를 포함한 여러 사용 가능한 데이터 소스에서 특징을 통합하고 추출했습니다.
*   **국소화된 시각 특징 추출**: 내시경 영상 데이터의 관심 영역(RoI)을 결정하기 위해 시각 기반 도구 추적 알고리즘을 구현하여 보다 국소화되고 효과적인 시각 특징 추출을 가능하게 했습니다.
*   **이력 수술 상태 시퀀스 활용**: 상태 예측을 위해 이력 수술 상태 시퀀스를 추론하는 상태 추정 모델(Fusion-KVE)을 적용하여 실시간 환경에서의 예측 모델 강건성을 높였습니다.
*   **시간적 정보 활용**: 학습 기반(learning-based) 방법을 사용하여 데이터 시퀀스의 시간적 정보를 통합함으로써 최대 2초까지의 정확한 궤적 및 상태 예측을 달성했습니다.
*   `JIGSAWS` 봉합 데이터셋 및 확장된 `RIOUS+` 영상 데이터셋을 사용하여 모델 성능을 평가하고, 다양한 특징 유형의 기여를 이해하기 위한 어블레이션 연구(ablation studies)를 수행했습니다.

## 📎 Related Works
*   **수술 로봇 자율성**: 가상 픽스처(virtual fixtures) [1], 자율 수술 작업 [2,3], 의료 로봇의 자율성 수준 정의 [7].
*   **수술 도구 궤적 예측**: 충돌 예측 및 회피, 다중 에이전트 수술 시스템. Weede et al. [8]의 마르코프 모델(Markov Model, MM) 기반 내시경 위치 최적화 및 장기 궤적 예측(움직임 클래스 예측에 집중, 수치 시퀀스 예측 아님), Staub et al. [13], Zhao et al. [14], Wang et al. [15].
*   **수술 상태 및 행동 예측/인식**: 유한 상태 기계(Finite State Machine, FSM) [17], 고전적인 마르코프 모델 [8,12,18-20] (시간 정보 포착 한계). 순환 신경망(RNN) [21] 및 합성곱 신경망(CNN) [22-24] 기반 상태 추정 모델 (상태 전환 정보 없이 독립적으로 수행, 전체 내시경 뷰에서 특징 추출).
*   **컴퓨터 비전 분야의 딥러닝 기반 경로/행동 예측**: 개인 시각 특징 및 LSTM [26,27]을 사용한 경로 예측, 행동 조기 인식 [28,29]. Liang et al. [30]의 멀티태스크 모델.
*   **저자들의 이전 연구**: `Fusion-KVE` [17]는 여러 유형의 입력 데이터를 통합하여 최첨단 상태 추정 성능을 초과 달성한 통합 수술 상태 추정 모델입니다.

## 🛠️ Methodology
`daVinciNet`은 종단 간 공동 예측 모델로, 말단 효과기 궤적과 수술 상태를 동시에 예측합니다. 이 모델은 자연어 처리에서 널리 사용되는 장단기 기억(LSTM) 인코더-디코더 모델 구조를 따릅니다.

*   **입력 데이터**: 관찰 윈도우 $T_{obs}$ 크기만큼의 내시경 영상, 로봇 운동학 및 시스템 이벤트 데이터 시퀀스를 동기화하여 입력으로 사용합니다 (데이터 스트리밍 주파수 10Hz).
*   **특징 추출 (인코더)**:
    *   **시각 특징 인코더**: 전역적 시각 정보(전체 내시경 뷰)와 국소적 RoI(관심 영역) 시각 정보를 모두 추출합니다.
        *   전역 시각 정보는 사전 훈련된 `VGG-16` 모델 [35]과 `LSTM` 인코더를 사용하여 `CNN` 특징을 추출합니다.
        *   RoI 시각 정보는 실루엣 기반 도구 추적 모델 [25]을 통해 말단 효과기 주변의 경계 상자(RoI)를 식별한 후, 별도의 `CNN-LSTM` 인코더(도구-장면 인코더)를 통해 특징을 추출합니다.
        *   이 두 인코더의 숨겨진 상태($H_{scene}$, $H_{RoI}$)가 특징 텐서의 일부를 형성합니다.
    *   **운동학 특징 인코더**: 입력 어텐션 메커니즘 [38]이 적용된 `LSTM` 인코더를 사용하여 다빈치(da Vinci$^{\text{R}}$) 수술 시스템의 운동학 데이터(말단 효과기의 병진 및 회전 위치 등)에서 특징을 추출합니다. 이 인코더의 숨겨진 상태($H_{kin}$)도 특징 텐서의 일부입니다.
    *   **통합 특징 텐서 $Q$**: 모든 인코더의 출력($H_{scene}$, $H_{RoI}$, $H_{kin}$)이 연결되어 통합 특징 텐서 $Q \in \mathbb{R}^{T_{obs} \times (n_{scene} + n_{RoI} + n_{kin})}$를 구성합니다.
    *   **이력 수술 상태**: 실시간 예측 환경에서는 실제(ground truth) 상태 시퀀스를 사용할 수 없으므로, 저자들이 이전에 제안한 `Fusion-KVE` [17] 모델을 사용하여 $T_{obs}$ 기간 동안의 이력 수술 상태 시퀀스를 추정하여 활용합니다.
*   **예측 (디코더)**:
    *   **어텐션 기반 `LSTM` 디코더**: 두 예측 작업 모두 입력 시퀀스 길이 증가에 따른 성능 저하를 완화하기 위해 시간적 어텐션(temporal attention) [39]을 갖춘 `LSTM` 디코더를 구현했습니다.
    *   **도구 경로 예측**: 말단 효과기의 내시경 기준 좌표계($x, y, z$) 내 Cartesian 경로를 예측합니다. 입력은 $Q$와 이력 목표 시퀀스입니다.
    *   **수술 상태 예측**: 미래 수술 상태(이산 상태)를 예측합니다. 입력은 $Q$와 `Fusion-KVE`에서 추정된 이력 상태 시퀀스입니다.
*   **훈련**: 특징 추출 및 예측 모듈을 포함한 전체 모델은 종단 간 훈련됩니다. 손실 함수는 예측된 궤적과 실제 궤적 간의 누적 $L_2$ 손실, 그리고 예측된 수술 상태와 실제 상태 간의 누적 범주형 교차 엔트로피(categorical cross-entropy) 손실을 최소화합니다.

## 📊 Results
*   **데이터셋**: `JIGSAWS` 봉합 데이터셋(9가지 행동)과 `RIOUS+` 초음파 영상 데이터셋(8가지 상태)에서 모델 성능을 평가했습니다. Leave-One-User-Out 교차 검증 방식을 사용했습니다.
*   **평가 지표**: 말단 효과기 궤적 예측에는 `RMSE`(제곱평균제곱근 오차), `MAE`(평균 절대 오차), `MAPE`(평균 절대 백분율 오차)를 사용했고, 수술 상태 예측에는 정확하게 예측된 프레임의 비율을 사용했습니다.
*   **주요 결과 (1초 예측 기준, $T_{pred} = 10$)**:
    *   **궤적 예측 (표 II, 그림 4)**:
        *   모든 데이터 스트림($Q$)을 사용한 `daVinciNet` 예측은 운동학 특징만 사용한 경우($H_{kin}$) 또는 장면 및 운동학 특징을 함께 사용한 경우($\{H_{scene}, H_{kin}\}$)보다 일관되게 최대 20% 더 나은 성능을 달성했습니다.
        *   `RIOUS+` 데이터셋에서 `Q`를 사용한 모델은 1초 예측 시 1.07mm의 단기(0.5초) 궤적 예측 오차와 1.64mm의 `MAE`를 기록했습니다. 이는 매우 높은 정확도를 보여줍니다.
        *   특히 RoI 시각 특징은 긴 예측 단계에서 궤적 예측 `MAE`를 현저히 감소시켰습니다.
    *   **상태 예측 (표 III, 그림 5, 그림 6)**:
        *   `Q`와 `Fusion-KVE`를 모두 입력으로 사용한 `daVinciNet`은 `RIOUS+`에서 91.02%, `JIGSAWS`에서 84.39%의 높은 정확도를 달성했습니다.
        *   `Q`만 사용했을 때는 정확도가 낮고 예측 시간이 길어질수록 기하급수적으로 감소하는 경향을 보였습니다. 이는 이전 예측 오류가 누적되기 때문입니다.
        *   `Fusion-KVE`만 사용했을 때는 정확도가 향상되었으나, 시각적/운동학적 신호가 부족하여 실제 상태 변화에 대한 반응이 지연될 수 있었습니다.
        *   `daVinciNet`은 두 정보를 통합하여 예측 정확도를 크게 향상시키고 예측 시간이 길어져도 성능 저하가 미미했습니다.
    *   **인간 주석 불일치**: 인간 주석가들 사이의 `RIOUS+` 상태 주석 일치율은 94.2%였습니다. 불일치는 주로 상태 전환 지점에서 발생했는데, 이는 `daVinciNet`의 예측 오류 중 일부가 인간 주석의 본질적인 모호성에서 기인할 수 있음을 시사합니다.

## 🧠 Insights & Discussion
*   **다중 모달 데이터의 필수성**: 운동학, 전역 시각, RoI 시각, 시스템 이벤트를 포함한 다중 데이터 소스와 이력 상태 시퀀스를 활용했을 때 성능이 크게 향상되는 것은, 견고하고 정확한 공동 예측을 위해 이러한 정보가 얼마나 중요한지를 보여줍니다.
*   **RoI 특징의 가치**: 말단 효과기 주변의 국소화된 시각 특징은 도구 움직임과 상태 변화에 대한 고급 단서를 제공하며, 특히 장기 궤적 예측의 정확도를 크게 향상시킵니다.
*   **추정된 상태의 견고성**: 실제(ground truth) 상태 시퀀스 대신 `Fusion-KVE`로 추정된 이력 상태를 사용하는 것은 모델이 실시간 수술 환경에서 실제로 적용 가능함을 보여줍니다.
*   **시간적 의존성 처리**: `LSTM` 인코더-디코더 아키텍처와 어텐션 메커니즘은 장기적인 시간적 의존성을 효과적으로 포착하고 더 긴 예측 구간에서의 성능 저하를 완화합니다.
*   **한계 및 향후 연구**:
    *   현재 오류는 주로 상태 전환 지점에 집중되어 있으며, 이는 수동 주석의 본질적인 모호성 때문일 수 있습니다.
    *   향후 연구에서는 내시경 영상의 시맨틱 세그멘테이션(semantic segmentation)을 통합하여 환경 노이즈를 줄이는 것을 고려할 수 있습니다.
    *   `daVinciNet`을 공유 제어 또는 감독 자율 수술 하위 작업과 같은 `RAS` 실제 응용 분야에 적용할 계획입니다.

## 📌 TL;DR
*   **문제**: 로봇 보조 수술에서 도구 궤적과 수술 상태를 실시간으로 동시에 정확하게 예측하는 것은 자율 시스템 구현의 핵심 과제입니다.
*   **해결책**: `daVinciNet`은 로봇 운동학, 내시경 영상, 시스템 이벤트 등 다중 모달 데이터를 입력으로 받아들이고, `Fusion-KVE`를 통해 이력 수술 상태를 추정하여 궤적 및 상태를 공동으로 예측하는 종단 간 `LSTM` 인코더-디코더 모델입니다.
*   **주요 성과**: 이 모델은 최대 2초 미래까지의 예측에서 높은 정확도를 달성했습니다(예: 0.5초 궤적 예측 오차 1.07mm, 2초 상태 예측 정확도 82.11%). 특히, 관심 영역(RoI)에 초점을 맞춘 시각 특징과 이력 상태 시퀀스를 활용하는 것이 예측 성능을 크게 향상시키며, 이는 인간 주석의 일관성 수준에 필적하는 견고성을 보여줍니다.