{
  "url": "http://arxiv.org/abs/2009.11937v1",
  "title": "daVinciNet: Joint Prediction of Motion and Surgical State in\n  Robot-Assisted Surgery",
  "authors": "Yidan Qin, Seyedshams Feyzabadi, Max Allan, Joel W. Burdick, Mahdi Azizian",
  "year": 2020,
  "abstract": "This paper presents a technique to concurrently and jointly predict the\nfuture trajectories of surgical instruments and the future state(s) of surgical\nsubtasks in robot-assisted surgeries (RAS) using multiple input sources. Such\npredictions are a necessary first step towards shared control and supervised\nautonomy of surgical subtasks. Minute-long surgical subtasks, such as suturing\nor ultrasound scanning, often have distinguishable tool kinematics and visual\nfeatures, and can be described as a series of fine-grained states with\ntransition schematics. We propose daVinciNet - an end-to-end dual-task model\nfor robot motion and surgical state predictions. daVinciNet performs concurrent\nend-effector trajectory and surgical state predictions using features extracted\nfrom multiple data streams, including robot kinematics, endoscopic vision, and\nsystem events. We evaluate our proposed model on an extended Robotic\nIntra-Operative Ultrasound (RIOUS+) imaging dataset collected on a da Vinci Xi\nsurgical system and the JHU-ISI Gesture and Skill Assessment Working Set\n(JIGSAWS). Our model achieves up to 93.85% short-term (0.5s) and 82.11%\nlong-term (2s) state prediction accuracy, as well as 1.07mm short-term and\n5.62mm long-term trajectory prediction error."
}