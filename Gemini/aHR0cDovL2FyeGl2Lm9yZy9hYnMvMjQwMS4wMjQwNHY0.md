# Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks

Hartwig H. Hochmair, Levente Juhász, and Takoda Kemp

## 🧩 Problem to Solve

기존의 대규모 언어 모델(LLM) 공간 작업 성능 평가는 주로 ChatGPT에 초점을 맞춰왔습니다. 이 연구는 이 연구 격차를 해소하고, 다양한 최신 챗봇(ChatGPT-4, Gemini, Claude-3, Copilot)이 공간 관련 작업을 얼마나 정확하게 수행하는지 비교 평가하고자 합니다. 특히, 챗봇이 공간적 이해, 프로그래밍, 추론 능력에서 어떤 강점과 약점을 보이는지 파악하는 것이 목표입니다.

## ✨ Key Contributions

* **광범위한 챗봇 비교:** ChatGPT-4, Gemini, Claude-3, Copilot 등 4가지 주요 챗봇에 대해 7가지 공간 작업 범주에 걸쳐 76개의 작업에 대한 제로샷(zero-shot) 정확성 평가를 수행했습니다.
* **성능 차이 식별:** 챗봇들은 공간 리터러시, GIS 이론, 코드 및 함수 해석 작업에서 전반적으로 우수했지만, 매핑, 코드 작성, 공간 추론에서는 약점을 드러냈습니다. 챗봇 간의 정확성에도 상당한 차이가 있었습니다.
* **일관성 평가:** 각 챗봇에 반복된 작업을 할당하여 응답 일관성을 평가했으며, 대부분의 작업 범주에서 80% 이상의 높은 일관성을 보였습니다.
* **상세한 성능 분석:** GPT-4가 전반적으로 가장 우수한 성능을 보였으나, 코딩 작업에서는 Copilot이, 공간 추론에서는 Gemini와 Copilot이 더 나은 결과를 나타내는 등 미묘한 차이를 발견했습니다. 또한 Claude-3가 가장 장황한 답변을 제공했습니다.

## 📎 Related Works

* **LLM의 다재다능함과 한계:** LLM은 프로그래밍, 산술 추론, 시계열 예측, 지명 인식, 이미지 분류 등 다양한 분야에서 활용 가능성을 보였으나, 환각(hallucination), 부정확한 수학 능력, 문맥 이해 부족 등의 한계도 지적되어 왔습니다 (Kung et al., 2023; Ray, 2023; Tyson, 2023).
* **GeoQA 벤치마크:** 지리공간 질의응답(GeoQA) 시스템 평가를 위한 GeoQuestions201, GeoAnQu, GeoQuestions1089와 같은 데이터셋이 존재하지만, LLM의 응답 정확성 평가에는 일부 제한이 있습니다 (Punjani et al., 2018; H. Xu et al., 2020).
* **기존 LLM 비교 연구:** GPT-4가 GPT-3.5, Bing Chat (Copilot), Bard (Gemini) 보다 전반적으로 우수한 성능을 보였으며, 언어 이해에 강하지만 수학, 코딩, 추론에 어려움을 겪는다는 연구 결과가 있었습니다 (Rudolph et al., 2023; Borji & Mohammadian, 2023). Claude 3 Opus는 특정 공학 문제에서 GPT-4를 능가하기도 했습니다 (Kevian et al., 2024).
* **공간 작업 LLM 평가 연구 격차:** 기존 LLM의 공간 작업(공간 추론, 공간 리터러시, GIS 작업 등) 평가는 주로 OpenAI의 챗봇(GPT-3.5, GPT-4)에 집중되었으며, 다른 챗봇에 대한 연구는 부족했습니다 (Cohn, 2023; Mooney et al., 2023).

## 🛠️ Methodology

1. **챗봇 선정:** ChatGPT-4, Gemini, Claude-3, Copilot의 4가지 주요 챗봇을 대상으로 했습니다.
2. **작업 설계 및 분류:**
    * 총 76개의 공간 관련 작업을 7가지 범주로 나눴습니다: 공간 리터러시 (16개), GIS 개념 (10개), 매핑 (10개), 함수 해석 (10개), 코드 설명 (10개), 코딩 (10개), 공간 추론 (10개).
    * 지리공간 지식, 시각화, 코드 작성/해석 능력, 공간 추론 기술을 평가하도록 설계했습니다.
    * 입력 모드는 텍스트 프롬프트만 사용했습니다.
3. **실험 방식:**
    * 각 작업마다 새로운 채팅을 시작하여 이전 컨텍스트의 영향을 배제했습니다.
    * 각 작업은 두 번씩 주어져 응답의 일관성을 평가했습니다.
    * 제로샷 프롬프트(zero-shot prompting) 방식을 사용하여 추가 예시 없이 모델의 훈련 데이터 기반 지식에 의존했습니다.
4. **분석 방법:**
    * **정확성 평가:** 두 번의 시도 모두 올바른 경우에만 정확한 것으로 간주했습니다. 코딩 및 매핑 작업의 경우 생성된 코드를 실행하여 검증했습니다.
    * **통계 분석:**
        * 챗봇 간 및 작업 범주 간 정확성 비율 차이의 통계적 유의성을 검정하기 위해 Pearson의 카이제곱($\chi^2$) 테스트를 사용했습니다.
        * 사후 테스트에는 Bonferroni 보정을 적용했습니다.
    * **응답 길이 분석:** 공간 리터러시 및 GIS 개념 작업에 대한 챗봇별 응답 길이를 Wilcoxon 순위합 테스트 및 Kruskal-Wallis 테스트로 분석했습니다.
    * **일관성 평가:** 반복 작업에서 응답의 일치율(matching rate)을 계산했습니다.
    * **정성적 분석:** 관찰된 어려움과 과제를 예시와 함께 논의했습니다.

## 📊 Results

* **전반적인 정확성:** GPT-4가 가장 높은 정확도(76.3%)를 보였고, Copilot (71.1%), Claude-3 (64.5%), Gemini (55.3%) 순이었습니다. 챗봇 간 정확도 차이는 통계적으로 유의미했으나, 개별 챗봇 쌍 간의 사후 분석에서는 유의미한 차이가 없었습니다 (GPT-4 vs. Gemini는 근접).
* **작업 범주별 정확성:**
  * **높은 정확도:** GIS 개념 (95.0%), 코드 설명 (95.0%), 공간 리터러시 (81.3%), 함수 해석 (82.5%).
  * **낮은 정확도:** 매핑 (25.0%), 공간 추론 (32.5%), 코딩 (47.5%).
  * 공간 리터러시, GIS 개념, 함수 해석, 코드 설명은 매핑, 코딩, 공간 추론보다 유의미하게 더 정확했습니다.
* **응답 일관성:** 대부분의 챗봇 및 작업 범주에서 80% 이상의 높은 일관성을 보였습니다. 단, Claude-3와 Copilot의 매핑 작업(60%), GPT-4의 공간 추론 작업(70%)에서는 일관성이 낮았습니다.
* **응답 길이:** Claude-3가 가장 장황한 답변을 제공했으며, GPT-4와 Gemini가 가장 간결했습니다. 공간 리터러시와 GIS 개념 질문 모두에서 GPT-4와 Claude-3, Gemini와 Claude-3 간의 응답 길이 차이가 통계적으로 유의미했습니다.
* **정성적 분석 하이라이트:**
  * **공간 리터러시:** UNESCO 세계 유산 목록 최신 정보 누락, 고속도로-강 교차점, 도시 정렬 등에서 어려움이 있었습니다.
  * **GIS 개념:** "그리드 북쪽이 북극 방향인가?" 질문에서 Gemini와 Claude-3가 오답을 제공했습니다.
  * **매핑:** 가장 어려운 범주로, Mapbox API 사용, R `tmap` 패키지를 이용한 세계 도시 매핑 등에서 대부분의 챗봇이 실패했습니다. GPT-4가 6/10으로 가장 잘 수행했습니다.
  * **함수 해석:** GPT-4가 10/10으로 가장 우수했으며, Gemini와 Copilot이 반변량 함수(semi-variogram) 식($\gamma(h) = \frac{1}{2} E(Z(s) - Z(s+h))^2$)을 인식하지 못했습니다.
  * **코드 설명:** GPT-4, Claude-3, Copilot은 10/10으로 우수했으나, Gemini는 두 개의 작업(Python 선 교차점 찾기, R 다각형 무게중심 찾기)에서 실패했습니다.
  * **코딩:** 전반적으로 어려웠으며, Copilot이 7/10으로 가장 강점을 보였습니다. Quicksort 알고리즘의 Python-R 번역은 모든 챗봇이 실패했습니다. Haversine 거리 계산 코드 번역은 모든 챗봇이 성공했습니다.
  * **공간 추론:** 정확도가 2/10(GPT-4) ~ 4/10(Gemini, Copilot)으로 가장 낮았습니다. Tic Tac Toe, RCC-8 관계 추론, 3D 상자 정렬 등에서 어려움을 겪었습니다.

## 🧠 Insights & Discussion

* **LLM 성능의 진화:** LLM의 성능은 빠르게 향상되고 있으며, 공간 작업 능력도 시간이 지남에 따라 개선될 것으로 예상됩니다.
* **챗봇별 강점과 약점:**
  * GPT-4는 전반적으로 가장 우수했지만, 공간 추론에서는 가장 낮은 성능을 보였고 코딩에서도 Copilot보다 낮은 점수를 받았습니다.
  * Copilot은 코딩 작업에서 가장 강한 성능을 보였습니다. 이는 Microsoft가 GitHub와 같은 광범위한 생태계와 코드 저장소에 접근할 수 있기 때문일 수 있습니다.
  * Claude-3는 더 많은 배경 정보를 제공하는 장황한 답변을 선호하는 사용자에게 적합할 수 있습니다.
* **공간 작업의 어려움:** 매핑, 코딩, 공간 추론 작업은 LLM에게 특히 어려웠습니다. 이는 Mapbox API 사용의 어려움, 라이브러리 함수 매개변수 오용, 인간과 같은 추론 및 문맥 이해의 부족(hallucination, 비일관성)과 관련이 있습니다.
* **향후 개선 방향:**
  * **다중 모달 통합:** 이미지, 지도, 공간 벡터와 같은 추가 데이터 유형을 통합하여 복잡한 시각적 데이터 및 추론을 요구하는 지리공간 작업에 대한 LLM의 성능을 향상시켜야 합니다.
  * **전문적인 훈련:** 지리공간 정보 및 공간 추론 작업을 포함하는 도메인별 데이터셋으로 모델을 미세 조정(fine-tune)해야 합니다.
  * **고급 프롬프트 엔지니어링:** 사고의 사슬(Chain-of-Thought) 및 자가 점검 프롬프트와 같은 기술을 활용하여 추론 및 의사 결정 능력을 개선할 수 있습니다.
* **연구의 한계:** 이 연구는 제로샷 프롬프트에 국한되었으며, 복합적인 간접 질의응답(indirect QA)이나 다중 모달 입력은 고려하지 않았습니다. 향후 연구에서는 이러한 측면을 탐구할 필요가 있습니다.

## 📌 TL;DR

이 연구는 ChatGPT-4, Gemini, Claude-3, Copilot의 4가지 주요 챗봇이 76가지 공간 관련 작업을 얼마나 정확히 수행하는지 제로샷 방식으로 비교 평가했습니다. 결과적으로 GPT-4가 전반적으로 가장 높은 정확도를 보였지만, 챗봇들은 매핑, 코딩, 특히 공간 추론 작업에서 상당한 약점을 드러냈습니다. 반면, 공간 리터러시, GIS 이론, 코드 해석에서는 비교적 우수했습니다. Copilot은 코딩에서 강점을 보였고, Claude-3는 더 장황한 답변을 제공했습니다. 응답 일관성은 대부분 높았으나, 어려운 범주에서는 낮아지는 경향을 보였습니다. 이는 LLM이 지리공간 작업에서 여전히 한계가 있으며, 다중 모달 통합, 전문적인 훈련, 고급 프롬프트 엔지니어링을 통한 개선이 필요함을 시사합니다.
