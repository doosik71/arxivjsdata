{
  "title": "RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series\n  Tasks",
  "authors": "Haowen Hou, F. Richard Yu",
  "year": 2024,
  "url": "http://arxiv.org/abs/2401.09093v1",
  "abstract": "Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and\nGRU, have historically held prominence in time series tasks. However, they have\nrecently seen a decline in their dominant position across various time series\ntasks. As a result, recent advancements in time series forecasting have seen a\nnotable shift away from RNNs towards alternative architectures such as\nTransformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs,\nwe design an efficient RNN-based model for time series tasks, named RWKV-TS,\nwith three distinctive features: (i) A novel RNN architecture characterized by\n$O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture\nlong-term sequence information compared to traditional RNNs. (iii) High\ncomputational efficiency coupled with the capacity to scale up effectively.\nThrough extensive experimentation, our proposed RWKV-TS model demonstrates\ncompetitive performance when compared to state-of-the-art Transformer-based or\nCNN-based models. Notably, RWKV-TS exhibits not only comparable performance but\nalso demonstrates reduced latency and memory utilization. The success of\nRWKV-TS encourages further exploration and innovation in leveraging RNN-based\napproaches within the domain of Time Series. The combination of competitive\nperformance, low latency, and efficient memory usage positions RWKV-TS as a\npromising avenue for future research in time series tasks. Code is available\nat:\\href{https://github.com/howard-hou/RWKV-TS}{\nhttps://github.com/howard-hou/RWKV-TS}"
}