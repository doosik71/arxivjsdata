{
  "title": "Linearizing Large Language Models",
  "authors": "Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar",
  "year": 2024,
  "url": "http://arxiv.org/abs/2405.06640v1",
  "abstract": "Linear transformers have emerged as a subquadratic-time alternative to\nsoftmax attention and have garnered significant interest due to their\nfixed-size recurrent state that lowers inference cost. However, their original\nformulation suffers from poor scaling and underperforms compute-matched\ntransformers. Recent linear models such as RWKV and Mamba have attempted to\naddress these shortcomings by proposing novel time-mixing and gating\narchitectures, but pre-training large language models requires significant data\nand compute investments. Thus, the search for subquadratic architectures is\nlimited by the availability of compute and quality pre-training datasets. As a\ncost-effective alternative to pre-training linear transformers, we propose\nScalable UPtraining for Recurrent Attention (SUPRA). We present a method to\nuptrain existing large pre-trained transformers into Recurrent Neural Networks\n(RNNs) with a modest compute budget. This allows us to leverage the strong\npre-training data and performance of existing transformer LLMs, while requiring\n5% of the training cost. We find that our linearization technique leads to\ncompetitive performance on standard benchmarks, but we identify persistent\nin-context learning and long-context modeling shortfalls for even the largest\nlinear models. Our code and models can be found at\nhttps://github.com/TRI-ML/linear_open_lm."
}