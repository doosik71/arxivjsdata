{
  "url": "http://arxiv.org/abs/2305.07239v2",
  "title": "T-former: An Efficient Transformer for Image Inpainting",
  "authors": "Ye Deng, Siqi Hui, Sanping Zhou, Deyu Meng, Jinjun Wang",
  "year": 2023,
  "abstract": "Benefiting from powerful convolutional neural networks (CNNs), learning-based\nimage inpainting methods have made significant breakthroughs over the years.\nHowever, some nature of CNNs (e.g. local prior, spatially shared parameters)\nlimit the performance in the face of broken images with diverse and complex\nforms. Recently, a class of attention-based network architectures, called\ntransformer, has shown significant performance on natural language processing\nfields and high-level vision tasks. Compared with CNNs, attention operators are\nbetter at long-range modeling and have dynamic weights, but their computational\ncomplexity is quadratic in spatial resolution, and thus less suitable for\napplications involving higher resolution images, such as image inpainting. In\nthis paper, we design a novel attention linearly related to the resolution\naccording to Taylor expansion. And based on this attention, a network called\n$T$-former is designed for image inpainting. Experiments on several benchmark\ndatasets demonstrate that our proposed method achieves state-of-the-art\naccuracy while maintaining a relatively low number of parameters and\ncomputational complexity. The code can be found at\n\\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\\_image\\_inpainting}"
}