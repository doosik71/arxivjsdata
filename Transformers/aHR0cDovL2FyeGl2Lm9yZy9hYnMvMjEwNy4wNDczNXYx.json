{
  "url": "http://arxiv.org/abs/2107.04735v1",
  "title": "Local-to-Global Self-Attention in Vision Transformers",
  "authors": "Jinpeng Li, Yichao Yan, Shengcai Liao, Xiaokang Yang, Ling Shao",
  "year": 2021,
  "abstract": "Transformers have demonstrated great potential in computer vision tasks. To\navoid dense computations of self-attentions in high-resolution visual data,\nsome recent Transformer models adopt a hierarchical design, where\nself-attentions are only computed within local windows. This design\nsignificantly improves the efficiency but lacks global feature reasoning in\nearly stages. In this work, we design a multi-path structure of the\nTransformer, which enables local-to-global reasoning at multiple granularities\nin each stage. The proposed framework is computationally efficient and highly\neffective. With a marginal increasement in computational overhead, our model\nachieves notable improvements in both image classification and semantic\nsegmentation. Code is available at https://github.com/ljpadam/LG-Transformer"
}