# Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning
Qiang Zhu, Kuan Lu, Menghao Huo, Yuxiao Li

## 🧩 Problem to Solve
이미지-간(image-to-image) 변환은 특정 도메인의 이미지를 다른 도메인의 이미지로 매핑하여 스타일 변환, 외관 변화, 도메인 적응과 같은 작업을 수행하는 중요한 컴퓨터 비전 과제입니다. 기존 GAN(Generative Adversarial Network) 기반 모델(예: Pix2Pix)은 훈련 불안정성, 모드 붕괴(mode collapse), 고해상도 또는 세부적인 이미지 생성의 어려움과 같은 문제를 겪으며, 특히 정확한 구조 보존 및 스타일 정확도가 요구되는 작업에서 이러한 한계가 두드러집니다. 확산 모델(Diffusion models)은 우수한 이미지 품질과 훈련 안정성을 제공하지만, 이를 조건부 이미지-간 변환에 적용하는 것은 여전히 활발한 연구 분야입니다.

## ✨ Key Contributions
*   클래스 레이블 대신 이미지 임베딩을 사용하여 조건 부여된 DiT(Diffusion Transformer) 프레임워크를 이미지-간 변환을 위해 도입했습니다.
*   생성 이미지의 품질과 스타일 충실도를 향상시키기 위해 지각적(LPIPS) 및 의미론적(CLIP) 일관성을 모두 통합하는 복합 손실 함수를 제안했습니다.
*   `face2comics` 및 `edges2shoes` 쌍별 데이터셋에서 제안된 접근 방식의 효과를 입증하여 기존 GAN 기반 방법론에 비해 고품질의 신원 보존 변환을 달성했습니다.

## 📎 Related Works
*   **GANs (Generative Adversarial Networks)**: 이미지 생성 및 도메인 변환의 발전을 이끈 [1]의 기본 모델. Pix2Pix [2]는 조건부 GAN을 사용하여 쌍별 이미지 변환에 효과적임을 입증했습니다.
*   **DDPMs (Denoising Diffusion Probabilistic Models)**: 잡음 제거 과정을 역전시켜 데이터를 합성하는 생성 모델 [5]. 높은 이미지 품질과 안정적인 훈련이 장점입니다.
*   **LDMs (Latent Diffusion Models)**: 고차원 픽셀 공간 대신 더 작은 잠재 공간에서 작동하여 확산 기반 이미지 생성의 계산 비용을 줄인 DDPM의 효율적인 확장 [6].
*   **Transformers 및 ViT (Vision Transformer)**: NLP 분야에서 시작된 [7] 트랜스포머는 셀프-어텐션 메커니즘을 통해 장거리 의존성과 전역적 컨텍스트를 효과적으로 포착합니다. ViT [19]는 이미지를 패치로 분할하여 트랜스포머 아키텍처를 이미지 데이터에 직접 적용합니다.
*   **DiT (Diffusion Transformer)**: 기존 LDM의 U-Net 백본을 트랜스포머 기반 백본으로 대체하여 확산 모델의 확장성 및 고해상도 이미지 생성 능력을 향상시킨 모델 [8].
*   **CLIP (Contrastive Language-Image Pre-training)**: 이미지 임베딩을 추출하여 의미론적 일관성을 강화하는 데 사용되는 사전 훈련된 모델 [9].
*   **LPIPS (Learned Perceptual Image Patch Similarity)**: 지각적 유사성을 측정하는 데 사용되는 손실 함수 [10].

## 🛠️ Methodology
본 연구는 CLIP 기반 이미지 컨디셔닝을 통해 쌍별 이미지-간 변환을 수행하는 DiT 프레임워크를 제안합니다.

1.  **데이터 전처리**:
    *   **타겟 도메인 이미지**: 사전 훈련된 VAE(Variational Autoencoder)를 사용하여 $256 \times 256$으로 크기를 조정한 후 저차원 잠재 공간으로 인코딩됩니다. 이 잠재 특징 맵은 비겹치는 패치로 분할되어 선형 투영을 통해 임베딩 공간으로 변환됩니다.
    *   **조건 부여 이미지 (소스 이미지)**: 사전 훈련된 CLIP-ViT-L/14 모델 [9]에 맞춰 $224 \times 224$로 크기를 조정한 후 의미론적 잠재 표현이 추출됩니다. 이 CLIP 임베딩은 동일한 히든 공간으로 투영되어 타임스텝(timestep) 임베딩과 합산됩니다.

2.  **아키텍처**:
    *   원래 DiT와 달리, 본 방법은 사전 훈련된 CLIP 인코더에서 추출된 이미지 임베딩을 사용하여 미세한 의미론적 컨디셔닝을 제공합니다.
    *   VAE를 통해 인코딩된 노이즈가 있는 잠재 표현은 패치로 나뉘고, Vision Transformer(ViT)의 패치화 전략에 따라 패치 임베딩으로 변환됩니다.
    *   이 패치 임베딩은 DiT 블록을 통과하며, 여기에는 멀티-헤드 셀프-어텐션(Multi-Head Self-Attention, MSA)과 피드-포워드 네트워크(Feed-Forward Network, FFN)가 포함됩니다.
    *   **CLIP 컨디셔닝**: CLIP 임베딩은 두 가지 핵심 메커니즘을 통해 확산 과정에 주입됩니다.
        *   **크로스-어텐션 (Cross-Attention)**: 노이즈가 있는 잠재 이미지의 패치 임베딩이 소스 이미지의 CLIP 임베딩에 주의를 기울여 의미론적 정렬을 유지합니다.
        *   **AdaLN-Zero 변조 (AdaLN-Zero Modulation)**: AdaLN-Zero 블록을 통해 피처 활성화가 적응적으로 변조되며, 잔여 연결에 대한 스케일링 파라미터($\alpha_1, \alpha_2$)와 레이어 정규화에 대한 게인/바이어스 파라미터($\gamma_1, \beta_1, \gamma_2, \beta_2$)를 도입하여 잡음 제거 역학을 미세하게 제어합니다.
    *   DiT 백본 후, 모델은 타임스텝 임베딩과 CLIP에서 파생된 이미지 피처에 조건부로 각 패치 임베딩에 추가된 잡음을 예측합니다. 반복적인 정제를 통해 모델은 잠재 표현을 점진적으로 잡음 제거합니다.
    *   마지막으로, 정제된 잠재 공간은 사전 훈련된 VAE 디코더를 사용하여 픽셀 공간으로 디코딩되어, 입력의 구조적 내용을 보존하면서 타겟 도메인 스타일을 반영하는 출력 이미지를 생성합니다.

3.  **손실 함수 설계**:
    *   전체 손실 함수는 픽셀 수준 정확도, 지각적 충실도, 의미론적 일관성을 균형 있게 조절하는 복합 손실로 구성됩니다:
        $$L_{\{total\}} = \lambda_{\{rec\}} \cdot L_{\{rec\}} + \lambda_{\{LPIPS\}} \cdot L_{\{LPIPS\}} + \lambda_{\{CLIP\}} \cdot L_{\{CLIP\}}$$
    *   **재구성 손실 ($L_{\{rec\}}$)**: 생성된 이미지와 원본 타겟 이미지 간의 L1 손실.
        $$L_{\{rec\}} = \|\hat{x}_{\{target\}} - x_{\{target\}}\|_1$$
    *   **지각적 손실 ($L_{\{LPIPS\}}$)**: 사전 훈련된 네트워크에서 추출된 깊은 특징을 비교하는 LPIPS 손실.
        $$L_{\{LPIPS\}} = \|\phi_{\{LPIPS\}}(\hat{x}_{\{target\}}) - \phi_{\{LPIPS\}}(x_{\{target\}})\|_2^2$$
    *   **의미론적 일관성 손실 ($L_{\{CLIP\}}$)**: 생성된 이미지와 소스 도메인의 조건 부여 이미지 간의 CLIP 기반 코사인 유사도 손실.
        $$L_{\{CLIP\}} = 1 - \cos(\phi_{\{CLIP\}}(\hat{x}_{\{target\}}), \phi_{\{CLIP\}}(x_{\{source\}}))$$

## 📊 Results
*   **데이터셋**: `face2comics` (실제 얼굴 -> 만화) 및 `edges2shoes` (엣지 맵 -> 신발 이미지) 두 가지 쌍별 데이터셋에서 모델을 평가했습니다.
*   **훈련 결과**: 훈련 손실 곡선은 시간이 지남에 따라 점진적으로 감소하여 모델이 데이터를 효과적으로 학습하고 있음을 보여줍니다. `edges2shoes`와 같은 더 큰 데이터셋에서 훈련된 모델이 `face2comics`에 비해 더 낮은 최종 손실에 수렴하여 더 나은 최적화와 일반화에 기여함을 시사합니다.
*   **정성적 비교**:
    *   CLIP-conditioned DiT 모델은 Pix2Pix 및 Pix2PixHD와 비교하여 현저히 높은 품질, 더 선명한 세부 사항, 더 적은 아티팩트를 가진 이미지를 생성했습니다.
    *   특히, 엣지, 하이라이트, 질감, 헤어스타일, 안경과 같은 미세한 세부 사항을 더 잘 보존했습니다.
    *   `edges2shoes` 데이터셋에서 `face2comics`보다 더 나은 결과를 보였습니다.
    *   작은 데이터셋(예: CMP Facade Database)에서는 Pix2Pix가 DiT 모델보다 우수했으며, 이는 DiT가 큰 훈련 데이터셋에서 더 큰 이점을 얻음을 시사합니다.
*   **계산 비용**:
    *   DiT 기반 모델은 GAN 기반 모델에 비해 훈련 및 추론 시간 모두에서 훨씬 더 높은 계산 비용을 발생시킵니다 (DiT 모델 크기: 12.92 GB, 훈련 속도: 0.48 steps/s, 추론 속도: 0.028 img/s).
    *   이러한 높은 비용에도 불구하고, DiT는 생성 품질, 구조 보존, 스타일 일관성 측면에서 주요 이점을 제공하며, GAN의 훈련 문제(모드 붕괴)를 피하여 더 안정적이고 견고한 수렴을 가능하게 합니다.

## 🧠 Insights & Discussion
본 연구는 CLIP 기반 이미지 컨디셔닝을 DiT 아키텍처에 적용하여 쌍별 이미지-간 변환에서 기존 GAN 기반 모델을 능가하는 고품질의 의미론적으로 일관된 결과를 달성했습니다. 특히, 대규모 데이터셋(`edges2shoes`)에서 확산 모델의 확장성과 안정성이 입증되었으며, 이는 적대적 훈련 방식에 대한 유망한 대안임을 보여줍니다.

주요 한계는 확산 샘플링의 반복적인 특성과 트랜스포머 기반 아키텍처로 인한 훈련 및 추론 시 상당한 계산 비용 증가입니다. 작은 데이터셋에서는 GAN 모델에 비해 성능이 떨어질 수 있다는 점도 지적됩니다. 그럼에도 불구하고, 품질 개선은 이러한 비용 트레이드오프를 정당화한다고 판단됩니다.

향후 연구에서는 주기가 일관된(cycle-consistent) 손실을 통합하여 `Cycle-DiT` 모델을 개발함으로써 비쌍별(unpaired) 이미지-간 변환으로 프레임워크를 확장할 계획입니다. 또한, 출력 충실도 및 수렴을 더욱 개선하기 위해 더 긴 훈련 스케줄과 더 큰 배치 크기를 탐색할 것입니다.

## 📌 TL;DR
**문제**: 기존 GAN 기반 이미지-간 변환 모델은 훈련 불안정성, 모드 붕괴, 세부적인 고품질 이미지 생성에 어려움이 있습니다.
**방법**: CLIP 이미지 임베딩을 조건으로 활용하는 Diffusion Transformer (DiT) 프레임워크를 제안합니다. 이 프레임워크는 L1 재구성 손실, LPIPS 지각적 손실, CLIP 의미론적 일관성 손실을 결합하여 변환 품질을 높입니다.
**결과**: `face2comics` 및 `edges2shoes` 데이터셋에서 GAN 기반 모델(Pix2Pix, Pix2PixHD)보다 더 높은 품질과 의미론적 충실도를 가진 이미지를 생성하며, 특히 대규모 데이터셋에서 뛰어난 성능을 보입니다. 다만, 훈련 및 추론 시 상당한 계산 비용이 발생합니다.