{
  "url": "http://arxiv.org/abs/2110.09408v3",
  "title": "HRFormer: High-Resolution Transformer for Dense Prediction",
  "authors": "Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, Jingdong Wang",
  "year": 2021,
  "abstract": "We present a High-Resolution Transformer (HRFormer) that learns\nhigh-resolution representations for dense prediction tasks, in contrast to the\noriginal Vision Transformer that produces low-resolution representations and\nhas high memory and computational cost. We take advantage of the\nmulti-resolution parallel design introduced in high-resolution convolutional\nnetworks (HRNet), along with local-window self-attention that performs\nself-attention over small non-overlapping image windows, for improving the\nmemory and computation efficiency. In addition, we introduce a convolution into\nthe FFN to exchange information across the disconnected image windows. We\ndemonstrate the effectiveness of the High-Resolution Transformer on both human\npose estimation and semantic segmentation tasks, e.g., HRFormer outperforms\nSwin transformer by $1.3$ AP on COCO pose estimation with $50\\%$ fewer\nparameters and $30\\%$ fewer FLOPs. Code is available at:\nhttps://github.com/HRNet/HRFormer."
}