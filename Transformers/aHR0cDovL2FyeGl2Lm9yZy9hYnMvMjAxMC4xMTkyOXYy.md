# AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby

## 🧩 Problem to Solve
자연어 처리(NLP) 분야에서 트랜스포머(Transformer) 아키텍처가 사실상의 표준이 되었지만, 컴퓨터 비전(CV) 분야에서의 적용은 여전히 제한적이었습니다. 기존의 시도들은 합성곱 신경망(CNN)과 어텐션(Attention)을 결합하거나, CNN의 특정 구성 요소를 대체하는 방식이었습니다. 본 논문은 이러한 CNN 의존성 없이, 이미지 패치(patch) 시퀀스에 직접 순수한 트랜스포머를 적용하여 이미지 분류 태스크에서 좋은 성능을 달성할 수 있는지 탐구하고자 합니다. 특히, CNN이 내재적으로 가지고 있는 국소성(locality) 및 이동 등변성(translation equivariance)과 같은 귀납적 편향(inductive bias)이 없는 트랜스포머가 대규모 데이터 없이도 잘 일반화될 수 있는지가 핵심 문제입니다.

## ✨ Key Contributions
*   **Vision Transformer (ViT) 제안:** 이미지를 고정 크기 패치 시퀀스로 변환하여 표준 트랜스포머 인코더에 직접 입력하는 순수한 트랜스포머 기반 이미지 인식 모델인 ViT를 제안했습니다.
*   **대규모 사전 학습의 효과 입증:** 중간 규모(ImageNet) 또는 소규모(CIFAR-100, VTAB) 벤치마크에서도 ViT가 뛰어난 성능을 달성할 수 있음을 대규모 데이터셋(ImageNet-21k, JFT-300M)을 통한 사전 학습으로 입증했습니다.
*   **SOTA CNN 모델 능가:** 충분한 규모로 사전 학습된 ViT는 최신 CNN 기반 모델(예: BiT, Noisy Student)과 비교하여 동등하거나 더 나은 성능을 달성하면서도, 사전 학습에 필요한 계산 자원은 훨씬 적음을 보였습니다.
*   **귀납적 편향 극복:** 트랜스포머가 CNN에 비해 이미지 특화 귀납적 편향이 적지만, 대규모 데이터 학습을 통해 이러한 한계를 극복하고 심지어 이점을 가져올 수 있음을 보여주었습니다.
*   **모델 내부 동작 분석:** ViT의 초기 선형 임베딩 필터, 학습된 위치 임베딩의 유사성, 그리고 어텐션 헤드의 어텐션 거리(receptive field와 유사) 변화를 분석하여 ViT가 이미지 정보를 처리하는 방식을 시각적으로 설명했습니다.

## 📎 Related Works
*   **NLP의 트랜스포머 성공:** Vaswani et al. (2017)의 트랜스포머 제안 이후 BERT (Devlin et al., 2019), GPT (Radford et al., 2018; 2019; Brown et al., 2020) 등 대규모 모델의 성공.
*   **컴퓨터 비전에서의 어텐션:**
    *   로컬 어텐션(Local attention) 적용: Parmar et al. (2018), Hu et al. (2019), Ramachandran et al. (2019), Zhao et al. (2020).
    *   스파스 트랜스포머(Sparse Transformers): Child et al. (2019).
    *   축별 어텐션(Axial attention): Weissenborn et al. (2019), Ho et al. (2019), Wang et al. (2020a).
*   **CNN과 어텐션 결합:** Bello et al. (2019), Carion et al. (2020), Wang et al. (2018), Sun et al. (2019), Wu et al. (2020) 등.
*   **ViT와 유사한 연구:** Cordonnier et al. (2020)은 $2 \times 2$ 패치를 사용했지만, 대규모 사전 학습을 통한 SOTA CNN과의 경쟁력은 입증하지 못함.
*   **생성 모델로서의 트랜스포머:** Image GPT (iGPT) (Chen et al., 2020a)는 비지도 학습 기반의 이미지 생성 모델.
*   **대규모 이미지 인식 및 전이 학습:** ImageNet-21k, JFT-300M 등 대규모 데이터셋을 활용한 연구 (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020; Kolesnikov et al., 2020).

## 🛠️ Methodology
ViT는 NLP의 표준 트랜스포머 구조를 가능한 한 적은 수정으로 이미지에 적용합니다.
1.  **이미지 패치 변환:**
    *   입력 이미지 $x \in \mathbb{R}^{H \times W \times C}$를 고정 크기의 2D 패치로 나눕니다.
    *   각 패치 $(P, P)$는 평탄화(flatten)되어 $P^2 \cdot C$ 차원의 벡터가 됩니다.
    *   총 $N = HW/P^2$개의 패치로 구성된 시퀀스 $x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$를 생성합니다.
2.  **선형 임베딩:** 평탄화된 각 패치는 학습 가능한 선형 투영(linear projection)을 통해 트랜스포머의 일정한 잠재 벡터 크기 $D$로 임베딩됩니다. 이를 패치 임베딩(patch embedding)이라고 합니다.
3.  **클래스 토큰 추가:** BERT의 `[class]` 토큰과 유사하게, 학습 가능한 특별한 클래스 임베딩을 패치 임베딩 시퀀스의 맨 앞에 추가합니다. 트랜스포머 인코더의 출력에서 이 클래스 토큰의 상태(representation)가 전체 이미지의 최종 표현으로 사용됩니다.
4.  **위치 임베딩:** 패치 임베딩에 1D 학습 가능한 위치 임베딩(position embedding)을 더하여 공간 정보를 유지합니다. (2D-aware 위치 임베딩은 성능 향상에 큰 영향을 주지 않음).
5.  **트랜스포머 인코더:** 위치 임베딩이 더해진 벡터 시퀀스는 표준 트랜스포머 인코더의 입력으로 사용됩니다. 인코더는 멀티헤드 셀프 어텐션(MSA)과 MLP 블록이 번갈아 가며 구성되며, 각 블록 앞에 LayerNorm이 적용되고, 각 블록 후에 잔차 연결(residual connection)이 추가됩니다. MLP는 GELU 비선형성을 포함하는 두 개의 레이어로 구성됩니다.
6.  **하이브리드 아키텍처 (선택 사항):** 순수한 이미지 패치 대신, CNN의 피처 맵(feature map)에서 패치를 추출하여 트랜스포머의 입력으로 사용할 수도 있습니다.
7.  **파인튜닝 및 고해상도 처리:**
    *   대규모 데이터셋으로 사전 학습 후, 작은 다운스트림 태스크에 파인튜닝합니다.
    *   사전 학습된 예측 헤드를 제거하고 새로운 선형 분류 레이어를 부착합니다.
    *   파인튜닝 시 더 높은 해상도의 이미지를 사용할 수 있습니다. 이때 패치 크기는 동일하게 유지되므로, 더 긴 시퀀스 길이가 됩니다. 사전 학습된 위치 임베딩은 원본 이미지에서의 위치에 따라 2D 보간하여 조정됩니다. (이러한 해상도 조정과 패치 추출은 2D 구조에 대한 귀납적 편향이 수동으로 주입되는 유일한 부분입니다.)
8.  **학습:** Adam 옵티마이저, 높은 가중치 감쇠(weight decay), 선형 학습률 웜업(warmup) 및 감쇠를 사용합니다. 파인튜닝에는 SGD를 사용합니다.

## 📊 Results
*   **SOTA 성능 달성:** JFT-300M 데이터셋으로 사전 학습된 ViT-L/16 모델은 동일한 데이터셋으로 사전 학습된 ResNet 기반의 BiT-L 모델보다 모든 태스크에서 더 나은 성능을 달성했으며, 사전 학습에 필요한 계산 자원은 현저히 적었습니다. 더 큰 ViT-H/14 모델은 ImageNet, CIFAR-100, VTAB와 같은 더 어려운 데이터셋에서 성능을 더욱 향상시켰습니다.
    *   ImageNet: 88.55%
    *   ImageNet-ReaL: 90.72%
    *   CIFAR-100: 94.55%
    *   VTAB (19 tasks): 77.63%
*   **데이터 규모의 중요성:**
    *   ImageNet과 같은 작은 데이터셋으로 사전 학습 시, ViT-Large 모델은 ViT-Base 모델이나 ResNet보다 성능이 저조했습니다.
    *   ImageNet-21k 규모에서는 성능이 비슷해졌고, JFT-300M과 같은 대규모 데이터셋에서는 대형 ViT 모델이 완전한 잠재력을 발휘하여 CNN을 능가했습니다.
    *   이는 트랜스포머가 CNN의 귀납적 편향이 없기 때문에 작은 데이터셋에서는 과적합 경향이 있지만, 대규모 데이터에서는 관련 패턴을 직접 학습하는 것이 충분하거나 오히려 유익함을 시사합니다.
*   **성능-계산 자원 효율성:** ViT는 ResNet에 비해 동일한 성능을 달성하는 데 약 2-4배 적은 계산 자원을 필요로 하여 성능/계산 효율성 측면에서 ResNet을 능가했습니다. 하이브리드 모델은 작은 계산 예산에서는 순수 ViT보다 약간 나았지만, 모델 규모가 커지면서 그 차이가 사라졌습니다.
*   **모델 내부 시각화:**
    *   초기 선형 임베딩 필터가 각 패치 내의 미세 구조에 대한 그럴듯한 기저 함수(basis functions)와 유사함을 보여주었습니다.
    *   학습된 위치 임베딩은 이미지 내의 거리를 잘 인코딩하며(가까운 패치일수록 유사한 임베딩), 행-열 구조도 나타냅니다.
    *   셀프 어텐션은 가장 낮은 레이어에서도 이미지 전체에 걸쳐 정보를 통합하는 능력을 보여주며, 일부 헤드는 이미지의 대부분에 어텐션하고 다른 헤드는 국소적으로 어텐션하는 등 다양한 어텐션 패턴을 보였습니다. 어텐션 거리는 네트워크 깊이에 따라 증가했습니다.
*   **자기 지도 학습 가능성:** 마스크 패치 예측(masked patch prediction) 방식의 자기 지도 사전 학습(self-supervised pre-training)을 통해 ViT-B/16 모델이 ImageNet에서 스크래치 학습 대비 2% 향상된 79.9%의 정확도를 달성하며 가능성을 보였습니다 (여전히 지도 학습에는 4% 뒤처짐).

## 🧠 Insights & Discussion
ViT는 기존 CNN과 달리 이미지에 특화된 귀납적 편향(예: 국소성, 2D 이웃 구조, 이동 등변성)이 훨씬 적습니다. 2D 구조에 대한 정보는 이미지 패치 추출과 파인튜닝 시 위치 임베딩의 2D 보간을 통해서만 수동으로 주입됩니다. 나머지 모든 패치 간의 공간적 관계는 모델이 데이터로부터 직접 학습해야 합니다.

이러한 접근 방식은 대규모 데이터셋(수백만 ~ 수억 개의 이미지)으로 사전 학습될 때 놀랍도록 잘 작동하며, 이는 충분한 데이터가 주어진다면 귀납적 편향이 약한 모델도 뛰어난 성능을 달성할 수 있음을 시사합니다. ViT는 동일한 성능을 내는 CNN보다 사전 학습 비용이 훨씬 저렴하여 효율성 면에서 큰 이점을 가집니다.

제한점과 향후 연구 방향으로는 다음이 있습니다.
*   **다른 컴퓨터 비전 태스크로의 확장:** 객체 탐지, 분할과 같은 다른 CV 태스크에 ViT를 적용하는 것입니다.
*   **자기 지도 사전 학습 방법의 개선:** 현재 자기 지도 학습과 대규모 지도 학습 간에는 성능 격차가 존재하며, 이를 줄이는 연구가 필요합니다.
*   **ViT의 추가적인 스케일링:** 현재 연구 범위 내에서 ViT의 성능이 포화되지 않았으므로, 모델 규모를 더욱 키우면 추가적인 성능 향상을 기대할 수 있습니다.

## 📌 TL;DR
*   **문제:** 이미지 인식에서 CNN 의존성을 깨고, 순수 트랜스포머가 대규모 데이터에서 SOTA 성능을 달성할 수 있는가?
*   **방법:** 이미지를 고정 크기 패치 시퀀스로 분할하고, 이를 선형 임베딩 및 위치 임베딩과 함께 표준 트랜스포머 인코더에 입력하는 ViT(Vision Transformer)를 제안한다. 이는 NLP의 토큰 처리 방식과 유사하다.
*   **결과:** JFT-300M과 같은 거대 데이터셋으로 사전 학습 시, ViT는 ImageNet, CIFAR-100, VTAB 등 다양한 이미지 분류 벤치마크에서 SOTA CNN을 능가하는 성능을 보였으며, 사전 학습에 필요한 계산 자원도 현저히 적었다. 이는 대규모 데이터 학습이 강력한 귀납적 편향의 부재를 극복하고 심지어 이점을 가져올 수 있음을 증명한다.