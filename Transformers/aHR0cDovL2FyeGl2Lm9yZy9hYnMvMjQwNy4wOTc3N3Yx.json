{
  "url": "http://arxiv.org/abs/2407.09777v1",
  "title": "Graph Transformers: A Survey",
  "authors": "Ahsan Shehzad, Feng Xia, Shagufta Abid, Ciyuan Peng, Shuo Yu, Dongyu Zhang, Karin Verspoor",
  "year": 2024,
  "abstract": "Graph transformers are a recent advancement in machine learning, offering a\nnew class of neural network models for graph-structured data. The synergy\nbetween transformers and graph learning demonstrates strong performance and\nversatility across various graph-related tasks. This survey provides an\nin-depth review of recent progress and challenges in graph transformer\nresearch. We begin with foundational concepts of graphs and transformers. We\nthen explore design perspectives of graph transformers, focusing on how they\nintegrate graph inductive biases and graph attention mechanisms into the\ntransformer architecture. Furthermore, we propose a taxonomy classifying graph\ntransformers based on depth, scalability, and pre-training strategies,\nsummarizing key principles for effective development of graph transformer\nmodels. Beyond technical analysis, we discuss the applications of graph\ntransformer models for node-level, edge-level, and graph-level tasks, exploring\ntheir potential in other application scenarios as well. Finally, we identify\nremaining challenges in the field, such as scalability and efficiency,\ngeneralization and robustness, interpretability and explainability, dynamic and\ncomplex graphs, as well as data quality and diversity, charting future\ndirections for graph transformer research."
}