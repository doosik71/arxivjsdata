{
  "title": "DeepSleepNet-Lite: A Simplified Automatic Sleep Stage Scoring Model with\n  Uncertainty Estimates",
  "authors": "Luigi Fiorillo, Paolo Favaro, Francesca Dalia Faraci",
  "year": 2021,
  "url": "http://arxiv.org/abs/2108.10600v1",
  "abstract": "Deep learning is widely used in the most recent automatic sleep scoring\nalgorithms. Its popularity stems from its excellent performance and from its\nability to directly process raw signals and to learn feature from the data.\nMost of the existing scoring algorithms exploit very computationally demanding\narchitectures, due to their high number of training parameters, and process\nlengthy time sequences in input (up to 12 minutes). Only few of these\narchitectures provide an estimate of the model uncertainty. In this study we\npropose DeepSleepNet-Lite, a simplified and lightweight scoring architecture,\nprocessing only 90-seconds EEG input sequences. We exploit, for the first time\nin sleep scoring, the Monte Carlo dropout technique to enhance the performance\nof the architecture and to also detect the uncertain instances. The evaluation\nis performed on a single-channel EEG Fpz-Cz from the open source Sleep-EDF\nexpanded database. DeepSleepNet-Lite achieves slightly lower performance, if\nnot on par, compared to the existing state-of-the-art architectures, in overall\naccuracy, macro F1-score and Cohen's kappa (on Sleep-EDF v1-2013 +/-30mins:\n84.0%, 78.0%, 0.78; on Sleep-EDF v2-2018 +/-30mins: 80.3%, 75.2%, 0.73). Monte\nCarlo dropout enables the estimate of the uncertain predictions. By rejecting\nthe uncertain instances, the model achieves higher performance on both versions\nof the database (on Sleep-EDF v1-2013 +/-30mins: 86.1.0%, 79.6%, 0.81; on\nSleep-EDF v2-2018 +/-30mins: 82.3%, 76.7%, 0.76). Our lighter sleep scoring\napproach paves the way to the application of scoring algorithms for sleep\nanalysis in real-time.",
  "citation": 108
}