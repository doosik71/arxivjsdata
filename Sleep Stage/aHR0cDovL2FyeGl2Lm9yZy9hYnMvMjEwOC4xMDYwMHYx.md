# DeepSleepNet-Lite: A Simplified Automatic Sleep Stage Scoring Model with Uncertainty Estimates

Luigi Fiorillo, Paolo Favaro, and Francesca Dalia Faraci

## 🧩 Problem to Solve

기존의 심층 학습 기반 자동 수면 단계 채점 알고리즘들은 대부분 계산적으로 부담이 큰 아키텍처를 사용하며, 많은 수의 훈련 파라미터와 최대 12분 길이의 긴 시간 시퀀스를 처리합니다. 이러한 특성 때문에 실시간 애플리케이션이나 가정 모니터링 환경에는 적합하지 않습니다. 또한, 대다수 아키텍처는 모델 불확실성 추정 기능을 제공하지 않아, 오분류된 수면 단계를 식별하거나 의사에게 예측의 신뢰도를 전달하는 데 어려움이 있습니다. 제한된 수면 데이터셋에서 과적합을 방지하고 초기부터 훈련하기 위해서는 더 가벼운 아키텍처가 필요합니다.

## ✨ Key Contributions

- **경량화된 자동 수면 단계 채점 아키텍처 DeepSleepNet-Lite 제안**: 90초의 단일 채널 EEG 입력 시퀀스만 처리하며, 기존 SOTA(State-Of-The-Art) 모델과 유사하거나 약간 낮은 성능을 유지하면서도 훈련 파라미터 수를 획기적으로 줄였습니다($\sim 0.6 \text{M}$ 대 기존 모델들의 $\sim 1.3 \text{M}$ - $\sim 24.7 \text{M}$). 이는 실시간 및 가정 모니터링 애플리케이션에 적합합니다.
- **Monte Carlo Dropout을 활용한 불확실성 추정 도입**: 수면 채점 분야에서 처음으로 Monte Carlo dropout 기술을 테스트 시간에 적용하여 모델 예측의 불확실성을 정량화하고, 이를 통해 모델 성능을 향상시킬 수 있음을 입증했습니다.
- **조건부 확률 분포 기반 레이블 스무딩 (Conditional Probability Distribution-based Label Smoothing) 기법 제안**: 수면 단계 전환의 중요성을 고려하여, 이전 및 다음 수면 단계에 따른 현재 수면 단계의 조건부 확률 분포를 기반으로 레이블 스무딩을 수행하여 모델 캘리브레이션을 개선했습니다.
- **불확실한 예측의 효과적인 식별**: Monte Carlo dropout을 통해 얻은 예측 분산($\sigma^2$) 또는 평균($\mu$)을 사용하여 불확실한 예측(즉, 오분류된 에포크)을 효과적으로 식별하고, 이를 제외할 경우 모델의 전체 성능이 향상됨을 보여주었습니다.

## 📎 Related Works

- **수면 채점 자동화 노력**: 1960년대부터 수면 채점 절차를 자동화하려는 시도가 있었으나, 아직까지 완전한 의사 대체 시스템은 부재합니다.
- **심층 학습 기반 수면 채점**: 오토인코더 [7], 심층 신경망(DNNs) [8], 합성곱 신경망(CNNs) [9]–[16], 순환 신경망(RNNs) [17] 및 이들의 조합 [18]–[26] 등 다양한 딥러닝 아키텍처가 제안되었습니다.
- **DeepSleepNet** [18]: DeepSleepNet-Lite의 아키텍처에 영감을 준 모델로, 원본 네트워크의 표현 학습 부분에 중점을 둡니다.
- **모델 불확실성 추정 연구**: [14], [21]과 같은 소수의 아키텍처에서 모델 불확실성 추정 기능을 제공했지만, 추가적인 분류 블록이나 여러 모델을 훈련하는 등 복잡한 방식을 사용했습니다.
- **최신 수면 채점 시스템**: FCNN+RNN [26], DeepSleepNet [18], IITNet [22], SleepEEGNet [23], SeqSleepNet+ [24], Naive Fusion [26], TinySleepNet [25], XSleepNet2 [26] 등이 비교 대상으로 언급됩니다.

## 🛠️ Methodology

DeepSleepNet-Lite는 DeepSleepNet [18]의 '표현 학습 (representation learning)' 아키텍처를 단순화하고 경량화한 모델입니다.

1. **아키텍처 (Architecture)**:

   - **입력**: 90초 길이의 단일 채널 EEG (Fpz-Cz) 시퀀스를 입력으로 받습니다. 이 시퀀스는 세 개의 30초 에포크로 구성되며, 모델은 중앙 에포크의 수면 단계를 예측합니다 (sequence-to-epoch 학습).
   - **병렬 CNNs**: 작은 필터($\text{CNN}_{\theta_{\text{S}}}$)와 큰 필터($\text{CNN}_{\theta_{\text{L}}}$)를 사용하는 두 개의 병렬 합성곱 신경망으로 구성됩니다.
     - 각 CNN 섹션은 4개의 합성곱 레이어와 2개의 최대 풀링(max-pooling) 레이어로 이루어집니다.
     - 합성곱 레이어는 1D 합성곱, 배치 정규화(Batch Normalization) [29], ReLU 활성화 함수를 포함합니다.
   - **특징 결합 및 출력**: 두 CNN의 출력 특징 벡터($h_{\text{i}}^{\text{S}}$ 및 $h_{\text{i}}^{\text{L}}$)를 연결한 후 소프트맥스(softmax) 레이어를 통해 5가지 수면 단계(W, N1, N2, N3, R)에 대한 확률을 출력합니다.

2. **훈련 알고리즘 (Training Algorithm)**:

   - **종단 간(End-to-end) 훈련**: Adam 옵티마이저 [30]를 사용하여 역전파 방식으로 훈련됩니다.
   - **클래스 불균형 해결**: 데이터 증강(수직 뒤집기)과 오버샘플링을 통해 각 수면 단계의 샘플 수를 균형 있게 맞춥니다.
   - **정규화**:
     - **드롭아웃 (Dropout)**: 훈련 중 뉴런의 50%를 무작위로 제거하여 과적합을 방지합니다.
     - **조기 종료 (Early Stopping)**: 검증 세트의 F1-점수가 일정 기간 동안 개선되지 않으면 훈련을 중단합니다.
     - **L2 가중치 감소 (L2 Weight Decay)**: 손실 함수에 가중치 페널티를 추가하여 과적합을 방지합니다 ($\lambda=10^{-3}$).

3. **모델 캘리브레이션 (Model Calibration)**:

   - **레이블 스무딩 (Label Smoothing)** [34]: 모델 캘리브레이션을 개선하는 데 사용됩니다.
   - **균일 분포 기반 스무딩 ($\text{LS}_{\text{u}}$)**: 표준 레이블 스무딩으로, 하드 타겟을 균일 분포($1/K$)와 가중 평균합니다 ($\text{y}_{\text{k}}^{\text{LS}_{\text{u}}} = \text{y}_{\text{k}} \cdot (1-\alpha) + \alpha/K$, $\alpha=0.1$).
   - **조건부 확률 분포 기반 스무딩 ($\text{LS}_{\text{s}}$)**: 새로운 접근 방식입니다. 수면 단계 시퀀스 ($P(\text{stage}(\text{t}) | \text{stage}(\text{t}-1), \text{stage}(\text{t}+1))$)의 조건부 확률 분포 $M$을 계산하여 이를 레이블 스무딩에 활용합니다 ($\text{y}_{\text{k}}^{\text{LS}_{\text{s}}} = \text{y}_{\text{k}} \cdot (1-\alpha) + \alpha \cdot M_{\text{W,K,N1}}$, $\alpha=0.2$).

4. **불확실성 추정 (Estimating Uncertainty)**:
   - **Monte Carlo (MC) Dropout** [36]: 테스트 시간에도 드롭아웃을 활성화하여 모델의 불확실성을 정량화합니다.
   - **N번의 샘플링**: 드롭아웃이 적용된 상태에서 입력 시퀀스마다 $N=30$번의 예측을 수행하여 $N$개의 다른 예측 $\hat{p}_{\text{n,i,k}}$를 얻습니다.
   - **평균 및 분산 계산**: 각 수면 단계 $k$에 대한 $N$개 예측의 평균 ($\mu_{\text{i,k}}$)과 분산 ($\sigma_{\text{i,k}}^2$)을 계산합니다.
     $$ \mu*{\text{i,k}} = \frac{1}{\text{N}} \sum*{\text{n}=1}^{\text{N}} \hat{p}_{\text{n,i,k}} $$
        $$ \sigma_{\text{i,k}}^2 = \frac{1}{\text{N}} \sum*{\text{n}=1}^{\text{N}} (\hat{p}*{\text{n,i,k}} - \mu\_{\text{i,k}})^2 $$
   - **불확실한 예측 식별**: 가장 낮은 $\mu$ 값 또는 가장 높은 $\sigma^2$ 값을 가진 에포크를 불확실한 것으로 간주하여 의사의 검토를 권장합니다.

## 📊 Results

- **평가 데이터셋**: Sleep-EDF v1-2013 및 v2-2018 확장 데이터셋의 단일 채널 EEG Fpz-Cz를 사용하여 벤치마킹했습니다.
- **전반적인 성능**:
  - **DeepSleepNet-Lite (MC dropout 적용, $LS_{\text{u}}$)**는 Sleep-EDF v1-2013 $\pm$30mins에서 Acc. 84.0%, MF1 78.0%, k 0.78을 달성했으며, Sleep-EDF v2-2018 $\pm$30mins에서 Acc. 80.3%, MF1 75.2%, k 0.73을 달성했습니다.
  - 이는 기존 SOTA 모델과 유사하거나 약간 낮은 수준이지만, 훈련 파라미터 수는 현저히 적습니다 ($\sim 0.6 \text{M}$ 대 SOTA 모델들의 $\sim 1.3 \text{M}$ - $\sim 24.7 \text{M}$).
- **캘리브레이션 및 성능 향상**:
  - 레이블 스무딩, 특히 균일 분포 스무딩 ($LS_{\text{u}}$)은 ECE(Expected Calibration Error) 값을 감소시켜 모델 캘리브레이션을 크게 개선했습니다.
  - 테스트 시간에 MC dropout을 적용한 결과, 전반적인 성능이 향상되고 ECE 값도 추가로 감소하여 모델 캘리브레이션이 더욱 개선되었습니다.
  - MC 샘플링은 약 30개 샘플 이후 성능이 수렴하는 경향을 보였습니다.
- **불확실성 추정을 통한 성능 개선**:
  - 불확실한 에포크($q\%=5\%$로 설정, 각 PSG 기록당 평균 54개 에포크)를 식별하여 예측에서 제외했을 때 모델의 성능이 향상되었습니다.
    - Sleep-EDF v1-2013 $\pm$30mins에서 Acc. 86.1%, MF1 79.6%, k 0.81 (제외 전: Acc. 84.0%, MF1 78.0%, k 0.78).
    - Sleep-EDF v2-2018 $\pm$30mins에서 Acc. 82.3%, MF1 76.7%, k 0.76 (제외 전: Acc. 80.3%, MF1 75.2%, k 0.73).
  - 불확실한 에포크를 식별하는 쿼리 절차에서는 예측 확률의 평균($\mu$)을 사용하는 것이 분산($\sigma^2$)을 사용하는 것보다 오분류된 에포크를 더 효과적으로 식별했습니다.
- **클래스별 성능**: N1 수면 단계는 F1-점수가 44.4% (v1-2013) 및 46.0% (v2-2018)로 가장 낮았으며, 주로 깨어있는 상태, N2, REM으로 오분류되는 경향을 보였습니다. 이는 N1 단계가 본질적으로 분류하기 어렵다는 기존 연구 결과와 일치합니다.

## 🧠 Insights & Discussion

- **경량 모델의 잠재력**: DeepSleepNet-Lite는 적은 파라미터로도 경쟁력 있는 성능을 보여, 긴 시간적 구조를 인코딩하는 것이 항상 최적은 아닐 수 있으며, 단기 PSG 기록의 본질적인 패턴만으로도 수면 단계 분류에 충분할 수 있음을 시사합니다. 이는 실시간 및 가정 모니터링 시나리오에 특히 유용합니다.
- **대규모 데이터셋에서의 한계**: 현재 모델은 소규모 데이터셋(Sleep-EDF)에서 효율적이지만, 더 크고 이질적인 데이터셋(예: Physio2018, SHHS)에서의 견고성에 대한 추가적인 조사가 필요합니다. 모델의 낮은 용량으로 인해 대규모 데이터셋에서는 과소적합이 발생할 수 있습니다.
- **Monte Carlo Dropout의 실용성**: MC dropout은 기존 아키텍처의 레이어를 활용하여 쉽게 구현할 수 있으며, 예측 확률의 평균 및 분산과 같은 해석 가능한 불확실성 추정치를 제공합니다. 비록 추론 시간이 $N$배 증가하지만, 단일 시퀀스 평가가 밀리초 단위이므로 실시간 애플리케이션에서도 여전히 실용적인 해결책입니다.
- **레이블 스무딩과 캘리브레이션**: 조건부 확률 분포 기반 레이블 스무딩은 모델 캘리브레이션을 개선하는 데 도움이 되었으나, MC dropout과 결합했을 때 반드시 더 높은 성능으로 이어지지는 않았습니다. 이는 모델 캘리브레이션 개선이 항상 예측 성능 향상이나 불확실성 추정의 정확도 향상과 직접적으로 연결되지 않을 수 있음을 시사합니다.
- **불확실성 추정의 가치**: 불확실성 쿼리 절차를 통해 오분류된 에포크의 상당수를 효과적으로 식별할 수 있으며, 이는 의사의 2차 검토를 통해 수면 채점의 신뢰도를 높이는 데 기여할 수 있습니다.

## 📌 TL;DR

본 논문은 계산적으로 효율적인 자동 수면 단계 채점 모델인 **DeepSleepNet-Lite**를 제안합니다. 이 모델은 90초의 단일 채널 EEG만 입력으로 사용하며, 적은 훈련 파라미터로 기존 최첨단 모델과 유사한 성능을 달성합니다. 핵심 기여는 수면 채점 분야에 **Monte Carlo Dropout**을 도입하여 예측 불확실성을 정량화하고, 이를 통해 오분류 에포크를 식별하며 모델 성능을 향상시킨 점입니다. 또한, 새로운 **조건부 확률 분포 기반 레이블 스무딩** 기법으로 모델 캘리브레이션을 개선했습니다. Sleep-EDF 데이터셋에 대한 평가 결과, 불확실한 예측을 제외할 경우 정확도 및 F1-점수 등 전반적인 성능이 유의미하게 향상되었음을 입증했습니다. 이 경량화된 접근 방식은 실시간 및 가정 수면 모니터링 애플리케이션의 가능성을 열어줍니다.
