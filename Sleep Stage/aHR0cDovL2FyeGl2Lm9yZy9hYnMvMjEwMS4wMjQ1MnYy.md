# RobustSleepNet: Transfer learning for automated sleep staging at scale

Antoine Guillot and Valentin Thorey

## 🧩 Problem to Solve

수면 장애 진단은 수면다원검사(PSG) 기록 분석에 의존하며, 수면 단계 분류는 필수적인 사전 단계입니다. 하지만 이 분류 작업은 30초 에포크(epoch)를 시각적으로 검사해야 하는 지루하고 비용이 많이 드는 과정입니다. 자동화된 접근 방식이 많이 개발되었지만, 대부분의 수면 클리닉에서는 활용되지 못하고 있는데, 주요 문제는 다음과 같습니다.

- **입력-형태 비호환성(Input-shape incompatibility)**: 각 수면 클리닉은 특정 PSG 몽타주(montage)를 사용하며, 기존 자동화 모델은 특정 입력 형태에만 학습되어 다른 몽타주와 호환되지 않습니다.
- **정확도 하락 문제(Accuracy-drop issue)**: 몽타주가 호환되더라도, 한 데이터셋으로 학습된 모델은 다른 인구 통계학적 특성(demographics)을 가진 미지의 데이터셋에서 성능이 현저히 떨어집니다. 이는 채널 불일치(channel mismatch)나 데이터셋 간의 인구 통계 분포 차이 때문으로 생각됩니다.

이러한 문제들로 인해 자동 수면 단계 분류 시스템은 실제 임상 환경에서 광범위하게 사용되지 못하고 있습니다.

## ✨ Key Contributions

이 논문은 자동 수면 단계 분류의 실용적 활용을 가로막는 주요 문제들을 해결하기 위해 **RobustSleepNet**을 제안하며 다음과 같은 핵심 기여를 합니다.

- **임의의 PSG 몽타주 처리**: 멀티 헤드 어텐션(multi-head attention) 레이어를 사용하여 임의의 입력 채널 수와 순서를 처리할 수 있는 아키텍처를 도입, 입력-형태 비호환성 문제를 해결합니다.
- **광범위한 데이터셋 기반의 강력한 일반화**: 8개의 이질적인 수면 단계 분류 데이터셋(다양한 몽타주 및 인구 통계 포함)으로 "leave-one-out-dataset" 방식으로 모델을 학습하고 평가하여, 인구 통계학적 변화에 대한 높은 견고성(robustness)을 달성했습니다.
- **뛰어난 직접 전이(Direct Transfer, DT) 성능**: 미지의 데이터셋에 대해, RobustSleepNet은 해당 데이터셋으로만 명시적으로 학습된 모델의 F1 점수 대비 평균 97%의 성능을 달성하여, 이전의 어떠한 문헌 연구보다도 우수한 즉시 사용 가능한(out-of-the-box) 성능을 제공합니다.
- **미세 조정을 통한 성능 향상**: 미지의 데이터셋 일부를 사용하여 모델을 미세 조정(fine-tuning)하면, 스크래치부터 학습(Learning From Scratch, LFS)한 모델보다 F1 점수가 2% 추가로 향상될 수 있음을 보여주어, 특정 인구 집단에 대한 최첨단 성능 달성 가능성을 제시합니다.
- **코드 공개**: 학습된 모델과 코드를 공개하여 연구 및 임상 적용을 지원합니다.

## 📎 Related Works

본 연구는 자동 수면 단계 분류 분야의 선행 연구들을 참조하며, 특히 딥러닝 기반 모델들의 한계를 지적합니다.

- **감독 학습(Supervised Learning) 기반 딥러닝 모델**: DeepSleepNet [7], SeqSleepNet [8], SimpleSleepNet [5], U-Time [9], XSleepNet [10] 등과 같은 모델들이 특정 데이터셋에서 인간 수준의 성능을 달성했음을 언급합니다.
- **전이 학습(Transfer Learning) 및 도메인 적응(Domain Adaptation) 시도**:
  - MASS 데이터셋으로 사전 학습하고 Sleep-EDF 데이터셋으로 미세 조정하는 방식 [15], [12]이 3~5% 성능 향상을 보였습니다.
  - 첫날 밤 데이터로 미세 조정한 개인화된 수면 단계 분류 연구 [16]는 강력한 정규화(regularization)를 통해 과적합(overfitting)을 피했습니다.
  - 소스 및 타겟 데이터셋 간의 분포 변화를 줄이기 위해 관측치를 정렬하는 방법 [11]도 있었습니다.
- **기존 연구의 한계**: 이러한 기존의 전이 학습 및 도메인 적응 방법들은 모두 타겟 데이터셋의 일부 데이터를 학습에 사용해야 하므로, 모델 재학습 및 적응에 대한 높은 고정 비용 문제를 완전히 해결하지 못했습니다. 또한, 입력-형태 비호환성 문제는 기존 문헌에서 다루어진 바가 없습니다.

## 🛠️ Methodology

RobustSleepNet은 계층적 시퀀스-투-시퀀스 분류기(hierarchical sequence-to-sequence classifier)로, 기존의 SimpleSleepNet [5] 및 SeqSleepNet [8] 아키텍처를 기반으로 하되, **어텐션 레이어(attention layer)**를 도입하여 임의의 PSG 몽타주를 처리할 수 있도록 개선되었습니다.

1. **문제 정의**:

   - PSG에서 얻은 신호는 $C$개의 채널과 $L$개의 샘플 포인트($L = 30 \times f_s$)를 가진 30초 에포크 시퀀스 $R^{C \times L}$로 표현됩니다.
   - 각 에포크는 AASM(American Academy of Sleep Medicine) 가이드라인에 따른 5가지 수면 단계(Wake, NREM1, NREM2, NREM3, REM) 중 하나로 원-핫 인코딩(one-hot encoding)되어 레이블링됩니다.
   - 모델은 $T$개의 연속 에포크 시퀀스 $Z \in R^{C \times L \times T}$를 입력받아, 해당하는 $T$개의 수면 단계 확률 $\hat{\pi} \in [0,1]^{5 \times T}$을 예측합니다.

2. **학습 설정**: 모델 성능 벤치마킹을 위해 세 가지 학습 설정을 사용했습니다.

   - **직접 전이(Direct Transfer, DT)**: 소스 데이터셋 $D_{sources}$으로만 학습하고 대상 데이터셋 $D_{target}$에서 평가합니다.
   - **스크래치부터 학습(Learning From Scratch, LFS)**: 대상 데이터셋 $D_{target}$으로만 K-fold 방식으로 학습 및 평가합니다.
   - **미세 조정(Finetuning, FT)**: DT 방식으로 사전 학습된 모델을 $D_{target}$으로 K-fold 방식으로 추가 학습 및 평가합니다.

3. **RobustSleepNet 아키텍처**: 4개의 주요 블록으로 구성됩니다.

   - **신호 정규화**:
     - 원시 신호 $X \in R^{C \times L}$에 대해 해밍 윈도우(Hamming window)를 사용하여 단시간 푸리에 변환(STFT)을 계산합니다 ($X_{fft} \in R^{C \times F_{fft} \times L_{fft}}$).
     - 각 채널 및 주파수 빈(frequency bin)에 대해 시간 축을 따라 제로-평균 및 단위 분산으로 정규화합니다.
   - **에포크 인코더**:
     - STFT 결과의 주파수 빈을 선형 투영하여 $X_{red} \in R^{C \times F_{red} \times L_{fft}}$를 얻습니다.
     - **멀티 헤드 어텐션 레이어**: 이 단계에서 가장 중요한 차이점이 발생합니다. [17]에서 소개된 어텐션 레이어를 사용하여 $C$개의 원본 채널을 고정된 수의 $N_{heads}$ 채널로 재결합합니다. 이 과정은 채널 중요도 $\alpha$를 가중치로 사용하여 수행되며, 임의의 채널 수와 순서를 처리할 수 있게 합니다.
     - 재결합된 출력 $X_{att} \in R^{N_{heads} \times F_{red} \times L_{fft}}$를 평탄화(flatten)한 후, 양방향 GRU [19]에 입력하고 드롭아웃(dropout)을 적용합니다.
     - GRU의 출력은 또 다른 어텐션 레이어를 통해 시간 축을 따라 최종 에포크 특징 벡터 $X_{features} \in R^{P}$로 축소됩니다.
   - **시퀀스 인코더**: $T$개 에포크의 특징 시퀀스 $Z_{features} \in R^{P \times T}$를 스킵 연결을 포함하는 2계층 양방향 GRU를 사용하여 $Z_{encoded} \in R^{Q \times T}$로 인코딩합니다. 각 계층 후 드롭아웃을 적용합니다.
   - **분류기**: 소프트맥스 레이어를 사용하여 $Z_{encoded}$를 각 수면 단계에 속할 확률 $\hat{\pi} \in [0,1]^{5 \times T}$로 매핑합니다.

4. **손실 함수**: 전체 시간 컨텍스트 $T$에 걸쳐 계산되는 교차 엔트로피(cross-entropy) 손실을 사용합니다:
   $$L(Z,y) = -\frac{1}{T}\sum_{i=0}^{T-1} y_i \cdot \log(\hat{\pi}_i)$$
   여기서 $y$는 정답 레이블입니다.

5. **다양한 채널 크기 처리**: 학습 중 각 배치(batch)에 여러 데이터셋의 관측치를 혼합하기 위해, 각 배치마다 최대 채널 수 $C_{max}$ 내에서 임의의 $C_{batch}$ 채널을 샘플링합니다. 이는 모델이 특정 채널에 과도하게 의존하는 것을 방지하여 정규화 효과를 제공합니다.

6. **추론**: 학습된 모델은 각 피험자에 대해 스트라이드(stride) 1로 평가됩니다. 시간적 컨텍스트에서 얻은 $T$개의 예측 $\hat{\pi}_i$는 기하 평균으로 취합되어 최종 수면 단계 예측으로 사용됩니다.

## 📊 Results

RobustSleepNet은 8개의 이질적인 데이터셋에서 세 가지 학습 설정(LFS, FT, DT)에 대해 벤치마크되었으며, 다음과 같은 주요 결과를 보였습니다.

- **LFS (스크래치부터 학습)**: RobustSleepNet은 대부분의 데이터셋에서 기존 최신 모델들과 유사하거나 더 나은 Macro F1 성능을 달성했습니다 (예: DOD-O 80.5%, Sleep-EDF SC-39 78.6%, MESA 77.8%). 이는 모델의 기본 성능이 강력함을 시사합니다.

- **FT (미세 조정)**:

  - LFS 설정과 비교하여 Macro F1 점수가 평균 2.5% 상대적으로 증가했습니다.
  - 대부분의 데이터셋에서 기존 문헌의 다른 미세 조정 방법들보다 우수한 성능을 보였습니다.
  - DT 설정과 비교하여 Macro F1 점수가 5.1% 상대적으로 증가했습니다.

- **DT (직접 전이)**:

  - 기존 문헌의 직접 전이 시도보다 훨씬 뛰어난 성능을 달성했습니다.
  - 특히 DOD-H, SC39, ST 데이터셋에서는 LFS 성능을 능가했습니다.
  - DOD-O 및 MASS SS1에서는 LFS와 동일한 수준의 성능을 보였습니다.
  - MrOS, MESA, SHHS에서는 LFS보다 3~4% 낮은 정확도를, CAP에서는 7% 낮은 정확도를 보였지만, 전반적으로 매우 우수한 전이 능력을 입증했습니다.

- **학습 레코드 수의 영향**:

  - DT 설정에서 학습에 사용되는 레코드 수가 증가할수록 성능이 꾸준히 향상되었습니다.
  - 단 14개의 레코드(각 소스 데이터셋당 2개)만으로도 LFS 성능의 84%에 도달했으며, 28개의 레코드(각 소스 데이터셋당 4개)로는 88%에 도달했습니다.
  - 특히 DOD-H의 경우, 56개의 다른 데이터셋 레코드만으로 LFS 기준선을 초과 달성했습니다.

- **어텐션 헤드 수의 영향**: 2~4개의 어텐션 헤드를 사용할 때 가장 최적의 평균 F1 성능(75.8%)을 보였습니다.

- **대상 데이터셋 채널의 영향**:

  - EEG와 EOG를 결합한 몽타주는 모든 데이터셋에서 베이스라인(모든 채널 사용)과 유사한 성능을 보였으며, 평균 F1 하락은 1%에 불과했습니다.
  - 단일 EEG 채널만 사용했을 때는 평균 5%의 F1 하락을 보였지만, 여전히 활용 가능한 결과를 제공했습니다.

- **평가자 간 변동성(Inter-scorer variability)의 영향**:
  - 여러 평가자의 합의(consensus)로 학습된 모델이 가장 높은 DT 성능을 보였습니다 (DOD-O에서 개별 평가자 대비 4%, DOD-H에서 2.5% 증가).
  - RobustSleepNet의 성능은 평가자 신뢰도와 상관관계를 보였으며, 합의된 레이블을 기준으로 평가할 때 가장 높은 F1 점수를 기록했습니다.

## 🧠 Insights & Discussion

RobustSleepNet은 자동 수면 단계 분류 분야에 중요한 통찰력을 제공하며, 실용적 적용 가능성을 크게 확장합니다.

- **획기적인 일반화 성능**: 멀티 헤드 어텐션 기반의 채널 재결합 아키텍처 덕분에 RobustSleepNet은 다양한 PSG 몽타주를 유연하게 처리할 수 있습니다. 8개 데이터셋의 방대한 코퍼스 학습을 통해 미지의 데이터에 대해 LFS 성능의 평균 97%에 달하는 뛰어난 직접 전이(DT) 성능을 달성했으며, 이는 이전 문헌의 어떠한 시도보다도 우수합니다. 이는 사전 학습된 RobustSleepNet이 별도의 학습 없이도 임상 환경에서 고품질의 자동 수면 단계 분류를 제공할 수 있음을 의미합니다.

- **학습 데이터 다양성의 중요성**: DT 학습 시 적은 수의 레코드로도 LFS 성능에 근접한 결과를 보인 것은 학습 데이터셋의 높은 인구 통계학적 다양성(연령, 몽타주, 병리 등) 덕분입니다. 특히 DOD-H와 같이 "쉬운" 데이터셋에서는 다른 데이터셋의 레코드만으로도 LFS 성능을 초과 달성하는 놀라운 결과를 보여, 다양한 모집단으로부터 얻은 데이터로 학습하는 것이 모델의 일반화 능력을 극대화하는 데 필수적임을 강조합니다.

- **채널 구성의 유연성 및 실용성**: 어텐션 레이어의 파라미터 구성이 성능에 미치는 영향은 미미했지만, 단일 EEG 채널만으로도 활용 가능한 결과를 얻을 수 있다는 점은 가정용 PSG 장치와 같은 미니멀리스트 센서 구성 설계에 매우 유용합니다. EEG와 EOG의 조합이 최상의 성능을 보였습니다.

- **평가자 합의의 중요성**: 평가자 간 변동성(inter-rater variability)에 대한 실험은 여러 수면 전문가의 합의된 레이블을 기반으로 모델을 학습하는 것이 잡음을 줄이고 RobustSleepNet의 견고성과 일반화 능력을 향상시키는 데 결정적임을 보여줍니다. 또한, 모델 평가 시에도 합의된 레이블을 기준으로 하는 것이 공정한 비교를 위해 중요합니다.

- **데이터셋 특성 및 전이 학습**: 데이터셋의 '용이성(easiness)'과 '일반화(generalization)' 지표는 연령과 역의 상관관계를 보였습니다. 이는 젊은 피험자 데이터는 노년층으로 일반화되기 어려운 반면, 노년층 데이터는 더 넓은 연령대의 정보를 포함할 수 있음을 시사합니다. 또한, 채널 수, 평가자 합의 여부, 병리 유형 등도 데이터셋의 일반화 능력에 영향을 미치는 요인임을 확인했습니다 (예: CAP 데이터셋의 낮은 일반화 및 용이성).

- **미세 조정의 전략적 활용**: RobustSleepNet은 LFS 및 특히 FT에서 유망한 성능을 보입니다. FT는 특히 소규모 데이터셋에서 LFS 성능을 향상시키며, 특정 인구 집단을 목표로 할 때 활용될 수 있습니다. 다만, 미세 조정을 위해 너무 적은 레코드를 사용하면 DT보다 성능이 저하될 수 있으므로, 충분한 수의 레코드(소규모 데이터셋의 경우 10~40개)를 사용하는 것이 중요합니다.

- **향후 연구 방향**: 현재 RobustSleepNet은 센서 위치 정보를 명시적으로 활용하지 않는데, 특히 EEG 채널의 머리 위치 정보 등을 통합하면 EMG와 같은 다양한 센서의 활용도를 높여 성능을 더욱 개선할 수 있을 것입니다. 또한, 자동 수면 단계 분류의 예측 신뢰도에 대한 메트릭을 제공하는 방향으로 모델을 확장하는 것도 고려될 수 있습니다.

## 📌 TL;DR

이 논문은 임상 환경에서 자동 수면 단계 분류 시스템의 보급을 막는 **다양한 PSG 몽타주 비호환성**과 **새로운 인구 통계에 대한 성능 저하** 문제를 해결하기 위해 **RobustSleepNet**을 제안합니다. RobustSleepNet은 **멀티 헤드 어텐션 레이어**를 활용하여 어떠한 입력 채널 구성도 처리할 수 있으며, 8개의 이질적인 대규모 수면 데이터셋으로 학습되어 뛰어난 일반화 능력을 갖춥니다. 그 결과, 보지 못한 데이터셋에 대한 **직접 전이(DT)**에서 해당 데이터셋으로 명시적으로 학습된 모델의 F1 점수의 97%에 달하는 성능을 달성했고, **미세 조정(FT)** 시 LFS 대비 2%의 F1 점수 향상을 보여줍니다. 이 모델은 어떤 임상 환경에서도 즉시 사용 가능한 고품질 자동 수면 단계 분류를 제공하며, 학습 시 데이터의 다양성과 전문가 합의의 중요성을 강조합니다.
