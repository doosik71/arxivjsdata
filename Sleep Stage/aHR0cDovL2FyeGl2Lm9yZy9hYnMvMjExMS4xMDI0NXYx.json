{
  "title": "Ubi-SleepNet: Advanced Multimodal Fusion Techniques for Three-stage\n  Sleep Classification Using Ubiquitous Sensing",
  "authors": "Bing Zhai, Yu Guan, Michael Catt, Thomas Ploetz",
  "year": 2021,
  "url": "http://arxiv.org/abs/2111.10245v1",
  "abstract": "Sleep is a fundamental physiological process that is essential for sustaining\na healthy body and mind. The gold standard for clinical sleep monitoring is\npolysomnography(PSG), based on which sleep can be categorized into five stages,\nincluding wake/rapid eye movement sleep (REM sleep)/Non-REM sleep 1\n(N1)/Non-REM sleep 2 (N2)/Non-REM sleep 3 (N3). However, PSG is expensive,\nburdensome, and not suitable for daily use. For long-term sleep monitoring,\nubiquitous sensing may be a solution. Most recently, cardiac and movement\nsensing has become popular in classifying three-stage sleep, since both\nmodalities can be easily acquired from research-grade or consumer-grade devices\n(e.g., Apple Watch). However, how best to fuse the data for the greatest\naccuracy remains an open question. In this work, we comprehensively studied\ndeep learning (DL)-based advanced fusion techniques consisting of three fusion\nstrategies alongside three fusion methods for three-stage sleep classification\nbased on two publicly available datasets. Experimental results demonstrate\nimportant evidence that three-stage sleep can be reliably classified by fusing\ncardiac/movement sensing modalities, which may potentially become a practical\ntool to conduct large-scale sleep stage assessment studies or long-term\nself-tracking on sleep. To accelerate the progression of sleep research in the\nubiquitous/wearable computing community, we made this project open source, and\nthe code can be found at: https://github.com/bzhai/Ubi-SleepNet.",
  "citation": 15
}