# Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou

## 🧩 Problem to Solve

대규모 언어 모델(LLM)의 규모를 확장하는 것이 다양한 이점을 가져왔지만, 산술, 상식, 상징적 추론과 같은 복잡한 작업에서는 여전히 어려움을 겪고 있습니다. 기존의 추론 능력 향상 기법(예: 설명 증강 학습 또는 미세 조정)은 고품질의 설명을 생성하는 데 많은 비용이 들며, 표준 퓨샷(few-shot) 프롬프팅은 추론이 필요한 작업에서 제대로 작동하지 않고 모델 규모가 커져도 성능이 크게 개선되지 않는 한계가 있습니다. 이 논문은 이러한 한계를 극복하고 LLM의 추론 능력을 간단한 방법으로 이끌어내는 것을 목표로 합니다.

## ✨ Key Contributions

- **사고의 사슬(Chain-of-Thought, CoT) 프롬프팅 도입:** 최종 답변에 이르는 일련의 중간 추론 단계를 자연어로 제공하는 퓨샷 프롬프팅 방식을 제안합니다.
- **추론 능력의 획기적 향상:** CoT 프롬프팅이 산술, 상식, 상징적 추론 작업에서 LLM의 성능을 크게 개선함을 입증했습니다.
- **모델 규모에 따른 emergent ability:** CoT 추론 능력이 충분히 큰 모델(약 $\sim 100$억 개 이상의 파라미터)에서 자연스럽게 나타나는(emergent ability) 특성을 발견했습니다. 작은 모델에서는 CoT 프롬프팅이 오히려 성능을 저하시키거나 영향을 주지 않았습니다.
- **최첨단 성능 달성:** PaLM 540B 모델이 단 8개의 CoT 예시만으로 GSM8K 수학 문제 벤치마크에서 기존의 미세 조정된 GPT-3 모델을 능가하는 SOTA(State-of-the-Art) 정확도를 달성했습니다.
- **길이 일반화(Length Generalization) 촉진:** 상징적 추론 작업에서 CoT 프롬프팅이 퓨샷 예시보다 긴 시퀀스(out-of-domain)에 대한 일반화 능력을 향상시킵니다.
- **모델 행동의 해석 가능성 제공:** CoT는 모델이 특정 답변에 도달한 과정을 보여주어 모델의 추론 경로를 이해하고 디버깅할 수 있는 해석 가능한 창을 제공합니다.

## 📎 Related Works

- **중간 단계를 활용한 추론 문제 해결:**
  - Ling et al. (2017)은 자연어 설명을 활용하여 수학 문제를 해결하는 아이디어를 개척했습니다.
  - Cobbe et al. (2021)은 큰 데이터셋과 사전 학습된 모델 미세 조정을 통해 이를 확장했습니다.
  - Nye et al. (2021)은 언어 모델이 파이썬 프로그램의 중간 계산 결과를 예측하여 최종 결과를 도출하는 '스크래치패드(scratchpads)' 방식을 제안했습니다.
  - 이러한 작업들은 자연어 추론 단계를 사용하며, 형식 언어(formal language)를 사용하는 신경-상징적(neuro-symbolic) 방법(Roy and Roth, 2015 등)과는 대조적입니다.
- **프롬프팅(Prompting):**
  - Brown et al. (2020)이 대중화한 퓨샷 프롬프팅 이후, 프롬프트 자동 학습(Lester et al., 2021) 또는 태스크 설명 지시문(Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022) 등 프롬프트 능력을 개선하려는 연구들이 활발합니다.
  - 이 논문은 기존 연구들이 프롬프트의 입력 부분을 개선하는 반면, 언어 모델의 '출력'을 CoT로 확장하는 직교적인 접근 방식을 취합니다.

## 🛠️ Methodology

1. **사고의 사슬 프롬프팅 (Chain-of-Thought Prompting):**
   - 표준 퓨샷 프롬프팅에서는 단순히 $\langle \text{질문}, \text{답변} \rangle$ 쌍을 제공합니다.
   - CoT 프롬프팅에서는 각 퓨샷 예시에 $\langle \text{입력}, \text{사고의 사슬}, \text{출력} \rangle$의 세 가지 요소를 포함합니다.
   - 여기서 '사고의 사슬(chain of thought)'은 최종 답변에 이르는 자연어 형태의 일련의 중간 추론 단계들을 의미합니다(예시: "5개의 테니스 공 + (2캔 \* 3개/캔) = 5 + 6 = 11개").
2. **예시 생성:**
   - 대부분의 데이터셋에는 평가 세트만 존재하므로, 저자들이 8개의 CoT 예시를 수동으로 작성하여 프롬프트에 사용했습니다(AQuA 제외).
   - AQuA와 같이 객관식 문제의 경우, 4개의 예시를 훈련 세트에서 가져와 사용했습니다.
3. **평가 모델:** GPT-3 (350M, 1.3B, 6.7B, 175B), LaMDA (422M, 2B, 8B, 68B, 137B), PaLM (8B, 62B, 540B), UL2 20B, Codex 모델을 활용하여 성능을 평가했습니다.
4. **평가 작업:**
   - **산술 추론:** GSM8K, SVAMP, ASDiv, AQuA, MAWPS
   - **상식 추론:** CSQA, StrategyQA, DateUnderstanding, SportsUnderstanding, SayCan
   - **상징적 추론:** Last Letter Concatenation, Coin Flip
5. **디코딩 방식:** 모델의 출력을 얻기 위해 그리디 디코딩(greedy decoding)을 사용했습니다.
6. **외부 계산기(External Calculator) 활용:** 산술 추론 작업의 경우, 모델이 생성한 CoT 내의 수식에 파이썬 `eval` 함수를 적용하여 계산 오류를 보정, 성능을 추가적으로 측정했습니다.
7. **어블레이션(Ablation) 및 강건성(Robustness) 연구:** CoT의 효과를 분석하기 위해 '수식만(Equation only)', '변수 계산만(Variable compute only)', '답변 후 추론(Reasoning after answer)' 등의 변형과 다른 주석자, 예시 집합, 예시 순서, 예시 개수에 따른 강건성을 평가했습니다.

## 📊 Results

- **산술 추론 (Arithmetic Reasoning):**
  - CoT 프롬프팅은 약 $\sim 100$억 파라미터 이상의 대규모 모델에서만 성능 향상을 보였으며, 작은 모델에서는 오히려 표준 프롬프팅보다 성능이 저하되거나 미미했습니다.
  - PaLM 540B는 CoT 프롬프팅을 통해 GSM8K에서 56.9%($+39.0\%$), SVAMP에서 79.0%($+9.6\%$), MAWPS에서 93.3%($+14.2\%$)의 정확도를 달성하며 기존 SOTA를 경신했습니다. (괄호 안은 표준 프롬프팅 대비 성능 향상 폭)
  - CoT의 성능 향상은 복잡한 문제(예: GSM8K)에서 가장 두드러졌고, 단일 또는 2단계 문제에서는 미미했습니다.
  - 수동 오류 분석 결과, 올바른 답변을 도출한 CoT의 98%는 논리적으로 정확했으며, 오답의 경우 46%는 사소한 오류(계산기 오류, 심볼 매핑 오류 등)를, 54%는 주요 의미론적 이해 또는 비일관성 오류를 보였습니다. 모델 규모가 PaLM 62B에서 540B로 커지면서 의미론적 이해 오류가 크게 개선되었습니다.
  - 어블레이션 연구(Figure 5)는 '수식만' 또는 '변수 계산만' 방식이 CoT 프롬프팅만큼의 이점을 제공하지 못하며, 자연어 중간 단계가 추론에 핵심적임을 시사했습니다.
  - CoT 프롬프팅은 다양한 주석자, 예시 집합, 예시 순서 및 개수에 대해 강건하게(robustly) 성능을 유지했습니다.
- **상식 추론 (Commonsense Reasoning):**
  - CoT 프롬프팅은 CSQA, StrategyQA, DateUnderstanding, SportsUnderstanding, SayCan 등 5가지 상식 추론 벤치마크에서 PaLM 540B의 성능을 향상시켰습니다.
  - PaLM 540B는 CoT를 통해 StrategyQA에서 SOTA(77.8% vs. 69.4%)를 달성하고, SportsUnderstanding에서는 인간 전문가(95.4% vs. 84%)를 능가하는 성능을 보였습니다.
- **상징적 추론 (Symbolic Reasoning):**
  - Last Letter Concatenation 및 Coin Flip 같은 상징적 추론 작업에서 CoT 프롬프팅은 모델이 주어진 논리 구조를 새로운 예시에 적용하는 능력을 크게 향상시켰습니다.
  - CoT는 퓨샷 예시보다 긴(OOD) 시퀀스에 대해서도 길이 일반화(length generalization)를 가능하게 했습니다.
  - 이러한 추상적인 조작 능력은 약 $\sim 100$억 파라미터 규모의 모델에서만 나타나는 특성이었습니다.

## 🧠 Insights & Discussion

- CoT 프롬프팅은 대규모 언어 모델에서 다단계 추론 행동을 이끌어내는 간단하고 광범위하게 적용 가능한 메커니즘입니다.
- 표준 프롬프팅에서 성능 스케일링 곡선이 평평하던 추론 작업에서 CoT 프롬프팅은 극적으로 증가하는 스케일링 곡선을 보여, LLM의 능력이 표준 프롬프팅이 제시하는 것보다 훨씬 넓을 수 있음을 시사합니다.
- **한계점:**
  - CoT가 인간의 사고 과정을 모방하지만, 신경망이 실제로 '추론'하는지 여부는 여전히 미해결 질문입니다.
  - 퓨샷 환경에서 수동 CoT 주석 비용은 최소이지만, 미세 조정을 위한 대규모 주석은 비용이 많이 들 수 있습니다.
  - 모델이 생성하는 추론 경로가 항상 정확하다는 보장이 없으며, 때로는 잘못된 추론으로 올바른 답을 도출하거나 잘못된 답을 낼 수 있습니다.
  - CoT 추론 능력은 대규모 모델에서만 나타나 실제 애플리케이션에 적용하는 비용이 높습니다. 미래 연구에서는 더 작은 모델에서 추론을 유도하는 방법을 탐구할 수 있습니다.

## 📌 TL;DR

거대 언어 모델(LLM)이 규모 확장만으로는 복잡한 추론 작업에 한계가 있음을 해결하기 위해, 이 논문은 **사고의 사슬(Chain-of-Thought, CoT) 프롬프팅**을 제안합니다. 이는 퓨샷 프롬프팅 예시에 최종 답변에 이르는 자연어 중간 추론 단계를 포함하는 방식입니다. 연구 결과, CoT 프롬프팅은 약 $\sim 100$억 개 이상의 파라미터를 가진 대규모 모델에서 **emergent ability**로 나타나며, 산술, 상식, 상징적 추론 작업에서 **성능을 획기적으로 향상**시켰습니다. 특히 PaLM 540B 모델은 GSM8K 수학 문제 벤치마크에서 SOTA를 달성했고, 모델이 더 많은 추론 단계를 거치도록 유도하여 OOD 일반화 능력까지 개선함을 보여주었습니다. 이는 LLM의 추론 능력이 표준 프롬프팅이 제시하는 것보다 훨씬 광범위할 수 있음을 시사합니다.
