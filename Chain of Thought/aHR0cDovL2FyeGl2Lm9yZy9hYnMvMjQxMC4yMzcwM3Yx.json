{
  "title": "OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large\n  Language Models",
  "authors": "Junda Wu, Xintong Li, Ruoyu Wang, Yu Xia, Yuxin Xiong, Jianing Wang, Tong Yu, Xiang Chen, Branislav Kveton, Lina Yao, Jingbo Shang, Julian McAuley",
  "year": 2024,
  "url": "http://arxiv.org/abs/2410.23703v1",
  "abstract": "Offline evaluation of LLMs is crucial in understanding their capacities,\nthough current methods remain underexplored in existing research. In this work,\nwe focus on the offline evaluation of the chain-of-thought capabilities and\nshow how to optimize LLMs based on the proposed evaluation method. To enable\noffline feedback with rich knowledge and reasoning paths, we use knowledge\ngraphs (e.g., Wikidata5m) to provide feedback on the generated chain of\nthoughts. Due to the heterogeneity between LLM reasoning and KG structures,\ndirect interaction and feedback from KGs on LLM behavior are challenging, as\nthey require accurate entity linking and grounding of LLM-generated chains of\nthought in the KG. To address the above challenge, we propose an offline\nchain-of-thought evaluation framework, OCEAN, which models chain-of-thought\nreasoning in LLMs as an MDP and evaluate the policy's alignment with KG\npreference modeling. To overcome the reasoning heterogeneity and grounding\nproblems, we leverage on-policy KG exploration and RL to model a KG policy that\ngenerates token-level likelihood distributions for LLM-generated\nchain-of-thought reasoning paths, simulating KG reasoning preference. Then we\nincorporate the knowledge-graph feedback on the validity and alignment of the\ngenerated reasoning paths into inverse propensity scores and propose KG-IPS\nestimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS\nestimator and provide a lower bound on its variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance chain-of-thought alignment. Our empirical study shows that\nOCEAN can be efficiently optimized for generating chain-of-thought reasoning\npaths with higher estimated values without affecting LLMs' general abilities in\ndownstream tasks or their internal knowledge.",
  "citation": 3
}