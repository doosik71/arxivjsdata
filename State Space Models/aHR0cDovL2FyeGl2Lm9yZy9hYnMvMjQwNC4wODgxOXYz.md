# The Illusion of State in State-Space Models

William Merrill, Jackson Petty, Ashish Sabharwal

## 🧩 Problem to Solve

Transformer 아키텍처는 본질적으로 순차적인 계산과 상태 추적에 있어 이론적인 한계를 가지고 있습니다. 이러한 한계를 극복하기 위해 순환 신경망(RNN)과 유사한 아키텍처를 가진 State-Space Model(SSM)이 등장했습니다. 이 연구의 핵심 질문은 S4 및 Mamba와 같은 SSM이 Transformer에 비해 상태 추적에서 표현력 우위를 진정으로 가지는지 여부입니다. 특히, 내러티브 내 엔티티 추적, 특정 표기법을 사용한 체스 이동 추적, 코드 평가와 같은 복잡한 상태 추적 문제는 대규모 언어 모델(LLM)의 핵심 역량에 해당합니다.

## ✨ Key Contributions

- **이론적 한계 증명:** S4, Mamba를 포함한 일반적인 선형 SSM은 Transformer와 유사하게 $L\text{-uniform TC}_0$ 복잡도 클래스 내의 표현력에 국한됨을 수학적으로 증명했습니다. 이는 이들 SSM이 순열 합성($S_5$ 워드 문제)과 같은 `NC_1`-hard 문제를 해결할 수 없음을 의미합니다.
- **실제 문제 해결 불가능성:** $S_5$ 문제의 해결 불가능성은 체스 이동 추적(특정 표기법 사용), 코드 평가, 긴 내러티브 내 엔티티 추적과 같은 실세계 상태 추적 문제 또한 해결할 수 없음을 시사합니다.
- **실험적 검증:** S4 및 Mamba가 이론적 예측과 일치하게 순열 합성 문제에서 어려움을 겪는 반면, RNN은 단일 레이어로도 해당 문제를 쉽게 학습함을 실험적으로 보여주었습니다.
- **표현력 확장 방안 제시:** 최소한의 변경(입력 의존적인 전이 행렬 $\bar{A}_i$)을 통해 SSM이 $NC_1$-hard 상태 추적 문제를 표현하고 학습할 수 있음을 이론적 및 실험적으로 보여주었습니다. 이는 새로운 SSM 아키텍처 개발의 가능성을 제시합니다.
- **'상태'의 환상 폭로:** 일반적인 SSM의 '상태'는 복잡한 상태 추적 능력 면에서 Transformer와 유사한 표현력 한계를 가지는 '환상'에 불과하며, 진정한 순환 모델과는 다르다는 점을 강조했습니다.

## 📎 Related Works

- **Transformer의 한계:** Merrill & Sabharwal (2023a, 2023b, 2024) 등은 Transformer가 `TC_0`에 국한되며 본질적으로 순차적인 계산을 표현할 수 없음을 보였습니다.
- **SSM 아키텍처:** Gu et al. (2021, 2022a, 2022b)은 RNN과 유사한 표현력을 통해 순차적이고 상태 의존적인 문제를 처리하기 위해 S4와 같은 SSM을 제안했습니다. Mamba (Gu & Dao, 2023)는 SSM의 한 변형입니다.
- **순열 합성 문제:** Liu et al. (2023)은 순열 합성 문제의 `NC_1`-hard 특성과 Transformer의 한계를 다루었습니다.
- **워드 문제와 복잡도:** Barrington (1989)은 유한 비가해(non-solvable) 그룹의 워드 문제가 `NC_1`-complete임을 확립했습니다.
- **확장된 SSM:** Hasani et al. (2023)의 Liquid S4는 입력 의존적인 전이 행렬을 사용하는 SSM의 한 형태입니다.

## 🛠️ Methodology

1. **복잡도 이론을 통한 분석:**
   - **회로 복잡도(Circuit Complexity):** 신경망의 표현력을 $L\text{-uniform TC}_0$ 복잡도 클래스를 사용하여 분석합니다. $TC_0$는 상수 깊이(constant-depth), 다항식 크기(polynomial-size)의 임계값 회로(threshold circuit)로 인식될 수 있는 문제들의 집합으로, 고도로 병렬적인 계산에 해당합니다.
   - **대수 형식 언어 이론(Algebraic Formal Language Theory):** 상태 추적 문제를 유한 모노이드(finite monoid)에 대한 워드 문제로 모델링하여, 어떤 워드 문제가 `TC_0`에 속하지 않을 가능성이 높은지(예: `NC_1`-complete 문제)를 파악합니다.
2. **일반화된 선형 SSM 레이어 정의:** S4와 Mamba의 S6 레이어를 포괄하는 일반화된 선형 SSM 레이어를 정의합니다. 이 레이어는 순환 형태($h_i = \bar{A}_i h_{i-1} + \bar{B}_i x_i$)와 합성 형태($h_i = \sum_{j=1}^{i} \left( \prod_{k=j+1}^{i} \bar{A}_k \right) \bar{B}_j x_j$)를 가집니다.
3. **$TC_0$ 포함 증명 전략:**
   - **Lemma 4.1:** SSM의 합성 형태가 $L\text{-uniform TC}_0$ 회로족으로 계산 가능하려면, 전이 행렬 $\bar{A}_i$의 반복 행렬 곱셈과 SSM 관련 행렬들($\bar{A}_i, \bar{B}_i, C_i, D_i$)이 $L\text{-uniform TC}_0$로 계산 가능해야 함을 보입니다.
   - **비게이팅 SSM (예: S4):** $\bar{A}_i$가 상수인 경우, 반복 행렬 곱셈은 행렬 제곱(matrix powering)으로 환원됩니다. 로그 정밀도 부동 소수점(log-precision float)에 대한 행렬 제곱이 $L\text{-uniform TC}_0$에 속함을 증명합니다.
   - **대각선 SSM (예: S6/Mamba):** $\bar{A}_i$가 대각 행렬인 경우, 반복 행렬 곱셈은 스칼라 곱셈으로 환원되며, 이는 $L\text{-uniform TC}_0$에 속합니다. S6의 파라미터화(exponential 및 softplus 비선형 함수 포함) 또한 $L\text{-uniform TC}_0$로 계산 가능함을 보입니다.
   - **동시 대각화 가능한 SSM:** $\bar{A}_i = W \text{diag}(\bar{a}_i) W^{-1}$ 형태로 동시에 대각화 가능한 경우에도 반복 곱셈이 $L\text{-uniform TC}_0$에 속함을 보입니다.
4. **SSM 표현력 확장 방안:**
   - **비선형성 추가:** $h_i = \text{sgn}(\bar{A}h_{i-1} + \bar{B}x_i)$와 같이 재귀 업데이트에 비선형성을 추가하여 RNN-SSM을 만듭니다. 이는 $S_5$ 워드 문제를 단일 레이어로 해결할 수 있지만, 병렬화가 어려워집니다.
   - **입력 의존적인 전이 행렬 (IDS4):** $\bar{A}_i$를 입력 $x_i$에 따라 달라지도록 합니다($\bar{A}_i = \pi_A(x_i)$). 이는 일반적인 행렬의 반복 곱셈을 허용하여 DFA를 시뮬레이션하고 $S_5$ 문제를 해결할 수 있는 $NC_1$ 표현력을 제공하며, SCAN 알고리즘을 통해 병렬화가 가능합니다.
5. **실험적 평가:**
   - **태스크:** $A_5$, $A_4 \times Z_5$, $Z_{60}$ 그룹에 대한 워드 문제(토큰 태깅 태스크)를 사용합니다.
   - **모델:** Transformer, RNN, S4, Mamba, IDS4를 훈련합니다.
   - **측정:** 시퀀스 길이에 따른 90% 이상의 검증 정확도를 달성하는 데 필요한 최소 레이어 수를 비교합니다.

## 📊 Results

- **이론적 결과:** $TC_0 \neq NC_1$ 가정을 전제로, S4, Mamba와 같은 일반적인 SSM은 $S_5$ 순열 합성 문제나 기타 `NC_1`-hard 상태 추적 문제를 해결할 수 없습니다. 이는 그들의 계산이 $L\text{-uniform TC}_0$ 내에 있기 때문입니다.
- **실험적 결과:**
  - **RNN 및 IDS4의 우수성:** 단일 레이어 RNN 및 IDS4 모델은 모든 그룹에 대해 임의로 긴 시퀀스에 대한 워드 문제를 효과적으로 학습했습니다.
  - **SSM 및 Transformer의 한계:** Transformer, S4, Mamba 모델은 비가환 그룹($A_5$, $A_4 \times Z_5$)에 대해 시퀀스 길이가 길어질수록 좋은 테스트 정확도를 얻기 위해 필요한 레이어 수가 단조적으로 증가했습니다. 이는 이 모델들이 $TC_0$에 속한다는 이론적 예측과 일치합니다.
  - **Transformer 대비 SSM의 미미한 우위:** $A_4 \times Z_5$ 또는 $A_5$와 같은 비가환 태스크에서 S4와 Mamba는 Transformer보다 경험적으로 더 적은 레이어로 유사한 정확도를 달성하며 약간 더 나은 성능을 보였습니다. 그러나 여전히 깊이 증가의 한계는 유지됩니다.

## 🧠 Insights & Discussion

- **상태의 환상:** S4 및 Mamba와 같은 일반적인 SSM은 순환 구조를 가지고 있지만, 복잡한 상태 추적 능력에서 Transformer와 유사한 근본적인 한계를 공유합니다. 이들의 "상태"는 진정한 `NC_1`-complete 문제를 해결할 수 있는 재귀적 계산 능력을 의미하지 않습니다.
- **이전 주장과의 충돌:** Gu et al. (2021)의 "SSM이 RNN을 시뮬레이션할 수 있다"는 주장은 무한 깊이를 가정했을 때 유효하며, 현실적인 유한 깊이 설정에서는 본 연구의 결과와 상충됩니다.
- **상태 추적의 난이도:** 현재 SSM은 '얕은 지름길'이 존재하는 간단한 상태 추적 문제만을 해결할 수 있으며, `NC_1`-complete와 같은 본질적으로 어려운 상태 추적 문제에는 취약합니다.
- **확장된 SSM의 가능성:** 재귀 업데이트에 비선형성을 추가하거나 입력 의존적인 전이 행렬을 사용하는 등의 최소한의 아키텍처 변경을 통해 SSM은 $NC_1$ 표현력을 얻을 수 있습니다.
- **실용적 고려사항:**
  - **병렬화:** 입력 의존적인 전이 행렬을 사용하는 IDS4는 SCAN 알고리즘을 통해 여전히 로그 깊이(log-depth)의 병렬 계산이 가능하지만, 비선형성을 추가하는 RNN-SSM은 병렬화가 어려워집니다.
  - **학습 역학:** 입력 의존적인 전이 행렬은 기울기 소실/폭발 문제를 일으킬 수 있지만, 이는 S6의 선택적 게이팅에서도 이미 존재할 수 있는 문제입니다.
- **미래 방향:** 본 연구는 Transformer와 SSM의 병렬성과 상태 추적을 위한 완전한 표현력 사이의 균형을 맞출 수 있는 새로운 신경망 아키텍처 개발의 중요성을 강조합니다. 이는 게임, 코드, 언어 추론에서 더 큰 역량을 가진 LLM의 발전을 가능하게 할 것입니다.

## 📌 TL;DR

- **문제:** State-Space Model(SSM)이 Transformer보다 복잡한 순차적 계산 및 상태 추적에서 이론적 우위를 가지는지 탐구.
- **방법:** S4, Mamba 등 일반적인 SSM의 계산 복잡도를 `L-uniform TC_0` 클래스 내에서 분석하고, `S_5` 순열 합성 문제와 같은 `NC_1`-complete 상태 추적 문제를 통해 이론적 한계를 증명. 실험적으로도 검증.
- **발견:**
  - S4, Mamba는 Transformer와 유사하게 $L\text{-uniform TC}_0$에 국한되어 `S_1`-hard 순열 합성 문제와 같은 복잡한 상태 추적 문제를 해결할 수 없음. 즉, 이들의 '상태'는 환상에 불과함.
  - RNN은 단 한 층으로 이러한 문제를 해결할 수 있는 반면, SSM과 Transformer는 입력 길이가 길어질수록 더 많은 층이 필요함.
  - 입력 의존적인 전이 행렬($\bar{A}_i$)을 사용하는 SSM (IDS4)은 단 한 층으로 `S_5` 문제를 해결하며, 기존 SSM의 표현력 한계를 극복할 수 있음을 입증.
