{
  "title": "Quamba: A Post-Training Quantization Recipe for Selective State Space\n  Models",
  "authors": "Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Diana Marculescu",
  "year": 2024,
  "url": "http://arxiv.org/abs/2410.13229v2",
  "abstract": "State Space Models (SSMs) have emerged as an appealing alternative to\nTransformers for large language models, achieving state-of-the-art accuracy\nwith constant memory complexity which allows for holding longer context lengths\nthan attention-based networks. The superior computational efficiency of SSMs in\nlong sequence modeling positions them favorably over Transformers in many\nscenarios. However, improving the efficiency of SSMs on request-intensive\ncloud-serving and resource-limited edge applications is still a formidable\ntask. SSM quantization is a possible solution to this problem, making SSMs more\nsuitable for wide deployment, while still maintaining their accuracy.\nQuantization is a common technique to reduce the model size and to utilize the\nlow bit-width acceleration features on modern computing units, yet existing\nquantization techniques are poorly suited for SSMs. Most notably, SSMs have\nhighly sensitive feature maps within the selective scan mechanism (i.e., linear\nrecurrence) and massive outliers in the output activations which are not\npresent in the output of token-mixing in the self-attention modules. To address\nthis issue, we propose a static 8-bit per-tensor SSM quantization method which\nsuppresses the maximum values of the input activations to the selective SSM for\nfiner quantization precision and quantizes the output activations in an\noutlier-free space with Hadamard transform. Our 8-bit weight-activation\nquantized Mamba 2.8B SSM benefits from hardware acceleration and achieves a\n1.72x lower generation latency on an Nvidia Orin Nano 8G, with only a 0.9% drop\nin average accuracy on zero-shot tasks. The experiments demonstrate the\neffectiveness and practical applicability of our approach for deploying\nSSM-based models of all sizes on both cloud and edge platforms."
}