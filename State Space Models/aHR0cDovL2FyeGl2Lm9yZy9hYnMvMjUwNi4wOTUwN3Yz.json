{
  "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary\n  Position Embedding",
  "authors": "Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, Yuyu Luo",
  "year": 2025,
  "url": "http://arxiv.org/abs/2506.09507v3",
  "abstract": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongr inuity their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance.To address this\nimpediment, we propose a unified rotary position embedding (Unified RoPE)\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this Unified RoPE, we\nintroduce TransXSSM, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4 sequenceK length, TransXSSM exhibits training and inference speeds that are\n42.3% and 29.5% faster, respectively, relative to standard Transformer models.\nIt also delivers higher accuracy: under comparable settings, it surpasses a\nTransformer baseline by over 4% on language modeling benchmarks.TransXSSM\nfurthermore scales more effectively: TransXSSM-1.3B gains 7.22% in average\naccuracy over its 320M version (versus about 6% gains for equivalent\nTransformers or SSMs). Our results show that unified positional encoding\nresolves positional incompatibility in hybrid models, enabling efficient,\nhigh-performance long-context modeling."
}