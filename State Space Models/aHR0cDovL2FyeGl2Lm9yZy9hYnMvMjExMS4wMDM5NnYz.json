{
  "url": "http://arxiv.org/abs/2111.00396v3",
  "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
  "authors": "Albert Gu, Karan Goel, Christopher RÃ©",
  "year": 2021,
  "abstract": "A central goal of sequence modeling is designing a single principled model\nthat can address sequence data across a range of modalities and tasks,\nparticularly on long-range dependencies. Although conventional models including\nRNNs, CNNs, and Transformers have specialized variants for capturing long\ndependencies, they still struggle to scale to very long sequences of $10000$ or\nmore steps. A promising recent approach proposed modeling sequences by\nsimulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t),\ny(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state\nmatrix \\( A \\), this system could handle long-range dependencies mathematically\nand empirically. However, this method has prohibitive computation and memory\nrequirements, rendering it infeasible as a general sequence modeling solution.\nWe propose the Structured State Space sequence model (S4) based on a new\nparameterization for the SSM, and show that it can be computed much more\nefficiently than prior approaches while preserving their theoretical strengths.\nOur technique involves conditioning \\( A \\) with a low-rank correction,\nallowing it to be diagonalized stably and reducing the SSM to the well-studied\ncomputation of a Cauchy kernel. S4 achieves strong empirical results across a\ndiverse range of established benchmarks, including (i) 91\\% accuracy on\nsequential CIFAR-10 with no data augmentation or auxiliary losses, on par with\na larger 2-D ResNet, (ii) substantially closing the gap to Transformers on\nimage and language modeling tasks, while performing generation $60\\times$\nfaster (iii) SoTA on every task from the Long Range Arena benchmark, including\nsolving the challenging Path-X task of length 16k that all prior work fails on,\nwhile being as efficient as all competitors."
}