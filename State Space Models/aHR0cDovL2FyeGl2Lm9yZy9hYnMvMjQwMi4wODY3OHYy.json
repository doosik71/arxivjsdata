{
  "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
  "authors": "Ali Behrouz, Farnoosh Hashemi",
  "year": 2024,
  "url": "http://arxiv.org/abs/2402.08678v2",
  "abstract": "Graph Neural Networks (GNNs) have shown promising potential in graph\nrepresentation learning. The majority of GNNs define a local message-passing\nmechanism, propagating information over the graph by stacking multiple layers.\nThese methods, however, are known to suffer from two major limitations:\nover-squashing and poor capturing of long-range dependencies. Recently, Graph\nTransformers (GTs) emerged as a powerful alternative to Message-Passing Neural\nNetworks (MPNNs). GTs, however, have quadratic computational cost, lack\ninductive biases on graph structures, and rely on complex Positional/Structural\nEncodings (SE/PE). In this paper, we show that while Transformers, complex\nmessage-passing, and SE/PE are sufficient for good performance in practice,\nneither is necessary. Motivated by the recent success of State Space Models\n(SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general\nframework for a new class of GNNs based on selective SSMs. We discuss and\ncategorize the new challenges when adapting SSMs to graph-structured data, and\npresent four required and one optional steps to design GMNs, where we choose\n(1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of\nBidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE\nand SE. We further provide theoretical justification for the power of GMNs.\nExperiments demonstrate that despite much less computational cost, GMNs attain\nan outstanding performance in long-range, small-scale, large-scale, and\nheterophilic benchmark datasets."
}