{
  "title": "Explainable Deep One-Class Classification",
  "authors": "Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus-Robert MÃ¼ller",
  "year": 2020,
  "url": "http://arxiv.org/abs/2007.01760v3",
  "abstract": "Deep one-class classification variants for anomaly detection learn a mapping\nthat concentrates nominal samples in feature space causing anomalies to be\nmapped away. Because this transformation is highly non-linear, finding\ninterpretations poses a significant challenge. In this paper we present an\nexplainable deep one-class classification method, Fully Convolutional Data\nDescription (FCDD), where the mapped samples are themselves also an explanation\nheatmap. FCDD yields competitive detection performance and provides reasonable\nexplanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet.\nOn MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps,\nFCDD sets a new state of the art in the unsupervised setting. Our method can\nincorporate ground-truth anomaly maps during training and using even a few of\nthese (~5) improves performance significantly. Finally, using FCDD's\nexplanations we demonstrate the vulnerability of deep one-class classification\nmodels to spurious image features such as image watermarks."
}