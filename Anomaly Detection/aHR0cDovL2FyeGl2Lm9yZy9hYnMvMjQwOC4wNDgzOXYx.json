{
  "url": "http://arxiv.org/abs/2408.04839v1",
  "title": "Adversarially Robust Industrial Anomaly Detection Through Diffusion\n  Model",
  "authors": "Yuanpu Cao, Lu Lin, Jinghui Chen",
  "year": 2024,
  "abstract": "Deep learning-based industrial anomaly detection models have achieved\nremarkably high accuracy on commonly used benchmark datasets. However, the\nrobustness of those models may not be satisfactory due to the existence of\nadversarial examples, which pose significant threats to the practical\ndeployment of deep anomaly detectors. Recently, it has been shown that\ndiffusion models can be used to purify the adversarial noises and thus build a\nrobust classifier against adversarial attacks. Unfortunately, we found that\nnaively applying this strategy in anomaly detection (i.e., placing a purifier\nbefore an anomaly detector) will suffer from a high anomaly miss rate since the\npurifying process can easily remove both the anomaly signal and the adversarial\nperturbations, causing the later anomaly detector failed to detect anomalies.\nTo tackle this issue, we explore the possibility of performing anomaly\ndetection and adversarial purification simultaneously. We propose a simple yet\neffective adversarially robust anomaly detection method, \\textit{AdvRAD}, that\nallows the diffusion model to act both as an anomaly detector and adversarial\npurifier. We also extend our proposed method for certified robustness to $l_2$\nnorm bounded perturbations. Through extensive experiments, we show that our\nproposed method exhibits outstanding (certified) adversarial robustness while\nalso maintaining equally strong anomaly detection performance on par with the\nstate-of-the-art methods on industrial anomaly detection benchmark datasets.",
  "citation": 1
}