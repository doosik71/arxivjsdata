# Toward Deep Supervised Anomaly Detection: Reinforcement Learning from Partially Labeled Anomaly Data

Guansong Pang, Anton van den Hengel, Chunhua Shen, Longbing Cao

## 🧩 Problem to Solve

이 논문은 부분적으로 레이블링된 이상치 데이터셋(소수의 레이블링된 이상치와 대규모의 레이블링되지 않은 데이터)을 사용하여 이상치를 탐지하는 문제를 다룹니다. 이는 사이버 보안, 질병 탐지, 사기 탐지 등 많은 중요한 실제 애플리케이션에서 흔히 발생하는 시나리오입니다. 기존 관련 방법들은 제한된 레이블링된 이상치 예제에만 과도하게 적합(overfit)되거나, 레이블링되지 않은 데이터에서 비지도 학습을 진행하여 알려지지 않은 이상치를 놓치거나 많은 오탐을 발생시키는 문제가 있습니다. 핵심 과제는 제한된 레이블링된 이상치 데이터를 활용하면서도 알려진 이상치뿐만 아니라 새로운 유형의 이상치(알려지지 않은 이상치)까지 효과적으로 탐지하는 방법을 찾는 것입니다.

## ✨ Key Contributions

- **현실적인 이상치 탐지 문제 해결:** 부분적으로 레이블링된 이상치 데이터를 활용하여 알려진 이상치와 알려지지 않은 이상치를 모두 탐지하는 '지도 학습' 이상치 탐지 문제를 해결하는 방법을 제안합니다.
- **새로운 DRL 기반 접근 방식 도입:** 이 문제를 위해 특별히 고안된 새로운 심층 강화 학습(DRL) 접근 방식을 소개합니다. 이 방식은 제한된 레이블링된 이상치 데이터를 자동적이고 상호작용적으로 활용하여 알려진 이상 징후를 학습하고, 동시에 레이블링되지 않은 희귀한 이상치를 적극적으로 탐색하여 학습된 이상 징후를 알려지지 않은 이상 징후로 확장합니다.
- **통합 최적화 달성:** 알려진 이상치와 알려지지 않은 이상치 탐지를 동시에 최적화하는 최초의 DRL 기반 모델인 DPLAN (Deep Q-learning with Partially Labeled ANomalies)을 제시합니다.
- **광범위한 실험 및 성능 검증:** 네 가지 실제 데이터셋에서 파생된 48개의 데이터셋에 대한 광범위한 실험을 통해 DPLAN이 다섯 가지 최신 경쟁 방법보다 훨씬 우수하고 안정적인 성능을 보이며, 정밀도-재현율(AUC-PR)에서 최소 7~12% 향상된 결과를 달성함을 입증합니다.

## 📎 Related Works

- **비지도 이상치 탐지:**
  - 전통적인 방법: LOF (Local Outlier Factor) [5], iForest (Isolation Forest) [11].
  - 심층 학습 기반: Autoencoder/GAN 기반 재구성 오류 측정 [24, 32, 33], 특정 이상치 측정에 특화된 특징 표현 학습 (REPEN [16], DAGMM [33]).
- **약지도 이상치 탐지:**
  - 제한된 레이블링된 이상치를 활용하여 정확도를 개선하는 방법들: 레이블 전파 [26], 종단 간 특징 학습 (DevNet [19], Deep SAD [22]).
  - 이러한 방법들은 레이블링된 이상치에 과도하게 의존하여 알려진 이상치에 과적합될 위험이 있습니다.
- **PU (Positive-Unlabeled) 학습:** 본 연구와 관련이 있으나, PU 학습은 긍정 인스턴스(이상치)가 동일한 매니폴드/구조를 공유한다고 가정하는 반면, 본 연구의 이상치는 다른 매니폴드/구조에 존재할 수 있어 근본적으로 다른 문제입니다.
- **능동적 이상치 탐지:** 인간 전문가의 피드백을 통해 반복적으로 이상치를 탐지하는 방법 [1, 25, 29]과 관련이 있으나, 본 연구는 인간 개입 없이 레이블링되지 않은 데이터를 자동적으로 탐색합니다.
- **DRL 기반 지식 발견:** Atari 게임 [12]에서의 성공 이후 추천 시스템 [30, 31], 자동화된 머신러닝 [7, 34], 역 강화 학습을 이용한 순차 이상치 탐지 [15], 신경 아키텍처 탐색 [10] 등 다양한 분야에 DRL이 적용되었습니다. 본 연구는 순차적 결정 과정 없이 '지도' 설정에서 이상치 탐지를 위한 DRL을 제안한다는 점에서 차이가 있습니다.

## 🛠️ Methodology

본 논문은 알려진 이상치와 알려지지 않은 이상치 탐지를 공동으로 최적화하기 위해 이상치 탐지 지향 심층 강화 학습(DRL) 접근 방식을 제안하며, DPLAN (Deep Q-learning with Partially Labeled ANomalies)이라는 모델로 구체화됩니다.

1. **문제 정의:**
   - 훈련 데이터 $D = \{D_a, D_u\}$로 구성됩니다. $D_a$는 소수의 레이블링된 이상치 집합이고, $D_u$는 대규모의 레이블링되지 않은 데이터(대부분 정상 데이터와 일부 알려진/알려지지 않은 이상치 포함)입니다.
   - 목표는 데이터 인스턴스에 이상치 점수를 할당하는 이상치 스코어링 함수 $\phi: D \to R$를 학습하는 것입니다.
2. **DRL 프레임워크 요소:**
   - **관측 공간 (Observation space):** 전체 훈련 데이터 $D$의 각 데이터 인스턴스 $s \in D$가 관측(상태)이 됩니다.
   - **행동 공간 (Action space):** $\{a_0, a_1\}$로 정의되며, $a_0$은 '정상'으로 레이블링하는 행동, $a_1$은 '이상'으로 레이블링하는 행동입니다.
   - **에이전트 (Agent):** 딥 신경망(Q-네트워크)으로 구현되며, 주어진 관측 $s$에 대해 최적의 행동을 선택합니다.
   - **이상치 편향 시뮬레이션 환경 (Anomaly-biased simulation environment) $E$:**
     - 데이터가 순차적 결정 프로세스를 포함하지 않으므로, 에이전트와 환경 간의 의미 있는 상호작용을 가능하게 하는 시뮬레이션 환경이 필요합니다.
     - **관측 샘플링 함수 $g(s_{t+1}|s_t, a_t)$:**
       - $D_a$에서 균일하게 샘플링하는 $g_a$와 $D_u$에서 근접성에 따라 샘플링하는 $g_u$로 구성됩니다.
       - $g_u$는 에이전트가 현재 관측 $s_t$를 이상치라고 판단($a_t=a_1$)하면 $s_t$에 가장 가까운 이웃을, 정상이라고 판단($a_t=a_0$)하면 가장 먼 이웃을 $D_u$에서 샘플링합니다. 이는 $D_u$의 잠재적 이상치를 적극적으로 탐색하는 역할을 합니다.
       - $g_a$와 $g_u$는 확률 $p$로 균형 있게 사용됩니다 ($p=0.5$ 기본).
   - **결합 보상 함수 (Combined reward function):** $r = c(r_e, r_i)$
     - **레이블링된 이상치 데이터 기반 외부 보상 $h(r^e_t|s_t, a_t)$:**
       - 에이전트가 $D_a$에 속하는 $s_t$를 정확히 '이상치'($a_t=a_1$)로 인식하면 양의 보상 (+1)을 제공합니다.
       - 정상 인스턴스를 정확히 '정상'으로 인식하면 보상 (0)을 주지 않습니다.
       - 오탐 또는 미탐 시 음의 보상 (-1)을 제공합니다. 이는 알려진 이상치에 대한 '활용(exploitation)'을 장려합니다.
     - **비지도 내재적 보상 $f(s_t)$:**
       - 현재 관측 $s_t$의 참신도(novelty)를 iForest [11]를 사용하여 측정합니다. iForest는 DQN의 마지막 은닉층에서 파생된 특징 임베딩 공간 $\Psi(\cdot; \theta_e)$에서 동작하여 계산 효율성을 높입니다.
       - 에이전트가 희귀하거나 새로운 관측을 발견할 때 높은 보상을 받도록 하여 레이블링되지 않은 데이터 $D_u$에서 '탐색(exploration)'을 장려합니다.
3. **DPLAN의 최적화:**
   - 에이전트는 Q-함수 $Q^*(s, a) = \max_{\pi} E[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots | s_t=s, a_t=a, \pi]$를 학습합니다.
   - 이는 딥 Q-네트워크(DQN) [12]를 사용하여 손실 함수 $L_j(\theta_j) = E_{(s,a,r,s') \sim U(E)} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-_j) - Q(s,a; \theta_j) \right)^2 \right]$를 최소화함으로써 이루어집니다.
   - 훈련 후, 학습된 Q-함수 $Q(\hat{s}, a_1; \theta^*)$가 테스트 데이터 $\hat{s}$에 대한 이상치 점수로 사용됩니다. 직관적으로, 레이블링된 이상치, 레이블링되지 않은 이상치, 정상 인스턴스 순으로 $Q(\hat{s}, a_1; \theta^*)$ 값이 커져야 이상치를 잘 구별할 수 있습니다.

## 📊 Results

- **데이터셋:** UNSW_NB15, Annthyroid, HAR, Covertype 등 4개 실제 데이터셋을 기반으로, 알려진 이상치 클래스의 수와 이상치 오염률을 조절하여 총 48개 시나리오 데이터셋을 구성했습니다.
- **경쟁 방법:** DevNet, Deep SAD (약지도), REPEN, iForest (비지도), DUA (DevNet + 의사 레이블링)와 비교했습니다.
- **평가 지표:** AUC-PR (정밀도-재현율 곡선 아래 면적)과 AUC-ROC (수신기 동작 특성 곡선 아래 면적)를 사용했습니다.
- **주요 결과:**
  - **전반적인 우수성:** DPLAN은 13개 데이터셋 중 10개에서 AUC-PR 및 AUC-ROC 모두에서 최고의 성능을 달성했으며, 나머지 데이터셋에서도 최고 성능에 근접했습니다. 평균적으로 DPLAN은 DevNet, Deep SAD, DUA보다 AUC-PR에서 7~12% 더 높은 성능을 보였고, REPEN 및 iForest와 같은 비지도 방법보다 거의 100% 향상된 성능을 보였습니다.
  - **안정성:** DPLAN은 모든 데이터셋에서 AUC 표준 편차가 경쟁 방법보다 현저히 작아 매우 안정적인 성능을 입증했습니다.
  - **알려진 이상치 클래스 수 증가:** 알려진 이상치 클래스 수가 증가함에 따라 DPLAN의 AUC-PR 성능이 크게 향상되었으며, 일부 데이터셋에서는 최대 91%의 상대적 개선을 보였습니다.
  - **이상치 오염률 증가에 대한 내성:** DPLAN은 다양한 이상치 오염률에서도 일관되게 경쟁 방법보다 우수한 성능을 유지했으며, 일부 데이터셋에서는 오염률 증가에도 불구하고 성능이 안정적이었습니다.
  - **심층 아키텍처 활용:** DevNet과 같은 이전 방법들은 깊은 네트워크 아키텍처 사용 시 성능이 크게 하락했지만, DPLAN의 깊은 Q-네트워크($\text{DQN}^{+}$) 변형은 원래 DPLAN보다 성능을 현저히 향상시켜, 제한된 레이블링 데이터로도 복잡하고 일반화된 모델 학습이 가능함을 보여주었습니다.
  - **학습 스텝 수의 영향:** DPLAN은 대부분 20,000 학습 스텝 부근에서 빠르게 수렴했지만, 일부 데이터셋(예: Hypothyroid, Subnormal)에서는 학습 스텝을 늘리면 AUC-PR이 23%에서 98%까지 추가적으로 향상될 수 있음을 보여주었습니다.

## 🧠 Insights & Discussion

- **일반화 능력 강화:** DPLAN은 제한된 레이블링된 이상치 예제를 활용하는 동시에 레이블링되지 않은 데이터에서 알려지지 않은 이상치를 적극적으로 탐색하여 기존 방법보다 훨씬 더 일반화된 이상 징후를 학습합니다. 이는 부분적으로 레이블링된 데이터를 효과적으로 활용하는 DRL의 강점을 보여줍니다.
- **강화 학습의 독특한 이점:** 제안된 이상치 편향 시뮬레이션 환경과 결합 보상 함수(외부 및 내재적)는 에이전트가 알려진 이상치를 '활용'하고 알려지지 않은 이상치를 '탐색'하는 균형 잡힌 전략을 가능하게 합니다. 이는 기존의 비연결적인 의사 레이블링 방식(DUA)보다 훨씬 효과적임을 입증했습니다.
- **심층 네트워크 확장성:** DPLAN의 구조는 기존 준지도 학습 방법이 직면했던 문제(제한된 레이블링 데이터로 인한 깊은 네트워크의 과적합)를 극복하고, 더 깊은 네트워크 아키텍처를 통해 모델 복잡성과 일반화 능력을 향상시킬 수 있는 가능성을 열어줍니다.
- **실제 적용 가능성:** DPLAN의 훈련 시간은 경쟁 방법에 비해 길 수 있지만(오프라인 처리 가능), 온라인 추론 시에는 단일 순방향 패스로 이상치 점수를 얻어 매우 효율적입니다. 이는 대규모 실시간 시스템에 적용 가능성을 높입니다.
- **한계점 및 미래 방향:** DPLAN은 레이블링된 데이터의 감독 정보가 특정 지점에서 바운딩될 수 있음을 보여주지만, 특정 데이터셋에서는 학습 스텝을 늘리거나 더 깊은 아키텍처를 사용하여 성능을 더 향상시킬 수 있는 여지를 남깁니다. 보상 함수와 환경 설계의 중요성을 강조하며, 이는 DRL 기반 이상치 탐지 연구의 중요한 방향이 될 것입니다.

## 📌 TL;DR

부분적으로 레이블링된 이상치 데이터(소수의 레이블된 이상치와 대규모 레이블 없는 데이터)에서 알려진 및 알려지지 않은 이상치를 모두 탐지하는 문제를 해결하기 위해, 본 논문은 DPLAN이라는 새로운 심층 강화 학습(DRL) 접근 방식을 제안합니다. DPLAN은 DQN 기반 에이전트가 이상치 편향 시뮬레이션 환경에서 레이블링된 이상치 데이터로부터 알려진 이상 징후를 '활용'하고, 비지도 내재적 보상을 통해 레이블링되지 않은 데이터에서 새로운 이상치를 '탐색'하도록 공동으로 최적화합니다. 48개 실제 데이터셋에 대한 실험 결과, DPLAN은 최신 경쟁 방법들을 AUC-PR에서 최소 7~12% 이상 능가하며, 더 깊은 신경망 아키텍처와 더 많은 학습 스텝을 통해 추가적인 성능 향상도 가능함을 보여주었습니다.
