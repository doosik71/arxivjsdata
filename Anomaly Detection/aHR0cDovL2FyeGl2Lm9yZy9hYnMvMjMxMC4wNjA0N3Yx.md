# Knowledge Distillation for Anomaly Detection
Adrian Alan Pol, Ekaterina Govorkova, Sonja Gr ̈onroos, Nadezda Chernyavskaya, Philip Harris, Maurizio Pierini, Isobel Ojalvo, Peter Elmer

## 🧩 Problem to Solve
비지도 학습 기반의 딥러닝 이상 탐지 모델은 높은 성능을 달성하기 위해 복잡하고 큰 모델을 요구합니다. 이러한 대형 모델은 에지 장치나 임베디드 시스템과 같이 리소스가 제한적인 환경(예: CERN의 고에너지 물리 실험에서 요구되는 100 ns 지연 시간)에 배포하기 어렵습니다. 따라서 모델의 크기와 메모리 사용량을 크게 줄이면서도 이상 탐지 성능을 유지하는 효과적인 압축 기법이 필요합니다.

## ✨ Key Contributions
*   **새로운 지식 증류 (Knowledge Distillation, KD) 절차 제안:** 비지도 이상 탐지 모델을 작고 배포 가능한 지도 학습 모델로 압축하는 방법을 제안합니다.
*   **비지도 학습 문제를 지도 학습 문제로 전환:** 대형 `teacher` 모델의 이상 점수를 `student` 모델이 직접 학습하도록 하여, `student` 모델이 해결해야 할 문제의 복잡도를 줄입니다.
*   **탐지 민감도 향상을 위한 기법 도입:**
    *   `teacher`와 `student` 모델을 동시에 학습시키는 **공동 학습 (Co-learning)** 방법을 제안합니다.
    *   `student` 모델의 일반화 능력을 향상시키기 위해 **아웃라이어 노출 (Outlier Exposure)** 기법을 활용합니다.
    *   **노이즈 주입 (Noise Injection)**을 통한 아웃라이어 노출 가능성을 탐색합니다.
*   **성능 유지 및 효율성 증명:** 압축된 `student` 모델이 원본 `teacher` 모델과 유사한 탐지 성능을 유지하면서 모델 크기와 메모리 사용량을 크게 줄일 수 있음을 입증합니다.

## 📎 Related Works
*   **전통적인 지식 증류:** `teacher` 모델의 지식을 `student` 모델로 전달하는 개념은 Bucilua et al. [4] 및 Hinton et al. [5]의 연구에서 시작되었습니다.
*   **이상 탐지 분야의 KD 적용:**
    *   Salehi et al. [7]은 중간 특징 맵을 사용하여 더 얕은 모델을 훈련했습니다.
    *   Liu et al. [8]은 KD를 통해 훈련 세트 내 이상치의 영향을 완화했습니다.
    *   Xiao et al. [9]는 `teacher`의 저차원 임베딩을 `student` 앙상블 학습에 활용했습니다.
*   **KD의 일반화 관련 연구:** Stanton et al. [6]은 지식 증류의 일반화 능력에 대한 의문을 제기했습니다.
*   **아웃라이어 노출:** Kulkarni et al. [12]의 연구에서 영감을 받아 아웃라이어 노출 전략을 적용했습니다.
*   **노이즈를 통한 KD:** Sau and Balasubramanian [13]은 노이즈가 있는 `teacher`로부터 지식을 증류하는 방법을 연구했습니다.
*   **지식 전달과 모델 용량 격차:** Mirzadeh et al. [14]은 모델 용량 격차가 지식 전달을 저하시킬 수 있음을 보여주었습니다.

## 🛠️ Methodology
본 논문은 대형 비지도 이상 탐지 모델(`teacher` Autoencoder)을 작고 배포 가능한 지도 `student` 모델로 압축하는 지식 증류 기반의 새로운 절차를 제안합니다.

1.  **`Teacher` 모델 훈련:**
    *   **모델:** 오토인코더(AE) 구조를 가진 `teacher` 모델을 사용합니다. 인코더와 디코더는 각각 5개의 컨볼루션 레이어와 평균 풀링 레이어로 구성되며, 20차원의 표현 벡터와 두 개의 완전 연결 레이어를 포함합니다.
    *   **데이터:** MNIST 및 Fashion-MNIST 데이터셋에서 단일 클래스 이미지를 '정상' 데이터로 간주하여 비지도 방식으로 훈련합니다 (각 클래스당 6000개 샘플).
    *   **훈련:** MSE(Mean Squared Error) 손실 함수를 사용하며, Adam 옵티마이저로 300 에포크 동안 훈련합니다.
    *   **이상 점수:** 입력 이미지 $x_i$에 대해 재구성 손실 $L(x_i, D(E(x_i)))$을 이상 점수 $S_i$로 정의합니다. `student` 학습을 용이하게 하기 위해 `teacher`의 손실 값에 로그 변환을 적용합니다.

2.  **`Student` 모델 훈련:**
    *   **모델:** `teacher` 모델보다 훨씬 작고 간단한 7가지 다른 컨볼루션 기반 아키텍처(S1-S7)를 실험합니다. `student` 모델은 오토인코더가 아니며, 입력을 재구성할 필요 없이 `teacher`의 이상 점수 $S_i$를 직접 회귀하도록 학습합니다 ($g(x_i) \approx S_i$).
    *   **훈련:** MAE(Mean Absolute Error) 손실 함수를 사용하며, Adam 옵티마이저로 300 에포크 동안 훈련합니다.
    *   **평가:** ROC-AUC와 EMD(Earth Mover's Distance, Wasserstein distance)를 사용하여 `student` 모델의 탐지 성능과 `teacher` 이상 점수 분포에 대한 재현 능력을 평가합니다.

3.  **성능 향상 기법:**
    *   **공동 학습 (Co-learning):** `teacher`의 재구성 손실과 `student`의 증류 손실을 결합하여 최소화합니다. `teacher` 모델과 `student` 모델을 동시에 훈련시켜 `teacher`의 출력을 `student`의 부드러운 목표(soft targets)로 활용합니다.
    *   **아웃라이어 노출 (Outlier Exposure):** `teacher`가 훈련된 정상 데이터셋 외에, 다른 비관련 데이터셋(예: MNIST에 훈련된 `teacher`의 `student` 학습에 Fashion-MNIST 데이터를 이상치로 사용)의 샘플을 `student` 훈련에 추가하여 `student`가 분포 외(out-of-distribution) 샘플에 노출되도록 합니다.
    *   **노이즈를 통한 아웃라이어 노출 (Denoising as Outlier Exposure):** 추가 데이터셋 없이 `student` 훈련 세트에만 노이즈($x + \epsilon \times N(0,1)$)를 주입하여 `student`가 이상치 분포를 더 잘 학습하도록 유도합니다.

## 📊 Results
*   **모델 압축 효과:** 제안된 지식 증류 접근 방식은 비지도 이상 탐지 모델을 효과적으로 압축했습니다. 압축된 `student` 모델은 `teacher` 모델에 비해 크기와 메모리 사용량이 크게 줄었지만, 높은 탐지 민감도를 유지했습니다.
*   **공동 학습의 이점:** `teacher`와 `student`를 함께 훈련하는 공동 학습 방식은 `baseline` (오프라인 학습) 대비 ROC-AUC 및 EMD 측면에서 평균적으로 성능을 향상시켰습니다.
*   **아웃라이어 노출의 효과:** 공동 학습과 함께 아웃라이어 노출 기법을 적용했을 때, `student` 모델이 분포 외 데이터로부터 학습할 수 있었기 때문에 특히 이상치 분포 거리(EMD)가 더욱 개선되었습니다. 이는 전반적인 결과 향상으로 이어졌습니다.
*   **노이즈 주입의 영향:** 노이즈 주입은 이상치 분포 거리를 줄였지만, 동시에 정상치 분포 거리를 증가시켜 전반적인 이상치 탐지 민감도를 감소시키는 `양날의 검`으로 나타났습니다.
*   **모델 크기와 성능:** `student` 모델의 매개변수 수가 증가할수록 성능(ROC-AUC)이 향상되고 이상치 EMD가 감소하는 일반적인 경향이 관찰되었습니다. 이는 모델 용량 격차가 줄어들수록 지식 전달이 향상된다는 기존 연구 결과와 일치합니다.
*   **최고 성능 조합:** 공동 학습과 아웃라이어 노출을 결합한 방법이 ROC-AUC와 EMD 모두에서 평균적으로 가장 좋은 결과를 보였습니다. MNIST 데이터셋에서 더 명확한 추세가 나타났지만, Fashion-MNIST 결과는 다소 불규칙했습니다.

## 🧠 Insights & Discussion
*   **실용적 의의:** 본 연구의 가장 중요한 시사점은 복잡한 비지도 이상 탐지 모델을 리소스 제약이 있는 환경, 특히 고에너지 물리 분야와 같이 초고속 추론이 요구되는 곳에 배포할 수 있게 한다는 점입니다. 지식 증류를 통해 모델의 크기와 복잡성을 크게 줄이면서도 핵심적인 탐지 성능을 유지할 수 있음을 입증했습니다.
*   **지도 학습으로의 전환:** 비지도 학습 문제를 `teacher`의 이상 점수를 예측하는 지도 학습 문제로 전환한 것이 `student` 모델의 학습을 단순화하고 효율성을 높이는 데 핵심적인 역할을 했습니다.
*   **일반화 및 강건성:** 아웃라이어 노출 기법은 `student` 모델의 이상치에 대한 일반화 능력을 향상시키는 데 효과적이었습니다. 그러나 노이즈 주입 방식의 아웃라이어 노출은 정상치 분포에 대한 학습을 방해하여 민감도를 떨어뜨릴 수 있음을 보여주었습니다.
*   **데이터 및 아키텍처 의존성:** 실험 결과는 데이터셋과 모델 아키텍처에 따라 다를 수 있음을 강조했습니다. 이는 일반적인 경향을 발견했지만, 실제 적용 시에는 특정 사용 사례에 대한 추가 검증이 필요함을 시사합니다.
*   **향후 연구:** 본 방법의 특정 응용 분야에서의 적용 가능성을 탐구하고, 컴퓨팅 효율성 및 에너지 소비와 같은 다른 평가 지표를 통해 실용성을 추가로 검증할 계획입니다.

## 📌 TL;DR
리소스 제약이 있는 환경에 대형 비지도 이상 탐지 모델을 배포하기 위해, 본 논문은 지식 증류(KD)를 활용하여 대규모 오토인코더 `teacher` 모델을 작고 배포 가능한 지도 `student` 모델로 압축하는 방법을 제안합니다. `student`는 `teacher`의 이상 점수를 직접 학습하며, 공동 학습 및 아웃라이어 노출과 같은 추가 기법을 통해 성능을 향상시킵니다. 실험 결과, 압축된 `student` 모델은 크기와 메모리 사용량을 크게 줄이면서도 `teacher`와 유사한 이상 탐지 성능(ROC-AUC)을 달성했으며, 특히 공동 학습과 아웃라이어 노출의 조합이 가장 좋은 성능을 보였습니다. 노이즈 주입은 오히려 민감도를 저해할 수 있음을 확인했습니다.