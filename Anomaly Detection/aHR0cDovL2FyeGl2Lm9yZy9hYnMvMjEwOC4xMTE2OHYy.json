{
  "title": "Adversarially Robust One-class Novelty Detection",
  "authors": "Shao-Yuan Lo, Poojan Oza, Vishal M. Patel",
  "year": 2021,
  "url": "http://arxiv.org/abs/2108.11168v2",
  "abstract": "One-class novelty detectors are trained with examples of a particular class\nand are tasked with identifying whether a query example belongs to the same\nknown class. Most recent advances adopt a deep auto-encoder style architecture\nto compute novelty scores for detecting novel class data. Deep networks have\nshown to be vulnerable to adversarial attacks, yet little focus is devoted to\nstudying the adversarial robustness of deep novelty detectors. In this paper,\nwe first show that existing novelty detectors are susceptible to adversarial\nexamples. We further demonstrate that commonly-used defense approaches for\nclassification tasks have limited effectiveness in one-class novelty detection.\nHence, we need a defense specifically designed for novelty detection. To this\nend, we propose a defense strategy that manipulates the latent space of novelty\ndetectors to improve the robustness against adversarial examples. The proposed\nmethod, referred to as Principal Latent Space (PrincipaLS), learns the\nincrementally-trained cascade principal components in the latent space to\nrobustify novelty detectors. PrincipaLS can purify latent space against\nadversarial examples and constrain latent space to exclusively model the known\nclass distribution. We conduct extensive experiments on eight attacks, five\ndatasets and seven novelty detectors, showing that PrincipaLS consistently\nenhances the adversarial robustness of novelty detection models. Code is\navailable at https://github.com/shaoyuanlo/PrincipaLS",
  "citation": 50
}