{
  "url": "http://arxiv.org/abs/1904.02639v2",
  "title": "Memorizing Normality to Detect Anomaly: Memory-augmented Deep\n  Autoencoder for Unsupervised Anomaly Detection",
  "authors": "Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, Anton van den Hengel",
  "year": 2019,
  "abstract": "Deep autoencoder has been extensively used for anomaly detection. Training on\nthe normal data, the autoencoder is expected to produce higher reconstruction\nerror for the abnormal inputs than the normal ones, which is adopted as a\ncriterion for identifying anomalies. However, this assumption does not always\nhold in practice. It has been observed that sometimes the autoencoder\n\"generalizes\" so well that it can also reconstruct anomalies well, leading to\nthe miss detection of anomalies. To mitigate this drawback for autoencoder\nbased anomaly detector, we propose to augment the autoencoder with a memory\nmodule and develop an improved autoencoder called memory-augmented autoencoder,\ni.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder\nand then uses it as a query to retrieve the most relevant memory items for\nreconstruction. At the training stage, the memory contents are updated and are\nencouraged to represent the prototypical elements of the normal data. At the\ntest stage, the learned memory will be fixed, and the reconstruction is\nobtained from a few selected memory records of the normal data. The\nreconstruction will thus tend to be close to a normal sample. Thus the\nreconstructed errors on anomalies will be strengthened for anomaly detection.\nMemAE is free of assumptions on the data type and thus general to be applied to\ndifferent tasks. Experiments on various datasets prove the excellent\ngeneralization and high effectiveness of the proposed MemAE.",
  "citation": 2086
}