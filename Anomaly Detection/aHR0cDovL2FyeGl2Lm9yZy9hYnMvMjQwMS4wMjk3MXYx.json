{
  "title": "Deep Anomaly Detection in Text",
  "authors": "Andrei Manolache",
  "year": 2023,
  "url": "http://arxiv.org/abs/2401.02971v1",
  "abstract": "Deep anomaly detection methods have become increasingly popular in recent\nyears, with methods like Stacked Autoencoders, Variational Autoencoders, and\nGenerative Adversarial Networks greatly improving the state-of-the-art. Other\nmethods rely on augmenting classical models (such as the One-Class Support\nVector Machine), by learning an appropriate kernel function using Neural\nNetworks. Recent developments in representation learning by self-supervision\nare proving to be very beneficial in the context of anomaly detection. Inspired\nby the advancements in anomaly detection using self-supervised learning in the\nfield of computer vision, this thesis aims to develop a method for detecting\nanomalies by exploiting pretext tasks tailored for text corpora. This approach\ngreatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG\nNews, for both semi-supervised and unsupervised anomaly detection, thus proving\nthe potential for self-supervised anomaly detectors in the field of natural\nlanguage processing.",
  "citation": 1
}