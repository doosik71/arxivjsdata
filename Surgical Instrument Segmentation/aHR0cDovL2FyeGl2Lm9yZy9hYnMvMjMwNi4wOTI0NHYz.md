# Text Promptable Surgical Instrument Segmentation with Vision-Language Models

Zijian Zhou, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Miaojing Shi

## 🧩 Problem to Solve

* **수술 도구의 다양성과 변화**: 최소 침습 수술(MIS)에서 수술 도구의 종류가 빠르게 증가하고 있지만, 기존 분할 모델은 새로운 도구 유형에 적응하기 어려워 새로운 도구 도입 시 매번 재레이블링 및 재훈련이 필요합니다.
* **유사한 도구 간 구별의 어려움**: 외관이 유사한 다양한 범주의 수술 도구를 구별하는 데 어려움을 겪으며, 이는 미묘한 시각적 차이와 수술 환경의 제약으로 인해 분할 성능을 저하시킵니다.
* **데이터셋 부족**: 수술 도구 분할을 위한 포괄적이고 대규모의 주석(annotation) 데이터셋이 부족합니다.

## ✨ Key Contributions

* **텍스트 프롬프트 가능한 수술 도구 분할 작업 재정의**: 수술 도구 분할 작업을 텍스트 프롬프트 가능(text promptable)한 방식으로 재정의하여 모델이 새로운 도구 유형에 더 잘 적응하고 이해할 수 있도록 했습니다. 이는 이 분야에서 최초로 시도되는 프롬프트 기반 접근 방식입니다.
* **텍스트 프롬프트 가능한 마스크 디코더 설계**: 사전 훈련된 비전-언어 모델(CLIP)의 이미지 및 텍스트 인코더를 백본으로 활용하고, 어텐션 기반 및 컨볼루션 기반 프롬프팅 스키마를 포함하는 마스크 디코더를 설계하여 수술 도구를 정밀하게 분할합니다.
* **"프롬프트 혼합(Mixture of Prompts)" 메커니즘 도입**: 각 수술 도구에 대해 여러 텍스트 프롬프트를 동시에 활용하는 새로운 메커니즘을 제안하여 분할 성능을 향상시켰습니다. 이는 시각-텍스트 게이팅 네트워크를 통해 다양한 프롬프트의 예측을 융합합니다.
* **"어려운 도구 영역 강화(Hard Instrument Area Reinforcement)" 모듈 도입**: 모델이 분할 오류가 발생하기 쉬운 "어려운 영역"에 대한 이미지 특징 이해도를 높이고 분할 정밀도를 개선하기 위한 보조 모듈을 제시했습니다.
* **최첨단 성능 및 일반화 능력 입증**: 여러 수술 도구 분할 데이터셋에 대한 광범위한 실험을 통해 제안된 모델이 최첨단 성능을 달성하며, 교차 데이터셋 실험에서 유망한 일반화 능력을 보임을 입증했습니다.

## 📎 Related Works

* **수술 도구 분할**: TernausNet, ISINet, MF-TapNet, MATIS 등 대부분의 기존 연구는 특정 도구 카테고리에 대한 주석이 달린 훈련 데이터에 의존하는 전통적인 비전 기반 모델입니다. 이는 새로운 도구 유형에 대한 적응력이 부족하고 재훈련 비용이 높다는 한계가 있습니다.
* **비전-언어 모델 (Vision-Language Models, VLMs)**: CLIP과 같은 대규모 사전 훈련 모델은 풍부한 데이터로 훈련되어 이미지-텍스트 특징을 정렬하고 제로샷 분류, 비디오 캡셔닝 등 다양한 다운스트림 비전 작업에서 강력한 성능을 보여줍니다. 이러한 모델은 수술 도구 데이터 부족을 보완하고 텍스트를 통해 도구 구별을 돕는 잠재력을 가지고 있습니다.
* **텍스트 프롬프트 가능 분할 (Text Promptable Segmentation)**: 자연어 표현을 프롬프트로 사용하여 이미지 분할을 수행하는 작업(referring expression segmentation)으로, 초기에는 CNN-RNN 조합을 사용했으며, 이후 어텐션 메커니즘과 트랜스포머 기반 접근 방식(CRIS, CLIPSeg)이 도입되었습니다. SAM과 같은 최근 모델도 텍스트 프롬프트 기능을 포함하지만, 수술 도구 분할과 같은 고도로 특수한 작업에 최적화되어 있지 않습니다.

## 🛠️ Methodology

본 논문은 사전 훈련된 비전-언어 모델(CLIP)을 기반으로 수술 도구 분할을 위한 텍스트 프롬프트 가능한 접근 방식을 제안합니다. 전체 모델은 이미지 및 텍스트 인코더, 텍스트 프롬프트 가능 마스크 디코더, 프롬프트 혼합 모듈, 어려운 도구 영역 강화 모듈로 구성됩니다.

* **1. 이미지 및 텍스트 인코더**:
  * **CLIP 백본**: 입력 내시경 이미지 $I$와 수술 도구 설명 $T$로부터 시각 특징 $F_I$와 텍스트 특징 $F_T$를 얻기 위해 CLIP [38]의 사전 훈련된 이미지 및 텍스트 인코더를 활용합니다.
  * **이미지 인코더**: CLIP의 Vision Transformer (ViT) 기반 이미지 인코더를 미세 조정하여 특징을 추출합니다. 이미지 내 다양한 스케일의 객체를 처리하기 위해 ViT의 4, 8, 12번째 레이어 출력을 FPN(Feature Pyramid Network) [27]으로 융합하는 **다중 스케일 특징 증강(Multi-scale Feature Augmentation, MSFA)**을 적용하여 $F_I \in \mathbb{R}^{N \times D}$를 얻습니다.
  * **텍스트 인코더**: CLIP의 Transformer 기반 텍스트 인코더는 고정(frozen)된 상태로 사용됩니다. 텍스트 설명의 전역 정보를 나타내는 `[CLS]` 토큰의 특징을 전역 텍스트 특징 $F_T \in \mathbb{R}^{1 \times D}$로 활용합니다.

* **2. 텍스트 프롬프트 가능 마스크 디코더**:
  * $F_I$와 $F_T$를 활용하여 $F_I$로부터 점수 맵(score map) $S \in \mathbb{R}^{H \times W}$를 디코딩합니다. 각 픽셀의 값은 해당 픽셀이 $F_T$에 의해 설명된 도구 클래스에 속할 확률을 나타냅니다.
  * **어텐션 기반 프롬프팅**: Self-Attention (SA)을 통해 이미지 내 전경 영역을 강화하고, Cross-Attention (CA)을 통해 $F_T$에 따라 $F_I$ 내의 도구 영역을 지역화합니다. Feed-Forward Network (FFN)으로 특징을 정제하여 어텐션 기반 특징 $F_I^A$를 생성합니다.
  * **컨볼루션 기반 프롬프팅**: $F_T$를 컨볼루션 커널 가중치 $w$와 바이어스 $b$로 변환하여 $\tilde{F}_I^A$를 국부적으로 컨볼루션합니다. 이는 어텐션 기반 디코딩의 전역적 특징을 국부적으로 정제하는 역할을 합니다.
    * $$S = \text{Sigmoid}(\text{Conv}(\tilde{F}_I^A | w, b))$$

* **3. 프롬프트 혼합 (Mixture of Prompts, MoP)**:
  * 다양한 텍스트 프롬프트의 강점을 효과적으로 활용하여 분할 정확도를 높입니다.
  * **프롬프트 획득**: 세 가지 유형의 프롬프트를 사용합니다: 1) 도구 클래스 이름, 2) CLIP의 템플릿 기반 프롬프트, 3) GPT-4 [36]로 생성된 자세한 설명.
  * **프롬프트 예측 융합**: 각 프롬프트 $T_i$에 대해 예측된 점수 맵 $S_i$들을 결합하기 위해 시각-텍스트 게이팅 네트워크 $G$를 사용합니다. $G$는 $F_I$와 $F_T$를 입력으로 받아 각 $S_i$에 대한 픽셀 단위 가중치 맵을 생성하고, 소프트맥스 정규화를 거쳐 최종 점수 맵 $S_{et}$를 얻습니다.

* **4. 어려운 도구 영역 강화 (Hard Instrument Area Reinforcement, HIAR)**:
  * 모델이 수술 환경의 복잡성으로 인해 도구의 정확한 범주 및 경계를 구별하는 데 어려움을 겪는 영역의 성능을 강화합니다.
  * **어려운 영역 마이닝(Hard Area Mining)**: 예측 마스크 $M_{et}$와 실제 마스크 $M_{gt}$를 비교하여 오예측 영역을 "어려운 영역"으로 식별합니다. 이 영역의 마스킹 비율을 조절하여 마스킹된 이미지(masked image)를 생성합니다.
  * **영역 강화**: 마스킹된 이미지를 이미지 인코더에 입력하고, MAE(Masked Autoencoder) [16]와 유사한 디코더 구조를 통해 전체 이미지를 재구성합니다. 이는 모델이 어려운 영역의 미묘한 세부 사항에 집중하도록 돕습니다 (테스트 시에는 사용되지 않음).

* **5. 모델 훈련**:
  * 텍스트 인코더는 고정하고 이미지 인코더만 미세 조정합니다.
  * 두 가지 손실 함수를 사용합니다: 이미지 분할을 위한 이진 교차 엔트로피 손실 $L_{seg}$와 어려운 도구 영역 강화를 위한 L2 손실 $L_{rec}$를 사용합니다.
  * 총 손실은 $L = L_{seg} + \lambda L_{rec}$ ($L_{seg}$와 $L_{rec}$는 각각 이진 교차 엔트로피 손실과 L2 손실)입니다.

## 📊 Results

* **최첨단 성능 달성**: EndoVis2017 및 EndoVis2018 데이터셋에서 기존의 감독 학습 기반 방법과 다른 텍스트 프롬프트 가능 비전-언어 모델 기반 방법(CRIS, CLIPSeg)을 모두 크게 능가하는 최첨단 성능을 달성했습니다. 예를 들어, EndoVis2017에서 Ch_IoU, ISI_IoU, mc_IoU에서 상당한 개선을 보였습니다.
* **SAM과의 비교**: `lang-segment-anything` 솔루션을 사용한 SAM 변형은 의료 도메인 프롬프트에 대한 이해 부족으로 본 논문의 모델보다 현저히 낮은 성능을 보였습니다.
* **교차 데이터셋 일반화**: EndoVis2018에서 훈련하고 EndoVis2017에서 테스트하는 등 교차 데이터셋 설정에서도 경쟁력 있는 결과를 얻어, 모델이 보지 못한 도구 카테고리나 환경에 대해서도 뛰어난 일반화 능력을 가짐을 입증했습니다.
* **어블레이션 연구**:
  * **다중 스케일 특징 증강 (MSFA)**: 제거 시 성능이 명확하게 하락하여 문맥 정보 강화의 중요성을 확인했습니다.
  * **`[CLS]` 토큰**: 텍스트 인코더에서 `[CLS]` 토큰 특징을 사용하는 것이 개별 단어 토큰의 평균 특징보다 더 나은 성능을 보였습니다.
  * **마스크 디코더 스키마**: 어텐션 기반과 컨볼루션 기반 프롬프팅 스키마를 순차적으로 사용하는 것이 각각 단독으로 사용하는 것보다 더 좋은 성능을 보여, 컨볼루션 기반의 국부적 정제 효과를 입증했습니다.
  * **프롬프트 혼합 (MoP)**: 단일 프롬프트보다 여러 프롬프트를 혼합하는 것이 성능 향상에 기여했으며, 특히 GPT-4로 생성된 프롬프트가 가장 효과적이었습니다. 픽셀 단위 가중치 맵 방식이 이미지 단위 방식보다 우수했습니다.
  * **어려운 도구 영역 강화 (HIAR)**: 이 모듈을 제거하거나 "어려운 영역 마이닝(Hard Area Mining)"을 제외하면 성능이 크게 하락하여, 어려운 영역에 집중하는 것이 중요함을 강조했습니다. 최적의 마스킹 비율 임계값 $r_t$는 0.25였습니다.
* **계산 효율성**: 본 모델은 다른 텍스트 프롬프트 가능 접근 방식(CRIS, CLIPSeg)과 유사한 FLOPs 및 FPS를 보여 실시간 임상 적용에 적합함을 입증했습니다.

## 🧠 Insights & Discussion

* **새로운 패러다임의 유효성**: 본 연구는 수술 도구 분할에 텍스트 프롬프트 기반의 접근 방식을 성공적으로 도입하여, 기존의 정적인 카테고리 기반 모델의 한계(예: 새로운 도구 유형에 대한 낮은 적응성)를 극복할 수 있는 새로운 길을 열었습니다.
* **비전-언어 모델의 강력한 활용**: CLIP과 같은 사전 훈련된 비전-언어 모델이 일반적인 이미지와 텍스트 데이터에서 학습한 풍부한 지식을 활용하여, 의료 영상처럼 데이터가 부족하고 도메인 특이성이 강한 환경에서도 강력한 성능을 발휘할 수 있음을 보여줍니다.
* **정교한 디코딩 및 정보 융합**: 어텐션 기반의 전역적 특징 추출과 컨볼루션 기반의 국부적 정제를 결합한 마스크 디코더, 그리고 다양한 텍스트 프롬프트를 지능적으로 융합하는 "프롬프트 혼합" 메커니즘은 복잡한 수술 도구의 미묘한 시각적 특징을 효과적으로 포착하고 활용하는 데 기여합니다.
* **어려운 영역에 대한 집중 학습**: "어려운 도구 영역 강화" 모듈은 모델이 분류하기 어려운 도구의 특정 부분(예: 클래스퍼, 샤프트)에 학습 초점을 맞춤으로써, 전반적인 분할 정확도와 정밀도를 향상시키는 효과적인 전략임을 입증합니다.
* **높은 일반화 및 오픈-셋 잠재력**: 교차 데이터셋 실험에서 보여준 견고한 성능은 본 모델이 새로운 수술 환경이나 이전에 학습하지 않은 도구 카테고리에도 잘 일반화될 수 있음을 시사하며, 이는 로봇 보조 수술에서의 "오픈-셋(open-set)" 분할 문제 해결에 큰 잠재력을 가집니다.
* **한계 및 향후 연구**: 범용 모델인 SAM이 의료 프롬프트에 어려움을 겪는 것은 의료 도메인 특화된 미세 조정의 필요성을 강조합니다. 또한, 모델이 어려운 영역으로 식별한 클래스퍼와 샤프트 간의 관계를 명시적으로 모델링하는 것이 향후 분할 성능 향상에 중요할 것으로 논의됩니다.
* **사회적 영향**: 본 연구의 발전은 수술 안전성 및 정밀도 향상, 외과 의사의 인지 부하 감소, 신규 의사 훈련 지원, 그리고 더 스마트한 AI 기반 수술 도구 개발로 이어져 궁극적으로 환자 치료의 질을 높이는 데 기여할 수 있습니다.

## 📌 TL;DR

본 연구는 수술 도구의 다양성 및 구별 난이도 문제를 해결하기 위해 **텍스트 프롬프트 가능한 수술 도구 분할**이라는 새로운 접근 방식을 제안합니다. 사전 훈련된 비전-언어 모델(CLIP)을 기반으로 **다중 스케일 특징 증강**을 포함한 인코더와 **어텐션 및 컨볼루션 기반 프롬프트 스키마로 구성된 마스크 디코더**를 개발했습니다. 또한, **"프롬프트 혼합" 메커니즘**으로 여러 프롬프트를 융합하고, **"어려운 도구 영역 강화" 모듈**로 모델이 오예측하기 쉬운 영역에 집중하도록 학습시킵니다. 실험 결과, 이 모델은 여러 수술 데이터셋에서 최첨단 성능을 능가하고 뛰어난 일반화 능력을 보였으며, 로봇 보조 수술 분야에 상당한 실용적 잠재력을 제공합니다.
