# Image Compositing for Segmentation of Surgical Tools without Manual Annotations

Luis C. Garcia-Peraza-Herrera, Lucas Fidon, Claudia D’Ettorre, Danail Stoyanov, Tom Vercauteren, and Sébastien Ourselin

## 🧩 Problem to Solve

수술 장면에서 픽셀 단위로 정확한 수술 도구 이미지 분할(Segmentation) 레이블을 수동으로 생성하는 것은 매우 지루하고 시간이 많이 소요됩니다. 이는 딥 러닝 모델 훈련에 필요한 대규모 레이블링된 데이터셋 확보에 병목 현상을 일으킵니다. 컴퓨터 비전 커뮤니티의 산업 표준(예: COCO)에 비견할 만한 대규모 수술 도구 분할 데이터셋이 현재로서는 존재하지 않아, 모델의 일반화 성능이 제한됩니다.

## ✨ Key Contributions

* 수술 도구 분할을 위한 현실적인 훈련 데이터셋을 자동으로 생성하는 새로운 기술 및 이론적 프레임워크를 제안했습니다.
* 크로마 키(chroma key)를 사용하여 촬영된 수술 도구(전경)와 도구가 없는 수술 장면(배경)의 두 가지 데이터 소스를 활용하여 반합성(semi-synthetic) 이미지를 생성합니다.
* 다양한 블렌딩 함수(Trivial, Gaussian feathering, Laplacian pyramid)를 확률적으로 조합하는 새로운 `mix-blend` 방식을 제안했습니다. 이는 훈련 중 실시간으로 다양한 반합성 이미지를 생성하여 블렌딩 메커니즘에 대한 불변성(invariance)을 학습하게 합니다.
* `mix-blend` 방식은 확률적 경사 하강법(SGD) 최적화에 적합한 몬테카를로(Monte Carlo) 방식을 통해 훈련 샘플을 실시간으로 생성합니다.
* 일반적인 U-Net 모델을 오직 반합성 데이터로만 훈련하고 간단한 GrabCut 후처리(post-processing)를 적용했을 때, 수동으로 레이블링된 실제 데이터셋으로 훈련한 네트워크와 동등한 분할 성능을 달성함을 입증했습니다.
* 새롭게 생성된 모든 데이터셋(100K 반합성 도구 분할 데이터셋, 14K 크로마 키 전경 데이터셋, 6K 배경 조직 데이터셋, 514개 수동 레이블링된 실제 임상 테스트셋인 RoboTool)을 공개했습니다.

## 📎 Related Works

* **기존 수술 도구 인식:** 위치 센서, 도구 내장 센서 등 초기 방식은 정확도 제한, 작업 흐름 변경, 멸균 문제 등의 단점이 있었습니다 [8]–[11].
* **딥 러닝 기반 접근법:** 최근 컨볼루션 신경망(CNN) 기반의 데이터 중심 접근 방식이 수술 도구 분할에서 뛰어난 성능을 보였으나 [3]–[5], [13], 대규모의 정교하게 주석이 달린 데이터셋 부족이 주요한 과제였습니다.
* **데이터 주석 노력 완화:**
  * **크라우드소싱:** 약한 레이블을 수정하는 데 크라우드소싱 활용 [20].
  * **약한 감독 학습:** 프레임별 도구 존재 분류 레이블을 분할을 위한 약한 감독으로 사용 [6].
  * **비레이블 데이터 활용:** 비레이블 내시경 비디오 데이터의 잠재력을 자기 지도 학습(self-supervised learning)으로 활용 [22] (재색상화), 수술 비디오의 시간적 일관성 활용 [24].
* **데이터셋 합성:**
  * **생성 모델:** 내시경 영상에서 데이터 부족 문제를 해결하기 위한 생성 모델 연구 [12], [25].
  * **가상 환경/합성 렌더링:** 3D 모델을 렌더링하여 데이터셋 생성 [27], 하지만 현실과의 도메인 차이를 줄이기 위한 고급 도메인 적응 기술이 필요 [28]–[30].
  * **이미지 합성(Image Compositing):** 서로 다른 소스의 시각적 요소를 결합하여 훈련 이미지 생성 [15]–[18], [31]. 합성 객체를 실제 이미지에 삽입하는 방법 [31].
  * **도메인 무작위화(Domain Randomization):** 합성 데이터를 확률적으로 변경하여 네트워크가 핵심 기능에 집중하도록 유도, 시뮬레이션과 실제 간의 격차 해소 [33]–[35].
  * **블렌딩 아티팩트 문제:** 합성 이미지에서 블렌딩 과정으로 인한 인위적인 특징(artifact)이 발생할 수 있으며 [15], 네트워크가 이를 활용하지 않도록 Dwibedi et al. [15]은 여러 블렌딩 방법을 제안했고, Tripathi et al. [18]은 "flying distractors" (배경 아티팩트)를 도입했습니다.

## 🛠️ Methodology

본 논문은 수술 도구 분할을 위한 반합성 데이터 생성 및 훈련 방식을 제안하며, 핵심 단계는 다음과 같습니다.

1. **반합성 학습 프레임워크 ($\hat{\theta} = \operatorname{argmin}_{\theta} E_{\hat{P}_{X,Y}}[\mathcal{L}(f_{\theta}(X), Y)]$):**
    * 수술 장면을 전경 $X_F$ (수술 도구)와 배경 $X_B$ (조직) 두 가지 독립적인 정보 요소로 모델링합니다.
    * 이상적인 블렌딩 함수 $\phi^*$가 알려져 있지 않으므로, 블렌딩 과정을 확률 변수 $\Phi$로 모델링하여 $X = \Phi(X_F, X_B)$로 정의합니다.
    * 블렌딩된 이미지의 레이블 $Y$는 전경 레이블 $Y_F$와 동일하다고 가정합니다.

2. **데이터 수집:**
    * **배경 데이터셋 ($x_b$):** 온라인에서 자유롭게 이용 가능한 50개 수술 절차에서 도구가 없는 6,130개의 프레임을 수동으로 선별했습니다. 이 배경에는 도구-조직 상호작용으로 인한 그림자, 혈액, 연기 등 간접적인 아티팩트가 포함될 수 있습니다.
    * **전경 데이터셋 ($x_f$):** 크로마 키(녹색 스크린) 위에 수술 도구를 놓고 다양한 조명 조건과 시점에서 촬영했습니다. 모바일 폰으로 13,613프레임, DSLR 카메라로 567프레임을 수집했습니다.
    * **전경 자동 분할:** HSV 색 공간에서 임계값 처리와 GrabCut [36]을 사용하여 크로마 키 배경에서 도구 마스크를 자동으로 추출했습니다.

3. **데이터 증강(Data Augmentation) 및 표준화:**
    * **전경 도구:** 무작위 확대/축소, 회전, 수직/수평 뒤집기 및 이동을 적용하고, 합성 혈액 방울 및 조직 잔해를 추가하며 밝기를 조절합니다.
    * **배경 이미지:** 수평/수직 뒤집기, 밝기 변경, 90도 무작위 회전을 적용합니다.
    * **블렌딩된 이미지:** Albumentations [38]의 다양한 기법(Cutout, 합성 연기 및 그림자, JPEG 압축, RGB/HSV 시프트, 노이즈, 블러)과 'flying distractors' (도구 모양으로 잘라낸 다른 배경), '내시경 패딩' (검은색 테두리 시뮬레이션)을 추가합니다.
    * 모든 이미지 쌍 $(x_f, x_b)$는 640픽셀 너비로 크기가 조정되고 높이를 일치시키기 위해 무작위로 잘립니다.

4. **블렌딩 방법론 ($\{\phi_m\}_{m=1}^{M}$):** 세 가지 기본 블렌딩 알고리즘을 사용합니다 ($M=3$).
    * **Trivial 블렌딩:** 도구 마스크에서 활성화된 픽셀을 전경 $x_f$에서 배경 $x_b$로 단순히 복사합니다.
    * **Gaussian Feathering:** 전경 분할 마스크를 침식(erode)하고 블러 처리한 후, 마스크를 이용하여 전경과 배경을 합성합니다.
    * **Laplacian Pyramid 블렌딩 [39]:** 두 이미지와 마스크에 대해 라플라시안 피라미드를 구성하고 가우시안 피라미드 노드를 가중치로 사용하여 결합합니다.
    * **`multi-blend` [15] 학습:** 각 $(x_f, x_b)$ 쌍에 대해 $M$개의 블렌딩 함수를 각각 적용하여 $M$개의 훈련 이미지를 생성합니다.
    * **`mix-blend` (제안) 학습:** $X = \sum_{m} \lambda_m \phi_m(X_F, X_B)$ 형태로 이미지를 생성합니다. 여기서 $\lambda_m$은 $Dir(\alpha)$ 분포에서 샘플링된 가중치 벡터 $\lambda = (\lambda_1, ..., \lambda_M)$의 $m$번째 요소이며, $\alpha = (1.0, 1.0, 1.0)$로 설정됩니다. 이는 각 미니배치에서 실시간으로 무한한 블렌딩 변형을 생성합니다.

5. **네트워크 아키텍처 및 훈련 프로토콜:**
    * U-Net [40] 아키텍처를 사용합니다.
    * 학습률 0.001, 배치 크기 32, 운동량 0.9의 SGD 최적화.
    * 조기 종료(Early Stopping)는 검증 세트의 평균 IoU를 기준으로 20 에포크 동안 개선이 없을 때 중단합니다.
    * 손실 함수로 픽셀 단위 교차 엔트로피($\mathcal{L}(\hat{y},y) = -\sum_{i=1}^{N_p} \sum_{k=1}^{K} y_{i,k} \log \hat{y}_{i,k}$)를 사용합니다.

6. **후처리(Post-processing):**
    * GrabCut [36]을 사용하여 네트워크 예측의 도메인 간 격차를 완화합니다.
    * 네트워크의 확률 맵(0.2 미만은 확실한 배경, 0.8 이상은 확실한 전경)을 GrabCut의 초기화 '스크리블'로 사용합니다. 이 임계값 내의 픽셀은 GrabCut에 의해 수정되지 않습니다.

## 📊 Results

* **실제 데이터 기반 모델의 일반화 문제:**
  * EndoVis2017 데이터셋(챌린지 훈련 세트)으로 훈련 후 EndoVis2017 자체 테스트 시 평균 mIoU 81.6% $[69.7, 89.6]$.
  * EndoVis2017으로 훈련 후 RoboTool(새로운 실제 임상 데이터셋) 테스트 시 mIoU 66.6% $[43.0, 87.2]$ (GrabCut 후처리 시 69.4% $[37.7, 91.8]$).
  * RoboTool로 훈련 후 EndoVis2017 테스트 시 mIoU 73.8% $[56.3, 86.6]$ (GrabCut 후처리 시 80.5% $[55.8, 94.1]$).
  * 이는 소규모 실제 데이터셋으로 훈련된 네트워크가 새로운 실제 데이터셋에 잘 일반화되지 않음을 시사합니다.
* **블렌딩 방법론별 성능 비교 (반합성 훈련, 실제 데이터 테스트):**
  * Trivial 블렌딩: EndoVis2017 53.7%, RoboTool 48.4%.
  * Gaussian 블렌딩: EndoVis2017 55.2%, RoboTool 48.7%.
  * Laplacian 블렌딩: EndoVis2017 68.3%, RoboTool 54.3% (Trivial, Gaussian보다 우수).
  * `multi-blend` [15]: EndoVis2017 64.3%, RoboTool 51.7% (Laplacian보다 낮음).
  * **`mix-blend` (제안): EndoVis2017 72.8%, RoboTool 56.1% (Laplacian보다 4%p, `multi-blend`보다 8%p 우수).**
* **`mix-blend`와 후처리 결합 성능:**
  * 반합성 `mix-blend` 훈련 + GrabCut 후처리: EndoVis2017 83.3% $[62.7, 93.9]$, RoboTool 68.1% $[42.6, 92.5]$.
  * RoboTool 테스트셋에 대한 `mix-blend` + GrabCut의 성능(68.1%)은 EndoVis2017으로 훈련된 네트워크의 성능(69.4%)과 거의 유사한 수준을 달성했습니다.

## 🧠 Insights & Discussion

* 수술 도구 분할 분야에서 수동 주석의 한계를 효과적으로 극복하기 위한 반합성 데이터 생성의 중요성을 보여주었습니다.
* 제안된 `mix-blend` 학습 전략은 여러 블렌딩 함수를 확률적으로 조합함으로써, 네트워크가 특정 블렌딩 방법에 과적합되는 것을 방지하고 다양한 블렌딩 메커니즘에 대한 불변성을 학습하도록 유도합니다. 이는 `multi-blend` 방식보다 넓은 블렌딩 공간을 탐색하기 때문에 더 높은 성능을 보입니다.
* 간단한 GrabCut 후처리 기법이 반합성 데이터와 실제 데이터 간의 도메인 격차를 효과적으로 줄여, 수동으로 레이블링된 실제 데이터로 훈련한 모델과 필적하는 성능을 달성할 수 있음을 입증했습니다.
* 이는 수술 도구 분할을 위한 대규모 수동 주석 데이터셋 없이도 강력하고 일반화 가능한 딥 러닝 모델을 훈련할 수 있는 실용적인 해결책을 제시합니다.
* **제한 사항:** 이미지 합성만으로는 특정 도구-조직 상호작용(예: 조직 견인, 열 소작 등)을 완벽하게 재현하기 어려울 수 있습니다. 이를 보완하기 위해 소량의 수동 주석 데이터를 추가하는 것이 고려될 수 있습니다.
* **향후 연구:** 도메인 적응(Domain Adaptation) 기술을 추가로 탐색하여 반합성 블렌딩으로 얻은 결과를 더욱 향상시킬 수 있습니다.

## 📌 TL;DR

**문제:** 수술 도구 분할은 대규모의 픽셀 단위 주석 데이터셋이 부족하여 딥러닝 모델 훈련에 어려움이 있습니다.
**방법:** 본 논문은 수동 주석 없이 수술 도구 분할을 위한 훈련 데이터를 자동으로 생성하는 반합성 접근 방식을 제안합니다. 크로마 키를 이용해 수술 도구(전경)를 촬영하고, 도구가 없는 수술 영상(배경)과 결합합니다. 이때, `mix-blend`라는 새로운 기법을 사용하여 여러 블렌딩 함수(Trivial, Gaussian, Laplacian)를 확률적으로 조합하여 실시간으로 다양한 훈련 샘플을 생성합니다. 이렇게 생성된 데이터로 U-Net을 훈련합니다.
**주요 결과:** 오직 반합성 데이터로 훈련하고 간단한 GrabCut 후처리를 적용한 U-Net이 수동으로 레이블링된 실제 데이터셋으로 훈련한 모델과 동등한 분할 정확도를 달성했습니다. 이는 수동 주석의 필요성을 크게 줄이면서 실제 데이터에 대한 모델의 일반화 성능을 향상시키는 효과적인 방법을 제시합니다.
