# Real-time Instance Segmentation of Surgical Instruments using Attention and Multi-scale Feature Fusion

Juan Carlos Ángeles-Cerón, Gilberto Ochoa-Ruiz, Leonardo Chang, Sharib Ali

## 🧩 Problem to Solve

최소 침습 수술(MIS)에서 수술 도구의 정확한 실시간 인스턴스 분할은 환자 안전과 수술 효율성 증진에 필수적입니다. 그러나 다음 두 가지 주요 요인으로 인해 달성하기 어려운 과제입니다:

1. **복잡한 수술 환경**: 출혈, 연기, 반사, 흐릿함 등 예측 불가능하고 어려운 시각적 조건.
2. **모델 설계**: 최적의 정확도와 실시간 처리 속도($\ge 30$ FPS)를 동시에 달성하는 모델 설계의 어려움. 기존의 대부분의 인스턴스 분할 모델(예: Mask R-CNN 기반의 2단계 모델)은 정확도는 높지만 추론 속도가 매우 느려 임상 적용에 한계가 있습니다.

## ✨ Key Contributions

* 어텐션 메커니즘을 통합한 실시간 단일 단계(single-stage) 인스턴스 분할 프레임워크를 제안합니다.
* 도메인 타겟 데이터 증강(data augmentation) 기법을 활용하여 모델의 강건성을 높였습니다.
* 차등 진화 검색 알고리즘을 통한 앵커 박스(anchor box) 최적화를 수행했습니다.
* 네트워크 백본에 다중 스케일 특징 융합(Multi-scale Feature Fusion, MSFF)을 통합하여 전역 컨텍스트 특징을 활용했습니다.
* 가장 성능이 낮은 샘플(worst-case samples)에 대한 철저한 분석을 통해 각 모델의 효과를 평가했습니다.

## 📎 Related Works

* **ROBUST-MIS 챌린지**: 수술 도구 분할을 위한 대규모 고품질 데이터셋을 제공하며, 모델의 강건성과 일반화 능력을 벤치마킹하는 데 중요한 역할을 합니다.
* **기존 인스턴스 분할 방법**: 대부분 Mask R-CNN과 같은 2단계 검출기 기반 모델이 사용되었으나, 높은 추론 시간($\approx 5$ FPS)으로 실시간 성능에 부적합했습니다.
* **단일 단계 인스턴스 분할**: PolyYolo, BlendMask, Solov2와 같은 초기 단일 단계 모델은 의료 영상 분야에서 성능 격차가 존재했습니다.
* **YOLACT/YOLACT++**: 실시간 인스턴스 분할을 위해 경량 마스크 조합을 사용하는 단일 단계 아키텍처로, Mask R-CNN과의 정확도 격차를 줄였습니다. 특히 YOLACT++는 변형 가능한 컨볼루션(deformable convolutions)을 도입하여 특징 샘플링을 개선했습니다.
* **어텐션 메커니즘**: Criss-cross Attention Modules (CCAM) 및 Convolutional Block Attention Modules (CBAM)과 같이 계산 효율적인 어텐션 모듈이 다양한 컴퓨터 비전 작업에서 모델 성능을 향상시키는 데 활용되었습니다.
* **다중 스케일 특징 융합**: 인스턴스 분할에서 객체의 다양한 스케일 변화에 대응하기 위해 인코더-디코더 아키텍처에서 고수준 및 저수준 특징을 결합하는 방식이 사용되었습니다.

## 🛠️ Methodology

본 연구는 YOLACT++를 기반으로 한 단일 단계 인스턴스 분할 프레임워크를 제안하며, 다음과 같은 개선 사항을 포함합니다:

1. **백본 네트워크**: ResNet-101을 사용합니다.
2. **다중 스케일 특징 융합 (MSFF)**: 백본 네트워크의 다양한 스케일 특징 맵($F_{s}$)을 전치 컨볼루션으로 가장 높은 해상도에 맞춰 업샘플링한 후, 하나의 텐서로 연결하고 컨볼루션 계층을 통과시켜 모든 스케일의 컨텍스트 정보를 통합합니다 ($F_{MS} = conv([F_{0}, F_{1}, F_{2}, F_{3}, F_{4}])$). 이후 $F_{MS}$를 각 $F_{s}$와 다시 연결하고 컨볼루션하여 다중 스케일 융합 특징 맵($F_{A}$)을 생성합니다.
3. **어텐션 메커니즘**:
    * **Criss-cross Attention Modules (CCAM)** 또는 **Convolutional Block Attention Modules (CBAM)**을 사용합니다.
    * 어텐션 모듈은 백본의 출력과 특징 피라미드 네트워크(FPN)의 출력에 적용되어 특징 표현을 정교화하고 중요한 특징에 집중하도록 합니다.
4. **손실 함수**:
    * 분류 손실 ($L_{cls}$, 소프트맥스 교차 엔트로피).
    * 바운딩 박스 회귀 손실 ($L_{box}$, Smooth-$L_{1}$ 손실).
    * 마스크 손실 ($L_{mask}$, 픽셀 단위 이진 교차 엔트로피).
    * 의미론적 분할 손실 ($L_{seg}$, 추가적인 특징 풍부함을 위해 훈련 중에만 적용).
    * 각 손실은 가중치(1, 1.5, 6.125, 1)로 조합됩니다.
5. **앵커 박스 최적화**: 600x600 픽셀 이미지에 대해 5개의 스케일과 종횡비를 (differential evolution search algorithm을 사용하여) 최적화합니다.
6. **데이터 증강**: 무작위 색상/대비/채도/색조/밝기 변화, 노이즈 변환, 무작위 스케일링, 무작위 샘플 크롭, 무작위 미러링 등 강한 데이터 증강 기법을 적용합니다.
7. **추론**: 최종적으로 NMS(Non-Maximum Suppression)를 수행하여 마스크 인스턴스를 예측하고, 프로토타입 마스크와 결합하여 최종 결과를 얻습니다.

## 📊 Results

* **ROBUST-MIS 챌린지 성능 능가**: 제안된 방법은 ROBUST-MIS 챌린지 상위 팀의 성능을 크게 뛰어넘었습니다.
  * 최종 모델인 **CBAM-Full + Aug + Anch + MS**는 MI_DSC (area-based multi-instance dice)에서 **44.7%**, MI_NSD (distance-based multi-instance normalized surface dice)에서 **48.9%**를 달성하여 챌린지 최고 성능 팀($0.31$ MI_DSC, $0.35$ MI_NSD) 대비 각각 **13.7%p**, **13.9%p** 높은 점수를 기록했습니다.
* **실시간 성능**:
  * **CBAM-Full + Aug + Anch** 모델은 $69$ FPS로 MI_DSC $0.425$, MI_NSD $0.471$을 달성하며 뛰어난 정확도와 함께 실시간 성능을 제공했습니다. 이는 기존 최고 성능 대비 **$13.8$배 더 빠른 속도**입니다.
  * 최종 모델인 **CBAM-Full + Aug + Anch + MS**는 $24$ FPS로 실행되며, 기존 SOTA보다 **$4.8$배 더 빠릅니다**.
* **점진적 개선**:
  * 어텐션 모듈 추가 (CBAM-Full): MI_DSC $0.338$, MI_NSD $0.383$, $65$ FPS (베이스라인 대비 $2.8\%$ MI_DSC 개선).
  * 데이터 증강 추가 (CBAM-Full + Aug): MI_DSC $0.382$, MI_NSD $0.429$, $63$ FPS (CBAM-Full 대비 $4.4\%$ MI_DSC 개선).
  * 앵커 최적화 추가 (CBAM-Full + Aug + Anch): MI_DSC $0.425$, MI_NSD $0.471$, $69$ FPS (CBAM-Full + Aug 대비 $4.3\%$ MI_DSC 개선).
  * MSFF 추가 (CBAM-Full + Aug + Anch + MS): MI_DSC $0.447$, MI_NSD $0.489$, $24$ FPS (CBAM-Full + Aug + Anch 대비 $2.2\%$ MI_DSC 개선).
* **정성적 분석**:
  * 베이스라인 모델은 연기, 혈흔, 움직임 흐림 등으로 인한 오탐지 및 누락이 많았습니다.
  * CBAM-Full 모델은 이러한 문제점을 일부 개선했습니다.
  * 데이터 증강은 시야 가장자리의 작은 기구, 투명하거나 부분적으로 가려진 기구, 수직 기구 등 도전적인 인스턴스 감지를 개선했습니다.
  * 앵커 최적화는 이전에 감지되지 않았던 객체와 전반적인 기구 감지 및 분할을 크게 향상시켰습니다.
  * MSFF는 노출 부족, 투명한 기구, 반사 등으로 인한 더욱 어려운 인스턴스를 인식하는 데 도움을 주었습니다.

## 🧠 Insights & Discussion

* **강건성과 실시간 성능의 중요성**: 수술 도구 분할에서 동적인 환경 변화(연기, 플레어, 반사)와 실시간 성능은 임상적 유용성에 매우 중요하며, 기존의 2단계 네트워크는 이 요구를 충족하기 어렵습니다.
* **어텐션 메커니즘의 효과**: CCAM보다 CBAM 기반 모델이 더 나은 성능을 보였으며, 어텐션 메커니즘을 통합한 모델은 항상 어텐션 없는 베이스라인보다 강건성 면에서 우수했습니다. 특히 CBAM은 채널 단위 및 블록 어텐션을 모두 강화하여 로컬 및 전역 특징을 더 잘 표현하는 데 효과적입니다.
* **단계별 개선의 중요성**: 데이터 증강, 앵커 가중치 최적화, 다중 스케일 특징 융합(MSFF)의 각 단계는 MI_DSC와 MI_NSD에서 지속적인 성능 향상을 가져왔으며, 특히 특정 유형의 어려운 샘플(예: 긴 기구, 작은 기구, 투명 기구)에서 큰 이점을 보였습니다.
* **MSFF의 역할**: MSFF 네트워크는 어텐션 맵과 결합되어 고수준 및 저수준 특징 표현을 융합함으로써 로컬 및 전역 컨텍스트를 전달하고, 이를 통해 약 $2\%$의 추가적인 MI_DSC 성능 향상을 달성했습니다.
* **속도와 정확도의 균형**: 최종적으로 가장 강력한 모델(CBAM-Full + Aug + Anch + MS)은 $24$ FPS를 달성했지만, 그 다음으로 성능이 좋은 모델(CBAM-Full + Aug + Anch)은 $69$ FPS로 대부분의 경우 요구되는 실시간 성능($45$ FPS)을 뛰어넘는 속도를 보였습니다. 이는 임상 환경에서 실용적인 선택지를 제공합니다.
* **CCAM과 CBAM의 차이**: CCAM은 혼합되지 않은 특징을 정교화하는 데 뛰어나지만, CBAM은 이전에 집계된 데이터를 정교화하는 데 더 효과적이어서 전반적으로 우수한 성능을 보였습니다.

## 📌 TL;DR

본 논문은 복잡한 수술 환경에서 수술 도구의 정확하고 실시간 인스턴스 분할이라는 난제를 해결하기 위해 YOLACT++ 기반의 단일 단계 모델을 제안합니다. 다중 스케일 특징 융합(MSFF)과 CBAM(Convolutional Block Attention Module) 어텐션 메커니즘을 통합하고, 데이터 증강 및 앵커 박스 최적화를 통해 모델의 강건성과 정확도를 향상시켰습니다. 제안된 방법은 ROBUST-MIS 챌린지 최고 성능을 MI_DSC에서 $13.7\%$p, MI_NSD에서 $13.9\%$p 능가하면서도 실시간 성능($24$ FPS 이상)을 달성하여, 임상 적용 가능성을 크게 높였습니다.
