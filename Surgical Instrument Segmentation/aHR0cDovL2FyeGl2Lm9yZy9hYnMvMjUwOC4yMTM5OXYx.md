# Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation

Sabrina Kletz, Klaus Schoeffmann, Jenny Benois-Pineau, Heinrich Husslein

## 🧩 Problem to Solve

복강경 수술 영상에서 수술 도구의 자동 식별 및 분할은 수술 품질 평가(SQA) 및 기타 자동화된 영상 분석을 위한 핵심 과제입니다. 특히, 전통적인 복강경 수술 영상은 로봇 수술 영상과 달리 반사, 흐릿함, 연기 등 특수한 콘텐츠 특성으로 인해 자동 콘텐츠 색인이 어렵습니다. 또한, 해당 분야의 수술 도구 인스턴스 분할을 위한 공개 데이터셋이 부족하여 딥러닝 연구에 제약이 있었습니다.

## ✨ Key Contributions

* **새로운 데이터셋 도입**: 부인과 자궁근종 절제술 및 자궁적출술에 사용되는 복강경 수술 도구 분할을 위한 맞춤형 데이터셋을 구축했습니다.
* **Mask R-CNN 기반 평가**: Mask R-CNN을 활용하여 복강경 수술 도구의 (1) 이진 인스턴스 분할 (도구/배경) 및 (2) 다중 클래스 도구 인식 성능을 평가했습니다.
* **데이터 증강 효과 분석**: 제한된 훈련 데이터셋의 샘플 크기를 보완하기 위해 다양한 데이터 증강 기법을 적용하고 그 효과를 분석했습니다.
* **성능 검증**: 적당히 적은 수의 훈련 예시에도 불구하고, 도구 영역을 높은 정확도로 분할할 수 있음을 입증했습니다.
* **과제 식별**: 수술 도구 간의 높은 시각적 유사성으로 인해 특정 도구 유형을 구분하는 것이 여전히 어렵다는 점을 밝혔습니다.

## 📎 Related Works

* **로봇 수술 도구 분할 및 추적**: 대부분의 이전 연구는 로봇 수술에서 도구의 의미론적 분할(semantic segmentation) 및 추적에 중점을 두었습니다 ([3]–[6], [12]).
* **담낭 절제술 도구 감지**: 일부 연구는 담낭 절제술에서 도구의 존재 여부 감지 또는 영역 기반 CNN을 이용한 도구 영역 학습에 초점을 맞췄습니다 ([8], [9]).
* **기존 방법론의 한계**: 의미론적 분할은 픽셀이 어떤 클래스에 속하는지 답하지만, 개별 인스턴스(객체)의 분리는 처리하지 못합니다. 인스턴스 분할은 이를 위한 추가 단계를 요구합니다.
* **연구 공백**: 부인과 복강경 수술 도구의 다중 클래스 인스턴스 분할 및 인식에 딥러닝을 적용한 연구는 이전에 없었습니다.

## 🛠️ Methodology

1. **네트워크 아키텍처**:
    * **Mask R-CNN [15]** 사용: Faster R-CNN [16]의 확장으로, Region Proposal Network (RPN)와 Fully Convolutional Network (FCN)를 결합합니다.
    * **백본**: ResNet-101을 특징 추출기로 사용했습니다.
    * **출력**: 각 제안 영역에 대해 경계 상자 좌표, 객체 클래스 확률, 그리고 객체에 대한 분할 마스크를 생성합니다.
    * **손실 함수**: 분류 손실, 지역화(localization) 손실, 평균 이진 교차 엔트로피 손실의 합계를 사용하여 엔드투엔드로 훈련됩니다.
2. **데이터셋 구축**:
    * 부인과 자궁근종 절제술 및 자궁적출술 영상에서 무작위로 추출한 **333개 프레임**($540 \times 360$ 해상도)으로 구성됩니다.
    * 각 프레임 내 보이는 모든 도구 인스턴스에 대해 수동으로 분할 마스크를 주석 처리하여 총 **561개의 인스턴스** 마스크를 생성했습니다.
    * **11가지 도구 유형** (예: Bipolar Grasper, Hook, Irrigator, Scissors 등)과 'Other' 클래스를 포함합니다.
3. **데이터 증강**:
    * 훈련 데이터 부족 문제를 해결하기 위해 **아핀 변환** (회전, 스케일링, 변환) 및 **블러링, 미러링**을 적용했습니다.
    * 증강은 두 단계로 진행됩니다:
        * **오프라인 증강**: 레이블 손실이 없거나 원본 마스크 영역과 크게 다르지 않은 회전, 스케일링, 변환을 적용합니다.
        * **온라인 증강**: 훈련 중 입력 이미지 시퀀스에 50% 확률로 가우시안 블러 및 플리핑을 적용합니다.
    * 최종적으로 333개의 원본 이미지는 3,274개의 증강된 이미지로, 561개의 인스턴스는 4,340개의 증강된 인스턴스로 확장되었습니다.
4. **평가 기준**:
    * COCO metrics [18] (Average Precision (AP) 및 Average Recall (AR))를 사용했습니다.
    * **IoU (Intersection over Union)**: $|T \cap D| / |T \cup D|$로 정의되며, 예측 마스크와 Ground-Truth 마스크 간의 유사도를 측정합니다.
    * **AP$_{50:95}$**: IoU 임계값 50%부터 95%까지 5% 간격으로 평균 낸 AP.
    * **AP$_{50}$**: IoU 임계값 50%에서의 AP.
    * **AR**: 검출의 평균 재현율.

## 📊 Results

* **이진 도구 분할 (Instrument vs. Background)**:
  * 데이터 증강 유무에 관계없이 높은 AP$_{50}$ 값 (약 **81%**)을 달성하여 도구를 배경과 안정적으로 분할할 수 있음을 보였습니다.
  * 데이터 증강은 훈련 시간을 증가시켰으나, 이진 분할의 AP$_{50}$ 성능에는 큰 영향을 미치지 않았습니다.
* **다중 클래스 도구 인식 (Specific Instrument Types)**:
  * 이진 분할보다 더 어려운 작업이었으나, 데이터 증강이 성능 향상에 크게 기여했습니다.
  * 데이터 증강을 통해 AP$_{50}$가 약 **51%에서 61%**로 향상되었습니다.
  * 일부 도구 (예: Bipolar Grasper, Hook, Irrigator, Sealer and Divider)는 높은 정밀도 (Sealer and Divider는 AP$_{50}$ 100%)로 분류되었지만, 다른 도구 (예: Grasper, Knot-Pusher, Needle, Needle-Holder, Scissors)는 시각적 유사성 때문에 식별하기 어려웠습니다.
  * 특히 Needle과 같이 얇은 도구는 배경과 구별하기 어려워 분할 성능이 낮았습니다 (AP$_{50}$ 0.218).
* **과적합 (Overfitting)**: 약 50 epoch 이후 검증 손실이 빠르게 증가하여 과적합 현상이 관찰되었습니다.

## 🧠 Insights & Discussion

* Mask R-CNN 기반 접근 방식은 복강경 수술 영상에서 도구 인스턴스를 배경과 구분하는 데 매우 효과적이며, 제한된 데이터셋에서도 높은 성능을 보였습니다.
* 다중 클래스 도구 인식은 도구 간의 높은 시각적 유사성 때문에 훨씬 더 도전적인 과제입니다. 특히 Grasper, Knot-Pusher, Needle-Holder, Scissors와 같은 도구는 구별하기 어렵습니다. 이는 도구의 미묘한 시각적 차이를 학습하기 위한 더 정교한 특징 추출이나 더 많은 데이터가 필요함을 시사합니다.
* 데이터 증강은 다중 클래스 인식 성능을 크게 향상시켜, 작은 데이터셋으로 딥러닝 모델을 훈련할 때 필수적인 요소임을 입증했습니다.
* 일부 도구는 완벽에 가까운 인식률을 보였지만, 다른 도구는 그렇지 않았습니다. 이는 수술 도구 인식을 위한 실제 시스템에서는 각 도구의 특성과 중요도를 고려한 맞춤형 접근 방식이 필요할 수 있음을 의미합니다.
* 본 연구에서 제안된 이진 도구 분할의 높은 정확도($\ge 81\%$)는 다른 자동화된 영상 분석 방법의 입력으로 사용될 수 있는 잠재력을 가집니다.

## 📌 TL;DR

**문제**: 복강경 수술 영상에서 수술 도구를 정확하게 분할하고 유형별로 식별하는 것은 데이터셋 부족과 도구 간 높은 시각적 유사성 때문에 어렵습니다.

**해결책**: 저자들은 부인과 복강경 수술 도구에 특화된 새로운 데이터셋을 구축하고, Mask R-CNN과 데이터 증강 기법을 사용하여 (1) 이진 도구 분할 (도구/배경)과 (2) 다중 클래스 도구 인식이라는 두 가지 작업을 수행했습니다.

**결과**:

* **이진 분할**: 적당히 적은 훈련 데이터로도 약 $81\%$의 높은 AP$_{50}$를 달성하여 도구를 배경과 안정적으로 구분했습니다.
* **다중 클래스 인식**: 더 어려운 작업이었으나, 데이터 증강을 통해 AP$_{50}$를 약 $51\%$에서 $61\%$로 크게 향상시켰습니다.
* **도구별 성능 편차**: Bipolar, Hook 등 일부 도구는 매우 높은 정확도로 인식되었으나, Grasper, Needle, Scissors와 같이 시각적으로 유사한 도구들은 구별하기 어려워 성능 편차가 컸습니다.
