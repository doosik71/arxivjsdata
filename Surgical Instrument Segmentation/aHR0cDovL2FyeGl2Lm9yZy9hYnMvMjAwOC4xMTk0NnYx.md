# Unsupervised Surgical Instrument Segmentation via Anchor Generation and Semantic Diffusion

Daochang Liu, Yuhui Wei, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei Shan, Ziyu Li

## 🧩 Problem to Solve

수술 도구 분할(Surgical instrument segmentation)은 상황 인지 수술실 개발에 필수적인 요소입니다. 하지만 기존의 방법들은 대규모 레이블링된 데이터에 크게 의존하는데, 이는 의료 분야에서 특히 비디오 데이터의 경우 많은 노동력과 비용, 그리고 시간을 요구합니다. 또한, 지도 학습 방법은 병원이나 시술 유형에 따른 도메인 차이로 인해 일반화 능력이 저해되는 문제가 있습니다. 이 연구는 이러한 문제를 해결하기 위해 수동 주석 없이 수술 도구 이진 분할을 수행하는 비지도 학습 접근 방식을 제안합니다.

## ✨ Key Contributions

* **비지도 수술 도구 이진 분할 접근 방식 제안:** 수동 주석 없이 수술 도구 분할을 수행하는 새로운 비지도 학습 방법을 개발했습니다.
* **앵커 생성 기반 훈련 전략:** 거친 수작업 큐(색상, 객체성, 위치 큐)를 융합하여 신뢰할 수 있는 의사(pseudo) 레이블인 앵커(anchor)를 생성하고 이를 초기 훈련 감독 신호로 활용하는 전략을 제시합니다.
* **의미론적 확산 손실(Semantic Diffusion Loss) 도입:** 인접 비디오 프레임 간의 의미론적 유사성(temporal coherence)을 활용하여 초기 앵커 신호를 모호한 픽셀로 확산시키는 새로운 손실 함수를 제안하여 모델의 예측 정확도와 완성도를 높입니다.

## 📎 Related Works

* **약지도 학습(Weakly-supervised learning):** 도구 존재 유무에 대한 이미지 수준 주석을 활용한 방법 [16, 26].
* **반지도 학습(Semi-supervised learning):** 모션 플로우를 사용하여 인접 비디오 프레임 간에 진실값(ground truth)을 전파하거나 [11], 재색상화를 사전 훈련 작업으로 활용하여 필요한 레이블 이미지 수를 줄이는 방법 [22].
* **자율지도 학습(Self-supervised learning):** 로봇 보조 수술의 운동학적 신호를 사용하여 레이블을 생성하는 방법 [5].
* **다른 도메인의 비지도 학습:** 수술 워크플로우 분석 [3] 및 수술 동작 예측 [6]과 같은 다른 수술 도메인에서 비지도 학습이 성공적으로 탐구되었습니다.
* **기존 지도 학습 기반 방법:** 초기 전통적인 방법 [25,4,20]부터 딥러닝 기반 접근 방식 [7,12,23,13,9,15,18,10,11,14,27]까지 언급하며 본 연구의 비지도 접근 방식과 차별성을 강조합니다.

## 🛠️ Methodology

본 연구의 비지도 학습 프레임워크는 크게 앵커 생성(Anchor Generation)과 의미론적 확산(Semantic Diffusion)의 두 가지 단계로 구성됩니다.

1. **앵커 생성 (Anchor Generation)**
    * **수작업 큐(Hand-crafted Cues) 계산:**
        * **색상 큐($c_{color_t}$):** LAB 공간의 반전된 A 채널과 HSV 공간의 반전된 S 채널을 곱하여 생성합니다. 수술 도구는 회색조이고 배경 조직은 붉은색이며 채도가 높다는 특성을 활용합니다.
        * **객체성 큐($c_{obj_t}$):** 클래스에 구애받지 않는 객체 탐지기 [1]를 사용하여 이미지 영역이 얼마나 객체와 유사한지(경계가 잘 정의되었는지)를 측정합니다.
        * **위치 큐($c_{loc_t}$):** 고정된 위치 사전(prior) 대신, 전체 비디오에 걸쳐 색상 맵을 평균하여 동영상별 적응형 위치 확률 맵을 얻습니다 ($c_{loc_t} = \frac{1}{T} \sum_t c_{color_t}$). 이는 해당 비디오에서 도구가 자주 나타나는 영역을 대략적으로 강조합니다.
    * **앵커 생성:** 위에서 얻은 거친 큐 맵들을 융합하여 노이즈를 억제한 앵커를 생성합니다.
        * **긍정 앵커($a_{pos_t}$):** 모든 큐의 원소별 곱 ($a_{pos_t} = c_{color_t}c_{obj_t}c_{loc_t}$). 신뢰할 수 있는 도구 영역을 나타냅니다.
        * **부정 앵커($a_{neg_t}$):** 모든 반전된 큐의 원소별 곱 ($a_{neg_t} = (1-c_{color_t})(1-c_{obj_t})(1-c_{loc_t})$). 신뢰할 수 있는 배경 영역을 나타냅니다.
    * **앵커 손실($L_{anc}$):** 생성된 앵커를 의사 레이블로 사용하여 U-Net [21] 분할 네트워크를 훈련합니다.
        $$L_{anc}(x_t) = \frac{1}{HW} \sum_i -a_{pos_{t,i}} p_{t,i} - a_{neg_{t,i}} (1-p_{t,i})$$
        여기서 $p_t$는 네트워크의 예측 맵이고, $i$는 픽셀 인덱스입니다. 이 손실은 신뢰할 수 있는 픽셀에만 감독을 가하여 노이즈에 의한 방해를 줄입니다.

2. **의미론적 확산 (Semantic Diffusion)**
    * **문제점:** 앵커는 이미지 픽셀의 소수 부분만 커버하므로, 나머지 모호한 픽셀에 대한 네트워크의 동작은 정의되지 않습니다.
    * **시간적 일관성 활용:** 인접 비디오 프레임의 도구들은 유사한 의미론적 특성을 공유하며(프레임 간 도구-도구 유사성), 이는 단일 프레임 내의 도구-배경 유사성보다 강하다는 가정을 활용합니다.
    * **특징 집계:** 사전 훈련된 CNN(VGG16의 relu5\_3 레이어)으로부터 얻은 특징 맵($F_t$)을 네트워크의 예측 맵($p_t$)을 사용하여 도구 영역($f_{fg_t}$)과 배경 영역($f_{bg_t}$)으로 각각 집계합니다.
        $$f_{fg_t} = \sum_i p_{t,i} F_{t,i}$$
        $$f_{bg_t} = \sum_i (1-p_{t,i}) F_{t,i}$$
    * **의미론적 확산 손실($L_{dif}$):** 쿼드러플렛(quadruplet) 형태로 제안되며, 프레임 간의 도구-도구 및 배경-배경 유사성이 프레임 내의 도구-배경 유사성보다 특정 마진($m_{fg}, m_{bg}$)만큼 더 높도록 강제합니다.
        * **전경(도구) 확산 손실:**
            $$L_{fg_{dif}}(x_t,x_{t+1}) = \max(\phi(f_{fg_t},f_{bg_t}) + \phi(f_{fg_{t+1}},f_{bg_{t+1}}) - 2\phi(f_{fg_t},f_{fg_{t+1}}) + m_{fg},0)$$
        * **배경 확산 손실:**
            $$L_{bg_{dif}}(x_t,x_{t+1}) = \max(\phi(f_{fg_t},f_{bg_t}) + \phi(f_{fg_{t+1}},f_{bg_{t+1}}) - 2\phi(f_{bg_t},f_{bg_{t+1}}) + m_{bg},0)$$
        여기서 $\phi(\cdot,\cdot)$는 두 특징 간의 코사인 유사도를 나타냅니다.
    * **전체 손실($L_{full}$):** 앵커 손실과 의미론적 확산 손실을 결합하여 최적화합니다.
        $$L_{full}(x_t,x_{t+1}) = L_{anc}(x_t) + L_{anc}(x_{t+1}) + L_{fg_{dif}}(x_t,x_{t+1}) + L_{bg_{dif}}(x_t,x_{t+1})$$

## 📊 Results

* **EndoVis 2017 데이터셋 성능:**
  * 2017 MICCAI EndoVis 로봇 도구 분할 챌린지 데이터셋의 이진 분할 작업에서 평가되었습니다.
  * **비지도 학습(0% 주석):** 0.71 IoU 및 0.81 Dice 계수를 달성했습니다. 이는 수동 주석을 전혀 사용하지 않고도 유망한 성능을 보여줍니다.
  * 특히, 의미론적 확산 손실($L_{bg_{dif}}$)을 추가했을 때 성능이 크게 향상되었으며, 배경 영역의 프레임 간 유사성이 도구 영역보다 분할 성능 향상에 더 큰 기여를 했습니다.
* **큐(Cues) 선택의 영향:**
  * 세 가지 큐(색상, 객체성, 위치)를 모두 조합했을 때 가장 좋은 네트워크 성능을 얻었습니다.
  * 색상 큐($c_{color_t}$)와 객체성 큐($c_{obj_t}$)가 위치 큐($c_{loc_t}$)보다 더 중요한 영향을 미쳤습니다.
* **지도 학습 방법과의 비교:**
  * 반지도 학습 설정(50% 주석)에서는 최첨단 방법들과 경쟁력 있는 성능(0.80 IoU, 0.88 Dice)을 보였습니다.
  * 완전 지도 학습(100% 주석) 설정에서는 U-Net 아키텍처를 기반으로 일부 초기 연구들과 유사한 상한선 성능을 달성했습니다.
* **다른 도메인으로의 확장성:**
  * ISIC 2016 피부 병변 분할 벤치마크에 대한 탐색적 실험에서, 비록 큐 설계를 조정했지만, 비지도 설정에서 0.64 IoU와 0.75 Dice를 달성하여 모델의 유연성을 입증했습니다.

## 🧠 Insights & Discussion

* 이 연구는 수술 도구 분할을 위한 비지도 학습의 잠재력을 강력하게 보여줍니다. 이를 통해 의료 이미지 분석에서 수동 주석의 높은 비용과 시간을 크게 줄일 수 있습니다.
* 앵커 생성은 거친 수작업 큐로부터 신뢰할 수 있는 의사 레이블을 효과적으로 추출하는 방법을 제공하며, 이는 레이블이 없는 환경에서 초기 감독 신호를 만드는 데 중요합니다.
* 의미론적 확산 손실은 인접 프레임 간의 시간적 일관성을 성공적으로 활용하여, 처음에 불확실했던 픽셀에 앵커의 감독 신호를 효과적으로 전파함으로써 정확하고 완전한 분할 마스크를 생성할 수 있게 합니다. 특히 배경 영역의 유사성이 분할 성능 향상에 큰 영향을 미쳤습니다.
* 현재 프레임워크는 이진 분할에만 국한된다는 한계가 있습니다. 향후 연구에서는 다중 클래스 또는 인스턴스 분할을 위해 여러 클래스별 앵커를 생성하거나 추가 그룹화 전략을 통합할 수 있습니다.
* 복잡한 장면, 불안정한 조명 또는 지상 진실(ground truth)에서 도구로 간주되지 않는 특수 프로브와 같은 경우에 실패 사례가 발생하기도 했습니다. 이는 큐의 신뢰성에 영향을 미칠 수 있는 요소입니다.
* 피부 병변 분할과 같은 다른 도메인으로의 확장 가능성을 보여주어, 이 방법이 수술 도구 분할 외의 다양한 이미지 분할 문제에도 적용될 수 있음을 시사합니다.

## 📌 TL;DR

**문제:** 수술 도구 분할은 중요하지만, 대규모 수동 주석은 비용과 시간이 많이 듭니다.
**방법:** 본 연구는 "앵커 생성"과 "의미론적 확산"을 통해 비지도 수술 도구 분할 방법을 제안합니다. 먼저, 색상, 객체성, 위치와 같은 수작업 큐를 융합하여 도구 및 배경에 대한 신뢰할 수 있는 "앵커"(의사 레이블)를 생성합니다. 다음으로, 인접 비디오 프레임 간의 의미론적 유사성을 활용하는 "의미론적 확산 손실"을 도입하여 앵커에서 얻은 초기 감독 신호를 나머지 모호한 픽셀로 확산시킵니다.
**결과:** EndoVis 2017 데이터셋에서 수동 주석 없이 0.71 IoU 및 0.81 Dice 점수를 달성하여, 수술 도구 분할을 위한 비지도 학습의 강력한 잠재력과 주석 비용 절감 가능성을 입증했습니다.
