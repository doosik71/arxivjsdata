# SOLVING CHALLENGING MATH WORD PROBLEMS USING GPT-4 CODE INTERPRETER WITH CODE-BASED SELF-VERIFICATION

Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li

## 🧩 Problem to Solve

최근 대규모 언어 모델(LLM)의 발전에도 불구하고, 복잡한 수학적 추론 문제 해결에서 LLM은 여전히 비논리적이거나 부정확한 내용, 그리고 계산 오류를 범하는 경향이 있습니다. 특히 GPT-4 Code Interpreter는 코드 생성 및 실행 능력이 뛰어나지만, 최종 답변의 정확성을 체계적으로 분석하고 검증하는 메커니즘이 부족합니다. 현재의 자가 디버깅(self-debugging)은 코드 실행 중 오류가 발생할 때만 작동하며, 추론 단계와 최종 답변 자체에 대한 검증은 이루어지지 않습니다. 본 연구는 GPT-4 Code Interpreter의 코드 생성 및 자가 디버깅 메커니즘을 최대한 활용하여, 외부 도움 없이 자동으로 해답을 검증하고 수정할 수 있는 방법을 찾는 것을 목표로 합니다.

## ✨ Key Contributions

* **코드 활용의 체계적 분석**: 수학 문제 해결에서 코드 생성, 실행 및 자가 디버깅의 역할에 대한 최초의 체계적인 분석을 제공합니다. GPT4-Code의 뛰어난 수학 문제 해결 능력이 단계별 코드 생성과 코드 실행 결과에 기반한 동적 솔루션 개선에서 비롯됨을 밝혀냅니다.
* **명시적 코드 기반 자가 검증(CSV) 프롬프트 제안**: GPT4-Code의 고급 코드 생성 메커니즘을 활용하여 답변을 검증하고 코드를 사용하여 솔루션을 재평가하도록 모델을 안내하는 혁신적인 프롬프트 기법을 도입합니다. CSV는 문제 해결 로직까지 검증을 확장하고, 검증 상태를 통합하여 다수결 투표 방법의 효율성을 향상시킵니다.
* **새로운 데이터셋 공개**: 오픈 소스 LLM의 수학적 추론 능력을 향상시키기 위한 MATH-code 및 MMLU-Math-code 두 가지 새로운 지시 따르기(instruction-following) 데이터셋을 구축하고 공개합니다.

## 📎 Related Works

* **사고의 사슬(Chain-of-Thought, CoT) 추론**: Wei et al. (2022)의 CoT 프롬프팅, Kojima et al. (2022)의 Zero-shot-CoT, Wang et al. (2023)의 다수결 투표를 통한 자기 일관성(self-consistency), Yao et al. (2023)의 Tree-of-Thoughts 등으로 LLM의 다단계 추론 능력을 향상시켰습니다.
* **코드를 활용한 수학 문제 해결**: LLM의 산술 계산 부정확성을 개선하기 위해 Python 인터프리터(`eval` 함수)를 활용한 GSM8K (Cobbe et al., 2021), 수학 문제를 Python 코드로 해석하여 외부 인터프리터로 실행하는 Program-Aided Language model (PAL) (Gao et al., 2023) 및 Program of Thoughts (PoT) (Chen et al., 2022)와 같은 연구가 있었습니다.
* **자가 검증(Self-Verification)**: 외부 검증기 모델을 훈련하여 최종 답변 (Cobbe et al., 2021) 또는 중간 단계 (Lightman et al., 2023)를 검증하거나, LLM이 여러 답변을 생성하고 자가 검증 점수로 순위를 매기는 (Weng et al., 2023) 접근법이 제시되었습니다. Madaan et al. (2023)의 SELF-REFINE은 자가 생성 피드백을 통해 출력을 반복적으로 개선합니다.

## 🛠️ Methodology

본 연구는 GPT4-Code의 수학 문제 해결 능력을 체계적으로 분석하고 향상시키기 위해 다음과 같은 방법을 제안합니다.

* **코드 사용 빈도(Code Usage Frequency) 분석을 위한 파일럿 실험**:
  * **프롬프트 1 (코드 사용 금지)**: GPT4-Code가 자연어(NL) 추론($C_{\text{NL}}$)에만 의존하게 하여 CoT 프레임워크와 유사하게 작동합니다.
  * **프롬프트 2 (코드 1회 사용 가능)**: GPT4-Code가 단일 코드 블록($C_{\text{SL}}$)을 사용하여 솔루션을 생성하도록 허용하며, PAL 접근 방식과 유사합니다. 이 경우 모델은 자가 디버깅 능력이 부족합니다.
  * **기본 프롬프트 (코드 사용 제한 없음)**: GPT4-Code의 일반적인 작동 패턴으로, 자연어 추론 단계 사이에 짧고 빈번한 코드 사용을 통해 자가 디버깅 및 코드 실행 결과에 따른 솔루션 수정을 수행합니다.
  * **핵심 통찰**: "코드 사용 빈도"가 높을수록 GPT4-Code의 성능이 향상됨을 확인했습니다. 특히 코드 실행 결과를 평가하고 솔루션 단계를 수정하는 자가 디버깅 메커니즘의 중요성을 발견했습니다.

* **명시적 코드 기반 자가 검증(Explicit Code-based Self-Verification, CSV) 프롬프트**:
  * GPT4-Code가 제안된 해답을 명시적으로 코드 생성을 통해 검증하도록 안내하는 제로샷(zero-shot) 프롬프트 기법입니다.
  * 검증 결과 $V$는 `True`, `False`, 또는 `Uncertain`으로 분류됩니다.
  * **재귀적 수정**: 만약 $V = \text{False}$인 경우, 모델은 자동으로 이전 솔루션을 수정하고 ($C_{\text{new}}$), 다시 검증하는 과정을 반복하여 `True` 또는 `Uncertain` 결과가 나올 때까지 진행합니다.
  * 이 검증 및 수정 과정은 모두 코드 기반으로 이루어지며, 코드 사용 빈도를 증가시켜 정확도를 높입니다.

* **검증 가이드 가중 다수결 투표(Verification-guided Weighted Majority Voting)**:
  * CoT의 자기 일관성(self-consistency)과 유사하게 여러 솔루션을 샘플링하지만, 각 솔루션의 검증 상태(`True`, `Uncertain`, `False`)에 따라 다른 가중치($w_{T}, w_{U}, w_{F}$)를 할당합니다.
  * 일반적으로 $w_{T} > w_{U} > w_{F}$로 설정하여 `True`로 검증된 답변에 더 높은 신뢰도를 부여합니다.
  * 각 후보 답변 $a$의 점수는 다음과 같이 계산됩니다:
        $$ \text{Score}(a) = \sum_{\{v_i\}} w_v (\#\{i|a_i=a \text{ and } v_i=v\}) $$
  * 가장 높은 점수를 얻은 답변을 최종 해답으로 선택합니다.

## 📊 Results

본 연구의 제안 방법은 GPT4-Code의 수학 문제 해결 능력을 크게 향상시켰습니다.

* **MATH 데이터셋**:
  * 기존 최신 GPT-4 모델의 53.9% 정확도를 GPT4-Code가 69.69%로 크게 능가했습니다.
  * GPT4-Code에 CSV를 적용하면 정확도가 73.54%로 향상되었습니다.
  * CSV와 샘플링 경로 수 $k=16$을 사용한 검증 가이드 가중 다수결 투표를 결합했을 때, MATH 데이터셋에서 84.32%라는 놀라운 제로샷 정확도를 달성했습니다. 이는 GPT4-Code 기본 모델 대비 14.63%의 향상입니다.
  * CSV는 모든 난이도 수준에서 기본 프롬프트보다 일관되게 높은 정확도를 보였으며, 코드 사용 빈도 증가와 함께 성능이 향상되었습니다. 특히 높은 난이도 문제에서 코드 사용량 증가에 따른 성능 향상이 두드러졌습니다.
* **GSM8K**: GPT4-Code + CSV + 투표 방식이 97.0%의 정확도로 최신 성능을 달성했습니다.
* **MMLU 데이터셋 (수학 및 STEM)**: GPT4-Code + CSV + 투표 방식이 MMLU-Math에서 89.2%, MMLU-STEM에서 87.0%의 최신 성능을 달성했습니다.
* **자연어 기반 검증과의 비교**: 코드 기반 자가 검증이 자연어 기반 검증보다 훨씬 우수함을 입증했습니다. 자연어 기반 검증은 오히려 정확도를 감소시키는 경우도 있었습니다.
* **가중 다수결 투표의 효과**: 검증 가이드 가중 다수결 투표는 자가 검증 결과의 높은 정밀도(평균 95.88%)를 활용하여 다수결 투표의 효율성을 크게 높였습니다.

## 🧠 Insights & Discussion

* **코드의 결정적 역할**: GPT4-Code의 뛰어난 수학 문제 해결 능력은 단순히 코드 생성 및 실행뿐만 아니라, 코드 실행 결과에 기반하여 동적으로 솔루션을 조정하고 수정하는 자가 디버깅 능력에서 기인합니다. "코드 사용 빈도"가 높을수록 성능이 향상되며, 특히 복잡한 문제일수록 이러한 경향이 뚜렷합니다.
* **명시적 검증의 중요성**: 명시적 코드 기반 자가 검증(CSV)은 모델이 최종 답변의 정확성을 주동적으로 확인하고 오류를 수정하도록 유도함으로써, GPT4-Code의 잠재력을 최대한 발휘하게 합니다. 이는 기존의 단계별 자가 디버깅을 넘어서는 포괄적인 검증을 가능하게 합니다.
* **코드 기반 검증의 우월성**: 수학 문제 해결에서 자연어 기반 검증은 오히려 정확도를 해칠 수 있는 반면, 코드 기반 검증은 모든 하위 주제에서 정확도를 향상시키는 데 기여합니다. 이는 정량적 추론에 있어 코드의 신뢰성을 강력하게 지지합니다.
* **가중 투표의 효율성**: 검증 상태에 따른 가중치를 부여한 다수결 투표는 신뢰도 높은 답변을 효과적으로 식별하여 전체 정확도를 크게 향상시키며, 샘플링 경로의 수를 줄이는 데도 기여합니다.

**한계점**:

* 현재 분석 및 개선은 GPT4-Code에만 초점을 맞추고 있으며, 향후 다른 LLM으로 확장할 계획입니다.
* 본 방법론을 통해 단계별 코드 기반 솔루션 생성 및 코드 기반 검증을 포함하는 더 정확한 데이터셋을 구축하여 LLaMA 2와 같은 오픈 소스 LLM의 수학적 능력을 향상시킬 가능성이 있지만, 이는 향후 연구 과제로 남겨져 있습니다.
* 기하학 문제에서는 멀티모달리티(multi-modality) 요구사항으로 인해 개선 효과가 상대적으로 미미했습니다.

## 📌 TL;DR

**문제**: LLM, 특히 GPT-4 Code Interpreter는 수학 문제 해결에서 코드를 활용하지만, 최종 답변에 대한 체계적인 자가 검증 및 수정 메커니즘이 부족했습니다.
**방법**: 본 연구는 GPT4-Code의 코드 생성, 실행 및 자가 디버깅 능력을 분석하여 "코드 사용 빈도"가 높을수록 성능이 향상됨을 확인했습니다. 이를 바탕으로 GPT4-Code가 **명시적으로 코드를 생성하여 답변을 검증하고, 검증 실패 시 스스로 해답을 수정하도록 유도하는 제로샷 프롬프트 기법인 "명시적 코드 기반 자가 검증(CSV)"**을 제안합니다. 또한, 이 검증 결과를 `True`, `Uncertain`, `False` 상태로 분류하고 가중치를 부여하여 다수결 투표의 정확도를 높이는 **"검증 가이드 가중 다수결 투표"**를 도입합니다.
**결과**: CSV와 가중 투표를 통해 MATH 데이터셋에서 GPT4-Code의 제로샷 정확도를 69.7%에서 84.3%로 크게 향상시켰습니다. 이는 수학적 추론 능력 향상에 있어 명시적이고 코드 기반의 자가 검증이 얼마나 중요한 역할을 하는지 보여줍니다.
