# T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations

Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen

## 🧩 Problem to Solve

인간의 움직임(모션)을 텍스트 설명으로부터 자동으로 생성하는 것은 게임, 영화 제작, 로봇 애니메이션 등 다양한 분야에서 활용될 수 있습니다. 기존의 모션 캡처는 비용이 많이 들며, 자연어 조건부 모션 생성은 모션과 텍스트가 다른 양식에 속하기 때문에 어려운 문제입니다. 이전 연구들은 복잡한 텍스트 설명에 대해 고품질 모션을 생성하는 데 한계가 있었고, 최근 확산 기반(diffusion-based) 모델들도 고전적인 VQ-VAE 기반 접근 방식에 비해 성능 향상이 크지 않을 수 있다는 문제 인식이 있었습니다.

## ✨ Key Contributions

* 텍스트 설명으로부터 모션을 생성하는 간단하면서도 효과적인 방법을 제안하며, HumanML3D 및 KIT-ML 데이터셋에서 최첨단(state-of-the-art) 성능을 달성했습니다.
* 이산 표현(discrete representations)을 통합한 GPT-유사 모델이 모션 생성에 있어 여전히 매우 경쟁력 있는 접근 방식임을 보여주었습니다.
* 양자화 전략(quantization strategies) 및 데이터셋 크기가 모델 성능에 미치는 영향에 대한 자세한 분석을 제공하여, 더 큰 데이터셋이 커뮤니티에 유망한 전망을 제공할 수 있음을 시사했습니다.

## 📎 Related Works

이 연구는 VQ-VAE [68] (이산 표현 학습을 위한 선구적인 작업)와 GPT [56, 69] (생성형 트랜스포머)의 최신 발전에 영감을 받았습니다.

* **VQ-VAE**: 이미지 합성 [19, 70], 텍스트-이미지 생성 [58], 음성 제스처 생성 [5], 음악 생성 [15, 16] 등 다양한 양식의 생성 작업에서 성공을 거두었습니다. 코드북 붕괴(codebook collapse) 문제를 해결하기 위해 EMA (Exponential Moving Average) 및 Code Reset [70]과 같은 기술이 사용됩니다.
* **인간 모션 합성**: RNN [12, 20], GAN [9, 30], GCN [49], Attention [48], MLP [11, 17] 기반의 모션 예측, VAE [4, 26]를 통한 다양한 모션 생성, 모션 인비트위닝(in-betweening) [18, 27] 및 음악-구동 춤 생성 [6, 13] 등이 포함됩니다.
* **텍스트-구동 인간 모션 생성**: Text2Action [2], Language2Pose [3], Ghost [21], MotionCLIP [65], ACTOR [52], TEMOS [53], TEACH [7]와 같은 초기 작업들이 있었으며, 최근 HumanML3D [22] 및 TM2T [23]와 같은 대규모 데이터셋 및 모델이 등장했습니다. MDM [66] 및 MotionDiffuse [74]와 같은 확산 기반 모델들이 이 분야를 지배하고 있었습니다.

## 🛠️ Methodology

T2M-GPT는 두 가지 주요 모듈로 구성된 2단계 접근 방식을 사용합니다.

1. **Motion VQ-VAE**:
    * **목표**: 모션 시퀀스 $X = [x_1, x_2, \dots, x_T]$를 이산 코드 인덱스 시퀀스 $S = [s_1, s_2, \dots, s_{T/l}]$로 매핑합니다. $x_t \in \mathbb{R}^{d}$는 각 프레임의 모션 벡터를 나타냅니다.
    * **아키텍처**: 1D Convolution, Residual Block [29], ReLU로 구성된 간단한 CNN 기반 오토인코더를 사용합니다.
    * **코드북**: $K$개의 코드 $\{c_k\}_{k=1}^{K}$를 포함하는 학습 가능한 코드북 $C$를 사용하며, 각 $c_k \in \mathbb{R}^{d_c}$입니다.
    * **최적화 목표**: 재구성 손실 $L_{re}$, 임베딩 손실 $L_{embed}$, 커밋먼트 손실 $L_{commit}$를 포함하는 표준 $L_{vq}$ 손실을 사용합니다.
        $$ L_{vq} = L_{re} + ||\text{sg}[Z] - \hat{Z}||^2 + \beta||Z - \text{sg}[\hat{Z}]||^2 $$
        재구성 손실 $L_{re}$는 $L_{\text{smooth}_1}(X, X_{re}) + \alpha L_{\text{smooth}_1}(V(X), V(X_{re}))$와 같이 모션 자체와 그 속도에 대한 Smooth L1 손실의 조합입니다.
    * **양자화 전략**: 코드북 붕괴(codebook collapse)를 완화하기 위해 EMA (Exponential Moving Average)와 Code Reset 전략을 활용합니다.

2. **T2M-GPT**:
    * **목표**: 학습된 VQ-VAE의 코드북에서 이산 코드 인덱스 시퀀스 $S$를 텍스트 설명 $c$에 조건부로 생성합니다.
    * **아키텍처**: 표준 GPT-유사 트랜스포머 [69] 모델을 사용합니다.
    * **End 토큰**: 모션 생성이 멈추는 시점을 나타내기 위해 특수 `End` 토큰을 추가하여 모션 길이를 예측합니다.
    * **최적화 목표**: 데이터 분포의 로그-우도(log-likelihood)를 직접 최대화합니다.
        $$ L_{trans} = E_{S \sim p(S)}[-\log p(S|c)] $$
    * **텍스트 임베딩**: CLIP [55]을 사용하여 텍스트 임베딩 $c$를 추출합니다.
    * **인과적 자기-어텐션 (Causal Self-attention)**: 트랜스포머에 인과적 마스킹(causal masking)을 적용하여 미래 정보가 현재 토큰 계산에 영향을 미치지 않도록 합니다.
    * **학습-테스트 불일치 완화**: 학습 시 시퀀스에 $\tau \times 100\%$의 무작위 코드 인덱스를 삽입하여 ground-truth 코드 인덱스를 손상시키는 간단한 데이터 증강 전략을 사용합니다. $\tau$는 하이퍼파라미터이거나 $U[0,1]$에서 무작위로 샘플링됩니다.

## 📊 Results

* **데이터셋**: HumanML3D [22] 및 KIT-ML [54] 데이터셋을 사용했습니다.
* **재구성 성능**: 제안된 VQ-VAE의 재구성 성능은 실제 모션과 거의 일치하여, 학습된 이산 표현의 고품질을 입증했습니다.
* **정량적 비교 (HumanML3D)**:
  * 텍스트-모션 일관성 (R-Precision, MM-Dist)에서 최첨단 확산 모델인 MotionDiffuse [74]와 비등하거나 더 나은 성능을 달성했습니다.
  * FID (Frechet Inception Distance) 면에서는 MotionDiffuse의 0.630 대비 **0.116**이라는 훨씬 우수한 성능을 보여, 생성된 모션의 품질이 확연히 높음을 입증했습니다.
  * 학습 중 손상된 시퀀스(corrupted sequences)를 사용하는 전략($\tau=0.5$)은 성능을 일관되게 향상시켰습니다.
* **정성적 비교**: MotionDiffuse [74], MDM [66], Guo et al. [22]과 비교했을 때, T2M-GPT는 훨씬 더 나은 품질의 인간 모션을 생성하며, 왜곡되거나 미끄러지는(sliding) 현상이 적었습니다.
* **아키텍처 및 손실 함수 분석**: T2M-GPT 트랜스포머의 스케일을 늘리면 성능이 향상되며, VQ-VAE의 재구성 손실로 L1 Smooth 손실에 속도 정규화($\alpha=0.5$)를 추가하는 것이 가장 좋은 성능을 보였습니다.
* **코드북 크기**: 512개 코드가 256개 또는 1024개 코드보다 약간 더 나은 성능을 보였습니다.

## 🧠 Insights & Discussion

* **VQ-VAE의 경쟁력**: 간단한 VQ-VAE 및 GPT 기반 프레임워크가 복잡한 확산 기반 모델에 비해 우수하거나 동등한 성능을 달성할 수 있음을 입증하여, 이산 표현 기반 접근 방식의 강력한 잠재력을 다시 확인시켰습니다.
* **양자화 전략의 중요성**: EMA와 Code Reset과 같은 표준 양자화 전략은 VQ-VAE의 코드북 활용률을 크게 높이고 최종 모션 생성 품질을 향상시키는 데 필수적입니다.
* **데이터셋 크기의 영향**: 모델의 성능은 학습 데이터의 양이 증가함에 따라 꾸준히 향상되는 경향을 보였습니다. 이는 HumanML3D와 같은 대규모 데이터셋도 여전히 모션 생성 모델을 더 개선할 수 있는 여지가 있음을 시사합니다.
* **학습-테스트 불일치 완화**: 학습 시 시퀀스 손상 전략은 모델이 추론 시 발생할 수 있는 오류에 더 강인하게 만들고 성능을 개선하는 데 효과적입니다.
* **한계**:
  * 지나치게 긴 텍스트의 경우, 생성된 모션이 텍스트 설명의 일부 세부 사항을 놓칠 수 있습니다.
  * 생성된 모션에서 다리나 손의 움직임에 약간의 떨림(jittering) 현상이 관찰되었습니다. 이는 VQ-VAE 아키텍처 개선이나 후처리(temporal smoothing filter)를 통해 완화될 수 있을 것으로 예상됩니다.

## 📌 TL;DR

텍스트 설명으로부터 고품질 인간 모션을 생성하는 문제에 대해, 이 연구는 **VQ-VAE**와 **GPT**를 결합한 간단하고 효과적인 **T2M-GPT** 프레임워크를 제안합니다. 이 프레임워크는 (1) EMA와 Code Reset을 통해 이산 모션 표현을 학습하는 Motion VQ-VAE와 (2) 텍스트 임베딩에 조건부로 이산 코드를 생성하는 트랜스포머 기반의 T2M-GPT로 구성됩니다. 학습-테스트 불일치를 완화하기 위해 학습 시 시퀀스 손상 전략이 적용되었습니다. 결과적으로 T2M-GPT는 HumanML3D 및 KIT-ML 데이터셋에서 확산 기반 모델을 포함한 최첨단 방법들을 **FID 측정에서 크게 능가**하며, VQ-VAE와 GPT 기반 모델이 모션 생성에서 매우 경쟁력 있는 접근 방식임을 입증했습니다. 또한, 데이터셋 크기가 성능 향상에 큰 영향을 미침을 보여주었습니다.
