# MTD-GPT: Unsignalized 교차로 자율주행을 위한 다중 작업 의사결정 GPT 모델

Jiaqi Liu, Peng Hang, Member, IEEE, Xiao Qi, Jianqiang Wang, and Jian Sun

## 🧩 Problem to Solve

자율주행 기술은 운송 시스템에 혁신을 약속하지만, 신호등 없는 교차로와 같은 복잡한 시나리오에서 안전하고 정확한 다중 작업 의사결정을 수행하는 것은 여전히 큰 과제입니다. 기존의 규칙 기반 및 게임 이론 기반 방법들은 복잡한 시나리오에 대처하기 어렵거나 계산 효율성 문제가 있습니다. 강화 학습(RL)은 강력한 학습 능력을 보이지만, 단일 모델로 다양한 자율주행 의사결정 시나리오와 작업을 동시에 처리하는 데는 한계가 있습니다. 본 논문은 이러한 다중 작업 의사결정 문제를 해결하고자 합니다.

## ✨ Key Contributions

* 자율주행 다중 작업 의사결정 문제를 시퀀스 모델링 문제로 추상화하고, GPT-2 기반의 Multi-Task Decision-Making GPT (MTD-GPT) 모델을 제안했습니다.
* RL 알고리즘을 사용하여 단일 작업 의사결정 전문가를 훈련하고, 이 전문가 데이터를 활용하여 MTD-GPT 학습을 안내하는 훈련 파이프라인을 설계했습니다.
* 신호등 없는 교차로의 다양한 의사결정 작업에서 MTD-GPT의 성능을 평가했으며, MTD-GPT 모델이 최첨단 단일 작업 의사결정 RL 모델보다 우수하거나 동등한 성능을 보임을 입증했습니다.

## 📎 Related Works

* **교차로에서의 자율주행 의사결정:** 규칙 기반 방법 (PET [4]), 게임 이론 기반 방법 [5], [13], 데이터 기반 기술 (강화 학습 [8], [9], [14]) 등이 연구되었습니다. 특히, Kai et al. [8]은 다중 작업 목표를 4차원 벡터로 공식화하고 벡터화된 보상 함수를 사용했으며, Liu et al. [15]은 다중 작업 안전 강화 학습 프레임워크를 제안했습니다.
* **강화 학습의 Transformer:** 자연어 처리(NLP) [16] 및 컴퓨터 비전(CV) [17] 분야에서 Transformer 모델의 성공에 힘입어, 많은 연구자들이 RL 문제에 Transformer를 적용하여 시퀀스 모델링 문제로 전환하여 학습하고 있습니다 [10], [18]–[21].

## 🛠️ Methodology

MTD-GPT의 훈련 파이프라인은 크게 세 가지 구성 요소로 이루어집니다: 전문가 데이터 수집, GPT 훈련, GPT 평가.

1. **전문가 데이터 수집 (Expert Data Collection):**
    * **RL 전문가 훈련:** 좌회전, 직진, 우회전의 세 가지 자율주행 의사결정 작업을 정의합니다. 각 작업에 대해 PPO-Attention 알고리즘을 사용하여 세 명의 RL 전문가를 훈련시킵니다.
        * **정책 네트워크:** MLP 인코더, 어텐션 모듈, MLP 디코더로 구성되어 관측치($O$)를 행동($a$) 분포로 매핑합니다. 어텐션 메커니즘 [23]을 통합하여 성능을 향상시킵니다.
        * **정책 최적화:** Clipped PPO [24]를 사용하여 정책 네트워크($\pi_{\theta}$)와 가치 네트워크($V_{\phi}$)를 훈련합니다. 손실 함수는 다음과 같습니다:
            $$L_{PPO}(\theta) = E_{t}\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_t\right)\right]$$
            여기서 $r(\theta) = \frac{\pi_{\theta}(a|s)}{\pi_{\theta'}(a|s)}$ 이고, $\hat{A}_t$는 어드밴티지 함수, $\epsilon$은 클리핑 범위입니다.
        * **보상 함수:** 충돌 보상($r_c$), 효율성 보상($r_e$), 도착 보상($r_a$)의 가중 합으로 정의됩니다: $r_i = w_c r_c + w_e r_e + w_a r_a$.
    * **데이터 샘플링:** 훈련된 RL 전문가들을 시뮬레이션 환경에 배포하여 행동 및 보상 데이터를 기록하고, 이를 오프라인 다중 작업 데이터셋 $D = \cup_{i=1}^{N} D_i$로 구성합니다. 각 에피소드의 데이터는 시퀀스($\tau = (s_1, a_1, r_1, s_2, a_2, r_2, ..., s_T, a_T, r_T)$)로 변환됩니다.

2. **오프라인 GPT 훈련 (Offline Training GPT):**
    * **입력 표현 (Input Representation):** 자율주행 다중 작업 의사결정 문제를 시퀀스 모델링 및 예측 문제로 추상화합니다.
        * 각 시점 $t$에서 'state-action-reward' 튜플을 GPT 모델의 입력 형식에 맞게 토큰화합니다.
        * 궤적($\tau$)은 'return-to-go' ($g_t = \sum_{t'=t}^{T} r_{t'}$)와 함께 $s_t, a_t, g_t$의 시퀀스로 표현됩니다:
            $$\tau' = (s_1, a_1, g_1, s_2, a_2, g_2, ..., s_T, a_T, g_T)$$
    * **아키텍처 (Architecture):** GPT-2 [11]와 동일한 디코더 전용 아키텍처를 사용합니다.
        * 토큰 $x_t = \langle s_t, a_t, g_t \rangle$는 MLP를 통해 연속적인 벡터 공간으로 매핑된 후, 입력 시퀀스의 순서를 유지하기 위해 Positional Encodings (PE) [23]가 추가됩니다.
        * 임베딩 결과는 Transformer 레이어를 통과하여 은닉 상태 $h_t$를 얻습니다. 여기서 마스킹된 셀프 어텐션 메커니즘 [23]이 사용되어 현재 시점 $t$의 입력이 $\langle 1, \dots, t-1 \rangle$에서만 상관관계를 가집니다.
        * 최종적으로 선형 예측 레이어를 통해 다음 행동 $a'_{t+1}$을 생성합니다.
    * **훈련 (Training):** 다중 작업 데이터셋 $D$의 모든 토큰을 MTD-GPT에 입력하며, 각 작업(좌회전, 직진, 우회전)은 데이터셋의 1/3씩 차지합니다. 교차 엔트로피(CE) 손실을 사용하여 정책 $\pi_{gpt}$를 훈련합니다:
        $$L_{CE} = \frac{1}{K} \sum_{t=1}^{K} P(a_t) \log\left(\pi_{gpt}(s_{-K,t}, g_{-K,t})\right)$$

3. **GPT 평가 (GPT Evaluation):**
    * 훈련된 MTD-GPT는 다양한 작업 시나리오에서 평가됩니다. 각 작업 $i$에 대해 원하는 성능 $g_i^1$와 초기 상태 $s_i^1$를 지정합니다.
    * MTD-GPT는 $s_i^1, g_i^1$에 따라 행동 $a_i^1 = \pi_{gpt}(s_i^1, g_i^1)$을 생성하고, 자율주행차(AV)가 이를 실행합니다.
    * 다음 상태 $s_i^{t+1}$와 보상 $r_i^t$를 얻고, 다음 RTG는 $g_i^{t+1} = g_i^t - r_i^t$로 계산됩니다. 이 과정을 에피소드가 종료될 때까지 반복하여 각 작업에 대한 의사결정 성공률을 측정합니다.

## 📊 Results

* **RL 전문가 성능:** 개발된 PPO-Attention 알고리즘은 DQN 및 PPO와 같은 다른 기준 알고리즘에 비해 모든 의사결정 작업에서 수렴 속도와 평균 보상 면에서 우수한 성능을 보였습니다. 이는 GPT 모델 학습을 위한 고품질 행동 데이터를 제공하는 RL 전문가의 견고성을 입증합니다.
* **MTD-GPT 성능 분석:**
  * 다양한 파라미터 수 (약 600K, 1.2M, 2.4M, 38M, 75M)를 가진 모델을 훈련했습니다.
  * 모델의 파라미터 수가 증가함에 따라 훈련 과정에서 더 빠른 수렴을 보였으며, 38M 및 75M 파라미터 모델이 최상의 수렴 및 학습 결과를 나타냈습니다.
  * 그러나 파라미터 수가 더 큰 모델이 반드시 더 나은 의사결정 성능을 보장하지는 않았습니다. 1.2M 파라미터 모델이 좌회전 70%, 직진 95%, 우회전 83%의 성공률로 가장 높은 의사결정 성공률을 달성했습니다.
  * 이는 대규모 모델이 고정된 오프라인 데이터에 과적합되어 테스트 작업에서 최적이 아닌 성능을 초래할 수 있음을 시사합니다.
* **MTD-GPT vs. RL 전문가:**
  * MTD-GPT (1.2M)는 단일 작업 성공률에서 RL 전문가를 능가하거나 동등한 성능을 보였습니다.
  * RL 전문가는 좌회전 49%, 직진 92%, 우회전 81%의 성공률을 보인 반면, MTD-GPT (1.2M)는 좌회전 70%, 직진 95%, 우회전 83%를 달성하여 모든 작업에서 향상된 성능을 보여주었습니다.

## 🧠 Insights & Discussion

* MTD-GPT는 신호등 없는 교차로에서 다중 작업 의사결정을 효과적으로 처리할 수 있으며, 최첨단 단일 작업 RL 모델보다 우수하거나 동등한 성능을 달성하여 그 잠재력을 입증했습니다.
* 모델의 파라미터 수가 증가할수록 훈련 속도는 빨라지지만, 특정 규모 이상에서는 고정된 오프라인 데이터에 대한 과적합으로 인해 실제 의사결정 성공률이 오히려 감소하는 현상이 관찰되었습니다. 이는 모델 일반화 능력을 향상시키기 위한 새로운 훈련 전략의 필요성을 강조합니다.
* 사례 분석을 통해 파라미터 규모에 따라 AV의 의사결정 스타일이 달라짐을 확인했습니다. 600K 파라미터 모델은 공격적인 주행 스타일을, 1.2M 모델은 보수적이면서 안전한 주행을, 75M 모델은 지나치게 보수적이라 비효율적인 주행을 보였습니다. 이는 모델 규모와 의사결정 행동 사이의 복잡한 관계를 보여줍니다.
* 향후 연구에서는 MTD-GPT 모델의 일반화 능력을 확장하여 램프 합류, 로터리 통과, 차선 변경 등 다양한 시나리오에서 뛰어난 성능을 발휘하도록 하는 것을 목표로 합니다. 또한, 실제 주행 데이터와 시뮬레이션 데이터를 결합한 하이브리드 데이터셋을 활용하고, RLHF(Reinforcement Learning with Human Feedback) 방법을 통합하여 보다 인간적이고 안전하며 해석 가능한 의사결정 행동을 생성할 계획입니다.

## 📌 TL;DR

이 논문은 신호등 없는 교차로에서 자율주행 차량의 안전하고 정확한 다중 작업 의사결정을 위한 MTD-GPT 모델을 제안합니다. RL 전문가로부터 고품질 데이터를 수집하고, 이를 GPT-2 기반의 Transformer 모델에 시퀀스 모델링 방식으로 학습시키는 파이프라인을 설계했습니다. 실험 결과, MTD-GPT는 기존 단일 작업 RL 전문가보다 우수하거나 동등한 성능을 보였으나, 대규모 모델에서 오프라인 데이터 과적합 문제가 관찰되어 향후 일반화 및 실제 데이터 활용 연구가 필요함을 시사했습니다.
