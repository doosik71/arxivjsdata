# SurgicalGPT: 수술 시각 질문 응답을 위한 End-to-End Language-Vision GPT

Lalithkumar Seenivasan, Mobarakol Islam, Gokul Kannan, Hongliang Ren

## 🧩 Problem to Solve

GPT 기반 대규모 언어 모델(LLM)은 자연어 처리 분야에서 혁신을 가져왔지만, 본래 시각 토큰을 처리하지 못하며 단방향(uni-directional) 어텐션을 사용합니다. 반면, 시각 질문 응답(VQA) 태스크는 비전과 언어 처리를 동시에 요구하며, 일반적으로 양방향(bi-directional) 어텐션 모델이나 융합 기법이 사용되어 모든 문맥을 한 번에 파악합니다. 의료, 특히 수술 분야에서는 의대생이나 환자의 수술 관련 질문에 답변할 수 있는 강력한 다중 모달 모델이 절실하지만, 기존 VQA 모델들은 종종 고정된 특징 추출기나 영역 제안 네트워크(region proposal network)를 필요로 합니다. 이러한 제한점을 극복하고 GPT 모델의 강력한 언어 처리 능력을 수술 VQA에 활용하는 것이 핵심 문제입니다.

## ✨ Key Contributions

* **End-to-End 학습 가능한 LV-GPT 모델 제안**: GPT2를 확장하여 시각 입력(이미지)을 포함하는 Language-Vision GPT (LV-GPT) 모델을 수술 VQA를 위해 개발했습니다.
* **비전 토큰 통합**: GPT 모델에 특징 추출기(비전 토크나이저)와 비전 토큰 임베딩(토큰 타입 및 위치)을 도입하여 시각 정보를 처리할 수 있게 했습니다.
* **새로운 토큰 시퀀싱 전략**: GPT의 단방향 어텐션을 활용하기 위해 시각 토큰보다 **단어 토큰을 먼저 시퀀싱**하는 방식을 제안했습니다. 이는 사람이 질문을 먼저 이해하고 이미지에서 답을 추론하는 방식과 유사합니다.
* **최첨단 성능 달성**: EndoVis18-VQA, Cholec80-VQA 등 두 가지 공개 수술 VQA 데이터셋과 새롭게 주석을 추가한 PSI-AVA-VQA 데이터셋에서 다른 최첨단 VQA 모델들보다 우수한 성능을 보였습니다.
* **새로운 데이터셋 공개**: Holistic surgical scene 데이터셋을 기반으로 PSI-AVA-VQA 데이터셋에 VQA 주석을 추가하고 질문 유형 주석을 포함하여 세 가지 데이터셋 모두에 대한 심층 분석을 가능하게 했습니다.
* **종합적인 분석**: 토큰 시퀀싱, 비전 토큰의 타입 및 위치 임베딩 효과에 대한 광범위한 연구를 수행했습니다.

## 📎 Related Works

* **GPT-기반 LLM**: GPT2, ChatGPT (GPT3.5), BARD (LaMDA)와 같은 언어 전용 단방향 트랜스포머 디코더 모델로, 텍스트 생성에 강력합니다.
* **VisualBert [12] 및 VisualBert RM [18]**: 다중 모달 태스크를 위해 주로 사용되는 양방향 인코더 모델입니다. VisualBert RM은 수술 로봇 VQA에 사용되었으며, 영역 제안 네트워크 없이 전체 이미지에서 특징을 추출합니다.
* **MedfuseNet [19]**: 의료 진단 VQA를 위한 어텐션 기반 모델입니다.
* **기타 VQA 모델**: Block [5], MUTAN [4], MFB [24], MFH [25]와 같은 모델들이 비교 대상으로 사용되었습니다.

## 🛠️ Methodology

LV-GPT(Language-Vision GPT) 모델은 GPT 모델의 언어 처리 능력을 활용하여 수술 VQA 태스크를 수행하도록 설계된 End-to-End 학습 가능한 다중 모달(언어 및 비전) 네트워크입니다.

1. **전체 네트워크 구조**:
    * 기존 GPT 모델에 **비전 토크나이저(특징 추출기)** 모듈과 **비전 임베딩**을 통합합니다.
2. **언어-비전 처리**:
    * **질문 처리**: 입력 질문은 GPT2의 내장 토크나이저로 토큰화됩니다. 이 단어 토큰들은 토큰 ID, 토큰 타입 (0), 토큰 위치를 기반으로 GPT2의 내장 단어 임베딩 레이어에 의해 임베딩됩니다.
    * **시각 장면 처리**: 입력 수술 장면(이미지)을 비전 토큰으로 토큰화하기 위해 LV-GPT는 ResNet18 (RN18), Swin, ViT 중 하나를 비전 토크나이저(특징 추출기)로 사용합니다. 각 비전 토큰은 이미지 패치의 시각적 특징을 포함합니다.
    * **비전 토큰 임베딩**: 비전 토큰들은 추가적으로 토큰 타입 (1)과 토큰 위치 (pos = 0) 임베딩을 통해 임베딩됩니다.
    * **최종 임베딩 수식**: 임베딩된 단어 토큰($w_e$)과 비전 토큰($v_e$)은 다음과 같이 표현됩니다:
        $$
        w_e = T_{t=0}(w_x) + P_{pos}(w_x) + w_x; \quad pos = 0,1,2,3, ..., n.
        $$
        $$
        v_e = T_{t=1}(v_x) + P_{pos=0}(v_x) + v_x; \quad v_x = \begin{cases} v_t & \text{if } \text{dim}(v_{i}^{t}) = \text{dim}(w_{i}^{x}) \\ f(v_t) & \text{else} \end{cases}
        $$
        여기서 $T_t()$는 타입 임베딩, $P_{pos}()$는 위치 임베딩, $w_x$와 $v_x$는 초기 단어 및 비전 임베딩, $v_t$는 비전 토큰입니다. 비전 토큰의 차원이 단어 토큰과 일치하지 않을 경우 추가 선형 레이어 $f()$를 거칩니다.
3. **토큰 시퀀싱**:
    * GPT의 강력한 언어 처리 능력과 단방향 어텐션을 활용하기 위해 **단어 토큰을 비전 토큰보다 먼저 시퀀싱**합니다. 이는 모델이 질문을 먼저 이해한 후 이미지에서 답을 추론하는 인간의 사고 과정을 모방합니다.
4. **분류**:
    * 전파된 다중 모달 특징은 일련의 선형 레이어를 거쳐 최종 답변을 분류합니다.

## 📊 Results

* **최첨단 성능 능가**: 제안된 LV-GPT 모델의 모든 변형(Swin, RN18, ViT)은 EndoVis18-VQA, Cholec80-VQA, PSI-AVA-VQA 세 가지 데이터셋에서 대부분의 최첨단 모델(VisualBert, VisualBert RM, Block, Mutan, MFB, MFH)보다 Acc (정확도), Recall, Fscore 측면에서 유의미하게 우수했습니다.
* 특히, LV-GPT (Swin) 변형은 EndoVis18-VQA 및 Cholec80-VQA 데이터셋에서 약 3-5%의 정확도 향상을 보여주었습니다.
* **End-to-End 학습 가능성**: LV-GPT 모델은 End-to-End로 학습 가능하여, 입력 이미지를 비전 토큰으로 처리하기 위해 영역 제안 네트워크가 필요한 대부분의 최첨단 모델과 차별화됩니다.
* **토큰 시퀀싱 효과**: 단어 토큰을 비전 토큰보다 먼저 처리하는 "Early word" 시퀀싱 방식이 "Early vision" 방식에 비해 모델 성능을 약 2-4% 향상시켰습니다. 이는 GPT의 언어 모델이 질문 문맥을 먼저 파악하는 능력과 관련이 깊습니다.
* **비전 토큰의 위치 임베딩 효과**: 비전 토큰에 **제로-위치 임베딩(pos=0)**을 적용했을 때 일반적으로 더 나은 성능을 보였습니다. 특히 Swin/ViT와 같은 트랜스포머 기반 비전 토크나이저는 이미 자체적으로 위치 임베딩을 포함하고 있어, GPT 수준에서 추가적인 실제 위치 임베딩을 적용하면 이중으로 처리되어 성능 저하를 가져올 수 있음을 시사합니다. CNN 기반(RN18) 모델은 실제 위치 임베딩에서 개선을 보였습니다.
* **하위 유형 분석**: 제로-위치 임베딩이 Cholec80-VQA의 모든 하위 유형에서 비슷하거나 약간 더 나은 성능을 보였고, PSI-AVA-VQA 데이터셋에서는 '도구 위치'와 같은 하위 유형을 포함하여 상당한 성능 향상을 보였습니다.
* **비전 토큰 임베딩에 대한 어블레이션 연구**: C-VE (custom embedding)와 VT-TY (vision token type) + VT-ZPE (vision token zero-position embedding) 조합이 가장 좋은 성능을 나타냈습니다.

## 🧠 Insights & Discussion

* **토큰 시퀀싱의 중요성**: 이 연구는 GPT와 같은 단방향 어텐션 모델에서 토큰 시퀀싱 순서의 중요성을 강조합니다. 단어 토큰을 먼저 처리하는 "Language-Vision" 접근 방식은 GPT의 강력한 언어 추론 능력을 극대화하며, 인간이 질문을 이해한 후 시각 정보를 통해 답을 찾는 인지 과정을 효과적으로 모방합니다.
* **GPT 확장 가능성**: GPT2 모델에 비전 처리 모듈을 성공적으로 통합함으로써, 기존 언어 모델이 다중 모달 태스크로 확장될 수 있음을 보여줍니다.
* **위치 임베딩의 미묘함**: 비전 토크나이저(Swin/ViT)가 이미 위치 정보를 처리하는 경우, GPT 단계에서 추가적인 위치 임베딩이 오히려 성능에 부정적인 영향을 미칠 수 있다는 점은 향후 다중 모달 모델 설계 시 고려해야 할 중요한 통찰입니다.
* **의료 분야의 잠재력**: SurgicalGPT는 수술 VQA를 통해 의대생 교육이나 의료 전문가의 업무 부담 경감에 크게 기여할 수 있습니다.
* **향후 연구**: 이 모델은 의료 이미지/비디오에서 직접 보고서를 생성하는 등 다양한 미래 응용 가능성을 열어줍니다.

## 📌 TL;DR

이 논문은 GPT 모델을 수술 시각 질문 응답(VQA) 태스크에 활용하기 위한 Language-Vision GPT(LV-GPT) 모델인 **SurgicalGPT**를 제안합니다. SurgicalGPT는 GPT2에 학습 가능한 비전 토크나이저와 비전 토큰 임베딩을 통합하며, **단어 토큰을 비전 토큰보다 먼저 처리**하는 새로운 시퀀싱 전략을 통해 GPT의 단방향 어텐션 능력을 최적화합니다. 이 모델은 세 가지 수술 VQA 데이터셋에서 기존 최첨단 모델들을 능가하는 성능을 보였으며, 토큰 시퀀싱 및 제로-위치 임베딩의 효과에 대한 중요한 통찰을 제공합니다. 이는 GPT 모델의 다중 모달 확장 가능성을 보여주며 의료 VQA 분야의 발전에 기여합니다.
