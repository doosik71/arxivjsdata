# ADAPTING GPT, GPT-2 AND BERT LANGUAGE MODELS FOR SPEECH RECOGNITION

Xianrui Zheng, Chao Zhang, Philip C. Woodland

## 🧩 Problem to Solve

자동 음성 인식(ASR) 시스템의 핵심 구성 요소인 언어 모델(LM)은 단어 시퀀스의 사전 확률을 통합하여 언어적 지식을 제공합니다. 기존 n-gram LM은 데이터 희소성 문제로 인해 제한된 문맥(n<5)만을 사용하며, 신경망 LM(FNN, RNN, LSTM, Transformer)은 더 긴 문맥을 활용할 수 있지만, 일반적으로 도메인 내(in-domain) 데이터만으로 처음부터 훈련됩니다.

GPT 및 GPT-2와 같은 대규모 사전 훈련된 단방향 LM은 자연어 처리(NLP)에서 큰 성공을 거두었으나, ASR에 대한 적용 연구는 제한적이었습니다. 특히 BERT와 같은 양방향 LM의 경우, 출력 확률을 단순히 곱하는 방식으로는 ASR에 필요한 유효한 문장 사전 확률 $P(w_{1:T})$을 얻을 수 없다는 문제가 있습니다. 따라서 이러한 강력한 사전 훈련 LM을 ASR에 효과적으로 활용하고, 특히 양방향 LM의 출력 확률을 정확히 변환하는 방법론이 필요합니다.

## ✨ Key Contributions

* **사전 훈련된 GPT 및 GPT-2의 ASR 적용:** 대규모 텍스트 데이터셋에 사전 훈련된 GPT 및 GPT-2 LM을 도메인 내 데이터로 미세 조정(fine-tuning)하여, 처음부터 훈련된 신경망 LM(FNN, LSTM, Transformer LM)의 조합보다 ASR 재채점(rescoring) 성능이 우수함을 입증했습니다.
* **GPT 및 GPT-2 조합의 효과:** 미세 조정된 GPT와 GPT-2를 결합했을 때 AMI 및 Switchboard 데이터셋에서 단어 오류율(WER)이 추가적으로 감소함을 보여주었습니다. 이는 베이스라인 4-gram 모델 대비 AMI eval 세트에서 4.6% 절대 WER 감소, Switchboard/Call Home 데이터에서 2.5%/4.3% 절대 WER 감소에 해당합니다.
* **양방향 LM 출력 확률 변환 방법론 제안:** BERT와 같은 양방향 LM의 출력 확률에서 ASR에 필요한 정확한 문장 사전 확률 $P(w_{1:T})$을 계산하는 수학적으로 엄밀한 재귀적 변환 방법(수식 7)을 제안했습니다. 이는 저자들의 지식 범위 내에서 최초로 제시된 방법입니다.
* **실용적인 근사 방법 도입:** 재귀적 변환의 계산 복잡도를 줄이기 위해, $M$($1 \le M \le T$)개의 조건부 확률 항목만 선택하는 근사 방법을 제안하여 계산 비용과 문맥 활용 사이의 균형을 맞췄습니다.
* **BERT의 ASR 성능 향상:** 제안된 변환 방법이 BERT의 ASR 성능을 추가로 3% 상대적 WER 감소시키는 데 기여하며, 특히 양쪽 문맥을 모두 활용했을 때 더욱 효과적임을 보여주었습니다.
* **GPT, GPT-2, BERT의 결합:** 미세 조정된 GPT, GPT-2, 그리고 제안된 변환 방법을 적용한 BERT를 모두 결합했을 때 가장 낮은 WER을 달성했으며, 이는 단방향 및 양방향 모델의 상보성을 입증합니다.

## 📎 Related Works

* **전통적인 n-gram LM:** ASR에서 언어 모델링의 기초로 사용 [1].
* **초기 신경망 LM (NNLM):**
  * Feed-Forward Neural Network (FNN) LM [9-12].
  * Recurrent Neural Network (RNN) 및 Long Short-Term Memory (LSTM) LM: 전체 기록을 기반으로 예측 가능 [13-17].
* **어텐션 메커니즘 및 Transformer:**
  * RNN의 대안으로 시퀀스 처리에 어텐션 메커니즘 도입 [18, 19].
  * Transformer: 기계 번역을 위해 제안된 어텐션 기반 인코더-디코더 모델 구조 [20], ASR용 단방향 LM으로 활용 가능 [21].
* **사전 훈련된 LM:**
  * **GPT (Generative Pre-trained Transformer):** Transformer 디코더 구조를 사용하여 대규모 텍스트 코퍼스에 사전 훈련된 단방향 LM [22].
  * **GPT-2:** GPT의 후속 모델, 더 큰 모델과 데이터셋 사용 [29].
  * **BERT (Bidirectional Encoder Representations from Transformers):** Transformer 인코더 구조를 사용하여 사전 훈련된 양방향 LM으로, 이전 및 미래 문맥을 모두 활용 [23].
  * RoBERTa [25], Transformer-XL [26], ELMo [27], XLNet [28], ALBERT [30], T5 [31], GPT-3 [32] 등 다양한 사전 훈련 LM 연구.
* **ASR에서의 사전 훈련 LM 적용:** GPT 및 BERT의 ASR 사용에 대한 연구는 제한적이었으며 [5-8], BERT 출력 확률 변환을 위한 Masked LM (MMLM) 스코어링 [5, 6, 38], 스무딩 기법 [33, 39], 순방향/역방향/양방향 LM 조합을 통한 시퀀스 확률 근사 [40] 등의 선행 연구가 있습니다.

## 🛠️ Methodology

1. **단방향 LM 결합:**
    * 각 단방향 LM의 훈련 목표는 다음 수식으로 주어지는 로그 퍼플렉서티(PPL)를 최소화하는 것입니다.
        $$\text{log}_2\text{PPL}=-\frac{1}{T}\sum_{t=1}^{T}\text{log}_2P(w_t|w_{1:t-1})$$
    * ASR의 `n`-best 재채점을 위해 여러 LM을 결합할 때, 각 LM의 로그 확률에 스케일링 계수 $\lambda_k$를 곱하여 음향 모델(AM) 점수와 선형적으로 합산합니다.
        $$\text{AMScore}+\sum_{k=1}^{K}\lambda_k \text{log}P^{(k)}(w_{1:T})$$
    * 스케일링 계수 $\lambda_k$는 Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [37]를 사용하여 개발 세트 WER을 최소화하도록 최적화됩니다.
2. **양방향 LM 출력 확률 변환 (BERT용):**
    * **문제:** BERT와 같은 양방향 LM은 $P(w_t | w_1, ..., w_{t-1}, w_{t+1}, ..., w_T)$와 같이 현재 토큰을 제외한 모든 토큰에 조건화된 확률을 출력합니다. 이러한 확률들을 단순히 곱하는 것 ($\Lambda$로 정의)은 유효한 문장 사전 확률 $P(w_{1:T})$이 아닙니다.
    * **제안된 변환 (정확한 방법):** 조건부 확률의 정의에 기반하여 $P(w_{1:T})$는 다음과 같이 재귀적으로 계산될 수 있습니다.
        $$P(w_{1:T}) = [ \Lambda \cdot P(w_{2:T}) \cdot P(w_1, w_{3:T}) \cdots P(w_{1:T-1}) ]^{\frac{1}{T}}$$
        여기서 $\Lambda = P(w_1|w_{2:T})P(w_2|w_1,w_{3:T})...P(w_T|w_{1:T-1})$입니다. 각 $P(w_{1:t-1}, w_{t+1:T})$ 항은 $w_{1:T}$에서 $t$번째 토큰이 제거된 문자열의 사전 확률입니다.
    * **효율적인 계산:** 수식 (7)의 계산은 $T^2$ 항을 포함하여 비효율적입니다. Fig. 2와 같은 유한 상태 오토마타 형태의 효율적인 계산 절차를 통해 계산량을 $0.5T^2 + 1.5T - 1$로 줄였지만, $T$가 클 때는 여전히 비실용적입니다.
    * **실용적인 근사:** 계산 비용을 줄이기 위해, 수식 (6)의 $T$개 항목 중 $M$개 ($1 \le M \le T$)만을 선택하는 근사 방법을 제안합니다. 예를 들어, $M=1$은 현재 문장 내에서 미래 토큰만 보거나 과거 토큰만 보는 단방향 LM과 유사하게 BERT를 사용하는 것에 해당합니다.
    * **스무딩 (Temperature Softmax):** 양방향 LM의 출력 분포가 과도하게 첨예할 경우, 온도 계수 $\alpha < 1$를 사용하여 스무딩합니다.
        $$\text{TempSoftmax}(z)_i = \frac{\text{exp}(\alpha z_i)}{\sum_j \text{exp}(\alpha z_j)}$$
3. **실험 설정:**
    * **데이터:** AMI 코퍼스 (911k 단어 토큰) 및 Switchboard/Fisher (SWB+Fisher, 27M 단어 토큰) 데이터셋을 사용했습니다.
    * **음향 모델:** 잔여 연결(residual connections)이 있는 Factorised Time-Delay Neural Network (TDNN) [41]을 lattice-free MMI [43] 기준으로 훈련했습니다.
    * **재채점:** 4-gram 모델로 생성된 100-best 리스트를 다양한 LM으로 재채점했습니다.
    * **LM 훈련 절차:**
        * 처음부터 훈련된 LM (FNN, LSTM, Transformer): 도메인 내 훈련 데이터만 사용하여 SGD로 최적화.
        * 사전 훈련된 LM (GPT, GPT-2, BERT): 도메인 내 텍스트 데이터에 대해 Adam 최적화 [47]를 사용하여 3 epochs 동안 미세 조정. BERT의 경우, 좌측 문맥은 이전 문장의 재채점된 1-best 리스트를 사용하고, 우측 문맥은 현재 문장의 1-best 가설 또는 참조 전사(reference transcriptions)를 사용합니다.

## 📊 Results

* **처음부터 훈련된 LM 결과 (표 1):**
  * AMI AEval 세트에서 LSTM LM이 4-gram 대비 2.3% 절대 WER 감소를 보였습니다.
  * FNN, LSTM, Transformer LM의 조합(F⊕L⊕T)은 AMI AEval에서 17.7%, SWB에서 6.5%, CH에서 13.5% WER을 달성하여 개별 LM보다 우수한 성능을 보였습니다.
* **GPT 및 GPT-2 결과 (표 2):**
  * **미세 조정 없는 사전 훈련된 GPT-2 (24 블록):** AMI AEval에서 17.3% WER을 기록하여 F⊕L⊕T(17.7%)를 능가했습니다.
  * **미세 조정된 GPT:** AMI AEval에서 16.0% WER을 달성하여 F⊕L⊕T 대비 1.7% 절대 WER 감소를 보였습니다.
  * **미세 조정된 GPT-2 (24 블록):** AMI AEval에서 15.7% WER을 달성하여 F⊕L⊕T 대비 2.0% 절대 WER 감소를 보였습니다.
  * **미세 조정된 GPT와 24 블록 GPT-2의 조합(GPT⊕GPT-2):** AMI AEval에서 15.6%, SWB에서 6.1%, CH에서 12.7% WER을 달성하여 단방향 모델 중 최고 성능을 기록했습니다. 이는 F⊕L⊕T 대비 최대 12%의 상대적 WER 감소에 해당합니다.
* **BERT 결과 (표 3 및 4):**
  * **변환 방법 비교 (문장 외 문맥 없음, 표 3):**
    * 미세 조정 후, 제안된 방법("Ours", $M=1, \alpha=0.7$)은 AMI AEval에서 17.9% WER을 기록하여 MMLM($\alpha=0.7$)의 18.1%보다 우수했습니다.
    * $M=2$를 사용한 제안된 방법은 AMI AEval에서 17.7% WER로 추가 개선을 보였습니다.
  * **문맥 활용 효과 (표 4):**
    * 제안된 방법($M=1, \alpha=0.7$)에서 좌측 50토큰, 우측 20토큰 문맥을 사용했을 때 AMI AEval에서 17.0% WER을 달성하여, 문맥을 많이 사용한 MMLM(17.5%)보다 우수했습니다.
    * $M=2$를 사용한 제안된 방법은 좌측 50토큰, 우측 20토큰 문맥에서 16.9% WER을 기록했습니다.
    * 우측 문맥으로 참조 전사(reference transcriptions)를 사용했을 때($M=2$의 50LC, 20RC†) AMI AEval에서 16.7% WER로 가장 낮은 BERT 단독 성능을 보였습니다.
* **GPT⊕GPT-2⊕BERT 결합 (표 4):**
  * 미세 조정된 GPT, 24 블록 GPT-2, 그리고 제안된 $M=2$ 변환 방법을 사용한 BERT를 결합했을 때, AMI AEval에서 15.5% WER을 달성했습니다. 이는 GPT⊕GPT-2 조합 대비 0.1%의 추가적인 절대 WER 감소이며, 전체 실험에서 가장 낮은 WER입니다.

## 🧠 Insights & Discussion

* **사전 훈련 LM의 강력함:** 대규모 텍스트 데이터에 사전 훈련된 LM(GPT, GPT-2)을 소규모 도메인 내 데이터로 미세 조정하는 것만으로도, 처음부터 훈련된 LM보다 훨씬 뛰어난 ASR 성능을 달성할 수 있음을 입증했습니다. 이는 ASR 분야에서 전이 학습(transfer learning) 패러다임의 강력한 가능성을 시사합니다.
* **모델 간 상보성:** GPT 및 GPT-2와 같은 단방향 LM은 서로 보완적이며, 이를 결합하면 WER이 더욱 감소합니다. 나아가, 양방향 LM인 BERT 역시 단방향 LM과 상보적 관계를 가지며, 세 모델을 모두 결합했을 때 추가적인 성능 향상을 가져왔습니다. 이는 다양한 아키텍처와 훈련 전략을 가진 LM들의 조합이 ASR 성능을 극대화하는 데 중요함을 보여줍니다.
* **양방향 LM 처리의 중요성:** BERT와 같은 양방향 LM의 출력을 ASR에 필요한 유효한 문장 사전 확률로 변환하는 제안된 수학적으로 정확한 방법은 BERT를 ASR에 효과적으로 활용하는 데 필수적입니다. 이 방법은 미세 조정 후에 기존 MMLM 스코어링 방식보다 일관되게 우수한 성능을 보입니다.
* **문맥 길이의 활용:** 사전 훈련된 모델은 처음부터 훈련된 LM보다 훨씬 긴 문맥(예: GPT의 경우 180 토큰)을 효과적으로 활용하여 퍼플렉서티 및 WER을 낮출 수 있습니다.
* **계산 비용 문제:** 대규모 사전 훈련 모델은 재채점 시간이 상당히 길다는 단점이 있습니다. 이는 추론 과정에서 최적화 방법론의 필요성을 제기합니다.
* **근사 방법의 실용성:** 제안된 재귀적 변환의 실용적인 근사 ($M < T$)는 계산 비용과 문맥 활용 사이의 합리적인 균형점을 제공하며, 특히 $M=2$가 $M=1$보다 더 나은 결과를 보여주었습니다.
* **향후 연구 방향:** 양방향 LM에서 디코딩 시 참조 전사 없이 우측 문맥의 품질을 향상시킬 수 있는 방법론에 대한 연구가 필요합니다.

## 📌 TL;DR

이 논문은 ASR 시스템에서 GPT, GPT-2, BERT와 같은 강력한 사전 훈련된 언어 모델(LM)을 효과적으로 적용하는 방법을 다룹니다. 특히, BERT와 같은 양방향 LM의 출력을 ASR에 필요한 유효한 문장 사전 확률로 변환하기 위한 수학적으로 정확한 재귀적 방법을 제안하고, 이의 실용적인 근사 방법을 제시합니다. 실험 결과, 미세 조정된 GPT와 GPT-2의 조합은 기존의 처음부터 훈련된 LM 조합보다 최대 12%의 상대적 WER 감소를 달성했으며, 제안된 변환 방법을 적용한 BERT는 ASR 성능을 추가로 향상시켰습니다. 궁극적으로 GPT, GPT-2, BERT를 모두 결합했을 때 가장 낮은 WER을 기록하며, 단방향 및 양방향 모델의 상보성을 성공적으로 입증했습니다.
