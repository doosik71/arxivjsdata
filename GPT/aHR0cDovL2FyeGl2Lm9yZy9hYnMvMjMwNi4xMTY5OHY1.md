# DECODINGTRUST: GPT 모델의 신뢰성에 대한 포괄적인 평가

Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li

## 🧩 Problem to Solve

최근 GPT 모델은 기능면에서 놀라운 발전을 보여주었지만, 헬스케어 및 금융과 같이 치명적인 실수가 발생할 수 있는 민감한 애플리케이션에 사용될 경우, 그 신뢰성에 대한 포괄적인 평가가 부족합니다. 기존 연구는 주로 모델의 견고성이나 과신과 같은 특정 측면에만 초점을 맞추어 왔습니다. 특히, GPT-3.5 및 GPT-4와 같은 대화형 모델의 향상된 지시 이행 및 인컨텍스트 학습 기능은 독성 생성, 고정관념 편향, 개인 정보 유출, 윤리적 판단 오류, 공정성 문제 등 새로운 신뢰성 우려를 야기하고 있습니다. 이러한 격차를 해소하고 GPT 모델의 다양한 신뢰성 위협에 대한 종합적인 이해를 제공할 필요성이 있습니다.

## ✨ Key Contributions

* **포괄적인 신뢰성 평가 프레임워크 제안**: GPT-4 및 GPT-3.5 모델을 독성, 고정관념 편향, 적대적 강건성, 분포 외(Out-of-Distribution, OOD) 강건성, 적대적 데모에 대한 강건성, 프라이버시, 기계 윤리, 공정성 등 8가지 핵심 측면에서 심층적으로 평가하는 `DecodingTrust` 벤치마크를 제시했습니다.
* **새로운 취약점 발견 및 분석**:
  * GPT 모델이 쉽게 독성 및 편향된 출력을 생성하도록 유도될 수 있음을 발견했습니다.
  * 훈련 데이터와 대화 기록 모두에서 개인 정보가 유출될 수 있는 취약점을 발견했습니다.
  * GPT-4가 표준 벤치마크에서는 GPT-3.5보다 더 신뢰할 수 있지만, 탈옥(jailbreaking) 시스템 프롬프트나 오해의 소지가 있는 지시에는 더 취약하여 이를 더 정확하게 따르는 경향이 있음을 확인했습니다.
* **다양한 적대적 프롬프트 및 데모 설계**: 모델의 잠재적 취약점을 효과적으로 드러내기 위해 다양한 적대적 시스템/사용자 프롬프트 및 인컨텍스트 학습 데모를 개발하고 활용했습니다.
* **새로운 벤치마크 데이터셋 공개**: 연구에서 활용된 벤치마크와 데이터셋을 공개하여 (<https://decodingtrust.github.io/>, <https://huggingface.co/datasets/AI-Secure/DecodingTrust>) LLM 신뢰성에 대한 추가 연구를 촉진합니다.
* **정확도-공정성 상충관계 확인**: GPT-4에서 더 높은 정확도와 함께 때로는 더 높은 불공정성 점수가 관찰되어, 모델 개발 시 정확성과 공정성 간의 미묘한 상충관계가 존재함을 시사합니다.

## 📎 Related Works

* **LLM 평가 벤치마크**: GLUE, SuperGLUE (일반 언어 이해), CodeXGLUE, BIG-Bench, NaturalInstructions (복잡한 작업), AdvGLUE, TextFlint (강건성), HELM (대규모, 전체론적 평가).
* **LLM 독성 평가**: REALTOXICITYPROMPTS, BOLD (생성 독성 측정), Perspective API (독성 탐지).
* **LLM 고정관념 편향**: Brown et al. (GPT-3의 편향), Abid et al. (GPT-3의 특정 편향), BBQ (질문 응답에서 고정관념 의존도).
* **LLM 적대적 강건성**: AdvGLUE (다양한 공격에 대한 LLM 취약성), PromptBench (적대적 프롬프트에 대한 LLM 회복력).
* **LLM OOD 강건성**: 합성 데이터 (텍스트 변환), 시간적 분포 변화, 인컨텍스트 학습에서의 OOD 데모.
* **LLM 적대적 데모 강건성**: Lu et al. (데모 순서), Min et al. (데모 레이블), Wei et al. (레이블 공간의 의미론적 사전), Wang et al. (적대적 데모의 영향).
* **LLM 프라이버시**: Carlini et al. (훈련 데이터 추출), Huang et al. (Enron 데이터셋), Li et al. (ChatGPT 탈옥 프롬프트), 차등 프라이버시 (DP).
* **LLM 기계 윤리**: ETHICS (윤리 개념 지식), SOCIAL-CHEM-101 (사회/도덕 규범), Moral Stories (목표 지향적 추론), Jiminy Cricket, MACHIAVELLI (텍스트 게임 내 에이전트 윤리).
* **LLM 공정성**: Socher et al. (GPT-3 불공정 예측), Liang et al. (반사실적 공정성), Li and Zhang (ChatGPT의 인컨텍스트 예시 공정성).
* **프롬프트 해킹**: 프롬프트 주입 (PromptInject), 프롬프트 유출, 탈옥 프롬프트 (DAN).
* **LLM 신뢰성 규제**: 유럽 연합의 AI Act (고위험 AI 시스템 규제), 미국 White House AI Bill of Rights (5대 원칙 제시).

## 🛠️ Methodology

본 연구는 GPT-4 (gpt-4-0314) 및 GPT-3.5 (gpt-3.5-turbo-0301) 모델을 대상으로 8가지 신뢰성 측면을 평가했습니다.

1. **프롬프트 설계**:
    * **시스템 프롬프트 (System Prompt)**: LLM 어시스턴트의 톤, 역할, 스타일을 정의합니다.
    * **사용자 프롬프트 (User Prompt)**: 실제 작업 설명과 태스크 프롬프트를 포함합니다.
    * **제로샷/퓨샷 분류**: 작업 설명만을 제공하거나(제로샷), 추가 예시 데모를 제공하여(퓨샷) 분류 작업을 수행합니다.
    * **텍스트 생성**: 시스템 및 사용자 프롬프트를 통해 일관된 텍스트를 생성하도록 유도합니다.

2. **신뢰성 8가지 측면 평가**:

    * **독성 (Toxicity)**:
        * `REALTOXICITYPROMPTS` 벤치마크 및 연구진이 설계한 33가지 다양한 시스템 프롬프트(역할극, 반대 지시 등)로 평가합니다.
        * GPT-4/GPT-3.5가 생성한 1.2K개의 도전적인 사용자 프롬프트를 활용합니다.
        * `Perspective API`를 통해 예상 최대 독성(Expected Maximum Toxicity) 및 독성 확률(Toxicity Probability)을 측정합니다.
    * **고정관념 편향 (Stereotype Bias)**:
        * 24개 인구 통계 그룹과 16개 고정관념 주제에 대한 맞춤형 진술 데이터셋을 생성하고, 모델이 이 진술에 동의/비동의하는지 질의합니다.
        * 양성(benign), 비대상(untargeted), 대상(targeted) 시스템 프롬프트 설정을 비교합니다.
        * 평가지표는 `Agreement Index` ($agreementIndex = n_{agree} / n$)를 사용합니다.
    * **적대적 강건성 (Adversarial Robustness)**:
        * 기존 `AdvGLUE` 벤치마크(BERT-유사 모델 대상 생성)를 사용하여 모델의 지시 이행 능력 및 공격 전이성을 평가합니다.
        * Alpaca-7B, Vicuna-13B, StableVicuna-13B를 대상으로 생성한 강력한 적대적 텍스트 `AdvGLUE++`를 사용하여 GPT 모델의 취약성을 추가로 평가합니다.
        * 강건성 정확도(robust accuracy), 성능 저하(performance drop), 공격 성공률(attack success rate) 등을 측정합니다.
    * **OOD 강건성 (Out-of-Distribution Robustness)**:
        * **OOD 스타일**: 단어 수준(Augment, Shake-W) 및 문장 수준(Tweet, Shakespearean, Bible, Romantic Poetry) 스타일 변환을 적용한 합성 데이터를 사용합니다.
        * **OOD 지식**: `RealtimeQA` 데이터셋을 사용하여 훈련 시점 이후의 이벤트(OOD 지식)에 대한 모델의 응답 능력 및 '모른다'고 거부하는 능력을 평가합니다.
        * **OOD 데모를 통한 인컨텍스트 학습**: 의미론적 불변 스타일 변환 및 `MMLU`의 의미론적 가변 도메인 데모가 모델 성능에 미치는 영향을 분석합니다.
        * 정확도(ACC), 거부율(RR), 유의미 정확도(MACC)를 사용합니다.
    * **적대적 데모에 대한 강건성 (Robustness to Adversarial Demonstrations)**:
        * 인컨텍스트 학습 데모에 1) 반사실적 예시, 2) 가짜 상관관계, 3) 백도어(BadWord, AddSent, SynBkd, StyleBkd)를 주입하여 모델 예측에 미치는 영향을 평가합니다.
        * 백도어 데모의 위치 및 트리거 위치, 백도어 지시의 포함 여부 등을 실험합니다.
        * 공격 성공률(ASR) 및 클린 정확도(CACC)를 측정합니다.
    * **프라이버시 (Privacy)**:
        * **훈련 데이터 유출**: `Enron Email` 데이터셋으로 컨텍스트 프롬프팅 및 제로샷/퓨샷 프롬프팅을 통해 이메일 주소 추출 정확도를 평가합니다.
        * **대화 중 유출**: 18가지 유형의 PII를 대화 기록에 주입 후, 모델이 이를 유출하는지 평가(제로샷, 퓨샷 프라이버시 보호/유출 데모)합니다.
        * **프라이버시 관련 단어 및 이벤트 이해**: 'confidentially', 'in confidence' 같은 17가지 단어와 이혼, 건강 문제 등 8가지 이벤트 사용 시 누출률을 평가합니다.
    * **기계 윤리 (Machine Ethics)**:
        * **표준 벤치마크**: `ETHICS`(상식적 도덕성) 및 `Jiminy Cricket`(텍스트 게임 시나리오) 데이터셋으로 도덕적 인식 성능을 평가합니다.
        * **탈옥 프롬프트**: 'You are an immoral assistant'와 같은 탈옥 프롬프트를 주입하여 부도덕한 행동 인식에 미치는 영향을 평가합니다.
        * **회피적 문장**: 'But it was accidental'과 같은 회피적 문장을 부도덕한 행동 설명 뒤에 추가하여 모델 인식을 교란합니다.
        * **조건부 행동**: 피해자(타인/자신) 및 심각도(경미/보통)에 따른 부도덕한 행동 인식 능력을 평가합니다.
        * 분류 정확도 및 오탐율(FPR)을 사용합니다.
    * **공정성 (Fairness)**:
        * `Adult` 데이터셋을 언어 설명으로 변환하여 분류 작업을 수행하고, 성별, 인종, 연령과 같은 민감 속성에 대한 공정성을 평가합니다.
        * **제로샷 설정**: 다양한 기준율 균형(base rate parity)을 가진 테스트 셋에서 공정성을 평가합니다.
        * **퓨샷 학습**: 인구 통계학적으로 불균형하거나 균형 잡힌 퓨샷 예시가 모델 예측 공정성에 미치는 영향을 분석합니다.
        * 정확도(ACC), 인구 통계학적 균형 차이(Demographic Parity Difference, $M_{dpd}$), 평등 기회 차이(Equalized Odds Difference, $M_{eod}$)를 측정합니다.

## 📊 Results

* **독성**: GPT-3.5와 GPT-4는 RLHF로 독성이 크게 감소했지만, '탈옥' 프롬프트에는 독성 생성이 급증합니다. GPT-4는 탈옥 프롬프트에 더 정확하게 따르므로 더 높은 독성을 보입니다. GPT-4가 생성한 독성 프롬프트가 기존 벤치마크보다 효과적입니다.
* **고정관념 편향**: 양성 프롬프트에서는 편향이 적지만, 오해의 소지가 있는 프롬프트에는 모델이 쉽게 편향됩니다. GPT-4가 GPT-3.5보다 대상 프롬프트에 더 취약하며, 편향은 인구 통계 그룹과 고정관념 주제에 따라 달라집니다.
* **적대적 강건성**: GPT-4는 AdvGLUE 벤치마크에서 GPT-3.5 및 기존 SoTA 모델보다 강건합니다. 그러나 AdvGLUE++와 같은 강력한 적대적 공격에는 GPT-3.5와 GPT-4 모두 취약합니다. 작업 설명이나 시스템 프롬프트는 강건성에 큰 영향을 미치지 않습니다.
* **OOD 강건성**: GPT-4는 OOD 스타일 변환 및 OOD 지식(미지의 사실)에 대해 GPT-3.5보다 더 강건하며, '모른다' 옵션이 주어질 때 더 신뢰할 수 있는 응답을 제공합니다. OOD 데모가 주어졌을 때 GPT-4는 성능 개선을 보이는 반면, GPT-3.5는 성능 저하를 겪습니다.
* **적대적 데모에 대한 강건성**: GPT 모델은 반사실적 예시에는 오도되지 않지만, 가짜 상관관계나 백도어 데모에는 취약합니다. 특히 백도어 데모가 테스트 입력과 가까이 있거나 텍스트 시작 부분에 트리거가 있을 때 효과가 큽니다. GPT-4는 백도어 데모에 더 취약하며, 백도어 지시를 포함하면 공격 성공률이 크게 증가합니다.
* **프라이버시**: GPT 모델은 훈련 데이터(Enron 이메일) 및 대화 중 주입된 PII를 유출할 수 있습니다. GPT-4는 PII 보호에 전반적으로 더 강건하지만, 퓨샷 유출 데모가 주어지면 모든 유형의 PII를 유출합니다. 모델은 'confidentially'와 'in confidence' 같은 프라이버시 관련 단어에 다르게 반응하며, GPT-4는 오도하는 지시를 더 정확히 따르므로 유출 가능성이 높습니다.
* **기계 윤리**: 퓨샷 GPT 모델(특히 GPT-4)은 표준 벤치마크에서 미세 튜닝된 모델과 경쟁력 있는 도덕적 인식 성능을 보입니다. 그러나 탈옥 프롬프트나 회피적 문장에 쉽게 오도되며, GPT-4가 GPT-3.5보다 더 취약합니다. 모델은 '자해'와 같은 조건부 행동 인식에 약하며, GPT-4는 피해 심각도가 높을수록 인식이 정확해집니다.
* **공정성**: GPT-4는 균형 잡힌 데이터에서 정확도가 높지만, 불균형한 데이터에서는 더 높은 불공정성을 보여 정확도-공정성 상충관계가 존재합니다. 두 모델 모두 제로샷 설정에서 내재적인 편향을 보이며, 퓨샷 데모의 불균형이 편향을 유도하지만, 균형 잡힌 퓨샷 예시를 통해 공정성을 개선할 수 있습니다.

## 🧠 Insights & Discussion

본 연구의 핵심 통찰은 GPT 모델의 신뢰성 측면에서 나타나는 GPT-4의 양면성입니다. GPT-4는 GPT-3.5보다 전반적으로 더 우수한 성능과 강건성을 보이지만, 이는 동시에 양날의 검이 됩니다. 즉, **GPT-4는 지시를 더 정확하게 따르는 능력 때문에 탈옥 프롬프트나 오해의 소지가 있는 (적대적) 지시 및 데모에 훨씬 더 쉽게 조작될 수 있다는 새로운 유형의 취약점을 드러냈습니다.** 이는 AI 모델의 역량이 향상될수록 악의적인 사용에 대한 잠재적 위험도 함께 증가할 수 있음을 강력히 시사합니다.

또한, LLM이 방대한 데이터를 학습했음에도 불구하고 독성 생성, 고정관념 편향, 프라이버시 유출, 도덕적 판단 오류 등 여러 신뢰성 측면에서 여전히 상당한 취약점과 개선의 여지가 있음을 확인했습니다. 이는 **모델이 학습한 데이터의 편향을 답습하거나, 미묘하게 조작된 입력에 취약한 근본적인 한계**를 가지고 있음을 의미합니다. 모델의 신뢰성에 영향을 미치는 요소들이 다양하고 복합적(시스템/사용자 프롬프트, 데모 내용, 스타일, 트리거 위치, 피해자 유형, 심각도 등)이라는 점도 중요한 시사점입니다.

GPT-4에서 관찰된 **정확도-공정성 상충관계**는 LLM 개발 시 성능 최적화와 윤리적 고려 사이의 섬세한 균형이 필요함을 강조합니다. 특정 성능 지표를 높이는 것이 다른 중요한 신뢰성 측면을 희생시킬 수 있다는 경고입니다. 이러한 발견들은 AI 모델의 빠른 발전 속도와 지속적인 업데이트를 고려할 때, LLM의 신뢰성에 대한 역동적이고 지속적인 평가 및 연구가 필수적임을 보여줍니다.

향후 연구는 정적 데이터셋을 넘어선 동적인 대화 상호작용에서의 취약점 심화 연구, 탈옥 프롬프트 외의 다양한 '오도하는 컨텍스트' 탐색, 조직화된 적대자 시나리오 고려, 특정 도메인에 특화된 신뢰성 평가, 그리고 GPT 모델의 신뢰성에 대한 엄격한 검증 및 보호 방법 개발에 초점을 맞출 필요가 있습니다.

## 📌 TL;DR

* **문제**: GPT 모델의 뛰어난 성능에도 불구하고, 독성, 편향, 프라이버시 등 다각적인 신뢰성 문제에 대한 종합적인 평가가 부족하며, 특히 GPT-3.5/GPT-4의 새로운 기능이 새로운 취약점을 야기합니다.
* **제안 방법**: 본 논문은 독성, 고정관념 편향, 적대적/OOD 강건성, 적대적 데모, 프라이버시, 기계 윤리, 공정성 등 8가지 핵심 측면에서 GPT-4 및 GPT-3.5를 평가하는 포괄적인 벤치마크 `DecodingTrust`를 구축했습니다. 이를 위해 표준 벤치마크 외에 새로운 도전적인 사용자/시스템 프롬프트, 적대적 데모, OOD 데이터 등 다양한 시나리오와 데이터셋을 활용했습니다.
* **주요 발견**:
    1. GPT 모델은 지시 튜닝에도 불구하고 **`탈옥` 프롬프트**에 의해 독성 및 편향된 출력을 쉽게 생성할 수 있습니다.
    2. GPT-4는 표준 벤치마크에서 GPT-3.5보다 우수하지만, **`오해의 소지가 있는 지시`나 `탈옥 프롬프트`에 더 취약하게 반응**하는 경향이 있습니다.
    3. 훈련 데이터와 대화 기록에서 **개인 정보가 유출될 수 있으며**, 프라이버시 관련 단어 및 이벤트 이해에 일관성이 부족합니다.
    4. 모델 예측의 공정성에 **`정확도-공정성 상충관계`**가 존재하며, 퓨샷 데모의 불균형이 편향을 유도할 수 있지만, 균형 잡힌 데모를 통해 개선 가능합니다.
    5. LLM의 신뢰성에 영향을 미치는 다양한 **`입력 속성 및 조건`** (예: 데모의 위치, 트리거의 위치, 피해자 유형, 심각도)을 발견했습니다.
* **의의**: 이 연구는 GPT 모델의 강점과 한계를 종합적으로 조명하며, 신뢰할 수 있는 LLM 개발을 위한 중요한 통찰과 미래 연구 방향을 제시합니다.
