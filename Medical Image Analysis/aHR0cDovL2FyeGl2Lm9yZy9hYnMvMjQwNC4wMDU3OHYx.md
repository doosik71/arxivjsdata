# M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models

Fan Bai, Yuxin Du, Tiejun Huang, Max Q.-H. Meng, Bo Zhao

## 🧩 Problem to Solve

최근 멀티모달 대규모 언어 모델(MLLM)은 의료 영상 분석에서 뛰어난 성능을 보였지만, 대부분 2D 의료 영상에 집중되어 3D 의료 영상은 충분히 탐색되지 않았습니다. CT, MRI와 같은 3D 영상은 더 풍부한 공간 정보를 담고 있음에도 불구하고, 기존 MLLM 방식은 3D 영상에 대해 슬라이스별 분석 등 높은 비용이 들거나 전혀 처리하지 못하는 한계가 있습니다. 이 논문은 3D 의료 영상 분석 분야를 MLLM으로 발전시키는 것을 목표로 합니다.

## ✨ Key Contributions

- **대규모 3D 멀티모달 의료 데이터셋 `M3D-Data` 구축:** 120K개의 영상-텍스트 쌍과 662K개의 지시-응답(instruction-response) 쌍으로 구성되며, 영상-텍스트 검색, 보고서 생성, 시각 질문 답변(VQA), 위치 파악, 분할 등 다양한 3D 의료 작업을 지원합니다.
- **다목적 3D MLLM `M3D-LaMed` 제안:** 3D 의료 영상 분석을 위한 다재다능한 모델로, 3D 영상에 대한 시각 언어 위치 파악 및 분할(referring expression segmentation) 작업을 처음으로 통합합니다.
- **종합 3D 멀티모달 벤치마크 `M3D-Bench` 개발:** 8가지 작업을 포괄하는 자동 평가를 가능하게 하는 포괄적인 벤치마크를 제공합니다.
- **강력한 성능 입증:** 제안된 방법은 기존 솔루션을 능가하는 강력한 3D 의료 영상 분석 모델임을 종합적인 평가를 통해 입증했습니다.

## 📎 Related Works

- **의료 멀티모달 데이터셋:**
  - `VQA-Med`, `MIMIC-CXR` 등 초기 연구는 개인 정보 보호 문제로 대규모 데이터셋 구축에 어려움을 겪었습니다.
  - `PMC-OA`는 2D 영상에 초점을 맞춰 1.6M개의 영상-텍스트 쌍을 웹 크롤링으로 수집했습니다.
  - `MedMD`의 `RP3D`는 51K개의 3D 영상-텍스트 쌍과 142K개의 LLM 생성 VQA 데이터를 포함합니다.
  - 본 연구의 `M3D-Data`는 현존하는 가장 큰 3D 의료 멀티모달 데이터셋으로, 가장 많은 종류의 태스크를 지원합니다 (표 1 참고).
- **의료 MLLM:**
  - `LLaVA-Med`, `Med-PaLM M`, `Med-Flamingo` 등은 기존 2D 오픈소스 MLLM을 의료 데이터로 파인튜닝한 모델입니다.
  - `PMC-VQA`는 2D 영상에 한정되지만, 대규모 데이터셋을 활용해 의료 MLLM을 처음부터 훈련했습니다.
  - `RadFM`은 2D 및 3D 영상을 지원하지만, 주로 텍스트 생성(VQA)에 사용되며 성능이 좋지 않습니다.
  - `M3D-LaMed`는 3D 의료 영상 분석을 위한 일반화된 MLLM으로, 보고서 생성 및 VQA뿐만 아니라 3D 의료 영상에서 시각 언어 위치 파악 및 분할과 같은 새로운 시각 작업을 개척합니다.

## 🛠️ Methodology

- **`M3D-Data` 구축:**
  - **영상-텍스트 쌍 데이터 (M3D-Cap):** Radiopaedia와 같은 공개된 전문 의료 웹사이트에서 120K개의 3D CT 영상-진단 보고서 쌍을 수집합니다. 영상-텍스트 검색 및 보고서 생성에 활용됩니다.
  - **지시-응답 쌍 데이터:**
    - **VQA 데이터 (M3D-VQA):** 5가지 핵심 주제(평면, 시기, 장기, 이상 징후, 위치)에 대한 객관식 질문을 LLM(Qwen-72B)이 진단 보고서에서 프롬프트 기반으로 생성합니다. 자체 필터링 및 전문가/LLM 검증(99.4% 통과율)을 거쳐 509K개의 쌍을 생성합니다.
    - **위치 파악 및 분할 데이터 (M3D-RefSeg, M3D-Seg):** 영상-마스크-텍스트 3중항 형식으로 통합됩니다.
      - **레이블 기반 지시 데이터:** 공개 분할 데이터셋의 영상-마스크 쌍에서 레이블 템플릿을 사용하여 생성됩니다.
      - **정의 기반 지시 데이터:** LLM이 생성한 용어 사전과 정의 템플릿을 사용하여 구축됩니다.
      - **주석 처리된 지시 데이터:** 참조 영역에 대한 텍스트 설명을 전문가가 직접 주석을 달아 생성합니다.
- **`M3D-LaMed` 모델 아키텍처:**
  - **3D 영상 인코더:** 입력 3D 영상 $I \in \mathbb{R}^{C \times D \times H \times W}$를 특징 임베딩 $v = E_{img}(I) \in \mathbb{R}^{n \times d}$로 변환하기 위해 3D Vision Transformer (3D ViT)를 사용합니다.
  - **3D Perceiver:** 높은 차원의 3D 영상 토큰 수를 줄이고 LLM 입력에 맞춰 임베딩 차원을 조정하는 효율적인 3D 공간 풀링 퍼시버(spatial pooling perceiver)를 제안합니다. 이는 공간 정보를 보존하면서 계산 비용을 완화합니다.
  - **LLM:** LLaMA-2-7B 모델을 기본 LLM으로 활용합니다.
  - **프롬프트 가능한 분할 모듈:** 출력 토큰에 `[SEG]` 토큰이 있을 경우, 이 토큰의 마지막 레이어 임베딩을 특징으로 추출하여 MLP를 통해 SegVol [13]과 같은 분할 모듈을 구동하여 3D 마스크를 생성합니다.
- **시각 인코더 사전 학습:**
  - `M3D-Cap` 데이터셋에 대해 CLIP [50]과 유사한 교차 모달 대조 학습 손실을 사용하여 3D 영상 인코더를 처음부터 사전 학습합니다. 텍스트 인코더는 사전 학습된 BERT [10]로 초기화됩니다.
- **MLLM 학습 (엔드-투-엔드 튜닝):**
  - **1단계:** 영상 인코더와 LLM을 고정하고, 3D 퍼시버만 영상-텍스트 쌍을 사용하여 파인튜닝합니다.
  - **2단계:** 영상 인코더, 3D 퍼시버, LLM, 분할 모듈 전체를 지시 데이터를 사용하여 파인튜닝합니다. `[SEG]` 토큰이 있을 경우 Dice 및 BCE 손실을 사용하여 분할 훈련을 수행합니다.
  - LLM 파인튜닝에는 LoRA [20] 전략을 사용하여 파라미터 효율적인 튜닝을 수행합니다.

## 📊 Results

- **영상-텍스트 검색:** 다양한 난이도에서 2D `PMC-CLIP` 모델보다 훨씬 뛰어난 성능을 보였습니다. (예: 가장 간단한 설정에서 IR R@10 54% 향상).
- **보고서 생성:** `RadFM`보다 BLEU 점수 2.92%, LLM 기반 점수 4.17% 향상 등 모든 지표에서 우수했습니다. 퍼시버에 MLP를 사용하는 것이 Linear보다 약간 더 좋은 성능을 보였습니다.
- **VQA (객관식 및 주관식):** 5가지 문제 유형 모두에서 `RadFM`을 크게 능가했습니다. 객관식 VQA의 평균 정확도는 `RadFM` 19.79% 대비 75.78%를 달성했습니다.
- **위치 파악 (REC & REG):** 파인튜닝 시 영상 인코더를 잠금 해제하는 것이 REG(bounding box 생성) 작업의 성능을 29.25% 크게 향상시켰습니다.
- **분할 (SS & RES):** 의미론적 분할(SS)에서 `SegVol`을 능가했으며, `SegVol`이 지원하지 않는 참조 표현 분할(RES) 기능을 최초로 달성했습니다.
- **어블레이션 연구:** 영상 사전 학습, 공간 풀링, MLP, 잠금 해제된 영상 인코더 등 모델의 모든 부분이 필수적이며, 영상 사전 학습을 시작점으로 활용하고 파인튜닝 시 영상 인코더를 잠금 해제하는 것이 더 나은 학습 솔루션임을 확인했습니다.
- **OOD(Out-Of-Distribution) 질문 사례 연구:** `M3D-LaMed`는 학습 데이터 범위를 벗어나는 OOD 질문에 대해서도 강력한 일반화 능력을 보여주었습니다. 이는 LLM의 원래 지식을 보존하는 LoRA 파인튜닝 전략 덕분입니다.

## 🧠 Insights & Discussion

- **3D 의료 MLLM의 기반 마련:** `M3D-Data`, `M3D-LaMed`, `M3D-Bench`는 3D 의료 시나리오에서 MLLM이 시각과 언어를 이해하는 강력한 기반을 구축했습니다. 이는 2D에 머물러 있던 의료 MLLM 연구를 3D 영역으로 확장하는 중요한 진전입니다.
- **강력한 일반화 능력:** 경량 LoRA 파인튜닝을 통해 LLM의 원래 지식을 보존함으로써, `M3D-LaMed`는 OOD 질문에 대해서도 뛰어난 일반화 능력을 보였습니다. 이는 강력한 LLM 기반 위에 의료 전문 지식을 효율적으로 통합하여 전문성과 일반화 능력을 동시에 향상시킬 수 있음을 시사합니다.
- **향후 연구 방향:** 현재는 주로 CT 영상에 초점을 맞추고 있지만, 향후 다른 3D 의료 모달리티(예: MRI, PET)로 확장하고, 더 큰 규모의 LLM 또는 다른 분할 백본과의 통합을 탐색할 수 있습니다.

## 📌 TL;DR

- **문제:** 기존 MLLM은 2D 의료 영상에 집중되어 3D 의료 영상의 풍부한 공간 정보를 활용하지 못하며, 관련 데이터셋과 모델, 벤치마크가 부족합니다.
- **방법:** 연구팀은 대규모 3D 멀티모달 의료 데이터셋 `M3D-Data`(120K 영상-텍스트, 662K 지시-응답), 다목적 3D MLLM `M3D-LaMed`(3D spatial pooling perceiver 및 프롬프트 가능한 분할 모듈 포함), 그리고 8가지 태스크를 위한 종합 3D 벤치마크 `M3D-Bench`를 제안했습니다. `M3D-LaMed`는 CLIP 스타일의 사전 학습된 3D 영상 인코더와 LoRA로 파인튜닝된 LLaMA-2-7B를 기반으로 합니다.
- **결과:** `M3D-LaMed`는 영상-텍스트 검색, 보고서 생성, VQA, 위치 파악, 분할을 포함한 8가지 3D 의료 태스크에서 기존 모델들을 크게 능가했으며, OOD 질문에 대해서도 강력한 일반화 능력을 입증하여 3D 의료 MLLM의 견고한 토대를 마련했습니다.
