# Explainable artificial intelligence (XAI) in deep learning-based medical image analysis

Bas H.M. van der Velden, Hugo J. Kuijf, Kenneth G.A. Gilhuijs, Max A. Viergever

## 🧩 Problem to Solve

딥러닝은 의료 영상 분석에서 혁혁한 발전을 이루었지만, 그 복잡하고 불투명한 "블랙박스" 특성으로 인해 의사결정 과정을 이해하기 어렵다는 비판을 받습니다. 특히 의료 분야와 같이 고위험 의사결정이 이루어지는 영역에서는 모델의 편향이나 오류가 환자에게 치명적인 결과를 초래할 수 있으므로, 딥러닝 모델의 의사결정 과정을 설명하고 신뢰를 확보할 수 있는 방법론인 설명 가능한 인공지능(Explainable Artificial Intelligence, XAI)에 대한 필요성이 강력히 대두되고 있습니다. 본 논문은 딥러닝 기반 의료 영상 분석 분야에서 XAI 기술의 현황을 종합적으로 분석하고 체계적으로 분류하는 것을 목표로 합니다.

## ✨ Key Contributions

- 딥러닝 기반 의료 영상 분석에 활용되는 설명 가능한 인공지능(XAI)에 대한 포괄적인 개요를 제공합니다.
- XAI 기술을 분류하기 위한 새로운 기준 프레임워크(모델 기반 vs. 사후 분석, 모델 특화 vs. 모델 독립, 전역 vs. 지역 설명)를 제시합니다.
- 제시된 프레임워크와 해부학적 위치에 따라 의료 영상 분석 분야의 XAI 관련 논문들을 체계적으로 조사하고 분류합니다.
- 컴퓨터 비전에서 유래한 XAI 기술들이 의료 영상 분석 분야의 특수성을 반영하여 어떻게 적용되고 변형되었는지를 상세히 분석합니다.
- XAI 평가 방법론, 현재 XAI에 대한 비판적 관점, 그리고 의료 영상 분석 분야에서 XAI의 미래 기회에 대한 심층적인 논의를 제공합니다.

## 📎 Related Works

- **Adadi and Berrada (2018), Murdoch et al. (2019)**: XAI에 대한 일반적인 설문조사 논문으로, 본 논문의 XAI 프레임워크를 수립하는 데 주요 기반이 되었습니다.
- **Reyes et al. (2020)**: 주로 컴퓨터 비전 분야의 XAI 기술을 다루었으며, 의료 영상 분석 분야에 대한 광범위한 평가가 부족했습니다. 본 설문조사는 이러한 한계점을 보완합니다.
- **Huff et al. (2021)**: 주로 시각적 설명(visual explanation) 예시에 초점을 맞추었으나, 본 설문조사는 비시각적 설명, XAI에 대한 비판, 평가 방법론 등을 포함한 보다 전체적인(holistic) 접근 방식을 취합니다.

## 🛠️ Methodology

본 설문조사는 딥러닝 기반 의료 영상 분석에 XAI를 활용한 논문들을 체계적으로 검토하기 위해 다음 단계를 따랐습니다.

1. **논문 검색 및 선정**:
   - SCOPUS에서 "(explainable deep learning OR interpretable deep learning OR XAI OR interpretable machine learning OR explainable machine learning) AND (medical imaging OR medical image analysis)" 검색어를 사용했습니다.
   - 액티브 러닝 기반의 체계적인 검토 도구를 활용하여 관련성이 높은 논문들을 선별했습니다.
   - 동료들과의 논의 및 스노우볼링(snowballing) 방식을 통해 포함된 논문들의 참고 문헌 및 해당 논문들을 인용한 논문들을 추가로 조사했습니다.
   - 2020년 10월까지 출판된 동료 검토 저널 논문 및 학회 발표 논문들을 포함했습니다.
2. **XAI 프레임워크 정의**:
   - Adadi and Berrada (2018), Murdoch et al. (2019)의 설문조사를 기반으로 XAI 기술을 분류하기 위한 3가지 기준을 제시했습니다.
     - **모델 기반(Model-based) vs. 사후 분석(Post hoc) 설명**: 설명 가능성이 모델 학습 시 내재되는지(모델 기반) 또는 학습된 모델 분석을 통해 얻어지는지(사후 분석)를 구분합니다.
     - **모델 특화(Model-specific) vs. 모델 독립(Model-agnostic) 설명**: 특정 모델 클래스에 한정되는지(모델 특화) 또는 모델의 입력-출력에만 의존하여 모든 모델에 적용 가능한지(모델 독립)를 구분합니다.
     - **설명의 범위 (전역(Global) vs. 지역(Local))**: 모델 전체의 일반적인 관계를 설명하는지(전역) 또는 특정 단일 입력에 대한 의사결정을 설명하는지(지역)를 구분합니다.
3. **논문 분류 및 분석**:
   - 선정된 논문들을 제시된 XAI 프레임워크에 따라 시각적(visual), 텍스트(textual), 사례 기반(example-based) 설명 유형으로 분류했습니다.
   - 각 XAI 기술이 어떤 프레임워크 기준에 해당하는지, 그리고 컴퓨터 비전 기술이 의료 영상 분석에 어떻게 적용되고 변형되었는지를 분석했습니다.
   - 논문들을 설명 방법 및 해부학적 위치에 따라 그룹화하여 분석했습니다.

## 📊 Results

총 223편의 논문이 검토되었으며, 주요 결과는 다음과 같습니다.

- **XAI 프레임워크 분류 경향**:
  - 대부분의 논문은 학습된 모델의 동작을 분석하는 **사후 분석(post hoc) 설명** 방식을 채택했습니다.
  - 설명의 범위는 주로 개별 사례에 대한 의사결정을 설명하는 **지역적(local) 설명**에 집중되었습니다.
  - **모델 특화(model-specific) 방법**과 **모델 독립(model-agnostic) 방법**이 모두 사용되었으나, CNN에 적합한 사후 분석 기술이 많아 모델 특화 방법이 널리 활용되었습니다.
- **주요 설명 기술 유형**:
  - **시각적 설명 (Visual explanation)**: 의료 영상 분석에서 가장 흔하며, saliency map을 통해 의사결정에 중요한 이미지 영역을 시각적으로 강조합니다.
    - 주로 역전파(backpropagation) 기반 접근 방식(CAM, Grad-CAM, LRP, Deep SHAP 등)과 교란(perturbation) 기반 접근 방식(Occlusion sensitivity, LIME, Meaningful Perturbation)이 활용되었습니다.
    - 멀티스케일 CAM, 변분 오토인코더(VAE)를 이용한 병변 대체 교란 등 컴퓨터 비전 기술을 의료 도메인에 맞게 변형한 사례가 많았습니다.
  - **텍스트 설명 (Textual explanation)**: 모델에 텍스트 설명을 추가하여 의료 보고서 생성, 고수준 개념(예: '미세동맥류')과의 연결 등을 수행합니다.
    - 이미지 캡셔닝(Image Captioning) 및 개념 활성화 벡터(TCAV)를 활용한 개념 귀속(concept attribution) 등이 대표적입니다.
  - **사례 기반 설명 (Example-based explanation)**: 현재 분석 중인 데이터와 유사한 훈련 사례를 제시하여 모델의 의사결정을 설명합니다.
    - 트리플렛 네트워크(Triplet Networks), 영향 함수(Influence Functions), 프로토타입(Prototypes), 잠재 공간(latent space)에서의 예시 추출 등이 사용되었습니다.
- **해부학적 위치 및 영상 양식별 분포**:
  - 대부분의 연구는 **흉부(Chest)** 또는 **뇌(Brain)** 영상을 대상으로 했으며, **X-ray** 또는 **MRI** 영상 양식이 주로 사용되었습니다. 이는 일반적인 의료 영상 딥러닝 연구 동향과 일치합니다.

## 🧠 Insights & Discussion

- **XAI 평가의 복잡성**: XAI 기술의 "좋은 설명" 기준을 정의하는 것은 정량적 성능 지표보다 어렵습니다.
  - **응용 기반 평가 (Application-grounded evaluation)**: 실제 응용 환경에서 도메인 전문가가 설명을 평가합니다. 가장 이상적이지만, 비용과 시간이 많이 소요됩니다.
  - **인간 기반 평가 (Human-grounded evaluation)**: 간단한 인간 실험을 통해 비용 효율적으로 설명 품질의 일반적 개념을 얻을 수 있으나, 실제 품질의 대리 평가라는 한계가 있습니다.
  - **기능 기반 평가 (Functionally-grounded evaluation)**: 인간 실험 없이 다른 대리 지표(예: 수동 주석)를 사용합니다. 이미 주석이 존재하는 경우 효율적이나, 주석 획득 자체가 자원 집약적입니다.
- **XAI에 대한 비판**:
  - **불완전한 충실도**: Rudin (2019)은 XAI가 원본 모델의 의사결정을 100% 충실하게 설명하지 못할 수 있음을 지적하며, 고위험 의사결정에는 본질적으로 해석 가능한 모델(예: 프로토타입 네트워크)을 사용할 것을 권고합니다.
  - **설명의 견고성 부족**: Adebayo et al. (2018)은 일부 saliency map 기술이 모델의 학습된 파라미터나 데이터 레이블에 독립적으로 유사한 결과를 보여줄 수 있으며, 이는 모델 의사결정의 실제 원인보다는 이미지의 가장자리와 같은 저수준 특징을 강조할 수 있다고 비판했습니다. 의료 영상 분석 분야에서는 이에 대한 추가적인 연구가 필요합니다.
- **미래 전망**:
  - **복합적(Holistic) XAI 접근 방식**: 시각, 텍스트, 사례 기반 설명을 결합하여 보다 포괄적인 이해를 제공하는 연구가 증가할 것으로 예상됩니다.
  - **생물학적 설명**: 딥러닝이 영상 특징으로부터 생물학적 프로세스를 예측하는 것을 넘어, 영상 표현형(imaging phenotypes)에 대한 경로 분석 등을 통해 생물학적 메커니즘을 설명하는 방향으로 발전할 수 있습니다.
  - **인과 관계와 XAI의 통합**: 현재 XAI는 주로 상관 관계에 기반하지만, 편향 제거 및 일반화 가능성 향상을 위해 인과 추론(causal reasoning)을 XAI에 통합하는 연구가 중요합니다.
  - **XAI를 위한 표본 크기 가이드라인**: 의료 영상 데이터셋 구축의 높은 비용을 고려할 때, 각 XAI 기술에 필요한 최소 표본 크기에 대한 명확한 가이드라인 마련이 필요합니다.

## 📌 TL;DR

딥러닝 기반 의료 영상 분석의 "블랙박스" 문제와 의사결정 투명성 요구에 따라, 이 논문은 의료 영상 분야의 설명 가능한 인공지능(XAI) 기술들을 종합적으로 검토했습니다. 모델 기반 vs. 사후 분석, 모델 특화 vs. 모델 독립, 전역 vs. 지역이라는 새로운 XAI 프레임워크를 제시하고, 시각적(예: Grad-CAM), 텍스트(예: 이미지 캡셔닝), 사례 기반(예: 트리플렛 네트워크) 기법들을 해부학적 위치(주로 흉부/뇌)와 영상 양식(X-ray/MRI)에 따라 분류하고 활용 동향을 분석했습니다. XAI 평가의 어려움과 기술의 견고성에 대한 비판적 논의를 포함하여, 복합적 XAI 접근, 생물학적/인과적 설명, 표본 크기 가이드라인 마련 등 미래 연구 방향을 제시하여 의료 분야 XAI의 중요성과 발전 가능성을 강조합니다.
