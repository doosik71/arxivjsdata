{
  "title": "TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis",
  "authors": "Sabera Talukder, Yisong Yue, Georgia Gkioxari",
  "year": 2024,
  "url": "http://arxiv.org/abs/2402.16412v2",
  "abstract": "This work studies the problem of time series analysis with generalist (or\nfoundation) models, which are models trained across many data domains. Drawing\ninspiration from the widespread success of large language models, we consider\nthe simple strategy of discretely tokenizing time series data drawn from a\nmyriad of datasets via self-supervision, then using the fixed tokenization to\nsolve a variety of tasks across many data domains. Canonically, time series\nmodels are either trained on a single dataset or built in a task-specific\nmanner (e.g., a forecasting-only model), where many use patches of time as\ninputs to the model. As such, performant generalist, discrete representation\ntime series models explored across many tasks are of value. Our method,\nTOkenized Time Series EMbeddings (TOTEM), produces such generalist time series\nmodels with minimal or no fine-tuning while exhibiting strong zero-shot\nperformance. We evaluate TOTEM extensively over nearly 500 experiments on three\ncommonly-studied time series tasks with real-world data: imputation (17\nbaselines, 12 datasets), anomaly detection (19 baselines, 25 datasets), and\nforecasting (14 baselines, 12 datasets). We conclude that TOTEM matches or\noutperforms existing state-of-the-art models in both the canonical specialist\nsetting (i.e., training one model on one domain) as well as the generalist\nsetting (i.e., training a single model on many domains), which demonstrates the\nefficacy of tokenization for general time series analysis. The open-source\nimplementation is available here: https://github.com/SaberaTalukder/TOTEM; a\nvideo summary is available here: https://www.youtube.com/watch?v=OqrCpdb6MJk.",
  "citation": 42
}