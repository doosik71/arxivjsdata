{
  "title": "Multi-modal Time Series Analysis: A Tutorial and Survey",
  "authors": "Yushan Jiang, Kanghui Ning, Zijie Pan, Xuyang Shen, Jingchao Ni, Wenchao Yu, Anderson Schneider, Haifeng Chen, Yuriy Nevmyvaka, Dongjin Song",
  "year": 2025,
  "url": "http://arxiv.org/abs/2503.13709v1",
  "abstract": "Multi-modal time series analysis has recently emerged as a prominent research\narea in data mining, driven by the increasing availability of diverse data\nmodalities, such as text, images, and structured tabular data from real-world\nsources. However, effective analysis of multi-modal time series is hindered by\ndata heterogeneity, modality gap, misalignment, and inherent noise. Recent\nadvancements in multi-modal time series methods have exploited the multi-modal\ncontext via cross-modal interactions based on deep learning methods,\nsignificantly enhancing various downstream tasks. In this tutorial and survey,\nwe present a systematic and up-to-date overview of multi-modal time series\ndatasets and methods. We first state the existing challenges of multi-modal\ntime series analysis and our motivations, with a brief introduction of\npreliminaries. Then, we summarize the general pipeline and categorize existing\nmethods through a unified cross-modal interaction framework encompassing\nfusion, alignment, and transference at different levels (\\textit{i.e.}, input,\nintermediate, output), where key concepts and ideas are highlighted. We also\ndiscuss the real-world applications of multi-modal analysis for both standard\nand spatial time series, tailored to general and specific domains. Finally, we\ndiscuss future research directions to help practitioners explore and exploit\nmulti-modal time series. The up-to-date resources are provided in the GitHub\nrepository: https://github.com/UConn-DSIS/Multi-modal-Time-Series-Analysis",
  "citation": 18
}