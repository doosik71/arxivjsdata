{
  "title": "Large Pre-trained time series models for cross-domain Time series\n  analysis tasks",
  "authors": "Harshavardhan Kamarthi, B. Aditya Prakash",
  "year": 2023,
  "url": "http://arxiv.org/abs/2311.11413v3",
  "abstract": "Large pre-trained models have been vital in recent advancements in domains\nlike language and vision, making model training for individual downstream tasks\nmore efficient and provide superior performance. However, tackling time-series\nanalysis tasks usually involves designing and training a separate model from\nscratch leveraging training data and domain expertise specific to the task. We\ntackle a significant challenge for pre-training a foundational time-series\nmodel from multi-domain time-series datasets: extracting semantically useful\ntokenized inputs to the model across heterogenous time-series from different\ndomains. We propose Large Pre-trained Time-series Models (LPTM) that introduces\na novel method of adaptive segmentation that automatically identifies optimal\ndataset-specific segmentation strategy during pre-training. This enables LPTM\nto perform similar to or better than domain-specific state-of-art model when\nfine-tuned to different downstream time-series analysis tasks and under\nzero-shot settings. LPTM achieves superior forecasting and time-series\nclassification results taking up to 40% less data and 50% less training time\ncompared to state-of-art baselines. Code: www.github.com/AdityaLab/Samay",
  "citation": 20
}