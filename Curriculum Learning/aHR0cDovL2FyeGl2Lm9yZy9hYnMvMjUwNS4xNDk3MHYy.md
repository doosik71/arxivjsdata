# Self-Evolving Curriculum for LLM Reasoning
Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, Ehsan Kamalloo

## 🧩 Problem to Solve
강화 학습(RL)은 대규모 언어 모델(LLM)의 추론 능력을 향상시키는 효과적인 파인튜닝 방법으로 입증되었습니다. 그러나 RL 파인튜닝 성공의 핵심 요소인 학습 커리큘럼(문제 제시 순서)은 여전히 최적화되지 못하고 있습니다. 무작위 커리큘럼은 최적의 성능을 보장하지 않으며, 수동으로 설계된 커리큘럼은 휴리스틱에 크게 의존하고 새로운 모델이나 작업에 대한 인간 개입이 필요합니다. 또한, 온라인 필터링 방법은 추가적인 온-정책(on-policy) 샘플 생성으로 인해 계산 비용이 매우 높다는 한계가 있습니다. 이러한 제약 사항을 해결하고 LLM의 학습 효율성 및 일반화 성능을 향상시키는 자동화되고 적응적인 커리큘럼 학습 방법이 필요합니다.

## ✨ Key Contributions
*   **Self-Evolving Curriculum (SEC) 제안**: LLM의 RL 파인튜닝 과정과 동시에 커리큘럼 정책을 학습하는 자동 커리큘럼 학습 방법을 제안합니다.
*   **비정상 다중-무장 강도(Multi-Armed Bandit, MAB) 문제 공식화**: 커리큘럼 선택을 비정상 MAB 문제로 공식화하여, 각 문제 카테고리(예: 난이도 수준 또는 문제 유형)를 개별 '팔(arm)'로 취급합니다.
*   **'절대 이점(Absolute Advantage)'을 학습 이득의 척도로 활용**: 정책 경사(Policy Gradient) 방법에서 기울기 크기가 이점 함수(advantage function)의 절댓값에 의해 가중된다는 점을 활용하여, 절대 이점($|\hat{A}_t|$)을 즉각적인 학습 이득에 대한 프록시 보상으로 정의합니다.
*   **TD(0) 업데이트를 통한 커리큘럼 정책 학습**: 각 학습 단계에서 이 보상 신호를 최대화하도록 카테고리를 선택하고, TD(0) 방법을 사용하여 커리큘럼 정책을 업데이트합니다.
*   **다양한 추론 도메인에서의 성능 향상**: 계획, 귀납적 추론, 수학의 세 가지 추론 도메인에서 SEC가 모델의 추론 능력을 크게 향상시키고, 더 어려운 분포 외(Out-of-Distribution, OOD) 테스트 문제에 대한 일반화를 가능하게 함을 입증합니다.
*   **다중 도메인 파인튜닝 시 기술 균형 개선**: 여러 추론 도메인에서 동시에 파인튜닝할 때 더 나은 기술 균형을 달성합니다.

## 📎 Related Works
*   **LLM 강화를 통한 파인튜닝 (RL Fine-tuning for LLMs)**: 결과 보상 모델(ORM), 프로세스 보상 모델(PRM), RLHF (Reinforcement Learning from Human Feedback), 검증 가능한 보상 기반 RL (RLVR) 등 다양한 패러다임이 있습니다. 정책 경사(Policy Gradient) 방법론 중 REINFORCE 변형 (RLOO), PPO (Proximal Policy Optimization), GRPO (Group Relative Policy Optimization)가 널리 사용됩니다.
*   **커리큘럼 학습 (Curriculum Learning)**: Bengio et al. [7]에 의해 도입되었으며, 쉬운 것부터 어려운 것 순서로 예시를 구성하여 비볼록 최적화를 완화하고 일반화를 개선합니다. RL 분야에서는 역 커리큘럼 생성(Reverse Curriculum Generation), 교사-학생 커리큘럼 학습(Teacher-Student Curriculum Learning, TSCL), POET, ACCEL, PAIRED와 같이 환경과 에이전트를 공동 진화시키는 방법 등이 있습니다.
*   **LLM 파인튜닝을 위한 자동 커리큘럼 학습**: R$^{3}$ (Chain-of-Thought 추론을 위한 역 커리큘럼), WEBRL (LLM이 자체적으로 새로운 작업 생성), Bae et al. [3]의 온라인 필터링, AdaRFT [46] (모델의 보상 신호 기반 난이도 조절) 등이 있습니다. 본 연구와 유사하게, DUMP [56]는 Knights and Knaves 논리 추론 퍼즐에 MAB 프레임워크와 절대 이점을 활용합니다.

## 🛠️ Methodology
SEC는 LLM 파인튜닝과 동시에 커리큘럼 정책을 적응적으로 학습합니다.
1.  **훈련 데이터 카테고리화**: 훈련 데이터셋을 $N$개의 개별 카테고리 $C = \{c_1, \ldots, c_N\}$로 나눕니다 (예: 난이도, 문제 유형).
2.  **커리큘럼 선택을 MAB 문제로 공식화**: 각 카테고리 $c_i$를 MAB의 '팔(arm)'로 취급하고, LLM 정책이 훈련되는 동안 기대 보상 $Q_t(c)$를 학습하는 비정상 MAB 문제로 정의합니다. $Q_{t+1}(c) = \alpha r_t(c) + (1-\alpha)Q_t(c)$와 같이 TD(0) 방법을 사용하여 $Q$ 값을 업데이트합니다. 여기서 $\alpha$는 학습률입니다.
3.  **학습 이득 측정 (보상 정의)**:
    *   최종 성능을 직접 측정하는 대신, 각 훈련 단계에서 효율적으로 계산할 수 있는 프록시 목적 함수를 사용합니다.
    *   정책 경사 알고리즘에서 기울기 크기는 이점 값($\hat{A}_t$)의 절댓값($|\hat{A}_t|$)에 비례하여 학습 이득을 반영합니다.
    *   따라서, 카테고리 $c$에 대한 보상 $r(c)$는 해당 카테고리에서 샘플링된 문제들의 롤아웃에 대한 평균 절대 이점으로 정의됩니다: $r(c) = E_{(s_t,a_t)\sim\pi_{\theta}(x_i),x_i\sim c} [|\hat{A}_t|]$.
    *   특히, 검증 가능한 보상(이진 보상: 0 또는 1)이 사용되는 경우, 기대 절대 이득 $E[|\tilde{r}_i|] = 2\sqrt{p(1-p)}$는 성공률 $p=0.5$일 때 최대가 됩니다. 이는 현재 모델에 너무 쉽지도 너무 어렵지도 않은 문제(근접 발달 영역, Zone of Proximal Development)를 우선시한다는 의미입니다.
4.  **배치 샘플링**: 현재 $Q_t(c)$ 값에 의해 정의된 볼츠만 분포 $p(c) = \frac{e^{Q_t(c)/\tau}}{\sum_{i=1}^{N}e^{Q_t(c_i)/\tau}}$에 따라 카테고리를 샘플링합니다 ($\tau$는 탐색-활용 균형을 제어하는 온도 매개변수). 선택된 카테고리에서 문제들을 균일하게 샘플링하여 훈련 배치를 구성합니다.
5.  **LLM 정책 업데이트**: 생성된 배치로 LLM 정책을 업데이트하고, 이 과정에서 얻은 이점 값을 사용하여 커리큘럼 정책의 $Q_t(c)$ 값을 업데이트합니다.

## 📊 Results
*   **단일 추론 도메인 (난이도 기준)**:
    *   Qwen2.5-3B 모델의 경우, SEC는 어려운 OOD 테스트 세트에서 일관되게 상당한 성능 향상을 보였습니다. Countdown에서 무작위 기준선 대비 약 13% (0.48$\to$0.54), 난이도 순서 기준선 대비 약 69% (0.32$\to$0.54)의 OOD 정확도 향상을 달성했습니다. Zebra에서는 무작위 대비 약 21% (0.29$\to$0.35), 수학 AIME24 데이터셋에서는 무작위 대비 약 33% (0.075$\to$0.10) 향상되었습니다.
    *   Qwen2.5-7B 모델은 이미 강력한 기본 성능을 가지고 있어 무작위 커리큘럼과 유사한 경쟁력을 보였지만, Zebra (OOD 0.32$\to$0.36, 약 11% 향상) 및 AIME24 (0.14$\to$0.18, 약 27% 향상)와 같은 더 어려운 작업에서는 명확한 개선을 보여주었습니다.
    *   난이도 순서 커리큘럼은 고정된 난이도 스케줄로 인해 종종 차선책의 성능을 보였습니다.
*   **커리큘럼 분석 (난이도 변화)**: SEC는 학습 진행에 따라 적응적으로 훈련 난이도를 조절합니다. 초기에는 쉬운 문제를 강조하다가, 모델의 능력이 향상됨에 따라 점진적으로 어려운 문제를 도입합니다.
*   **다중 커리큘럼 카테고리 (문제 유형 + 난이도)**: Countdown, Zebra, ARC를 혼합한 다중 작업 훈련에서 SEC-2D (9개 조합의 카테고리)는 모든 추론 작업에서 무작위 커리큘럼보다 일관되게 뛰어난 성능을 보였습니다. 무작위 커리큘럼은 훈련 중간에 성능 저하를 보인 반면, SEC-2D는 안정적이고 견고한 성능을 유지하며 다중 학습 목표를 효과적으로 균형 있게 맞추는 능력을 보여주었습니다.
*   **대체 RL 알고리즘**: PPO (Proximal Policy Optimization) 및 RLOO (REINFORCE Leave-One-Out)와 같은 다른 RL 알고리즘과 함께 사용했을 때도 SEC는 Countdown 작업에서 성능을 일관되게 향상시켰습니다.

## 🧠 Insights & Discussion
SEC는 모델의 현재 학습 진행 상황에 맞춰 문제 난이도를 동적으로 조절함으로써 학습 결과를 극대화합니다. 이는 교육 심리학의 '근접 발달 영역(Zone of Proximal Development)' 이론과도 일맥상통하며, LLM의 RL 파인튜닝을 위한 유망한 전략임을 시사합니다. 특히 어려운 OOD 문제에 대한 일반화 성능과 다중 추론 도메인에서의 기술 균형 개선은 SEC의 주요 강점입니다.

**한계점**:
*   **사전 정의된 카테고리 의존**: 현재 SEC는 미리 정의된 커리큘럼 카테고리에 의존합니다. 자동으로 추론되거나 명확하게 정의되지 않은 카테고리에서의 효과는 탐구되지 않았습니다.
*   **하이퍼파라미터 튜닝**: SEC는 온도 매개변수($\tau$) 및 학습률($\alpha$)과 같은 추가 하이퍼파라미터 튜닝이 필요합니다.

**향후 연구**: 임베딩 기반 문제 클러스터링이나 경량 모델을 사용하여 커리큘럼 보상을 직접 추정하는 등 더 유연한 커리큘럼 정의를 탐색하고, UCB (Upper Confidence Bound) 또는 톰슨 샘플링(Thompson sampling)과 같은 불확실성 측정 방법을 커리큘럼 선택에 통합하는 방안을 모색할 수 있습니다.

## 📌 TL;DR
**문제**: LLM RL 파인튜닝 시 학습 커리큘럼이 중요하지만, 기존 방법은 비효율적(무작위, 수동 설계, 온라인 필터링의 높은 비용).
**제안**: Self-Evolving Curriculum (SEC)은 커리큘럼 선택을 비정상 다중-무장 강도(Multi-Armed Bandit, MAB) 문제로 공식화하여 LLM 파인튜닝과 동시에 커리큘럼 정책을 학습.
**방법**: 각 문제 카테고리를 '팔'로 간주하고, 정책 경사 방법의 '절대 이점'($|\hat{A}_t|$)을 즉각적인 학습 이득의 보상 신호로 활용. 이 보상을 최대화하도록 TD(0) 방식으로 커리큘럼 정책을 업데이트하며, 이는 현재 모델에 '적절히 어려운' 문제(성공률 0.5 근처)를 우선시하는 효과.
**결과**: SEC는 계획, 귀납적 추론, 수학 등 다양한 추론 도메인에서 모델의 추론 능력과 어려운 OOD 문제에 대한 일반화 성능을 크게 향상시키며, 다중 도메인 학습 시 기술 균형도 개선함.