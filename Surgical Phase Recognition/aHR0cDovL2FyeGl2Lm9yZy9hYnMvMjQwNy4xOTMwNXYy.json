{
  "title": "GP-VLS: A general-purpose vision language model for surgery",
  "authors": "Samuel Schmidgall, Joseph Cho, Cyril Zakka, William Hiesinger",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.19305v2",
  "abstract": "Surgery requires comprehensive medical knowledge, visual assessment skills,\nand procedural expertise. While recent surgical AI models have focused on\nsolving task-specific problems, there is a need for general-purpose systems\nthat can understand surgical scenes and interact through natural language. This\npaper introduces GP-VLS, a general-purpose vision language model for surgery\nthat integrates medical and surgical knowledge with visual scene understanding.\nFor comprehensively evaluating general-purpose surgical models, we propose\nSurgiQual, which evaluates across medical and surgical knowledge benchmarks as\nwell as surgical vision-language questions. To train GP-VLS, we develop six new\ndatasets spanning medical knowledge, surgical textbooks, and vision-language\npairs for tasks like phase recognition and tool identification. We show that\nGP-VLS significantly outperforms existing open- and closed-source models on\nsurgical vision-language tasks, with 8-21% improvements in accuracy across\nSurgiQual benchmarks. GP-VLS also demonstrates strong performance on medical\nand surgical knowledge tests compared to open-source alternatives. Overall,\nGP-VLS provides an open-source foundation for developing AI assistants to\nsupport surgeons across a wide range of tasks and scenarios. The code and data\nfor this work is publicly available at gpvls-surgery-vlm.github.io."
}