# BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation

Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, Youliang Yan

## 🧩 Problem to Solve

최근 완전 컨볼루션(fully convolutional) 기반의 인스턴스 분할(instance segmentation) 방법론들은 Mask R-CNN과 같은 2단계(two-stage) 접근법보다 구조가 단순하고 효율적이라는 장점으로 많은 주목을 받고 있습니다. 하지만 모델의 연산 복잡도가 유사할 때, 이러한 1단계(one-stage) 방법론들은 마스크 예측의 정밀도에서 여전히 Mask R-CNN에 뒤처지는 경향이 있어 개선의 여지가 큽니다.

기존 방법론들의 한계는 다음과 같습니다:

* **2단계 접근법 (예: Mask R-CNN)**: 인스턴스가 많을수록 추론 시간이 비례하여 증가하며, RoI(Region of Interest) 특징 및 결과 마스크의 해상도가 제한됩니다.
* **1단계 상향식(top-down) 접근법 (예: DeepMask)**: 특징과 마스크 간의 지역적 일관성(local-coherence) 손실, 중복된 특징 표현, 다운샘플링으로 인한 위치 정보 손상 등의 문제가 있습니다.
* **1단계 하향식(bottom-up) 접근법**: 밀집 예측(dense prediction) 품질에 크게 의존하여 성능이 저하되거나 마스크가 파편화되거나 연결될 수 있고, 복잡한 장면에서의 일반화 능력이 제한적이며, 복잡한 후처리 기술이 필요합니다.
* **하이브리드 접근법 (예: FCIS, YOLACT)**: 상위 및 하위 수준 특징의 표현력을 제대로 균형 잡히게 결합하지 못하는 너무 단순화된 통합 설계를 사용합니다. 특히 YOLACT는 인스턴스 정보를 인코딩하기 위해 스칼라 계수를 사용하여 최적화되지 못합니다.

## ✨ Key Contributions

* **블렌더(Blender) 모듈 개발**: 풍부한 인스턴스 수준 정보와 정확한 픽셀 수준 밀집 특징을 효과적으로 통합하여 인스턴스 마스크를 생성하는 유연한 방법을 제안합니다. YOLACT 및 FCIS의 병합 기술보다 COCO 데이터셋에서 각각 1.9, 1.3 mAP 포인트 향상을 달성했습니다.
* **BlendMask 아키텍처 제안**: 최신 1단계 객체 탐지기인 FCOS에 최소한의 계산 오버헤드만 추가하여 간단한 구조를 유지합니다.
* **향상된 추론 효율성**: 기존 2단계 방법과 달리 예측 인스턴스 수에 비례하여 추론 시간이 증가하지 않아 실시간 시나리오에서 더욱 견고합니다.
* **최고 수준의 성능 달성**: Mask R-CNN과 동일한 훈련 스케줄 하에 정확도 면에서 Mask R-CNN을 능가하고, 동시에 약 20% 더 빠릅니다. TensorMask보다 마스크 mAP에서 1.1 포인트 높은 성능을 달성하면서도 훈련 반복 횟수는 절반, 추론 시간은 1/5에 불과합니다.
* **파놉틱 분할(Panoptic Segmentation) 지원**: 별도의 수정 없이 파놉틱 분할을 자연스럽게 해결할 수 있습니다.
* **고품질 마스크 생성**: Mask R-CNN의 일반적인 $28 \times 28$ 마스크 해상도보다 훨씬 높은 해상도의 마스크를 출력할 수 있어 더욱 정확한 경계선을 생성합니다.
* **높은 일반화 및 유연성**: 최소한의 수정으로 키포인트 탐지(keypoint detection)와 같은 다른 인스턴스 수준 인식 작업에도 적용할 수 있습니다.

## 📎 Related Works

* **Anchor-free 객체 탐지 (FCOS)**: 바운딩 박스 앵커를 제거하여 탐지 파이프라인을 크게 단순화하며, 미리 정의된 앵커 형태의 제약 없이 타겟을 예측 특징에 자유롭게 매칭할 수 있습니다. BlendMask는 이러한 앵커 프리 프레임워크를 기반으로 구축됩니다.
* **Detect-then-segment 인스턴스 분할 (Mask R-CNN, R-FCN, MaskLab)**: 객체를 먼저 탐지한 후 각 제안에 대해 전경 마스크를 예측하는 2단계 방법론입니다. RoIAlign과 같은 정렬 연산으로 지역적 일관성을 유지하지만, 인스턴스 수에 비례하는 추론 시간과 제한된 마스크 해상도라는 한계가 있습니다.
* **FCIS 및 YOLACT**: 제안 기반의 마스크 조합을 시도하는 하이브리드 방법론입니다. FCIS는 위치에 민감한 스코어 맵을 사용하고, YOLACT는 마스크 계수를 학습하여 하위 마스크 베이스(bases)를 선형 조합합니다. BlendMask는 이들의 단순화된 병합 설계를 개선하여 상위-하위 특징 간의 균형을 맞춥니다.
* **저수준 특징으로 거친 마스크 개선 (MaskLab, IMP)**: 마스크 예측을 백본(backbone) 특징의 저수준 레이어와 연결하여 개선하는 아이디어입니다. BlendMask는 이와 유사하게 상위 수준의 거친 인스턴스 정보를 하위 수준의 미세한 입자감과 병합하지만, 더 효율적이고 가벼운 방식으로 작동합니다.

## 🛠️ Methodology

BlendMask는 크게 탐지기 네트워크(detector network)와 마스크 브랜치(mask branch)로 구성됩니다. 마스크 브랜치는 하단 모듈(bottom module), 상단 레이어(top layer), 그리고 블렌더 모듈(blender module)의 세 부분으로 나뉩니다.

1. **하단 모듈(Bottom Module)**:
    * DeepLabV3+의 디코더와 유사한 구조를 사용하며, 입력으로 백본 특징(C3, C5) 또는 FPN 특징(P3, P5)을 받습니다.
    * $N \times K \times H_s \times W_s$ 형태의 '베이스(bases)' $B$를 예측합니다. 여기서 $K$는 베이스의 수이며, $H_s \times W_s$는 스코어 맵 출력 스트라이드입니다.

2. **상단 레이어(Top Layer)**:
    * 각 탐지 타워(detection tower) 위에 단일 컨볼루션 레이어를 추가하여 상위 수준의 '어텐션(attentions)' $A$를 예측합니다.
    * $N \times (K \cdot M \cdot M) \times H_l \times W_l$ 형태의 텐서로, $M \times M$은 어텐션 해상도입니다. 이는 객체의 거친 형태와 자세와 같은 인스턴스 수준 정보를 인코딩합니다.
    * FCOS 후처리 과정을 거쳐 상위 $D$개의 바운딩 박스 제안 $P$와 해당 어텐션 $A$를 선택합니다.

3. **블렌더 모듈(Blender Module)**: BlendMask의 핵심으로, 위치에 민감한 베이스를 어텐션에 따라 결합하여 최종 마스크를 생성합니다.
    * **베이스 크롭(Base Cropping)**: Mask R-CNN의 RoIPooler(RoIAlign)를 사용하여 각 바운딩 박스 제안 $p_d$로 하단 모듈의 베이스 $B$를 크롭하고, 고정된 $R \times R$ 크기의 특징 맵 $r_d$로 리사이징합니다.
        $$r_d = \text{RoIPool}_{R \times R}(B, p_d), \forall d \in \{1...D\}$$
    * **어텐션 보간(Attention Interpolation)**: 상단 어텐션 $a_d$를 $M \times M$에서 $R \times R$ 해상도로 보간하여 $a'_d$를 얻습니다.
        $$a'_d = \text{interpolate}_{M \times M \to R \times R}(a_d), \forall d \in \{1...D\}$$
    * **어텐션 정규화(Attention Normalization)**: $a'_d$를 $K$ 차원을 따라 소프트맥스(softmax) 함수로 정규화하여 스코어 맵 $s_d$를 만듭니다.
        $$s_d = \text{softmax}(a'_d), \forall d \in \{1...D\}$$
    * **마스크 생성(Mask Generation)**: 각 엔티티 $r_d$와 $s_d$ 사이에 요소별 곱셈(element-wise product)을 수행한 후, $K$ 차원을 따라 합산하여 최종 마스크 로짓 $m_d$를 얻습니다.
        $$m_d = \sum_{k=1}^{K} s^k_d \circ r^k_d, \forall d \in \{1...D\}$$

**주요 설정 및 하이퍼파라미터**:

* $R$ (하단 RoI 해상도), $M$ (상단 예측 해상도), $K$ (베이스 수)
* 하단 모듈 입력 특징 (백본 C 또는 FPN P)
* 하단 베이스 샘플링 방법 (최근접 이웃 또는 바이리니어 풀링)
* 상단 어텐션 보간 방법 (최근접 이웃 또는 바이리니어 업샘플링)

## 📊 Results

* **병합 방법 비교**: 제안된 블렌더 모듈은 YOLACT (Weighted-sum) 및 FCIS (Assembler)의 병합 전략보다 훨씬 뛰어난 성능을 보였습니다. (예: Mask AP에서 각각 1.9, 1.3 포인트 높음).
* **해상도 및 베이스 수**:
  * 상단 어텐션 해상도를 높이면 성능이 향상되지만, RoI 크기의 약 1/4 해상도에서 포화됩니다.
  * 하단 베이스 풀링 해상도를 높여도 추론 시간 증가는 미미하며 AP가 향상됩니다.
  * 인스턴스 수준 정보가 상단 어텐션으로 잘 표현되기 때문에, $K=4$개의 베이스만으로도 최적의 정확도를 달성했습니다.
* **특징 위치 및 보간**: FPN 특징을 하단 모듈 입력으로 사용하면 성능이 개선되고 실행 시간이 단축됩니다. 바이리니어 보간(RoIAlign)은 하단 수준 정렬에 더 중요하며, 약 2 AP 향상을 가져왔습니다.
* **COCOtest-dev 벤치마크**:
  * ResNet-50 백본 사용 시, BlendMask는 Mask R-CNN(34.6%) 및 TensorMask(35.5%)를 뛰어넘는 37.0% mAP를 달성했습니다.
  * ResNet-101 백본 사용 시, BlendMask는 Mask R-CNN(36.2%) 및 TensorMask(37.3%)를 뛰어넘는 38.4% mAP를 달성했으며, DCN(Deformable Convolutional Networks)을 추가하면 41.3%까지 향상되었습니다.
  * BlendMask는 Mask R-CNN보다 약 20% 빠르며, TensorMask보다 훨씬 효율적입니다.
* **실시간 설정 (BlendMask-RT)**: YOLACT-700보다 7ms 빠르면서 3.3 AP 높은 34.2% mAP (25 FPS)를 달성하여 실시간 인스턴스 분할에서도 경쟁력을 보였습니다.
* **정성적 결과**: Mask R-CNN 및 YOLACT보다 더 높은 품질의 마스크, 특히 복잡한 형태, 가려진 객체, 겹치는 객체에서 더 정확한 경계선을 생성했습니다. YOLACT의 동일 클래스 인스턴스 분리 문제(leakage)를 효과적으로 해결했습니다.
* **파놉틱 분할**: Panoptic-FPN보다 지속적으로 더 나은 파놉틱 품질(PQ)을 달성했습니다 (예: R-101에서 44.3 PQ vs. 43.0 PQ).
* **LVIS 어노테이션 평가**: 고품질 LVIS 어노테이션에 대해 PointRend보다 높은 44.1 AP$^{?}$를 달성하여 BlendMask가 생성하는 마스크의 고품질을 입증했습니다.

## 🧠 Insights & Discussion

* **상위-하위 특징의 효과적인 결합**: BlendMask는 상위 수준의 인스턴스 정보(예: 객체의 거친 형태와 자세)와 하위 수준의 미세한 픽셀 정보를 블렌더 모듈을 통해 효과적으로 융합합니다. 이를 통해 기존 1단계 방법론의 한계를 극복하고 2단계 방법론에 필적하는, 혹은 능가하는 마스크 정밀도를 달성합니다.
* **학습된 베이스 및 어텐션의 의미**: 학습된 베이스는 객체 여부(의미론적 마스크) 및 객체의 특정 부분(위치에 민감한 특징)에 대한 지역 정보를 인코딩합니다. 어텐션 맵은 이러한 베이스들을 인스턴스별로 섬세하게 조합하는 가이드 역할을 하여 겹치는 인스턴스를 효과적으로 분리하고 마스크의 품질을 높입니다.
* **Mask R-CNN 대비 장점**:
  * **효율성**: RoI 샘플링 전에 R-CNN 헤드의 계산을 이동시켜 중복된 마스크 표현 및 계산을 피합니다.
  * **고해상도 마스크**: 출력 마스크 해상도가 상위 수준 샘플링에 의해 제한되지 않아 더 깊은 헤드 구조나 연산 비용 증가 없이 고품질의 마스크를 생성할 수 있습니다.
  * **실시간 성능**: 블렌더 모듈이 매우 효율적이어서 (1080Ti에서 0.6ms), 탐지된 인스턴스 수 증가에 따른 추가 추론 시간이 거의 무시할 만합니다.
* **유연성**: 블렌더 모듈은 단일 컨볼루션 레이어로 구성되어 있어 대부분의 최신 객체 탐지기에 쉽게 추가될 수 있으며, 다른 인스턴스 수준 인식 작업에도 적용 가능합니다.
* **파놉틱 분할에서의 시너지**: 인스턴스 분할과 의미론적 분할 간의 특징 공유 및 일관된 스케일 예측 덕분에 Panoptic-FPN보다 우수한 파놉틱 분할 성능을 달성합니다.

## 📌 TL;DR

BlendMask는 1단계 인스턴스 분할의 마스크 정밀도 한계를 해결하기 위해 상위 수준 인스턴스 정보와 하위 수준 픽셀 정보를 효과적으로 결합하는 **블렌더 모듈**을 제안합니다. 이 모듈은 **베이스(bases)**와 **어텐션(attentions)**을 학습하여 위치에 민감한 특징을 동적으로 조합하여 최종 마스크를 생성합니다. 그 결과, BlendMask는 Mask R-CNN보다 정확도가 높으면서 약 **20% 더 빠른 추론 속도**를 달성했으며, 실시간 버전인 BlendMask-RT는 YOLACT보다 뛰어난 성능을 보였습니다. BlendMask는 높은 마스크 품질과 효율성을 제공하며, 다양한 인스턴스 수준 예측 작업을 위한 강력하고 유연한 기준선이 될 수 있습니다.
