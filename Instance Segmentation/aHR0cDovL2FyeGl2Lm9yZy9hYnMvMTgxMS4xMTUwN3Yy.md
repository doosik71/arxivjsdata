# One-Shot Instance Segmentation

Claudio Michaelis, Ivan Ustyuzhaninov, Matthias Bethge, Alexander S. Ecker

## 🧩 Problem to Solve

이 논문은 "원샷 인스턴스 분할(one-shot instance segmentation)"이라는 새로운 도전 과제를 다룹니다. 이 과제의 목표는 이전에 본 적 없는 새로운 객체 범주에 대한 **단 하나의 예시 이미지(참조 이미지)**가 주어졌을 때, 복잡한 장면 이미지(쿼리 이미지) 내에서 해당 범주에 속하는 모든 객체를 찾아 바운딩 박스를 생성하고 정확하게 분할하는 것입니다.

기존의 강력한 컴퓨터 비전 시스템은 미리 정의된 제한된 수의 객체 범주에 대해서만 감지 및 분할을 수행할 수 있습니다. 반면 인간은 소수의 예시만으로도 새로운 개념을 빠르게 학습하고 해당 객체를 식별할 수 있습니다. 이러한 인간의 능력을 기계에 부여하는 것은 대규모 주석 데이터셋이 없는 자율 에이전트(예: 가정용 로봇), 의료 영상, 지리 과학 위성 이미지와 같은 다양한 실제 응용 분야에 매우 중요합니다.

기존의 퓨샷 학습(few-shot learning)은 주로 판별적 분류 문제(이미지 중심의 간단한 객체)에 초점을 맞추었으며, 객체 감지나 인스턴스 분할까지 확장하기 어렵습니다. 따라서 이 논문은 실제 복잡한 장면에서 새로운 객체를 감지하고 분할하는 문제를 해결하고자 합니다.

## ✨ Key Contributions

* **새로운 원샷 과제 정의:** 단일 시각적 예시를 기반으로 객체 감지 및 인스턴스 분할을 요구하는 "원샷 인스턴스 분할"이라는 새로운 과제를 도입했습니다.
* **Siamese Mask R-CNN 제안:** 원샷 인스턴스 분할을 수행할 수 있는 시스템인 Siamese Mask R-CNN을 제시했습니다. 이 모델은 최신 인스턴스 분할 모델인 Mask R-CNN에 메트릭 학습(metric learning)의 개념(Siamese 네트워크)을 통합했습니다.
* **평가 프로토콜 수립 및 MS-COCO 평가:** 이 과제를 위한 평가 프로토콜을 수립하고, MS-COCO 데이터셋을 사용하여 제안된 모델을 평가했습니다.
* **도전 과제 식별:** 모델에서 참조 범주를 대상으로 하는 감지(detection)가 주된 과제인 반면, 올바르게 식별된 객체를 분할(segmentation)하는 것은 비교적 잘 작동함을 보였습니다.

## 📎 Related Works

* **객체 감지 및 인스턴스 분할:** Mask R-CNN [21], Faster R-CNN [49], YOLO [44, 45, 46], SSD [34]와 같은 SOTA 모델들이 이 분야의 배경이 됩니다.
* **퓨샷 학습(Few-shot Learning):** Omniglot [27] 및 MiniImagenet [62]과 같은 데이터셋에서 분류 문제를 해결하기 위해 메트릭 학습(예: Siamese 네트워크 [25], Prototypical 네트워크 [57]) 및 메타 학습 [16] 접근 방식이 연구되었습니다.
* **관련 과제와의 차별점:**
  * **퓨샷 객체 감지 [13, 7, 24, 55]:** 이 논문은 소량의 레이블링된 데이터셋으로 객체 탐지기를 학습하는 대신, **예시 기반 검색** 과제로 정의하며 인스턴스 분할까지 확장합니다.
  * **제로샷 객체 감지 [3, 42, 11, 69]:** 참조 범주가 **텍스트 설명**으로 정의된다는 점에서 시각적 참조를 사용하는 본 연구와 다릅니다.
  * **원샷 의미론적 분할 [56, 43, 12, 37] 등:** 이러한 모델은 픽셀 수준의 **의미론적 분류**를 출력하여 개별 인스턴스를 구별할 수 없습니다. 본 연구는 **인스턴스 수준** 마스크를 생성합니다.
  * **특정 객체 추적/분할 [5, 6, 65]:** 특정 객체 인스턴스(예: 비디오 내의 특정 객체)를 지역화하는 데 중점을 두는 반면, 본 연구는 동일한 **객체 범주**의 인스턴스를 찾는 데 초점을 맞춥니다.

## 🛠️ Methodology

본 연구는 단일 시각적 예시를 기반으로 객체를 감지하고 분할하기 위해 **메트릭 학습(metric learning)** 접근 방식을 채택합니다. 참조 이미지와 장면 내의 이미지 영역 간의 유사도 메트릭을 학습하고, 이 메트릭을 기반으로 객체 제안을 생성하고 일치/불일치로 분류합니다.

* **Siamese Mask R-CNN 아키텍처:**
  * **Siamese 백본 네트워크:** Mask R-CNN [21]을 기반으로 하며, 참조 이미지와 쿼리 이미지 모두에서 특징을 추출하기 위해 **동일한 백본(ResNet50 [22] + FPN [31])을 공유된 가중치로 사용**합니다.
  * **특징 매칭(Feature Matching) 과정 (그림 3):**
        1. 참조 이미지의 특징을 전역 평균 풀링(global average pooling)하여 임베딩 벡터를 생성합니다.
        2. 쿼리 이미지의 각 $(x,y)$ 위치에서 추출된 특징 임베딩과 참조 임베딩 간의 **절대 차이($|x-y|$)**를 계산합니다.
        3. 이 차이 특징을 장면 표현에 연결(concatenate)합니다.
        4. $1 \times 1$ 컨볼루션을 사용하여 특징 수를 줄입니다.
        5. 이 결과 특징은 원본 Mask R-CNN 특징을 대체하여 장면 콘텐츠뿐만 아니라 참조 이미지와의 유사성을 인코딩합니다.
  * **헤드 아키텍처 (수정된 Mask R-CNN 헤드):**
    * 기존 RPN(Region Proposal Network) 및 ROI 풀링(ROI pooling)을 동일하게 사용합니다.
    * 분류 헤드는 80개 범주 판별 대신 **이진(match/non-match) 분류**를 수행하며, 단일 클래스 불가지론적 바운딩 박스 좌표 세트를 생성합니다.
    * 마스크 브랜치는 잠재적 범주별 마스크 대신 단일 인스턴스 마스크를 예측합니다.
* **학습 상세:**
  * 백본은 MS-COCO에 대응되지 않는 687개 ImageNet 범주로 구성된 부분집합(ImageNet-687)으로 사전 학습됩니다.
  * 에피소드 방식(episodic training)으로 학습됩니다. 각 미니 배치에서 이미지에 있는 훈련 범주 중 무작위 참조 범주를 선택하고, 다른 훈련 이미지에서 해당 범주의 무작위 인스턴스를 잘라 참조로 사용합니다. 다른 모든 객체는 배경으로 처리됩니다.
  * 손실 함수는 Mask R-CNN의 다중 작업 목적 함수와 동일하지만, 분류 손실은 이진 교차 엔트로피를 사용하고, 각 손실 구성 요소에 다른 가중치를 적용합니다 (예: RPN 분류 손실: 2, RPN 바운딩 박스 손실: 0.1, RoI 분류 손실: 2, RoI 바운딩 박스 손실: 0.5, 마스크 손실: 1).
* **평가 절차:**
    1. 테스트 세트에서 이미지를 선택합니다.
    2. 해당 이미지에 존재하는 (새로운) 테스트 범주 각각에 대해 무작위 참조 이미지를 추출합니다.
    3. 각 참조 이미지에 대해 개별적으로 바운딩 박스를 예측합니다.
    4. 계산된 예측을 해당 참조 이미지의 범주에 할당합니다.
    5. 테스트 세트의 모든 이미지에 대해 이 과정을 반복합니다.
    6. 표준 객체 감지 도구를 사용하여 mAP50 [14]을 계산합니다. 참조 이미지의 무작위 선택으로 인한 변동성을 정량화하기 위해 5회 반복 평가하여 평균 및 95% 신뢰 구간을 보고합니다.

## 📊 Results

* **무작위 박스 기준선:** 객체 감지 mAP50은 1.2%, 인스턴스 분할 mAP50은 0.5%를 기록했습니다.
* **학습 범주에 대한 성능:**
  * 1-샷 설정: 객체 감지 37.6% mAP50, 인스턴스 분할 34.9% mAP50.
  * 5-샷 설정: 객체 감지 41.3% mAP50, 인스턴스 분할 38.4% mAP50.
* **새로운 범주에 대한 성능 (원샷 인스턴스 분할):**
  * 1-샷 설정: 객체 감지 16.3% mAP50, 인스턴스 분할 14.5% mAP50.
  * 5-샷 설정: 객체 감지 18.5% mAP50, 인스턴스 분할 16.7% mAP50.
  * 이 값들은 학습 범주에 비해 상당히 낮지만, 무작위 박스 기준선보다 훨씬 높으며 원샷 설정의 어려움에도 불구하고 강력한 성능을 보여줍니다.
* **정성적 분석:** 모델은 전반적으로 우수한 바운딩 박스와 분할 마스크를 생성하지만, 종종 올바른 참조 범주를 대상으로 하는 데 실패합니다 (오탐지 증가).
* **오탐지 (False Positives):** 새로운 범주를 검색할 때 학습 세트에서 자주 나타나는 범주(예: 사람, 자동차, 비행기, 시계)가 오탐지로 발생하는 경향이 있습니다.
* **클러터(Clutter)의 영향:** 이미지 내 인스턴스 수가 적을수록 감지 및 분할 점수가 상당히 높게 나타나, 복잡한 장면 처리의 어려움을 보여줍니다.

## 🧠 Insights & Discussion

* **일반화 능력:** Siamese Mask R-CNN은 학습되지 않은 새로운 객체 범주에 대해서도 무작위 성능을 훨씬 상회하는 결과를 보여주며, 일반화 능력을 입증했습니다. 이는 메트릭 학습 접근 방식이 학습 범주를 넘어설 수 있음을 시사합니다.
* **주요 난제:** 모델의 주요 어려움은 감지 네트워크가 참조 범주를 정확하게 **타겟팅**하는 것입니다. 일단 객체가 올바르게 식별되면, 해당 객체에 대한 분할 마스크 생성 성능은 양호합니다.
* **한계점:**
  * 학습 범주에 대한 과적합(overfitting)으로 인해 새로운 범주에서의 성능 저하가 발생합니다.
  * 클러터가 심한 장면에서 성능이 크게 감소합니다.
  * 평가 절차가 참조 범주가 이미지에 존재한다고 가정하므로, 이는 실제 응용 시나리오보다 다소 단순화된 설정입니다.
* **시사점:** 본 연구는 원샷 인스턴스 분할이라는 어려운 문제에 대한 강력한 기준선(baseline)을 제공하며, 인간과 같은 유연성을 가진 시각 검색 알고리즘 개발을 위한 중요한 첫걸음이 될 것으로 기대됩니다. 향후 연구는 복잡한 장면에서 견고하게 작동하는 모델 개발에 초점을 맞춰야 할 것입니다.

## 📌 TL;DR

**문제:** 이 논문은 단일 예시 이미지(참조)만을 사용하여 이전에 학습되지 않은 객체 범주에 속하는 모든 인스턴스를 복잡한 장면(쿼리)에서 감지하고 분할하는 "원샷 인스턴스 분할" 과제를 제시합니다.

**방법:** Mask R-CNN에 메트릭 학습 개념을 통합한 **Siamese Mask R-CNN**을 제안합니다. 이 모델은 Siamese 백본을 사용하여 참조 이미지와 쿼리 이미지에서 공유 특징을 추출하고, 특징 매칭 모듈을 통해 참조와 쿼리 영역 간의 유사성을 계산합니다. 이 유사성을 기반으로 Mask R-CNN의 분류 헤드를 이진(일치/불일치) 분류로 변경하여 새로운 범주에 대한 일반화를 가능하게 합니다.

**결과:** MS-COCO 데이터셋의 새로운 범주에 대해 원샷 설정에서 객체 감지 16.3% mAP50, 인스턴스 분할 14.5% mAP50을 달성하여 무작위 성능보다 훨씬 뛰어나지만 학습 범주보다는 낮습니다. 주요 발견은 올바른 참조 범주를 **타겟팅하여 객체를 감지하는 것**이 가장 어려운 과제인 반면, 일단 객체가 식별되면 **정확하게 분할하는 것**은 비교적 잘 작동한다는 것입니다. 모델은 학습 범주에 대한 오탐지와 복잡한 장면에서 성능 저하를 겪는 한계가 있지만, 이 어려운 과제를 위한 강력한 기준선을 제시합니다.
