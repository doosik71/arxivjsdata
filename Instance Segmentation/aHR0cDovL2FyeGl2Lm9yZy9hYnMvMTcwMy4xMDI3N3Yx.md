# Semantic Instance Segmentation via Deep Metric Learning

Alireza Fathi, Zbigniew Wojna, Vivek Rathod, Peng Wang, Hyun Oh Song, Sergio Guadarrama, Kevin P. Murphy

## 🧩 Problem to Solve

본 논문은 이미지 내에서 각 객체의 개별 인스턴스(예: '사람-1', '사람-2')와 해당 카테고리를 식별하는 **시맨틱 인스턴스 분할(Semantic Instance Segmentation)** 문제를 다룹니다. 기존 접근 방식의 한계는 다음과 같습니다:

* **바운딩 박스 기반 방법**: 먼저 객체 바운딩 박스를 예측한 후 해당 박스 내에서 분할을 수행하는 방식입니다. 이 방법은 하나의 박스 안에 여러 인스턴스가 있을 경우 실패할 수 있으며, 불규칙한 객체 모양(예: 구부러진 사람, 의자)에 대해 바운딩 박스가 부적합할 수 있습니다.
* **"Box-free" 방법**: Faster R-CNN 아키텍처를 수정하여 직접 마스크를 예측하는 방식입니다. 그러나 이 방법은 전체 객체 인스턴스가 예측을 수행하는 단위의 수용 필드(receptive field) 내에 들어와야 하며, 일부 객체 카테고리에서는 "중심(center)"의 개념이 잘 정의되지 않는 문제가 있습니다.
본 논문은 이러한 기존 방법의 한계를 극복하고, 픽셀 간의 유사성을 기반으로 보다 자연스럽고 유연하게 객체 인스턴스를 분할하는 방법을 제안합니다.

## ✨ Key Contributions

* **새로운 인스턴스 분할 접근 방식**: 두 픽셀이 동일한 객체에 속할 가능성을 계산한 후 유사한 픽셀들을 그룹화하여 인스턴스 분할을 수행합니다.
* **심층 임베딩 모델 기반 유사성 메트릭 학습**: 픽셀의 로컬 컨텍스트를 고려하여 픽셀 간의 유사성을 예측하는 심층 완전 컨볼루션 임베딩 모델을 학습합니다.
* **"시드 포인트(Seed Points)" 기반 효율적인 마스크 생성**: "시드 포인트"에서 시작하여 충분히 유사한 모든 픽셀을 선택하여 마스크를 생성하는 그룹화 방법을 사용합니다. 시드 포인트는 별도의 심층 완전 컨볼루션 스코어링 모델로부터 "시드니스(seediness)" 점수를 통해 선택됩니다.
* **경쟁력 있는 결과 달성**: Pascal VOC 인스턴스 분할 벤치마크에서 기존의 Box-free 방식보다 훨씬 우수하며, Box-based 방식과도 경쟁력 있는 결과를 보였습니다.

## 📎 Related Works

* **바운딩 박스 기반 인스턴스 분할**:
  * **MNC [6], R2-IOS [16]**: Faster R-CNN [23]을 기반으로, 박스 감지 후 ROI 풀링을 통해 마스크를 예측하고 박스 위치를 개선하는 반복적인 방식을 사용합니다.
  * **SDS [10], Chen et al. [4]**: 유사하게 박스 제안 후 분할을 수행합니다.
* **"Box-free" 또는 슬라이딩 윈도우 기반 인스턴스 분할**:
  * **DeepMask [20], SharpMask [21], FCN-based [15]**: 각 위치에서 마스크 및 카테고리 확률을 예측합니다. 보통 이미지 피라미드나 특징 피라미드 네트워크(FPN) [18, 13]를 활용하여 다중 스케일 객체를 처리합니다.
* **순차적 인스턴스 분할**:
  * **Ren & Zemel [22], Romera-Paredes & Torr [24]**: RNN을 사용하여 한 번에 하나의 객체 인스턴스를 추출하는 방식입니다. 속도가 느리고 메모리 사용량이 많다는 단점이 있습니다.
* **워터셰드(Watershed) 알고리즘 기반**:
  * **Bai & Urtasun [2], Kirillov et al. [14]**: 픽셀별 에너지 값이나 인스턴스 인지 에지 맵에 워터셰드 알고리즘을 적용하여 객체 인스턴스를 분할합니다. 하지만 가려진 객체와 같이 연결되지 않은 영역을 하나의 인스턴스로 묶기 어렵습니다.
* **어소시에이티브 임베딩(Associative Embedding) [19]**:
  * 본 논문과 가장 유사한 접근 방식입니다. 픽셀별 객체성(objectness) 점수와 1차원 임베딩을 사용하여 클러스터링을 수행합니다. 본 논문은 [19]와 달리 D차원 임베딩을 사용하고, 다른 손실 함수 및 마스크 생성 방법을 통해 훨씬 더 나은 성능을 달성합니다.
* **메트릭 학습(Metric Learning)**:
  * **FaceNet [25]**: 얼굴 인식 및 클러스터링을 위한 통합 임베딩을 학습합니다. 본 논문은 이를 픽셀 유사성 학습에 적용합니다.
  * **N-pairs loss [27]**: 메트릭 학습에 사용되는 손실 함수입니다.

## 🛠️ Methodology

본 논문은 시맨틱 분할을 위해 사전 학습된 모델을 기반으로 두 가지 출력 "헤드(head)"를 추가하여 인스턴스 분할을 수행합니다.

1. **공유 특징 추출기 (Shared Feature Extractor)**
    * DeepLab v2 (ResNet-101 기반) 모델을 COCO 데이터셋에서 시맨틱 분할을 위해 사전 학습한 후, 최종 레이어를 제거하여 특징 추출기로 사용합니다.
    * 입력 이미지 $(2h, 2w, 3)$에 대해 $(h/4, w/4, 2048)$ 크기의 특징 맵을 출력하며, 이 특징 맵은 임베딩 모델과 분류/시드니스 모델의 입력으로 사용됩니다.

2. **임베딩 모델 (Embedding Model)**
    * **임베딩 벡터 생성**: 특징 추출기로부터 받은 특징 맵을 입력으로 받아 각 픽셀에 대한 $d$차원 임베딩 벡터 $e_p$를 출력합니다 (실험에서는 $d=64$ 사용). 이상적으로는 동일한 객체 인스턴스에 해당하는 픽셀들은 임베딩 공간에서 가깝고, 다른 객체(배경 포함)에 해당하는 픽셀들은 멀리 떨어져 있도록 학습됩니다.
    * **픽셀 유사성 계산**: 두 픽셀 $p, q$의 유사도 $\sigma(p, q)$는 다음 식을 통해 계산됩니다:
        $$
        \sigma(p,q) = \frac{2}{1 + \exp(||e_p - e_q||_{2}^{2})}
        $$
        임베딩 공간에서 가까운 픽셀 쌍은 $\sigma(p,q) \approx 1$이고, 멀리 떨어진 픽셀 쌍은 $\sigma(p,q) \approx 0$입니다.
    * **임베딩 손실 ($L_e$)**: 시그모이드 교차 엔트로피 손실을 사용하여 네트워크를 학습시킵니다. 학습 시 이미지 내 각 객체 인스턴스에서 $K$개의 픽셀을 무작위로 샘플링하여 픽셀 쌍을 구성하고, 동일 인스턴스 내 픽셀 쌍에는 목표 값 1, 다른 인스턴스 픽셀 쌍에는 0을 부여합니다. 손실은 인스턴스 크기에 반비례하도록 가중치를 부여하여 큰 객체에 편향되지 않도록 합니다.

3. **분류 및 시드니스 모델 (Classification and Seediness Model)**
    * **마스크 분류**: 특징 맵을 입력으로 받아 각 픽셀이 "시드"로 선택되었을 때 생성될 마스크의 클래스 레이블과 신뢰도 점수를 출력합니다 ($C+1$ 클래스, 0은 배경).
    * **분류 손실 ($L_{cls}$)**: 이미지 내 각 객체 인스턴스에서 $K=10$개의 무작위 픽셀을 선택하고, 각 픽셀로부터 마스크를 생성합니다. 생성된 마스크가 특정 IoU(Intersection over Union) 임계값 이상으로 실제 마스크와 겹치면 해당 픽셀에 실제 마스크의 레이블을 할당하고, 그렇지 않으면 배경 레이블을 할당합니다. 이 할당된 레이블을 사용하여 소프트맥스 교차 엔트로피 손실로 학습합니다.
    * **시드니스 점수 ($S_p$)**: 각 픽셀 $p$의 시드니스 점수는 다양한 유사도 임계값 $\tau \in \{0.25, 0.5, 0.75, 0.9\}$와 전경 클래스 $c \in 1:C$에 대한 마스크 분류 확률 $C^\tau_{pc}$ 중 최댓값으로 정의됩니다:
        $$
        S_p = \max_{\tau \in T, c \in 1:C} C^\tau_{pc}
        $$
        (배경 클래스는 제외하고 전경 클래스에 대해서만 최댓값을 계산합니다.) 높은 $S_p$는 해당 픽셀이 고품질 전경 마스크를 생성하기에 좋은 시드임을 나타냅니다.

4. **마스크 생성 (Mask Creation)**
    * **시드 픽셀 선택**: 시드니스 히트맵 $S_p$를 기반으로 시드 픽셀을 탐욕적으로 선택합니다. 공간적 다양성을 장려하기 위해 이전에 선택된 시드 픽셀들로부터 임베딩 공간에서 멀리 떨어진 픽셀을 선택합니다.
        $$
        p_t = \arg \max_{p \notin p_{1:t-1}} [\log(S_p) + \alpha \log(D(p, p_{1:t-1}))]
        $$
        여기서 $D(p, p_{1:t-1}) = \min_{q \in p_{1:t-1}} ||e_p - e_q||_{2}^{2}$ 이고, $\alpha$는 다양성을 조절하는 계수입니다 (실험적으로 $\alpha=0.3$이 최적으로 발견됨).
    * **마스크 확장**: 선택된 시드 픽셀 $p$와 임계값 $\tau$를 사용하여, $p$와 유사도 $\sigma(p,q)$가 $\tau$보다 크거나 같은 모든 픽셀 $q$를 포함하는 마스크 $m(p,\tau) = \{q : \sigma(p,q) \ge \tau\}$를 생성합니다.
    * **효율적인 구현**: 모든 픽셀의 임베딩 텐서 $A$와 $K$개 시드 포인트의 임베딩 텐서 $B$를 사용하여 $A^2 + B^2 - 2A \cdot B$ 연산을 통해 효율적으로 거리를 계산하고 임계값을 적용합니다.

5. **다중 스케일 처리 (Handling Scale)**
    * 4가지 스케일(0.25, 0.5, 1.0, 2.0)의 이미지 피라미드를 구성하고 각각에 대해 특징 추출기를 실행한 후, 특징 맵을 동일한 크기로 재조정하고 평균하여 두 헤드의 입력으로 사용합니다.

6. **결합 학습 (Joint Training)**
    * 전체 손실 함수 $L = L_e + \lambda L_{cls}$를 사용하여 두 헤드를 공유 바디(body)와 함께 공동으로 학습합니다. $\lambda$는 임베딩 모델이 먼저 학습되도록 0에서 시작하여 점차 0.2까지 증가시킵니다. 공유 특징의 학습률은 출력 헤드보다 작게 설정하여 사전 학습된 특징이 크게 변하지 않도록 합니다.

## 📊 Results

* **벤치마크**: PASCAL VOC 2012 검증 세트에서 평가되었습니다.
* **mAP (평균 정확도)**:
  * IoU 임계값 0.5에서 62.1% mAP를 달성했습니다. 이는 이전 Box-free 방식인 PFN [17] (58.7%)이나 Associative Embedding [19] (35.1%)보다 월등히 높으며, 상위 Box-based 방법들과도 경쟁력 있는 수준입니다 (MNC [6] 63.5%, Li et al. [15] 65.7%, R2-IOS [16] 66.7%).
  * IoU 임계값 0.6에서 53.3% mAP를 달성하여 R2-IOS [16] 다음으로 2위를 기록했습니다.
  * IoU 임계값 0.7에서 41.5% mAP를 달성하여 MNC [6] 및 PFN [17]과 비슷한 수준을 기록했습니다.
* **시드 샘플링 전략의 영향**: 다양성 계수 $\alpha$의 최적 값은 0.3으로 나타났습니다 (표 2 참조).
* **시드 포인트 개수의 영향**: 단 10개의 시드 포인트만으로도 59.7%의 mAP를 달성하며, 시드 포인트 수가 증가함에 따라 성능이 꾸준히 향상됩니다 (표 3 참조).
* **클래스별 성능**: 기차, 개, 오토바이와 같은 큰 객체에서는 매우 좋은 성능을 보였으나, 자전거와 같이 섬세한 부분이 많은 객체에서는 성능이 좋지 않았습니다. 이는 훈련 세트와 테스트 세트의 마스크 주석 품질 불일치(훈련 세트는 자전거 전체를 덮는 거친 마스크, 테스트 세트는 개별 바큇살까지 분할)에 기인합니다.

## 🧠 Insights & Discussion

* 본 논문의 접근 방식은 픽셀 간의 유사성을 학습하고 이를 통해 인스턴스를 그룹화하는 새로운 패러다임을 제시하며, 기존의 바운딩 박스나 객체 중심에 의존하는 방식의 한계를 극복합니다. 이는 특히 길고 복잡한 형태의 객체에 강점을 가질 수 있습니다.
* "시드니스" 점수와 임베딩 공간에서의 다양성을 고려한 시드 선택 전략은 높은 재현율과 정밀도를 동시에 달성하는 데 중요한 역할을 합니다.
* 경쟁력 있는 성능을 달성했지만, PASCAL VOC 벤치마크에서 일부 최신 Box-based SOTA 모델에는 미치지 못합니다. 이는 이 접근 방식이 상대적으로 초기 단계이며, 더 많은 최적화 가능성이 있음을 시사합니다.
* 제한 사항으로는 훈련 데이터의 마스크 주석 품질이 결과에 미치는 영향이 크다는 점이 있습니다 (예: 자전거 클래스).
* 향후 연구 방향으로는 COCO 및 Cityscapes와 같은 더 큰 데이터셋에서의 평가, 그리고 마스크 생성 과정 자체를 미분 가능하게 만들어 종단 간(end-to-end) 학습을 가능하게 하는 방법 고안이 제안되었습니다.

## 📌 TL;DR

본 논문은 딥 메트릭 학습을 활용한 시맨틱 인스턴스 분할 방법을 제안합니다. 픽셀 간의 유사성을 학습하는 임베딩 모델과 마스크의 품질을 예측하는 시드니스 모델을 결합하여, 지능적으로 선택된 시드 포인트에서 시작하여 유사한 픽셀들을 그룹화함으로써 각 객체 인스턴스를 분할합니다. PASCAL VOC 2012에서 기존의 Box-free 방식보다 훨씬 우수하며 Box-based 방식과도 경쟁력 있는 62.1% <mAP@0.5IoU> 성능을 달성하여, 바운딩 박스나 객체 중심에 의존하지 않는 유연하고 효과적인 인스턴스 분할 접근 가능성을 보여주었습니다.
