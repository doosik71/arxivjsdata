# A Simple Framework for Open-Vocabulary Segmentation and Detection

Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lei Zhang

## 🧩 Problem to Solve

최근 CLIP과 같은 대규모 이미지-텍스트 쌍 모델을 활용하여 새로운 개념이나 도메인으로 전이가 가능한 개방형 어휘(open-vocabulary) 비전 시스템 개발이 중요해지고 있습니다. 하지만 객체 탐지(object detection)와 이미지 분할(image segmentation)이라는 핵심 비전 작업들은 다음과 같은 본질적인 불일치를 가지고 있어, 두 작업을 단일 모델로 효과적으로 통합하는 것이 어렵습니다:

1. **어휘 및 어노테이션 세분성(granularity) 불일치**: 탐지 데이터셋(예: Objects365)은 훨씬 더 많은 카테고리와 박스 어노테이션을 가지는 반면, 분할 데이터셋(예: COCO)은 더 적은 카테고리와 마스크 어노테이션을 가집니다.
2. **작업 불일치(Task Discrepancy)**: 분할(특히 팬옵틱 분할)은 전경(foreground) 객체("things")와 배경(background) 영역("stuff") 모두의 마스크를 요구하는 반면, 탐지는 주로 전경 객체에만 관심을 가집니다.
3. **데이터 불일치(Data Discrepancy)**: 박스 어노테이션은 마스크 어노테이션보다 공간적 세분성이 낮아 직접 상호 변환하기 어렵습니다 (마스크에서 박스는 쉽지만, 박스에서 마스크는 어려움).

기존 연구들은 주로 탐지나 분할 중 한 가지에 초점을 맞추거나, 폐쇄 집합(closed-set) 설정에서 동작했습니다. 따라서 본 연구는 이러한 간극을 해소하고, 탐지 및 분할 데이터를 함께 학습하여 두 작업 모두에 대해 **단일하고 강력한 개방형 어휘 모델**을 구축하는 것을 목표로 합니다.

## ✨ Key Contributions

* **탐지 및 분할 공동 학습을 위한 강력한 베이스라인 제시**: 탐지 및 분할 데이터를 동시에 학습하여 두 작업 모두를 위한 개방형 어휘 모델을 제시하는 최초의 강력한 베이스라인 모델인 OpenSeeD를 소개합니다.
* **작업 및 데이터 불일치 해소를 위한 기법 제안**: 두 작업/데이터셋 간의 불일치를 완화하기 위해 다음과 같은 세 가지 핵심 기법을 제안합니다:
  * **공유 의미 공간(Shared Semantic Space)**: 사전 학습된 단일 텍스트 인코더를 사용하여 모든 시각적 개념을 공통 의미 공간에 임베딩합니다.
  * **분리된 전경 및 배경 디코딩(Decoupled Foreground and Background Decoding)**: 전경/배경 쿼리를 분리하고, 언어 가이드 전경 쿼리 선택(Language-guided foreground query selection)을 통해 전경 객체 인식을 최적화합니다.
  * **조건부 마스크 디코딩(Conditioned Mask Decoding)**: 주어진 박스에서 마스크를 생성하는 능력을 학습하여 탐지 데이터에 대한 마스크 감독을 보조하고, 상호작용적 분할(interactive segmentation) 인터페이스를 제공합니다.
* **새로운 SOTA 성능 달성**: OpenSeeD는 공동 훈련을 통해 다양한 데이터셋에서 개방형 어휘 분할 및 태스크 전이(task transfer)에서 새로운 SOTA(State-of-the-Art) 성능을 달성했으며, 개방형 어휘 객체 탐지에서도 경쟁력 있는 성능을 입증했습니다.

## 📎 Related Works

* **일반적인 분할 및 탐지 (Generic Segmentation and Detection)**: Mask R-CNN, DETR(Detection Transformer) 기반 모델들은 뛰어난 성능을 보였지만, 폐쇄 집합(closed-set) 환경에서 제한된 어휘 크기에 국한되었습니다.
* **개방형 어휘 분할 (Open-Vocabulary Segmentation)**: CLIP [39]과 같은 대규모 비전-언어 모델을 활용하여 시각-의미 지식을 전이하는 방식이 연구되었습니다 (예: X-Decoder [60], ODISE [46]).
* **개방형 어휘 탐지 (Open-Vocabulary Detection)**: OV-DETR [53], VILD [13]와 같이 파운데이션 모델을 활용하거나, GLIP [29]처럼 탐지를 그라운딩(grounding) 문제로 공식화하여 성능을 개선했습니다.
* **약한 감독 분할 (Weakly-Supervised Segmentation)**: BoxInst [42], Box2Mask [30] 등 박스 어노테이션만을 사용하여 분할을 생성하는 연구들이 있었으나, 폐쇄 집합에 국한되며 마스크 감독을 사용하는 모델보다 성능이 떨어집니다.
* **박스 및 마스크로부터 학습 (Learning from Box and Mask)**: Mask R-CNN [15]은 단일 데이터셋에서 박스와 마스크를 공동 학습했지만 전경 인스턴스에만 해당됩니다. Mask DINO [28] 등은 박스 감독으로 사전 학습 후 분할로 전이하는 방식(pre-train-then-fine-tune)을 사용했으나, 이는 두 개의 분리된 닫힌 집합 모델을 생성합니다.

OpenSeeD는 이러한 기존 연구들과 달리, 탐지 및 분할 데이터를 동시에 활용하여 두 작업 모두에 대해 **단일하고 개방형 어휘를 지원하는 모델**을 개발하는 데 중점을 둡니다.

## 🛠️ Methodology

OpenSeeD는 단일한 인코더-디코더 프레임워크를 기반으로 하며, 이미지 인코더($\text{Enc}_{\text{I}}$), 텍스트 인코더($\text{Enc}_{\text{T}}$), 그리고 세 가지 유형의 쿼리(query)를 처리하는 디코더로 구성됩니다.

1. **기본 손실 함수 및 공유 의미 공간**:
    * 이미지 $I$와 어휘 $V$를 각각 이미지 인코더($\text{Enc}_{\text{I}}$)와 텍스트 인코더($\text{Enc}_{\text{T}}$)로 인코딩하여 이미지 특징 $O$와 텍스트 특징 $T = \{t_1, \dots, t_K\}$를 얻습니다.
    * 디코더는 $L$개의 쿼리 $Q$와 $O$를 입력으로 받아 마스크 $P_m$, 박스 $P_b$, 의미론 $P_s$를 예측합니다.
    * 예측된 의미론 $P_s$와 텍스트 특징 $T$ 간의 유사도 $\text{Sim}(P_s, T)$를 계산하여 분류 점수 $P_c$를 얻고, 이를 통해 시각-의미 매칭을 수행합니다.
    * 분할 데이터($D_m$)와 탐지 데이터($D_b$)의 손실을 합산하여 훈련하며, 분할 데이터의 마스크($m$)로부터 박스($\hat{b}$)를 유도하여 박스 손실($L_b$)을 계산합니다. 이를 통해 두 작업 간의 공통 의미 공간을 학습합니다.

2. **분리된 전경 및 배경 디코딩 (Decoupled Foreground and Background Decoding)**:
    * **목표**: 분할(전경+배경)과 탐지(전경만) 간의 작업 불일치를 해결.
    * 디코더 쿼리 $Q$를 **전경 쿼리($Q_f$)**, **배경 쿼리($Q_b$)**, **조건부 쿼리($Q_d$)**로 나눕니다.
    * **언어 가이드 전경 쿼리 선택**:
        * 이미지 특징 $O$와 텍스트 특징 $T$를 사용하여 경량 모듈이 각 특징에 대한 박스 $E_b$와 분류 점수 $E_c = \text{Sim}(O, T)$를 예측합니다.
        * $E_c$의 점수를 기준으로 상위 $L_f$개의 이미지 특징과 박스를 전경 쿼리로 선택하여, 주어진 텍스트 개념과 관련된 토큰만 디코딩하도록 유도합니다.
    * **학습 가능한 배경 쿼리**:
        * 배경 스터프는 비교적 적은 수의 카테고리를 가지며, 선택된 참조점이 넓고 비볼록한 배경 영역을 넘어설 수 있으므로, 학습 가능한 쿼리 임베딩을 사용하여 배경 쿼리를 처리합니다.
    * 분할 데이터에 대해서는 전경 및 배경 디코딩을 모두 사용하고, 탐지 데이터에 대해서는 전경 디코딩만 사용합니다. 두 쿼리 유형은 동일한 디코더를 공유하며 상호 작용합니다.

3. **조건부 마스크 디코딩 (Conditioned Mask Decoding)**:
    * **목표**: 탐지 데이터에 마스크 어노테이션이 없는 데이터 불일치를 해결.
    * 분할 데이터에서 지상 진실(GT) 개념($c$)과 박스($b$)가 주어졌을 때 마스크($m$)를 디코딩하는 매핑 ($(\text{c}, \text{b}) \to \text{m}$)을 학습합니다: $P_m = \text{Dec}((t, b); O)$, 여기서 $t$는 개념 $c$의 텍스트 특징입니다.
    * 이 학습된 조건부 마스크 디코딩 능력은 다음과 같이 탐지 데이터를 보조하는 데 사용됩니다:
        * **온라인 마스크 보조(Online Mask Assistance)**: 훈련 중에 실시간으로 탐지 데이터에 대한 의사 마스크($\hat{m}$)를 생성하여 예측과 GT 인스턴스 매칭을 돕습니다.
        * **오프라인 마스크 보조(Offline Mask Assistance)**: 수렴된 OpenSeeD 모델을 사용하여 Objects365와 같은 탐지 데이터셋에 대한 의사 마스크 어노테이션을 생성하고, 이를 활용하여 분할 모델을 추가로 훈련할 수 있습니다.
    * 이는 주어진 박스와 개념으로부터 마스크를 생성하는 상호작용적 분할 인터페이스로도 활용될 수 있습니다.

## 📊 Results

OpenSeeD는 COCO와 Objects365 데이터를 공동으로 사전 학습한 후, 다양한 벤치마크에서 뛰어난 성능을 보였습니다.

* **제로샷 개방형 어휘 벤치마킹 (Table 2)**:
  * ADE20K, Cityscapes, LVIS 등 60개 이상의 데이터셋에서 제로샷 평가 결과, 인스턴스 분할에서 이전 SOTA 모델들(ODISE, X-Decoder)을 크게 능가했습니다. 특히 Cityscapes에서는 X-Decoder 대비 10.2 mask AP, LVIS에서는 9.8 mask AP를 개선했습니다.
  * 팬옵틱 및 의미 분할에서도 X-Decoder와 비교하여 경쟁력 있는 성능을 달성하거나 4개 데이터셋 중 3개에서 우위를 보였습니다.
  * 객체 탐지에서는 LVIS에서 GLIP-T 대비 2.8 AP(평균) 향상으로 합리적인 성능을 입증했습니다.
  * **핵심**: OpenSeeD는 분할과 탐지 데이터를 함께 사전 학습하고 두 작업 모두에 제로샷 전이가 가능한 최초의 모델입니다.

* **직접 및 태스크별 전이 (Table 3)**:
  * 사전 학습 후 추가적인 파인튜닝 없이 COCO 팬옵틱 분할에서 **59.5 PQ**라는 새로운 SOTA 성능을 달성했습니다.
  * 데이터별 파인튜닝 후, ADE20K 팬옵틱 (53.7 PQ), 인스턴스 분할 (42.6 AP) 및 Cityscapes 인스턴스 분할 (48.5 AP)에서 새로운 SOTA를 기록했습니다.

* **Wild 환경에서의 분할 및 탐지 (Table 4, 5)**:
  * **Segmentation in the Wild (SeginW)** 벤치마크 (25개 데이터셋)에서 이전 SOTA 대비 평균 mask AP 10 이상 개선하여, 탐지 감독이 인스턴스 분할 성능을 크게 향상시킴을 입증했습니다.
  * **Object Detection in the Wild (ODinW)** 벤치마크 (35개 데이터셋)에서 GLIP-T 대비 평균 2.8 AP 개선을 보였습니다.

* **Ablation Studies (Table 6, 7, 8, 9)**:
  * **기본 구성 요소**: 탐지 데이터를 활용하고 다양한 백본을 적용했을 때 분할 성능이 유의미하게 향상됨을 확인했습니다.
  * **오프라인 마스크 감독**: 의사 어노테이션(pseudo-annotations)을 활용했을 때 마스크 AP 및 박스 AP가 모두 개선되었으며, COCO 제로샷 분할 성능이 완전 학습 모델에 필적하는 수준에 도달했습니다.
  * **분리된 디코딩 및 온라인 마스크 보조**: 이들을 제거했을 때 ADE20K의 인스턴스 마스크 및 박스 성능이 유의미하게 하락하여, 제안된 기법들이 분할-탐지 데이터 간의 간극을 줄이는 데 효과적임을 확인했습니다.

## 🧠 Insights & Discussion

OpenSeeD는 단일 모델로 객체 탐지와 분할 작업을 통합하는 개방형 어휘 학습의 강력한 잠재력을 보여주었습니다. 이 연구는 다음과 같은 중요한 시사점을 제공합니다:

* **통합 모델의 효율성**: 기존에 분리되어 연구되던 탐지와 분할 작업을 단일 프레임워크 내에서 효과적으로 학습함으로써, 다양한 태스크와 데이터셋에서 경쟁력 있는 성능을 제공하는 효율적인 모델을 제시했습니다.
* **탐지 데이터의 가치**: 탐지 데이터에 내재된 풍부한 인스턴스 레벨 지식이 분할 작업, 특히 인스턴스 분할 성능을 크게 향상시키는 데 기여함을 명확히 보여주었습니다.
* **상호작용적 분할**: 조건부 마스크 디코딩 능력은 사용자가 박스 힌트를 제공하여 마스크를 생성하는 새로운 이미지 분할 인터페이스를 가능하게 하며, 이는 분할 어노테이션 과정을 가속화할 잠재력을 가집니다.
* **향후 연구 방향**: 현재 OpenSeeD는 대규모 이미지-텍스트 쌍이나 그라운딩(grounding) 데이터와 같은 외부 데이터 소스를 활용하지 않습니다. 이는 훈련 데이터와 의미론적 커버리지를 더욱 풍부하게 확장하여, 더 큰 규모의 공동 훈련을 탐색할 수 있는 향후 연구 방향을 제시합니다.

## 📌 TL;DR

**문제**: 객체 탐지와 분할은 어휘 및 어노테이션 세분성, 전경/배경 초점, 박스/마스크 형태 등의 불일치로 인해 단일 개방형 어휘 모델로 통합하기 어렵습니다.

**제안 방법**: OpenSeeD는 이러한 불일치를 해결하고 탐지 및 분할 데이터를 함께 학습하는 단일 엔드-투-엔드(end-to-end) 프레임워크를 제안합니다. 핵심 기법은 다음과 같습니다: 1) 모든 개념에 대한 **공유 의미 공간** 학습, 2) 전경 객체와 배경 스터프를 분리하여 처리하는 **분리된 디코딩**, 3) 주어진 박스로부터 마스크를 생성하여 탐지 데이터 훈련을 보조하는 **조건부 마스크 디코딩**입니다.

**주요 발견**: OpenSeeD는 제로샷 개방형 어휘 분할(인스턴스, 팬옵틱, 의미론적)에서 이전 SOTA 모델들을 능가하고, 객체 탐지에서도 경쟁력 있는 성능을 보였습니다. 또한, 사전 학습된 모델은 추가 파인튜닝 없이 COCO 팬옵틱 분할에서 새로운 SOTA를 달성했으며, 다양한 데이터셋에서 태스크 전이 성능도 크게 향상시킴을 입증했습니다.
