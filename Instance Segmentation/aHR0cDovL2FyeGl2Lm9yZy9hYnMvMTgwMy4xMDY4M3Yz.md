# Pose2Seg: Detection Free Human Instance Segmentation

Song-Hai Zhang, Ruilong Li, Xin Dong, Paul Rosin, Zixi Cai, Xi Han, Dingcheng Yang, Haozhi Huang and Shi-Min Hu

## 🧩 Problem to Solve

기존 객체 인스턴스 분할(instance segmentation) 방법은 주로 객체 감지(object detection)에 기반하며, 바운딩 박스를 사용하여 인스턴스를 분리합니다. 하지만 이 접근 방식은 두 객체가 심하게 겹치거나 가려진(occlusion) 상황에서 인스턴스를 정확하게 분리하기 어렵다는 한계가 있습니다. 특히 '인간' 카테고리는 포즈 스켈레톤으로 잘 정의될 수 있는 고유한 특성을 가지고 있음에도 불구하고, 기존 연구들은 이를 충분히 활용하지 못했습니다. 또한, 심하게 가려진 인간에 대한 포괄적인 주석이 포함된 공개 데이터셋이 부족하여 이 문제를 심층적으로 연구하기 어려운 실정이었습니다.

## ✨ Key Contributions

* **포즈 기반 인간 인스턴스 분할 프레임워크 제안:** 기존의 감지 기반 방식 대신 인간 포즈에 기반하여 인스턴스를 분할하는 새로운 프레임워크를 제시합니다. 이 프레임워크는 특히 가려진 상황에서 기존 감지 기반 프레임워크보다 더 나은 정확도를 달성합니다.
* **Affine-Align 모듈 개발:** 인간 포즈를 기반으로 이미지 영역(RoI)을 균일한 스케일과 방향으로 정렬하는 새로운 포즈 기반 정렬 모듈인 Affine-Align을 제안합니다. 이는 기존의 바운딩 박스 기반 정렬보다 겹친 인스턴스 분리에 효과적입니다.
* **스켈레톤 특징(Skeleton Features) 명시적 활용:** 인간 스켈레톤 특징을 분할 모듈에 명시적으로 연결(concatenate)하여 분할 정확도를 더욱 향상시킵니다. 이 특징은 네트워크가 특정 인스턴스에 집중하도록 안내하는 역할을 합니다.
* **새로운 벤치마크 데이터셋 OCHuman 공개:** 심하게 가려진 인간 인스턴스에 초점을 맞춘 새로운 벤치마크 데이터셋인 OCHuman을 소개합니다. 이 데이터셋은 바운딩 박스, 인간 포즈, 인스턴스 마스크를 포함하는 포괄적인 주석을 제공하여 가려짐 문제 연구를 위한 도전적인 환경을 제공합니다.

## 📎 Related Works

* **다중 인물 포즈 추정 (Multi-Person Pose Estimation):**
  * 상향식(Top-down) 방법은 먼저 인물 감지 후 각 바운딩 박스 내에서 포즈를 추정하며 (예: [2, 6, 14]), 가려짐에 취약합니다.
  * 하향식(Bottom-up) 방법은 먼저 모든 신체 부위 키포인트를 감지한 후 이들을 인스턴스로 그룹화합니다 (예: OpenPose [1], Associative Embedding [26]). 이는 객체 감지에 의존하지 않아 겹치는 인물 분리에 유리합니다.
* **인스턴스 분할 (Instance Segmentation):**
  * 초기에는 감지 후 의미론적 분할을 적용하는 다단계 파이프라인이 있었으며 (예: [4, 10, 12]), 이후 감지와 분할을 엔드투엔드(end-to-end)로 통합하는 방법들이 등장했습니다 (예: [3, 19, 22, 30]). Mask R-CNN [14]이 이 분야의 최첨단 기술 중 하나입니다.
* **인간 포즈 추정을 활용한 인스턴스 분할:**
  * Mask R-CNN [14]은 객체 감지, 인스턴스 분할, 포즈 추정을 동시에 수행하지만, 분할 작업에서는 마스크만 사용했을 때 키포인트와 마스크를 결합한 것보다 성능이 좋다고 언급되었습니다.
  * Pose2Instance [38]는 인스턴스 분할을 위해 포즈 추정을 활용하는 캐스케이드 네트워크를 제안합니다.
  * PersonLab [28]은 인스턴스 분할을 픽셀 단위 클러스터링으로 다루고 포즈를 사용하여 클러스터링 결과를 개선하지만, 분할 성능은 Mask R-CNN에 미치지 못했습니다.

## 🛠️ Methodology

본 연구의 프레임워크는 이미지와 인간 포즈를 입력으로 받아 인스턴스 분할을 수행합니다.

1. **전체 구조:**
    * **특징 추출:** 베이스 네트워크(예: ResNet50-FPN)가 입력 이미지에서 특징을 추출합니다.
    * **Affine-Align:** 추출된 특징 맵과 인간 포즈 키포인트를 기반으로 Affine-Align 모듈이 각 인간 인스턴스의 영역(RoI)을 $64 \times 64$ 크기의 균일한 형태로 정렬합니다.
    * **스켈레톤 특징 통합:** 각 인스턴스에 대해 생성된 Skeleton features(스켈레톤 특징)가 Affine-Align된 RoI 특징 맵에 연결됩니다.
    * **분할 예측:** SegModule이라는 분할 모듈이 연결된 특징을 받아 최종 인스턴스 마스크를 예측합니다.
    * **역변환:** Affine-Align 과정에서 추정된 어파인 변환 행렬을 역적용하여 최종 분할 결과를 원래 이미지 해상도로 복원합니다.
2. **Affine-Align 연산:**
    * **인간 포즈 표현:** 각 인물의 포즈는 17개의 신체 관절 (예: 눈, 코, 귀, 어깨, 팔꿈치, 손목 등)의 2D 좌표 $(x, y)$와 가시성 $(v)$으로 구성된 벡터 $P = (C_1, C_2, \dots, C_m) \in \mathbb{R}^{m \times 3}$로 표현됩니다.
    * **포즈 템플릿:** 학습 데이터셋의 다양한 포즈들을 K-means 클러스터링하여 대표적인 포즈 템플릿 $P_\mu$을 생성합니다. 본 연구에서는 K=3일 때 반신, 전신 후면, 전신 전면 템플릿을 사용하여 좌우 반전을 고려한 정렬에 충분하다고 판단했습니다.
    * **어파인 변환 행렬 추정:** 감지된 포즈 $P$를 템플릿 $P_\mu$에 가장 가깝게 변환하는 어파인 변환 행렬 $H$를 최적화하여 추정합니다.
        $$ H^* = \arg \min_H \| H \cdot P - P_\mu \| $$
        $H$는 회전, 스케일, x/y축 이동, 좌우 반전의 5가지 독립 변수를 포함하는 $2 \times 3$ 행렬입니다. 최적의 $H^*$는 최소 오류 값을 기반으로 선택되며, 이를 통해 각 RoI가 표준화된 포즈로 정렬됩니다.
3. **스켈레톤 특징 (Skeleton Features):**
    * OpenPose [1]의 PAF(Part Affinity Fields)를 활용하여 각 스켈레톤을 2채널 벡터 필드 맵으로 표현합니다 (COCO 데이터셋 기준 총 38채널).
    * 신체 부위 키포인트 주변의 중요성을 강조하기 위해 17채널의 Part Confidence Maps를 추가합니다.
    * 총 55채널의 스켈레톤 특징 맵을 Affine-Align된 이미지 특징 맵에 연결하여 네트워크가 특정 인물에 대한 구조적 정보를 명시적으로 활용하도록 돕습니다.
4. **SegModule:**
    * 정렬된 $64 \times 64$ RoI의 특징을 처리하기 위해 ResNet [15]의 잔차(residual) 유닛을 기반으로 설계되었습니다.
    * $7 \times 7$, stride-2 합성곱 층으로 시작하여 여러 개의 잔차 유닛을 쌓아 약 50 픽셀의 충분한 수용장(receptive field)을 확보합니다.
    * 이후 이중 선형 업샘플링 층으로 해상도를 복원하고, 추가 잔차 유닛 및 $1 \times 1$ 합성곱 층을 통해 최종 이진 마스크를 예측합니다.

## 📊 Results

* **가려짐 상황에서의 성능 (OCHuman 데이터셋):**
  * Mask R-CNN 대비 약 50% 높은 AP(Average Precision)를 달성하여 가려짐 문제에 대한 본 방법의 우수성을 입증했습니다 (Mask R-CNN: 0.163 AP vs. Ours: 0.222 AP).
  * Ground-truth (GT) 키포인트를 입력으로 사용했을 때 정확도가 0.544 AP로 두 배 이상 향상되어, 더 정교한 키포인트 검출기가 있다면 가려짐 문제에서 훨씬 더 뛰어난 성능을 보일 수 있음을 시사했습니다.
* **일반 상황에서의 성능 (COCOPersons 데이터셋):**
  * Mask R-CNN (0.532 AP)보다 높은 0.555 AP를 달성했습니다.
  * PersonLab [28] (ResNet101 기반, 0.476 AP)과 비교했을 때, 더 가벼운 ResNet50 백본을 사용했음에도 불구하고 더 나은 성능을 보였습니다.
  * GT 키포인트 입력 시 0.582 AP를 달성했습니다.
* **Ablation 연구:**
  * **Affine-Align vs. RoI-Align:**
    * OCHuman 데이터셋(가려짐)에서 GT 바운딩 박스 기반 RoI-Align(0.476 AP)보다 GT 포즈 기반 Affine-Align(0.544 AP)이 더 우수했습니다. 이는 Affine-Align의 회전 허용이 겹친 인물 분리에 효과적임을 보여줍니다.
    * COCOPerson 데이터셋(일반)에서도 Affine-Align과 Skeleton features를 결합한 경우가 RoI-Align 단독보다 성능이 우수했습니다.
    * 포즈 키포인트로부터 바운딩 박스를 생성하여 RoI-Align을 적용하는 직관적인 방법은 Affine-Align보다 성능이 낮았습니다.
  * **스켈레톤 특징 유무:** Skeleton features를 사용했을 때 모든 정렬 전략에서 AP가 일관되게 향상되었습니다. 이는 스켈레톤 특징이 네트워크가 특정 인물에 집중하고 분할 정확도를 높이는 데 명시적인 정보를 제공함을 의미합니다.
  * **SegModule 깊이:** $64 \times 64$ 정렬 크기에 대해 10개의 잔차 유닛(수용장 약 50 픽셀)이 충분했으며, 더 깊은 아키텍처는 성능 향상에 미미한 영향을 미쳤습니다.

## 🧠 Insights & Discussion

본 연구는 '인간'이라는 객체 카테고리의 고유한 특성인 포즈 스켈레톤을 활용하는 것이 인스턴스 분할, 특히 가려진 상황에서 기존 바운딩 박스 기반 접근법보다 훨씬 효과적임을 명확히 입증했습니다. 새로운 Affine-Align 모듈은 회전 변환을 통해 겹친 인물들을 더 잘 분리하는 '차별적인(discriminative)' RoI를 생성함으로써 분할 네트워크의 성능 향상에 크게 기여합니다. 또한, 스켈레톤 특징을 명시적으로 사용하는 것은 네트워크에 강력한 가이드를 제공하여, 복잡하게 얽힌 여러 인스턴스 사이에서도 특정 인물을 정확하게 분할할 수 있는 능력을 부여합니다.

도입된 OCHuman 데이터셋은 가려짐 문제를 벤치마킹하는 데 중요한 역할을 하며, 기존 데이터셋의 한계를 극복하고 실제 시나리오에서의 알고리즘 강건성 연구를 장려합니다. 제안된 프레임워크는 감지 기반 방법의 NMS(Non-maximum Suppression) 한계를 극복할 뿐만 아니라, 향후 더 정교한 포즈 추정기와의 결합 시 더욱 큰 잠재력을 가질 것으로 기대됩니다. 본 연구는 인스턴스 분할 분야에서 객체 고유의 특성을 깊이 있게 이해하고 활용하는 것의 중요성을 강조합니다.

## 📌 TL;DR

**문제:** 기존 인스턴스 분할 방법은 바운딩 박스 기반 객체 감지를 사용하여 겹치거나 가려진 인간 인스턴스를 정확히 분리하는 데 한계가 있으며, 가려진 인간 데이터셋이 부족합니다.
**방법:** 이 연구는 인간 포즈에 기반한 인스턴스 분할 프레임워크 `Pose2Seg`를 제안합니다. `Affine-Align`을 통해 바운딩 박스 대신 포즈를 사용하여 RoI를 정렬하고, `Skeleton features`를 네트워크에 통합하여 분할을 안내하며, `SegModule`로 마스크를 예측합니다. 또한 심하게 가려진 인간에 대한 새 벤치마크 데이터셋 `OCHuman`을 도입했습니다.
**결과:** `Pose2Seg`는 `OCHuman` 및 `COCOPersons` 데이터셋에서 Mask R-CNN 및 PersonLab을 능가하는 성능을 보여주며, 특히 가려진 상황에서 뛰어난 강건성을 입증했습니다. `Affine-Align`과 `Skeleton features`가 성능 향상에 핵심적인 역할을 함을 Ablation 연구를 통해 확인했습니다.
