# Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth

Davy Neven, Bert De Brabandere, Marc Proesmans, Luc Van Gool

## 🧩 Problem to Solve

최신 인스턴스 분할(Instance Segmentation) 방법들은 자율 주행과 같은 실시간 응용 분야에 적합하지 않습니다.

* **제안 기반(Proposal-based) 방법 (예: Mask R-CNN)**: 높은 정확도를 제공하지만, 실행 속도가 느리고 고정된 낮은 해상도의 마스크를 생성합니다.
* **제안 없는(Proposal-free) 방법**: 마스크를 고해상도로 생성할 수 있고 더 빠르지만, 제안 기반 방법만큼의 정확도를 달성하지 못합니다.
이러한 문제로 인해, 높은 정확도와 고해상도 마스크를 실시간으로 생성할 수 있는 인스턴스 분할 방법이 필요합니다.

## ✨ Key Contributions

* **새로운 클러스터링 손실 함수 제안**: 픽셀의 공간 임베딩(spatial embeddings)을 동일 인스턴스에 속하는 픽셀끼리 모으고, 각 인스턴스에 특화된 클러스터링 대역폭($\sigma_k$)을 공동으로 학습하는 새로운 손실 함수를 제안합니다.
* **IoU 직접 최적화**: Lovasz-hinge 손실 함수를 사용하여 결과 인스턴스 마스크의 IoU (Intersection-over-Union)를 직접적으로 최대화합니다.
* **실시간 고정확도 달성**: 빠른 아키텍처와 결합하여, Cityscapes 벤치마크에서 2MP 이미지에 대해 10fps 이상의 속도(11fps)로 Mask R-CNN보다 5% 향상된 27.6 AP를 달성하며 실시간 인스턴스 분할을 수행합니다. 이는 고정확도를 유지하면서 실시간으로 작동하는 최초의 제안 없는 인스턴스 분할 방법입니다.
* **학습 가능한 마진**: 객체의 크기에 따라 최적의 클러스터링 마진을 유동적으로 학습함으로써, 큰 객체의 가장자리 픽셀에 대한 손실을 완화합니다.

## 📎 Related Works

* **제안 기반 인스턴스 분할**: Faster R-CNN [24] 프레임워크에 기반하며 Mask R-CNN [9], PANet [17] 등이 대표적입니다. 높은 정확도를 보이지만, 저해상도 마스크 생성 및 느린 속도가 단점입니다.
* **제안 없는 인스턴스 분할**: 조밀 예측(dense-prediction) 네트워크에 의존하며, 주로 임베딩 손실 함수 [7, 19, 12, 6, 20]나 픽셀 유사도 학습에 기반합니다.
  * Kendall et al. [11] (및 [14]): 픽셀을 객체 중심으로 연결하여 객체를 할당하는 아이디어를 제안했으며, 본 논문의 방법은 이 개념을 기반으로 합니다.
  * Novotny et al. [20]: 유사한 개념을 사용하지만 다른 손실 함수와 감지 우선(detection-first) 원칙을 적용합니다.
  * Box2Pix [26]: 단일 샷 감지 후 픽셀을 객체 중심에 할당하는 방식으로 실시간 인스턴스 분할에 중점을 둡니다.
* **불확실성 통합**: [21, 10]은 손실 함수에 불확실성(aleatoric uncertainty)을 통합하며, 본 논문은 학습된 마진을 직접 테스트 시점에 사용합니다.

## 🛠️ Methodology

본 논문은 인스턴스 분할을 픽셀 할당 문제로 다루며, 각 픽셀에 대해 해당 객체의 중심으로 향하는 오프셋 벡터를 학습합니다.

1. **픽셀 임베딩 및 오프셋 벡터 학습**:
    * 각 픽셀 $x_i$에 대해 오프셋 벡터 $o_i$를 학습하여 공간 임베딩 $e_i = x_i + o_i$가 해당 인스턴스 중심 $C_k$를 가리키도록 합니다.
    * 기존의 직접적인 회귀 손실($L_{\text{regr}}$) 대신, 클러스터링 단계를 손실 함수에 통합합니다.

2. **학습 가능한 인스턴스별 마진 ($\sigma_k$)**:
    * 고정된 마진 $\delta$를 사용하는 힌지 손실($L_{\text{hinge}}$)의 한계를 극복하기 위해, 인스턴스별로 학습 가능한 마진을 도입합니다.
    * 가우시안 함수 $\phi_k(e_i)$를 사용하여 픽셀 임베딩 $e_i$와 인스턴스 중심 $C_k$ 간의 거리를 해당 인스턴스에 속할 확률로 변환합니다:
        $$ \phi_k(e_i) = \exp\left(-\frac{\left\|e_i - C_k\right\|^2}{2\sigma_k^2}\right) $$
    * $\sigma_k$는 인스턴스 $k$에 속하는 모든 픽셀 $\sigma_i$의 평균으로 정의되며, $\sigma_k$ 값에 따라 클러스터링 마진이 조절됩니다.
    * 이 가우시안 함수의 출력(전경/배경 확률 맵)을 ground-truth 이진 맵과 비교하여 **Lovasz-hinge 손실 함수** [4, 28]로 최적화합니다. 이는 IoU를 직접적으로 최적화하며 전경/배경 클래스 불균형 문제를 해결합니다.

3. **손실 확장**:
    * **타원형 마진 (Elliptical margin)**: 스칼라 $\sigma$ 대신 2차원 $\sigma_{xy}$를 사용하여 타원형 마진을 학습할 수 있도록 확장합니다. 이는 보행자처럼 길쭉한 객체에 더 적합합니다.
        $$ \phi_k(e_i) = \exp\left(-\frac{\left(e_{ix} - C_{kx}\right)^2}{2\sigma_{kx}^2} - \frac{\left(e_{iy} - C_{ky}\right)^2}{2\sigma_{ky}^2}\right) $$
    * **학습 가능한 매력 중심 (Learnable Center of Attraction)**: 인스턴스 중심 $C_k$를 고정된 도형 중심 대신, 해당 인스턴스에 속하는 임베딩들의 평균으로 정의하여 네트워크가 최적의 매력 중심 위치를 학습할 수 있도록 합니다.

4. **시드 맵(Seed Map) 학습**:
    * 추론 시 객체 중심을 찾기 위해 각 의미론적 클래스별로 시드 맵을 학습합니다.
    * 시드 맵 $s_i$는 픽셀 임베딩이 인스턴스 중심에서 얼마나 떨어져 있는지 나타내며, 이상적으로 $s_i = \phi_k(e_i)$여야 합니다.
    * 이 시드 맵은 회귀 손실 함수($L_{\text{seed}}$)로 훈련됩니다.

5. **후처리 (Post-processing) 및 평활화 손실**:
    * 추론 시, 각 클래스별 시드 맵에서 가장 높은 값을 가진 픽셀을 인스턴스 중심 $\hat{C}_k$ 및 $\hat{\sigma}_k$로 샘플링합니다.
    * 샘플링된 중심과 시그마를 사용하여 가우시안 함수가 0.5보다 큰 픽셀 임베딩을 클러스터링합니다.
    * 클러스터링된 픽셀을 시드 맵에서 제거하고 모든 시드가 마스킹될 때까지 반복합니다.
    * $\hat{\sigma}_k \approx \sigma_k$를 보장하기 위해 평활화 항 $L_{\text{smooth}}$을 전체 손실에 추가합니다.

6. **네트워크 아키텍처**: 실시간 의미 분할에 최적화된 ERFNet [25]을 인코더-디코더 기반의 2개 브랜치 네트워크로 사용합니다.
    * 첫 번째 브랜치는 $\sigma$ 및 오프셋 값을 예측합니다.
    * 두 번째 브랜치는 각 의미론적 클래스별 시드 맵을 출력합니다.

## 📊 Results

* **Cityscapes 벤치마크 (Test Set)**:
  * **AP 27.6**: `fine-only` 훈련 방법을 사용하는 모델 중 Mask R-CNN (AP 26.2)을 능가하며 2위를 차지했습니다. 특히 `person` (34.5 vs 30.5), `rider` (26.1 vs 23.7), `car` (52.4 vs 46.9) 클래스에서 Mask R-CNN보다 우수한 성능을 보였습니다.
  * Mask R-CNN (`fine+COCO` 훈련)과 비교했을 때, `person` (34.5 vs 34.8), `rider` (26.1 vs 27.0)에서 유사한 결과를, `car` (52.4 vs 49.1), `bicycle` (18.9 vs 18.7)에서 더 나은 결과를 보였습니다.
* **실시간 성능**:
  * 2MP (2048x1024) 이미지에서 **11 FPS**를 달성하여, Cityscapes에서 고정확도와 실시간 성능을 동시에 달성한 최초의 방법입니다.
  * Forward pass: 65ms, Clustering: 26ms.
* **Ablation Study (Validation Set)**:
  * **학습 가능한 $\sigma$의 중요성**: 학습 가능한 $\sigma$ (38.7 AP)는 고정된 $\sigma$ (28.0 AP)보다 훨씬 우수한 성능을 보였습니다.
  * **학습 가능한 매력 중심**: 고정된 도형 중심 대신 학습 가능한 매력 중심을 사용했을 때 (40.5 AP vs 39.1 AP), 정확도가 향상되었습니다.
  * **타원형 마진**: 스칼라 $\sigma$ (원형 마진)보다 2차원 $\sigma_{xy}$ (타원형 마진)를 사용했을 때 더 나은 성능을 달성했습니다.
  * **$\sigma$와 객체 크기의 상관관계**: 학습된 $\sigma$ 값이 객체 크기에 비례하는 양의 상관관계가 있음을 확인했습니다.

## 🧠 Insights & Discussion

* **학습 가능한 마진의 중요성**: 인스턴스별로 최적의 클러스터링 마진을 학습하는 능력은 객체 크기 변화에 강건하게 대응하고, 특히 큰 객체의 가장자리 픽셀에 대한 손실 제약을 완화하는 데 핵심적입니다. 이는 Mask R-CNN과 같은 고정 해상도 마스크 방식의 한계를 극복합니다.
* **IoU 직접 최적화의 효과**: Lovasz-hinge 손실 함수를 통한 IoU의 직접 최적화는 마스크 품질을 극대화하는 데 기여합니다.
* **실시간 고정확도 균형**: 이 방법은 기존 인스턴스 분할 방법들이 가졌던 정확도-속도 트레이드오프 문제를 효과적으로 해결하여, 자율 주행과 같은 실시간 응용 분야에 적합한 실용적인 솔루션을 제공합니다.
* **제한 사항**: `truck`, `bus`, `train`과 같이 훈련 데이터셋에서 매우 적게 표현된 클래스에서는 다른 클래스에 비해 성능이 상대적으로 낮았습니다. 이는 데이터셋 불균형으로 인한 것으로, 더 많은 훈련 데이터(예: coarse 또는 COCO 데이터셋)를 사용하면 개선될 수 있습니다.

## 📌 TL;DR

**문제**: 기존 인스턴스 분할은 정확하지만 느리거나(제안 기반), 빠르지만 정확도가 낮은(제안 없음) 문제가 있어 실시간 고정확도 요구사항을 충족하지 못했습니다.
**방법**: 픽셀의 공간 임베딩과 인스턴스별 클러스터링 대역폭($\sigma_k$)을 공동으로 학습하는 새로운 클러스터링 손실 함수를 제안합니다. Lovasz-hinge 손실을 통해 마스크의 IoU를 직접 최적화하며, ERFNet 기반 아키텍처는 픽셀 오프셋, $\sigma_k$, 그리고 객체 중심을 찾는 시드 맵을 예측합니다.
**발견**: Cityscapes 벤치마크에서 Mask R-CNN을 능가하는 27.6 AP와 11 fps의 실시간 성능을 동시에 달성하여, 고해상도 마스크를 생성하면서 실시간 고정확도를 제공하는 최초의 제안 없는 인스턴스 분할 방법임을 입증했습니다.
