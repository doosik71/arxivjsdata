# MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features

Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang, Hartwig Adam

## 🧩 Problem to Solve

본 논문은 객체 감지(object detection)와 시맨틱 분할(semantic segmentation)을 동시에 해결하여 픽셀 수준의 정확도로 객체 인스턴스를 찾아내는 인스턴스 분할(instance segmentation) 문제를 다룹니다. 특히 다음과 같은 도전 과제들을 해결하고자 합니다:

* **서로 다른 시맨틱 클래스 구분**: 배경을 포함하여 서로 다른 시맨틱 클래스의 객체(예: 사람과 배경)를 구별합니다.
* **동일 시맨틱 클래스 내 인스턴스 분리**: 같은 시맨틱 클래스 내의 개별 인스턴스(예: 두 명의 사람)를 분리합니다.
* **기존 방법의 한계 극복**: FCIS와 같은 기존 모델의 중복 배경 인코딩 문제를 해결하고, [70]의 복잡한 템플릿 매칭 없이 방향 정보를 활용합니다.

## ✨ Key Contributions

* **세 가지 출력의 통합 모델 MaskLab**: Faster R-CNN 기반으로 박스 감지, 시맨틱 분할, 방향 예측의 세 가지 출력을 생성하여 인스턴스 분할을 수행하는 새로운 모델 MaskLab을 제안합니다.
* **정교한 ROI 내 분할**: Faster R-CNN으로 예측된 박스 내에서 시맨틱 및 방향 예측을 결합하여 전경/배경 분할을 수행합니다.
  * **시맨틱 분할**: 서로 다른 시맨틱 클래스(배경 포함)를 구별하여 FCIS의 중복 배경 인코딩을 제거합니다.
  * **방향 예측**: 각 픽셀이 해당 인스턴스 중심으로 향하는 방향을 예측하여 동일 시맨틱 클래스의 인스턴스를 분리할 수 있게 합니다. 이는 [16,44]와 유사한 어셈블링 연산을 사용하여 [70]의 복잡한 템플릿 매칭을 피합니다.
* **최신 기술 통합**: Atrous convolution을 통한 고밀도 피처 맵 추출, hypercolumn 피처를 활용한 마스크 정제, multi-grid를 통한 다양한 스케일의 컨텍스트 캡처, 그리고 변형 가능한 RoIAlign과 유사한 "deformable crop and resize" TensorFlow 연산을 도입하여 성능을 향상시킵니다.
* **경쟁력 있는 성능**: COCO 인스턴스 분할 벤치마크에서 다른 최신 모델들과 필적하거나 더 나은 성능을 달성합니다.

## 📎 Related Works

본 논문은 인스턴스 분할 방법을 크게 두 가지로 분류하고, MaskLab이 이들의 장점을 결합했음을 강조합니다.

* **감지 기반(Detection-based) 방법**: Fast R-CNN [25], Faster R-CNN [62], R-FCN [19] 등 최첨단 객체 감지 모델을 활용하여 마스크 영역을 분류하거나 예측된 박스를 정제하여 마스크를 얻습니다. FCIS [44] (position-sensitive score maps를 확장하여 inside/outside score maps 사용) 및 Mask R-CNN [31] (Faster R-CNN 박스 예측에 마스크 브랜치 추가) 등이 있습니다. MaskLab은 Faster R-CNN 위에 구축됩니다.
* **분할 기반(Segmentation-based) 방법**: 일반적으로 분할 모듈로 픽셀 수준 예측을 얻은 후 클러스터링을 통해 각 객체 인스턴스에 대한 그룹화를 수행합니다. DeepLab [10]의 분할 결과를 스펙트럼 클러스터링하는 제안 [45]이나, [70]처럼 시맨틱, 깊이 정보 외에 인스턴스 중심 방향을 예측하는 FCN을 훈련하는 방법이 있습니다. MaskLab은 [70]의 방향 예측 아이디어를 활용하되, 복잡한 템플릿 매칭 대신 pooling 연산을 사용합니다.

## 🛠️ Methodology

MaskLab은 ResNet-101을 피처 추출기로 사용하는 Faster R-CNN 기반의 아키텍처를 가집니다.

* **세 가지 출력 생성**:
    1. **박스 예측**: Faster R-CNN의 박스 분류기에서 정제된 박스를 반환합니다.
    2. **시맨틱 분할 로짓**: 픽셀별 분류(배경 클래스 포함)를 위한 로짓입니다.
    3. **방향 예측 로짓**: 각 픽셀이 해당 인스턴스 중심을 향하는 방향을 예측하는 로짓입니다 [70].
  * 시맨틱 및 방향 예측 로짓은 ResNet-101의 `conv5` 블록의 마지막 피처 맵 뒤에 추가된 $1 \times 1$ 컨볼루션을 통해 계산됩니다.
* **ROI 내 전경/배경 분할**:
  * 예측된 박스(관심 영역)에 대해, Faster R-CNN이 예측한 클래스에 해당하는 시맨틱 채널을 선택하고, 예측된 박스에 따라 영역을 잘라냅니다.
  * 방향 정보를 활용하기 위해, [16,44]와 유사한 어셈블링(assembling) 연산을 수행하여 각 방향 채널에서 지역 로짓을 수집합니다 (예: 360도를 8방향으로 양자화하고, 거리를 4개 빈으로 양자화하여 $8 \times 4 = 32$ 채널 생성).
  * 잘라낸 시맨틱 로짓과 풀링된 방향 로짓을 `concatenate`한 후, 클래스 불가지론적(class-agnostic)인 $1 \times 1$ 컨볼루션을 적용하여 최종 전경/배경 분할을 수행합니다.
* **마스크 정제(Mask Refinement)**:
  * 생성된 초기 마스크 로짓(시맨틱 및 방향 피처만 활용)을 ResNet-101의 하위 레이어(예: `conv1`, `conv2`)에서 추출된 hypercolumn 피처 [29]와 `concatenate`합니다.
  * 이를 세 개의 추가 $5 \times 5$ 컨볼루션 레이어로 구성된 작은 컨볼루션 네트워크에 통과시켜 최종 정제된 마스크를 예측합니다.
* **변형 가능한 Crop and Resize (Deformable Crop and Resize)**:
  * 객체 분류에 사용되는 "crop and resize" TensorFlow 연산(Mask R-CNN의 RoIAlign과 유사)을 변형 가능한 형태로 수정합니다.
  * 피처 맵에서 지정된 경계 박스 영역을 자르고, 이를 지정된 크기(예: $4 \times 4$)로 이중 선형 리사이즈합니다.
  * 영역을 여러 개의 서브 박스(예: $2 \times 2$ 크기의 4개 서브 박스)로 나눕니다.
  * 작은 네트워크를 사용하여 각 서브 박스에 대한 오프셋(offset)을 학습합니다.
  * 변형된 서브 박스에 대해 "crop and resize"를 다시 수행하여 변형 가능한 풀링(deformable pooling) [20]을 구현합니다.
* **훈련 세부 사항**: Atrous convolution (output stride=8)을 사용하여 고밀도 피처 맵을 추출합니다. $1 \times 1$ 컨볼루션의 가중치를 $(0.5, 1)$로 초기화하여 방향 피처에 약간 더 큰 가중치를 부여합니다. 훈련 중에는 오직 ground truth 박스만을 사용하여 시맨틱 및 방향 로짓을 훈련하고, coarse 및 refined 마스크 결과에 sigmoid 함수를 사용합니다.

## 📊 Results

COCO 데이터셋에 대한 실험 결과는 다음과 같습니다.

* **마스크 크기**: 마스크 분할을 위해 `crop size = 41`이 충분했습니다.
* **시맨틱 및 방향 피처의 영향**:
  * 방향 피처만 사용했을 때 (mAP@0.75 27.4%) 시맨틱 피처만 사용했을 때 (mAP@0.75 24.44%)보다 성능이 우수하여 방향 피처가 더 중요함을 입증했습니다.
  * 두 피처를 모두 사용하고 방향 풀링에 4개의 거리 빈을 적용했을 때 (총 32채널) 가장 좋은 성능(mAP@0.75 30.57%)을 얻었습니다.
* **방향의 개수**: 360도를 양자화하기 위해 8개의 방향이 충분한 성능을 제공했습니다.
* **마스크 정제**: `conv1`과 `conv2` (ResNet-101의 `res2x` 블록 마지막 피처 맵) 피처를 활용하여 마스크를 정제했을 때 mAP@0.75가 30.57%에서 33.89%로 크게 향상되었습니다.
* **멀티-그리드**: 박스 분류기 블록에 다른 atrous rate를 적용하는 것이 더 효과적이었으며, 이는 현재 평가 지표($\text{mAP}^\text{r}$)가 감지 기반 방법을 선호할 수 있음을 시사합니다.
* **사전 훈련(Pretraining)**: ImageNet 사전 훈련, COCO 시맨틱 분할 주석을 이용한 사전 훈련, JFT-300M 데이터셋을 이용한 사전 훈련 모두 성능 향상에 기여했습니다 (최대 mAP@0.75 41.59%).
* **변형 가능한 Crop and Resize**: 약 1%의 성능 향상을 가져왔습니다.
* **스케일 증강(Scale Augmentation)**: 훈련 중 무작위 입력 스케일링을 통해 성능이 크게 향상되었습니다 (mAP@0.75 40.41%).
* **`test-dev` 마스크 mAP**:
  * MaskLab (ResNet-101 기반)은 FCIS+++ [44] 및 ResNet-101 기반 Mask R-CNN [31]을 능가하며, ResNet-101-FPN 기반 Mask R-CNN과 유사한 성능을 보입니다.
  * `MaskLab+` (스케일 증강)는 ResNeXt-101-FPN 기반 Mask R-CNN과 필적하는 37.3% mAP를 달성했으며, JFT 사전 훈련 시 38.1% mAP까지 향상되었습니다.
* **`test-dev` 박스 mAP**:
  * MaskLab은 G-RMI [35] 및 TDM [66] (더 강력한 Inception-ResNet-v2 사용)보다 우수하거나 필적하는 박스 감지 성능을 보입니다.
  * JFT 사전 훈련을 사용한 `MaskLab+`는 43.0% mAP를 달성했습니다.
* **정성적 결과**: 시맨틱 로짓은 학습 데이터의 한계로 비-객체 영역에서 활성화될 수 있지만, 박스 감지 브랜치에 의해 걸러집니다. 변형 가능한 서브 박스는 원형으로 변형되어 더 긴 컨텍스트를 포착하려는 경향을 보였습니다. 주요 실패 원인은 감지 실패(오탐지, 오분류)와 경계가 거친 분할 결과였습니다.

## 🧠 Insights & Discussion

* MaskLab은 객체 감지와 시맨틱 분할의 장점을 효과적으로 결합하여 인스턴스 분할 문제를 해결하는 강력한 프레임워크를 제공합니다.
* 시맨틱 피처와 방향 피처의 조합이 서로 다른 클래스 및 동일 클래스 내의 인스턴스를 분리하는 데 결정적인 역할을 함을 확인했습니다. 특히 방향 피처의 중요성이 두드러집니다.
* 단순화된 "방향 풀링" 연산은 기존의 복잡한 템플릿 매칭 없이도 방향 정보를 효과적으로 활용할 수 있게 합니다.
* Atrous convolution, hypercolumn, multi-grid, deformable crop and resize와 같은 최신 기술의 통합이 모델의 전반적인 성능 향상에 크게 기여했습니다.
* "Deformable crop and resize"가 객체 부분에 집중하는 것을 넘어 박스 분류를 위한 더 넓은 컨텍스트를 학습하는 능력은 흥미로운 발견입니다.
* 현재 COCO 평가 지표($\text{mAP}^\text{r}$)가 감지 기반 방법의 개선에 더 유리하게 작용할 수 있다는 점이 논의되었습니다.
* 모델의 한계로는 여전히 감지 실패(오탐지, 잘못된 클래스 예측)와 마스크의 거친 경계 문제가 남아있습니다.

## 📌 TL;DR

**문제**: 픽셀 수준의 객체 감지와 시맨틱 분할을 동시에 수행하는 인스턴스 분할.
**방법**: MaskLab은 Faster R-CNN을 기반으로 박스 감지, 시맨틱 분할, 방향 예측의 세 가지 출력을 생성합니다. 예측된 박스 내에서 시맨틱 피처(다른 클래스 구분)와 방향 피처(동일 인스턴스 분리)를 결합하여 전경/배경 분할을 정제합니다. 또한 마스크 정제 및 "deformable crop and resize" 연산을 통합하여 성능을 향상시킵니다.
**주요 결과**: COCO 벤치마크에서 최첨단 성능을 달성했으며, 특히 방향 피처는 동일 클래스 인스턴스 분리에 필수적임을 보여줍니다. 다양한 구성 요소와 사전 훈련, 스케일 증강 등이 전반적인 성능 향상에 기여합니다.
