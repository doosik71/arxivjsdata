# YOLACT: Real-time Instance Segmentation

Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee

## 🧩 Problem to Solve

기존의 인스턴스 분할(Instance Segmentation) 방법론들은 Mask R-CNN이나 FCIS와 같이 정확도에 중점을 두어 실시간 처리(예: 30 FPS 이상)가 어렵습니다. 이들은 주로 특징 재풀링(feature repooling)과 같은 순차적인 특징 지역화(feature localization) 단계를 포함하고 있어, 연산 비용이 높습니다. 객체 탐지(object detection) 분야에서 SSD나 YOLO와 같은 실시간 1단계(one-stage) 모델이 성공했음에도 불구하고, 인스턴스 분할 분야에는 이와 동등한 실시간 모델이 부족한 상황입니다. 이 연구는 이 격차를 메우는 것을 목표로 합니다.

## ✨ Key Contributions

* MS COCO 데이터셋에서 29.8 mAP의 정확도를 33.5 FPS로 달성한 최초의 경쟁력 있는 실시간(>30 fps) 인스턴스 분할 알고리즘을 제안했습니다. 이는 이전의 어떤 경쟁 모델보다 훨씬 빠릅니다.
* 명시적인 특징 지역화 단계 없이, 인스턴스 분할을 두 가지 병렬 서브태스크(서브작업)로 분리하는 새로운 1단계 완전 컨볼루션(fully-convolutional) 모델인 YOLACT를 제안합니다:
    1. 전체 이미지에 대한 프로토타입 마스크(prototype masks) 세트 생성
    2. 인스턴스별 마스크 계수(mask coefficients) 예측
* 프로토타입과 마스크 계수를 선형 결합하여 인스턴스 마스크를 생성하는 방식은 재풀링(repooling)에 의존하지 않아 매우 고품질의 마스크를 생성하며, 추가 비용 없이 시간적 안정성(temporal stability)을 보입니다.
* 프로토타입의 "자율적 행동(emergent behavior)"을 분석하여, 이들이 완전 컨볼루션 방식임에도 불구하고 번역 변형(translation variant) 방식으로 인스턴스를 스스로 지역화하는 법을 학습한다는 것을 보여줍니다.
* 표준 NMS(Non-Maximum Suppression)보다 12ms 더 빠른 `Fast NMS`를 제안하며, 이는 성능 저하가 미미합니다.
* YOLACT의 코드는 공개적으로 접근 가능합니다 (<https://github.com/dbolya/yolact>).

## 📎 Related Works

* **인스턴스 분할:**
  * **2단계(Two-stage) 방법론:** Mask R-CNN, FCIS 등. ROI(Region-of-Interest)를 생성한 후 각 ROI에 대해 특징을 재풀링하여 마스크를 예측하는 방식으로, 정확도는 높지만 실시간 처리에 한계가 있습니다.
  * **1단계(One-stage) 방법론:** FCIS처럼 위치 감지 맵(position-sensitive maps)을 생성하거나 Masklab처럼 의미론적 분할(semantic segmentation) 및 방향 예측을 결합합니다. 개념적으로는 더 빠르지만, 여전히 재풀링이나 마스크 투표(mask voting)와 같은 비 trivial한 계산이 필요하여 실시간 성능을 저해합니다.
  * **기타 방법론:** 의미론적 분할 후 경계 탐지, 픽셀 클러스터링, 임베딩 학습(embedding learning) 등. 대부분 다단계이거나 값비싼 클러스터링 절차를 포함합니다.
* **실시간 인스턴스 분할:** 실시간 객체 탐지나 의미론적 분할에 비해 연구가 적습니다. Straight to Shapes, Box2Pix 같은 연구가 있었으나, 현대적인 기준에 비해 정확도가 현저히 낮습니다.
* **프로토타입 학습:** 텍스톤(textons), 시각적 단어(visual words) 등 컴퓨터 비전 분야에서 특징 표현이나 객체 탐지를 위해 널리 탐구되었습니다. YOLACT는 이를 인스턴스 마스크를 조립하는 데 사용하며, 전체 데이터셋에 공유되는 전역 프로토타입 대신 각 이미지에 특화된 프로토타입을 학습합니다.

## 🛠️ Methodology

YOLACT는 기존의 1단계 객체 탐지 모델(RetinaNet)에 마스크 브랜치를 추가하되, 명시적인 특징 지역화 단계를 제거합니다.

1. **프로토타입 생성 브랜치 (Protonet):**
    * FPN(Feature Pyramid Network)의 가장 큰 특징 맵(P$_3$)에 연결된 FCN(Fully Convolutional Network)으로 구현됩니다.
    * 이미지 전체에 대한 $k$개의 프로토타입 마스크를 예측합니다. 마지막 레이어는 $k$개의 채널을 가집니다.
    * 명시적인 손실(loss) 없이 최종 마스크 손실을 통해 간접적으로 학습됩니다.
    * 출력은 `ReLU`를 사용하여 무한대(unbounded)이며, 작은 객체 성능 향상을 위해 입력 이미지 크기의 1/4 해상도로 업샘플링됩니다.

2. **마스크 계수 예측 브랜치:**
    * 객체 탐지 브랜치에 추가된 병렬 헤드로, 각 앵커(anchor)에 대해 $k$개의 마스크 계수 벡터를 예측합니다.
    * `tanh` 활성화 함수를 사용하여 프로토타입의 뺄셈이 가능하도록 합니다.

3. **마스크 조립 (Mask Assembly):**
    * NMS(Non-Maximum Suppression)를 통과한 각 인스턴스에 대해, 프로토타입 마스크 행렬 $P$와 예측된 마스크 계수 행렬 $C$를 선형 결합합니다.
    * 수식: $M = \sigma(PC^T)$, 여기서 $\sigma$는 시그모이드(sigmoid) 함수입니다.
    * 평가 시에는 예측된 바운딩 박스로 최종 마스크를 자르고(crop), 학습 시에는 ground truth 바운딩 박스로 자릅니다.

4. **손실 함수:**
    * 분류 손실($L_{cls}$), 박스 회귀 손실($L_{box}$): SSD와 유사하게 정의됩니다.
    * 마스크 손실($L_{mask}$): 조립된 마스크 $M$과 ground truth 마스크 $M_{gt}$ 간의 픽셀 단위 이진 교차 엔트로피(Binary Cross Entropy)입니다.
    * **선택적 의미론적 분할 손실 (Semantic Segmentation Loss):** 학습 시에만 사용되는 1x1 컨볼루션 레이어를 통해 P$_3$ 특징 맵에 적용됩니다. 이는 특징 풍부도(feature richness)를 높이는 데 기여합니다.

5. **백본 탐지기 (Backbone Detector):**
    * ResNet-101과 FPN을 기반으로 한 RetinaNet 구조를 사용하며, 속도에 중점을 둡니다.
    * 가벼운 예측 헤드(prediction head)를 설계하여 속도를 더욱 향상시킵니다.
    * OHEM(Online Hard Example Mining)과 Smooth-L$_1$ 손실을 사용합니다.

6. **Fast NMS:**
    * 기존 NMS의 순차적 특성을 개선하여 병렬 처리가 가능하도록 합니다.
    * 점수가 높은 탐지 결과가 낮은 탐지 결과들을 억제하는 방식이며, GPU 행렬 연산을 통해 효율적으로 구현됩니다.

## 📊 Results

* **MS COCO `test-dev` 결과:**
  * YOLACT-550 (ResNet-101 백본, 550x550 이미지 크기)은 33.5 FPS에서 29.8 mAP를 달성하여, 이전의 가장 빠른 인스턴스 분할 방법보다 3.8배 빠르면서도 경쟁력 있는 성능을 보입니다.
  * Mask R-CNN (R-101-FPN)은 8.6 FPS에서 35.7 mAP를 달성했습니다. YOLACT는 전체 mAP에서는 약간 낮지만, 고품질 마스크(AP$_{95}$)에서는 Mask R-CNN을 능가합니다 (1.6 AP vs 1.3 AP).
  * **마스크 품질:** 재풀링이 없어 원본 특징에서 마스크를 직접 생성하기 때문에 Mask R-CNN보다 대형 객체에 대해 더 높은 품질의 마스크를 생성합니다.
  * **시간적 안정성:** 훈련 시 시간적 평활화(temporal smoothing)를 적용하지 않았음에도 불구하고, 비디오에서 Mask R-CNN보다 마스크의 시간적 안정성이 더 높습니다.
  * **Fast NMS:** 표준 NMS보다 11.8ms 빠르며, 성능은 0.1 mAP만 감소합니다.
  * **프로토타입 수($k$):** $k=32$가 성능과 속도의 균형이 가장 좋았습니다. $k$를 증가시켜도 성능 향상은 미미했습니다.
  * **이미지 해상도 및 백본:** 더 높은 해상도(700x700)는 mAP를 증가시키지만 속도를 감소시키고, 낮은 해상도(400x400)는 속도는 빠르지만 mAP가 크게 떨어집니다. ResNet-50 또는 DarkNet-53 백본 사용 시 더 빠른 속도를 얻을 수 있습니다.
* **객체 탐지(Box) 성능:** 마스크 브랜치를 평가하지 않고 박스만 탐지할 경우, YOLACT는 YOLOv3와 유사한 속도에서 경쟁력 있는 탐지 성능을 보이며, 마스크 연산은 전체 평가 시간 중 단 6ms만 차지합니다.

## 🧠 Insights & Discussion

* **프로토타입의 자율적 행동:** YOLACT의 프로토타입은 FCN이 번역 불변적(translation invariant)이라는 일반적인 통념에도 불구하고, 이미지 패딩(padding)의 존재로 인해 내재적으로 번역 변형(translation variant) 특징을 학습합니다. 이는 프로토타입들이 이미지를 공간적으로 분할하거나, 인스턴스를 지역화하거나, 윤곽선을 감지하는 등 다양한 역할을 수행하게 합니다. 이러한 프로토타입들의 선형 결합을 통해 네트워크는 겹치는 동일 클래스의 인스턴스를 구분할 수 있습니다.
* **제한 사항 및 오류:**
  * **지역화 실패 (Localization Failure):** 한 지점에 너무 많은 객체가 밀집해 있을 때, 각 객체를 독립적인 프로토타입으로 지역화하지 못하고 전경 마스크와 유사한 결과를 낼 수 있습니다.
  * **누수 (Leakage):** 예측된 바운딩 박스가 부정확하거나 너무 클 경우, 잘라낸 영역 외부의 노이즈나 멀리 떨어진 다른 인스턴스의 마스크가 포함될 수 있습니다.
* **AP 격차 분석:** Mask R-CNN과의 mAP 격차는 마스크 생성 방식 자체보다는 YOLACT의 탐지기 성능(box mAP)이 상대적으로 낮기 때문이라고 분석됩니다. 두 방법론 모두 마스크 mAP와 박스 mAP 간의 차이가 유사합니다.

## 📌 TL;DR

**문제:** 기존의 인스턴스 분할 방법은 정확도 중심이며 속도가 느려 실시간 적용이 어렵습니다.
**방법:** YOLACT는 명시적인 특징 지역화 없이, 이미지 전체에 대한 `프로토타입 마스크`와 인스턴스별 `마스크 계수`를 병렬로 예측하고 이들을 선형 결합하여 최종 인스턴스 마스크를 생성하는 1단계 완전 컨볼루션 모델입니다. 또한, `Fast NMS`를 도입하여 NMS 속도를 향상시킵니다.
**주요 결과:** YOLACT는 MS COCO에서 29.8 mAP의 정확도를 33.5 FPS로 달성하며, 최초의 경쟁력 있는 실시간 인스턴스 분할 성능을 제공합니다. 고품질 및 시간적으로 안정적인 마스크를 생성하며, 프로토타입들이 인스턴스를 자율적으로 지역화하는 자율적 행동을 보임을 입증했습니다.
