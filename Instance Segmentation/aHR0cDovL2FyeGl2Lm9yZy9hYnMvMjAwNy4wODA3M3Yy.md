# Unseen Object Instance Segmentation for Robotic Environments

Christopher Xie, Yu Xiang, Arsalan Mousavian, Dieter Fox

## 🧩 Problem to Solve

로봇이 구조화되지 않은 환경에서 효과적으로 기능하려면 이전에 본 적 없는(unseen) 물체를 인식하고 조작할 수 있어야 합니다. 특히, 본 연구는 테이블 위 환경에서 **미지 물체 인스턴스 분할(Unseen Object Instance Segmentation, UOIS)**이라는 문제를 다룹니다. 이 과제의 핵심적인 어려움은 다음과 같습니다:

* **대규모 데이터셋 부족:** 미지 물체 인스턴스 분할을 위한 대규모의 실제 데이터셋이 존재하지 않아 딥러닝 모델 훈련에 필요한 다양한 물체 데이터를 확보하기 어렵습니다.
* **합성 데이터의 도메인 간극:** 데이터 수집 및 주석의 어려움 때문에 합성 데이터 활용이 매력적이지만, 많은 시뮬레이터가 사실적인 RGB 이미지를 제공하지 못해 합성 데이터와 실제 데이터 사이에 **도메인 간극(domain gap)**이 발생하여 직접적인 Sim-to-Real 전이에 어려움이 있습니다.
* **일반화 능력:** 로봇은 사전에 학습되지 않은 물체 카테고리에 대해서도 "물체"의 개념을 학습하고 일반화하여 모든 임의의 물체 인스턴스를 분할할 수 있어야 합니다.

이 연구는 이러한 문제를 해결하여 로봇이 테이블 위에서 처음 보는 물체들을 효과적으로 분할하고, 이를 파악(grasping)과 같은 로봇 조작 작업에 활용하는 것을 목표로 합니다.

## ✨ Key Contributions

* **UOIS-Net 제안:** 합성 RGB-D 데이터로부터 미지 물체 인스턴스 분할을 위한 새로운 2단계 딥 네트워크 아키텍처인 UOIS-Net을 제안합니다. 이 네트워크는 **합성 RGB와 합성 깊이(depth) 정보를 분리하여 활용**하는 독특한 방식을 채택합니다.
* **깊이 기반 초기 마스크 생성 (DSN):** 첫 번째 단계인 Depth Seeding Network (DSN)는 깊이 정보만을 사용하여 물체 인스턴스 중심 투표(center votes)를 2D 또는 3D로 예측하고, 이를 바탕으로 대략적인 초기 인스턴스 마스크를 생성합니다.
* **RGB 기반 마스크 정제 (RRN):** 두 번째 단계인 Region Refinement Network (RRN)는 DSN에서 생성된 초기 마스크를 RGB 이미지를 사용하여 정교하게 다듬습니다. 놀랍게도 **비사실적인(non-photorealistic) 합성 RGB 이미지**로 훈련된 RRN이 명시적인 Sim-to-Real 적응 기법 없이도 실제 이미지에 효과적으로 일반화됨을 보였습니다.
* **새로운 3D DSN 아키텍처 및 손실 함수:** 2D 방식의 한계를 극복하기 위해 3D 공간에서 추론하는 새로운 DSN 아키텍처를 도입하고, 특히 **물체 중심 투표들의 분리를 유도하는 새로운 분리 손실($\mathcal{L}_{sep}$)**을 제안하여 혼잡한 환경에서 정확도를 크게 향상시켰습니다.
* **대규모 합성 데이터셋 (TOD) 구축:** 로봇 환경에서 대규모 훈련 데이터 부족을 해결하기 위해, 무작위 ShapeNet 물체들을 테이블 위에 배치한 4만 개의 합성 장면으로 구성된 **Tabletop Object Dataset (TOD)**를 도입했습니다.
* **최첨단 성능 달성:** 제안하는 방법은 미지 물체 인스턴스 분할에서 기존 최첨단 방법들(Mask R-CNN [12], PointGroup [13] 등)을 능가하는 선명하고 정확한 분할 마스크를 생성하며, 실제 로봇 파악 작업에서도 그 효용성을 입증했습니다.

## 📎 Related Works

본 연구는 다음 분야의 기존 연구들로부터 영향을 받았습니다.

* **카테고리 수준 물체 분할 (2D/3D/RGB-D):**
  * **2D:** FCN [16], Dilated Convolution [17], Encoder-Decoder [18, 19] 등 픽셀별 클래스를 할당하는 연구.
  * **3D:** Sparse Convolution [23, 24], PointNet [25], PointNet++ [26] 등 3D 공간에서의 의미론적 분할 연구.
  * **RGB-D:** HHA 인코딩 [31], Depth-aware CNN [32], Graph Neural Networks [33] 등 RGB-D 센서를 활용한 융합 연구.
* **인스턴스 수준 물체 분할 (2D/3D):**
  * **2D (Top-down):** Mask R-CNN [12], FCIS [34] 등 바운딩 박스 제안과 결합된 방식.
  * **2D (Bottom-up):** Discriminative Loss [37, 38], ClusterNet [40] 등 픽셀을 인스턴스에 할당하는 방식.
  * **Class-agnostic:** 물체 카테고리에 구애받지 않고 모든 물체를 전경으로 처리하는 [11, 45, 46] 등의 접근법.
  * **3D:** RGB와 기하 정보를 융합한 [53], 그리고 중심 투표(center voting) 기반의 [55, 56, 57, 58, 13] 등 3D 인스턴스 분할 연구. UOIS-Net은 이 중심 투표 방식에서 영감을 받았습니다.
* **Sim-to-Real 지각:**
  * 시뮬레이션에서 학습된 모델을 실제 환경으로 전이하는 문제. 도메인 랜덤화(domain randomization) [9], 도메인 적응(domain adaptation) [7, 8] 등의 기술.
  * 깊이 이미지로 훈련된 모델은 단순한 환경에서 비교적 잘 일반화되는 것으로 알려져 있습니다 [10, 11, 64]. 본 연구는 이러한 배경 위에서 합성 RGB를 직접적으로 적응시키거나 랜덤화하지 않고 Sim-to-Real 문제를 해결하고자 합니다.

## 🛠️ Methodology

UOIS-Net 프레임워크는 깊이 정보를 처리하는 Depth Seeding Network (DSN)와 RGB 정보를 처리하는 Region Refinement Network (RRN)의 두 가지 주요 네트워크로 구성됩니다. 이들은 Initial Mask Processor (IMP)를 통해 연결됩니다. DSN과 RRN은 미분 불가능한 단계 때문에 종단 간(end-to-end)으로 훈련되지 않고 각각 독립적으로 훈련됩니다.

1. **Depth Seeding Network (DSN):**
    * **입력:** 깊이 맵을 카메라 내부 파라미터로 역투영하여 얻은 3채널의 구성된 포인트 클라우드 $D \in \mathbb{R}^{H \times W \times 3}$ (XYZ 좌표).
    * **목표:** 초기 물체 인스턴스 분할 마스크 생성.
    * **두 가지 추론 방식:**
        * **2D 추론:**
            * **아키텍처:** U-Net 인코더-디코더 구조. 출력은 의미 분할 마스크 $F \in \mathbb{R}^{H \times W \times C}$ (배경, 테이블, 물체)와 물체 중심을 가리키는 2D 방향 벡터 $V \in \mathbb{R}^{H \times W \times 2}$.
            * **초기 마스크 생성:** Hough voting layer (Algorithm 1)를 사용하여 $F$와 $V$로부터 물체 중심을 선정하고, 각 픽셀을 가장 가까운 중심으로 할당하여 초기 마스크를 생성.
            * **손실 함수:** `전경 손실 ($\mathcal{L}_{fg}$)` (가중 교차 엔트로피)과 `방향 손실 ($\mathcal{L}_{dir}$)` (가중 코사인 유사도 손실) 사용.
        * **3D 추론:**
            * **아키텍처:** 2D DSN과 유사하나, 더 넓은 수용 필드(receptive field)를 위해 ESP 모듈 [70]을 포함하는 dilated convolution을 사용. 출력은 $F$와 물체 중심에 대한 3D 오프셋 $V' \in \mathbb{R}^{H \times W \times 3}$. $D + V'$가 각 픽셀에 대한 "중심 투표(center votes)"가 됨.
            * **초기 마스크 생성:** 중심 투표 $D+V'$에 대해 3D 공간에서 평균 이동 클러스터링(Gaussian Mean Shift, GMS)을 수행하여 초기 마스크 생성.
            * **손실 함수:** `전경 손실 ($\mathcal{L}_{fg}$),` `중심 오프셋 손실 ($\mathcal{L}_{co}$)` (Huber 손실), `클러스터링 손실 ($\mathcal{L}_{cl}$)` (GMS 언롤링 기반), 그리고 핵심적으로 **`분리 손실 ($\mathcal{L}_{sep}$)`**을 사용. `$\mathcal{L}_{sep}$`은 중심 투표가 해당 물체 중심에 가까우면서 다른 물체의 중심으로부터 멀리 떨어지도록 유도하여 혼잡한 장면에서 클러스터링을 용이하게 합니다.

2. **Initial Mask Processor (IMP):**
    * DSN에서 생성된 초기 마스크의 노이즈(예: Salt/Pepper 노이즈, 잘못된 구멍)를 제거하고 견고성을 높이기 위해 표준 이미지 처리 기법을 사용합니다.
    * **단계 (각 인스턴스 마스크에 개별 적용):** Opening 연산 (침식 후 팽창), Closing 연산 (팽창 후 침식), 가장 큰 연결 구성 요소(connected component) 선택.

3. **Region Refinement Network (RRN):**
    * **입력:** IMP를 거친 초기 인스턴스 마스크의 경계 박스 주변으로 여백을 추가하여 자른 4채널 이미지(RGB와 초기 마스크가 연결됨), $224 \times 224$로 크기 조절된 $I \in \mathbb{R}^{224 \times 224 \times 4}$.
    * **아키텍처:** DSN과 동일한 U-Net 구조.
    * **출력:** 정제된 마스크 확률 $R \in \mathbb{R}^{224 \times 224}$.
    * **손실 함수:** 두 클래스(전경 vs. 배경)에 대한 `전경 손실 ($\mathcal{L}_{fg}$)` 사용.
    * **마스크 증강:** 훈련 시 실제 마스크를 초기 마스크와 유사하게 변형하기 위해 이동/회전, 추가/제거, 형태학적 연산, 무작위 타원 추가 등의 데이터 증강 기법을 사용.

4. **Tabletop Object Dataset (TOD):**
    * PyBullet 시뮬레이터 [14]를 사용하여 SUNCG 환경 [74]의 무작위 테이블 위에 ShapeNet [4]의 무작위 물체들을 배치한 4만 개의 합성 장면으로 구성된 대규모 합성 데이터셋.
    * 비사실적인(non-photorealistic) RGB-D 이미지 ($640 \times 480$)와 인스턴스 마스크를 렌더링.

## 📊 Results

* **정량적 평가:**
  * **최첨단 성능:** UOIS-Net-3D는 OCID [79] 및 OSD [80] 데이터셋에서 Overlap 및 Boundary F-measure 모두에서 최상의 성능을 달성하여 기존 베이스라인 및 SOTA 방법(Mask R-CNN [12], PointGroup [13])들을 능가했습니다. 이는 순수하게 합성 데이터로 훈련되었음에도 불구하고 달성된 결과입니다.
  * **3D 추론의 이점:** UOIS-Net-2D 대비 UOIS-Net-3D는 Recall에서 상당한 이득을 얻어 Overlap F-measure에서 4.5%, Boundary F-measure에서 5.5%의 상대적 성능 향상을 보였습니다.
  * **RRN의 효과:** OSD (수동 주석 데이터)에서 RRN은 Boundary P/R/F를 크게 향상시켰습니다. OCID (노이즈 있는 주석)에서는 정량적 성능 저하가 발생하기도 했으나, 시각적으로는 마스크를 더 선명하게 만들었습니다. RRN은 Mask R-CNN에 적용했을 때도 Boundary P/R/F를 향상시키는 효과를 보였습니다.
  * **Sim-to-Real 일반화:** 비사실적인 합성 TOD 데이터로 훈련된 RRN이 실제 이미지(Google OID [82])로 훈련된 RRN과 유사한 성능을 보이며, 마스크 정제 작업이 Sim-to-Real 전이에 비교적 쉽다는 것을 시사합니다.
  * **손실 함수 효과:** 새로운 **`분리 손실 ($\mathcal{L}_{sep}$)`**은 3D DSN의 Overlap F-measure를 10.1%, Boundary F-measure를 14.4% 향상시키는 데 결정적인 역할을 했습니다.
  * **ESP 모듈 효과:** DSN에 ESP 모듈을 추가하여 수용 필드를 넓히는 것이 UOIS-Net-2D와 UOIS-Net-3D 모두의 성능을 향상시켰습니다 (Overlap F-measure 3.0-4.9% 향상).
  * **IMP의 견고성:** Initial Mask Processor (IMP)는 DSN의 노이즈가 많은 초기 마스크를 정제하여 시스템의 견고성을 높이는 데 필수적임을 확인했습니다.
* **정성적 평가:**
  * **선명하고 정확한 마스크:** UOIS-Net은 깊이 센서 노이즈로 인한 부정확한 경계를 RGB를 사용하여 수정하고, 쌓여있거나 혼잡한 환경에서도 복잡한 물체 구조를 정확하게 분할했습니다.
  * **3D 추론의 장점:** 2D 중심이 가려진 물체나 가늘고 작은 물체(예: 펜, 바나나)를 2D DSN보다 3D DSN이 더 잘 감지하고 분할했습니다.
  * **`$\mathcal{L}_{sep}$`의 시각적 효과:** `분리 손실 ($\mathcal{L}_{sep}$)`이 적용되면 3D 중심 투표가 훨씬 더 밀집되고 잘 분리되어 후처리 클러스터링이 용이해지고, 결과적으로 매우 혼잡한 장면에서도 거의 완벽한 분할을 가능하게 함을 시각적으로 보여주었습니다.
* **Sim-to-Real 일반화 정량화:** 합성 TOD 테스트셋에서는 Mask R-CNN과 PointGroup이 UOIS-Net보다 성능이 좋았지만, 실제 데이터셋에서는 UOIS-Net이 더 좋은 성능을 보여 도메인 변화에 대한 우수한 처리 능력을 입증했습니다.
* **로봇 파악 적용:** Franka 로봇과 RGB-D 카메라를 사용하여 혼잡한 테이블에서 미지 물체를 파악하는 로봇 조작 작업에 UOIS-Net을 적용하여 51개 물체 중 41개(80.3%)를 성공적으로 파악했습니다.

## 🧠 Insights & Discussion

* **분리형 RGB-D 처리의 효율성:** 본 연구의 핵심 통찰은 깊이와 RGB 정보를 **분리하여 활용**하는 2단계 접근 방식(깊이로 초기 마스크 생성, RGB로 정교화)이 비사실적인 합성 RGB 데이터를 사용할 때 Sim-to-Real 미지 물체 인스턴스 분할에 매우 효과적이라는 점입니다.
* **비사실적 RGB의 재발견:** RRN이 비사실적인 합성 RGB로 훈련되었음에도 실제 이미지에 잘 일반화된다는 놀라운 결과는, 마스크 정제(국소적이며 단일 객체에 집중하는 문제)가 전체 장면을 RGB로부터 직접 분할하는 것보다 시뮬레이션에서 실제 환경으로 전이하기 쉬운 작업임을 시사합니다. 이는 사실적인 렌더링이나 복잡한 도메인 적응 기법의 필요성을 줄여줍니다.
* **3D 추론의 중요성:** 물체 중심을 3D 공간에서 추론하는 것은 가려진 물체나 얇은 물체에 대한 감지 성능을 크게 향상시키며, 2D 투영 방식의 한계를 극복합니다.
* **혼잡 환경을 위한 새로운 손실:** `분리 손실 ($\mathcal{L}_{sep}$)`은 혼잡한 환경에서 중심 투표가 명확한 클러스터를 형성하도록 유도하여 후처리 클러스터링의 효율성을 높이는 데 결정적인 역할을 합니다.
* **IMP를 통한 견고성:** 간단한 이미지 처리 기술로 구성된 IMP는 DSN이 생성한 초기 마스크의 노이즈를 효과적으로 제거하여 RRN의 성능을 안정화하고 전체 시스템의 견고성을 높입니다.
* **한계점:**
  * 매우 밀접하게 붙어 있거나 넓고 평평한 표면을 가진 물체(예: 나란히 정렬된 시리얼 상자)에 대한 과소 분할(under-segmentation)이 발생할 수 있습니다.
  * 매우 비볼록(non-convex)한 물체(예: 전동 드릴)는 과대 분할(over-segmentation)될 수 있습니다.
  * 가리는 물체에 의해 마스크가 여러 조각으로 나  었을 때 정확하게 분할하는 데 어려움이 있습니다.
  * 복잡한 질감을 가진 물체나 RRN의 입력 크롭이 물체 전체를 포함하지 못할 경우 성능이 저하될 수 있습니다.
  * GMS 클러스터링의 `$\sigma$` 값과 같은 하이퍼파라미터가 장면의 혼잡도에 따라 민감하게 반응합니다.
* **의의:** 본 연구는 실제 데이터 주석의 부담 없이 로봇이 복잡한 실제 환경에서 새로운 물체를 인식하고 조작할 수 있는 실용적인 솔루션을 제공함으로써 로봇 인지 능력을 발전시킵니다.

## 📌 TL;DR

**문제:** 로봇이 처음 보는 물체를 조작하기 위해선 정확한 물체 인스턴스 분할이 필요하지만, 대규모 실세계 훈련 데이터 부족과 합성 데이터의 낮은 현실성(비사실적 RGB)이 걸림돌입니다.

**제안 방법:** UOIS-Net은 깊이와 RGB를 개별적으로 활용하는 2단계 접근 방식을 제안합니다. Depth Seeding Network (DSN)가 깊이 정보로 초기 마스크를 생성하고, Initial Mask Processor (IMP)가 이를 정제합니다. Region Refinement Network (RRN)는 비사실적 합성 RGB로 훈련되었음에도 불구하고 이 초기 마스크를 실제 물체 경계에 맞춰 정교하게 다듬습니다. 특히, 3D DSN과 새로운 분리 손실($\mathcal{L}_{sep}$)은 혼잡한 환경에서 성능을 크게 향상시킵니다.

**주요 결과:** UOIS-Net은 합성 데이터만으로 훈련되었음에도 불구하고, OCID 및 OSD와 같은 실제 데이터셋에서 Mask R-CNN, PointGroup 등의 기존 최첨단 방법들을 뛰어넘는 선명하고 정확한 미지 물체 인스턴스 분할 성능을 달성했습니다. 로봇 파악 실험을 통해 그 실용성도 입증했습니다. 이는 마스크 정제 작업이 Sim-to-Real 전이에 효과적이며, 비사실적 합성 RGB도 활용될 수 있음을 보여주는 중요한 통찰입니다.
