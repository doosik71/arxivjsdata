# Learning to See the Invisible: End-to-End Trainable Amodal Instance Segmentation

Patrick Follmann, Rebecca König, Philipp Härtinger, Michael Klostermann

## 🧩 Problem to Solve

이 연구의 주된 목표는 이미지 내 모든 객체 인스턴스의 가시 영역(visible mask)뿐만 아니라 가려진 부분(invisible mask)까지 포함하는 전체적인 형상(amodal mask)을 예측하는 **시맨틱 비가시 인스턴스 분할(semantic amodal instance segmentation)** 문제를 해결하는 것입니다. 기존의 인스턴스 분할은 주로 보이는 영역에만 초점을 맞추었으나, 자율주행 로봇이나 산업용 비전 시스템과 같이 주변 환경을 깊이 이해해야 하는 응용 분야에서는 객체의 가려진 부분까지 파악하는 것이 필수적입니다. 특히, 가려진 부분은 시각적 단서가 거의 없어 예측하기 매우 어려운 문제입니다.

## ✨ Key Contributions

* **최초의 통합 엔드-투-엔드 학습 모델**: 비가시(amodal) 마스크, 가시(visible) 마스크, 그리고 가려짐(occlusion) 마스크를 단일 포워드 패스로 동시에 예측하는 최초의 엔드-투-엔드 학습 가능한 다중 작업(multi-task) 모델을 제안합니다.
* **새로운 시맨틱 비가시 데이터셋 공개**: 고품질 비가시 주석(amodal, visible, invisible mask)을 포함하며 깊이 순서 정보까지 제공하는 `D2S amodal` 데이터셋을 공개합니다. 또한, `COCO amodal` 데이터셋에 클래스 정보를 추가한 `COCOA cls` 데이터셋을 생성합니다.
* **기존 베이스라인 능가**: `COCO amodal` 데이터셋에서 기존 베이스라인 모델을 큰 폭으로 능가하는 성능을 달성하며, 새로운 데이터셋(`D2S amodal`, `COCOA cls`)에 대한 강력한 베이스라인 성능을 제공합니다.
* **비가시 훈련 데이터 없이 학습 가능성 입증**: 특수 데이터 증강 기법을 사용하여 비가시 주석 데이터(amodal training data)가 전혀 없는 경우에도 합리적인 성능으로 비가시 분할이 가능함을 `D2S amodal` 데이터셋에서 보여줍니다.

## 📎 Related Works

* **시맨틱 분할 및 3D 장면 재구성**: 배경 영역의 비가시 보완(Guo & Hoiem [5]), 3D 표면 재구성에서의 비가시 보완(Gupta et al. [6], Silberman et al. [18]) 등. 본 연구는 2D 이미지 내 객체 인스턴스의 비가시 보완에 집중합니다.
* **객체 감지**: 객체의 비가시 경계 상자(amodal bounding box)를 예측하는 연구(Kar et al. [10]). 마스크는 예측하지 않습니다.
* **인스턴스 분할**: 각 객체 인스턴스의 가시 마스크를 예측하는 Mask R-CNN (He et al. [7])과 같은 방법론. 깊이 순서를 예측하나 가려짐 영역은 예측하지 않는 연구(Yang et al. [19]), 가려짐 처리를 통해 가시 마스크 품질을 개선하는 연구(Chen et al. [1]).
* **비가시 인스턴스 분할**: 초기 연구로 가시 경계 상자를 확장하여 비가시 영역을 추론하는 방식(Li & Malik [14]). `COCO amodal` 데이터셋과 `AmodalMask` 베이스라인을 제안한 연구(Zhu et al. [20]). 이들은 주로 클래스에 구애받지 않는(class-agnostic) 예측을 수행하며, 본 논문의 모델과 달리 엔드-투-엔드 학습이 아니거나 가시/비가시 마스크를 동시에 예측하지 않습니다.

## 🛠️ Methodology

### Architecture

* **Occlusion R-CNN (ORCNN)**: Mask R-CNN (MRCNN)을 기반으로 확장된 아키텍처입니다.
* **추가 헤드**: 비가시 마스크 예측을 위한 **비가시 마스크 헤드(amodal mask head)**와 가려짐 마스크 예측을 위한 **가려짐 마스크 헤드(occlusion mask head)**를 추가합니다.
* **마스크 헤드 공유**: 가시 마스크 헤드와 비가시 마스크 헤드는 동일한 아키텍처를 공유하며 RoIAlign 레이어에서 추출된 특징을 입력으로 받습니다. RPN(Region Proposal Network)은 비가시 인스턴스의 경계 상자를 제안하며, 이 RoI를 공유하여 비가시 마스크와 가시 마스크가 동일한 객체 예측에 대응하도록 보장합니다.
* **가려짐 마스크 계산**: 가려짐 마스크 헤드는 비가시 마스크 로짓에서 가시 마스크 로짓을 빼는 방식으로 가려짐 마스크 로짓을 얻습니다. 이때, 음수 값을 방지하고 유의미한 가려짐 예측을 위해 가시 마스크 로짓에 ReLU 연산을 적용하는 것이 중요합니다.
* **경량화 및 초기화**: 기존 MRCNN 구조에 5개의 추가 컨볼루션 모듈과 2개의 시그모이드 레이어만 필요하므로 경량화됩니다. 또한, 비가시 및 가시 마스크 예측 헤드가 동일한 아키텍처를 공유하여 `COCO`와 같은 대규모 데이터셋으로 사전 학습된 가중치로 초기화할 수 있습니다.

### Training

* **손실 함수**: 각 마스크 유형(비가시, 가시, 비가시)에 대해 유사한 시그모이드-교차 엔트로피 손실을 사용합니다.
* **총 손실 $L$**: 클래스($L_{cls}$), 경계 상자($L_{box}$) 손실과 결합하여 다음과 같이 정의됩니다.
    $$L = L_{cls} + L_{box} + L_{AM} + L_{VM} + L_{IVM}$$
    여기서 $AM, VM, IVM$은 각각 비가시(amodal), 가시(visible), 비가시(invisible) 마스크를 의미합니다.
* **손실의 중요성**: 이론적으로는 $IVM = AM - VM$ 관계 때문에 세 가지 마스크 손실 중 하나는 중복될 수 있습니다. 그러나 모든 마스크에 대한 손실을 추가하는 것이 비가시 마스크 로짓과 가시 마스크 로짓의 스케일을 동일하게 유지하고 각 마스크의 예측 품질을 높이는 데 필수적입니다. 특히, 가려짐 마스크에 대한 추가 손실은 가시 마스크의 품질을 떨어뜨리지 않으면서도 안정적인 가려짐 예측을 가능하게 합니다.

### Evaluation

* **확장된 AP/AR 측정**: 기존 인스턴스 분할에 사용되는 평균 정밀도(AP) 및 평균 재현율(AR) 측정 지표를 확장하여 사용합니다.
* **개별 마스크 평가**: 비가시 마스크($AP_{A}$), 가시 마스크($AP_{V}$)에 대해 개별적으로 AP 값을 계산합니다.
* **결합된 AP 측정 $AP_{AV}$**: 진정한 긍정(True Positive) 인스턴스를 정의할 때, 올바른 클래스 예측과 함께 비가시 마스크($IoU(AM_G, AM_P) > t$) 및 가시 마스크($IoU(VM_G, VM_P) > t$) 모두 주어진 IoU 임계값 $t$를 넘어야 합니다.
* **비가시 마스크 평가의 어려움**:
  * 가려지지 않은 객체의 경우 비가시 마스크가 존재하지 않아 재현율 정의가 어렵습니다.
  * 대부분의 `COCO amodal` 또는 `D2S amodal` 객체에서 비가시 마스크 영역은 비가시 또는 가시 마스크에 비해 매우 작습니다. 따라서 작은 예측 오차도 IoU 값에 큰 영향을 미쳐 `AP_{AIVV}`와 같은 종합 지표가 오해를 줄 수 있습니다.
* **가려짐 품질 측정**: 가려짐 예측의 품질을 측정하기 위해 가려지지 않은 모든 GT 객체를 무시하고, 가려진 객체에 대해서만 비가시 마스크의 $AP^{0.5}_{IV}$를 계산합니다. 이때, 비가시 마스크가 종종 매우 작다는 점을 고려하여 낮은 IoU 임계값(0.5)을 사용합니다.

## 📊 Results

### COCOA

* **성능 우위**: `AmodalMRCNN` (Mask R-CNN을 비가시 주석으로 학습시킨 모델)이 기존 `AmodalMask` 베이스라인보다 평균 정밀도에서 크게 앞섰습니다 (예: `AmodalMRCNN-50`의 $AP_A$ 29.9% vs `AmodalMask` 5.7%).
* **데이터 증강 효과 없음**: `COCOA` 데이터셋에서 인공적으로 생성된 훈련 데이터를 사용한 증강 기법은 성능 개선에 도움이 되지 않았습니다. 모델이 객체의 맥락(context)을 이해하는 것이 중요해 보입니다.
* **ORCNN의 다중 작업 이점**: `ORCNN`은 `AmodalMRCNN` 대비 비가시 마스크 예측($AP_A$)에서 약간 낮은 성능을 보였지만, 가시 마스크 품질($AP_V$)을 개선하고 동시에 가려짐 마스크($AP^{0.5}_{IV}$)를 예측하는 능력을 입증했습니다.

### COCOA cls

* **클래스 정보의 영향**: `COCOA` 대비 `COCOA cls`에서 모델의 성능이 전반적으로 훨씬 좋았습니다. 이는 비가시 마스크 예측이 클래스별 작업임을 시사합니다.
* **ORCNN vs AmodalMRCNN**: `AmodalMRCNN`이 비가시 마스크 예측에서 `ORCNN`을 약간 능가했지만, `ORCNN`은 가시 및 비가시 마스크를 모두 제공합니다. 이는 `ORCNN`이 세 가지 마스크를 모두 예측하려고 시도하면서 오류의 원인이 더 많아지기 때문일 수 있습니다.
* **클래스별/클래스 비종속**: `COCOA cls`에서는 클래스별 마스크 제안(class-specific mask proposals)을 사용하는 것이 대부분의 지표에서 도움이 되었습니다. 다만, 가려진 객체에 대해서는 클래스 비종속 `ORCNN (cls-agn)`이 가시 및 비가시 마스크 측면에서 가장 좋은 평균 정밀도를 보였습니다.

### D2S amodal

* **정성적 결과의 유망함**: `ORCNN`은 `D2S amodal`에서 객체들이 완전히 겹쳐 있거나 이미지 경계를 벗어나는 경우에도 가려짐을 정확하게 예측하는 등 매우 유망한 정성적 결과를 보여주었습니다. 이는 모델이 객체 클래스의 일반적인 형상을 학습할 수 있음을 의미합니다.
* **비가시 마스크 예측의 어려움**: 정량적 분석에서는 비가시 마스크 예측($AP^{0.5}_{IV}$)이 여전히 매우 어려운 작업임을 보여줍니다. 작은 비가시 마스크의 경우 작은 예측 차이도 IoU 값에 큰 영향을 미치며, 큰 비가시 마스크의 경우 정확한 형태를 생성하기 어렵기 때문입니다.
* **ORCNN의 절충**: `ORCNN`은 가시 및 비가시 마스크를 동시에 예측하는 최적의 절충안(최고의 $AP_{AV}$ 및 $AP_V$)을 제공하지만, 비가시 마스크만 예측하는 `AmodalMRCNN` 대비 $AP_A$ 값이 약간 낮습니다.
* **클래스 비종속 vs 클래스별**: `D2S amodal`에서는 `ORCNN (cls-agn)`이 클래스별 `ORCNN` 모델보다 우수한 성능을 보였는데, 이는 `COCOA` 및 `COCOA cls` 결과와 대조됩니다.
* **비가시 주석 없는 학습**: `D2S amodal`의 `modal augmented` 데이터로만 `ORCNN`을 학습시켰을 때, `D2S amodal train` 데이터로 학습한 `ORCNN`에 비해 약간만 낮은 성능을 보였습니다. 이는 비가시 주석 데이터 없이도 비가시 분할이 가능함을 시사하는 중요한 결과입니다.

## 🧠 Insights & Discussion

* **"보이지 않는 것을 보는 학습"**: `ORCNN`은 시각적 단서가 없는 영역에서도 가려진 객체의 비가시 마스크를 예측할 수 있음을 입증하여, 모델이 실제로 "보이지 않는 것을 학습"할 수 있음을 보여줍니다.
* **엔드-투-엔드 통합의 강점**: 단일 포워드 패스에서 비가시, 가시, 비가시 마스크를 동시에 예측하는 엔드-투-엔드 학습 모델은 효율성 및 모델 간 일관성 측면에서 큰 이점을 제공합니다.
* **비가시 마스크 예측의 난이도**: 비가시 마스크 예측은 여전히 매우 어려운 과제입니다. 작은 마스크 크기, 이미지 노이즈(반사, 조명 변화)로 인한 오탐지(false positive)가 발생하며, 인접한 객체로 마스크가 확장되는 등의 실패 사례가 보고되었습니다. 낮은 $AP^{0.5}_{IV}$ 값은 이러한 어려움을 잘 보여줍니다.
* **데이터셋 특성의 중요성**: `COCOA`에서 데이터 증강이 효과 없었던 반면, `D2S amodal`에서는 `modal augmented` 데이터만으로도 합리적인 성능을 달성한 것은 데이터셋의 특성(예: 배경의 균질성, 복잡도)이 모델 학습에 미치는 영향을 시사합니다.

## 📌 TL;DR

이 논문은 이미지 내 객체의 보이는 부분뿐만 아니라 가려진 부분까지 포함하는 **비가시 인스턴스 분할** 문제를 해결하기 위해 **ORCNN(Occlusion R-CNN)**이라는 최초의 엔드-투-엔드 학습 모델을 제안합니다. ORCNN은 Mask R-CNN 기반으로, 비가시, 가시, 가려짐 마스크를 단일 포워드 패스로 동시에 예측하며, 모든 마스크에 대한 손실을 사용하여 학습합니다. `COCO amodal`, 새로운 `COCOA cls`, `D2S amodal` 데이터셋에서 기존 베이스라인을 뛰어넘는 성능을 달성했으며, 특히 `D2S amodal`에서는 비가시 훈련 데이터 없이도 합리적인 성능을 보여 "보이지 않는 것을 학습"할 수 있음을 입증했습니다. 다만, 비가시 마스크 예측은 여전히 어려운 과제로 남아 있습니다.
