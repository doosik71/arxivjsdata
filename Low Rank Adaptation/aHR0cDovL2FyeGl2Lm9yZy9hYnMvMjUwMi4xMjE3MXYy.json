{
  "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
  "authors": "Haonan He, Peng Ye, Yuchen Ren, Yuan Yuan, Luyang Zhou, Shucun Ju, Lei Chen",
  "year": 2025,
  "url": "http://arxiv.org/abs/2502.12171v2",
  "abstract": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning\nlarge language models (LLMs), with its effectiveness influenced by two key\nfactors: rank selection and weight initialization. While numerous LoRA variants\nhave been proposed to improve performance by addressing one of these aspects,\nthey often compromise usability or computational efficiency. In this paper, we\nanalyze and identify the core limitations of existing approaches and propose a\nnovel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that\nsimultaneously adapts both the rank and initialization strategy within a\nunified framework. GoRA leverages gradient information during training to\ndynamically assign optimal ranks and initialize low-rank adapter weights in an\nadaptive manner. To our knowledge, GoRA is the first method that not only\naddresses the limitations of prior approaches -- which often focus on either\nrank selection or initialization in isolation -- but also unifies both aspects\nwithin a single framework, enabling more effective and efficient adaptation.\nExtensive experiments across various architectures and modalities show that\nGoRA consistently outperforms existing LoRA-based methods while preserving the\nefficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for\nmathematical reasoning, GoRA achieves a 5.13-point improvement over standard\nLoRA and even outperforms full fine-tuning by 2.05 points under high-rank\nsettings."
}