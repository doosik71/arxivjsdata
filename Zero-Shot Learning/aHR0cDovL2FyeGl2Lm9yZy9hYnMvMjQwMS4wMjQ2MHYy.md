# Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions

Oindrila Saha, Grant Van Horn, Subhransu Maji

## 🧩 Problem to Solve

기존 Vision-Language Model (VLM)은 CLIP [29]과 같은 대규모 모델조차 특정 도메인의 세분화된(fine-grained) 이미지 분류에서 시각적 속성을 인코딩하는 데 어려움을 겪습니다. 예를 들어, CUB [42] 데이터셋에서 시각적 속성 설명을 추가해도 제로샷 분류 성능 향상이 미미합니다. 이는 VLM 훈련에 사용되는 대규모 데이터셋이 전문가가 요구하는 세부 정보를 부족하게 담고 있기 때문입니다. 또한, 이러한 세분화된 도메인에서 대규모 이미지-캡션 데이터셋을 수집하는 것은 상당한 노력을 필요로 합니다.

## ✨ Key Contributions

* **LLM 생성 텍스트 활용**: Large Language Model (LLM)이 생성한 카테고리 설명을 풍부한 세분화된 이미지 분류 데이터셋과 결합하여 VLM의 제로샷 분류 성능을 향상시키는 새로운 방법을 제시합니다.
* **"Bag-level" 이미지-텍스트 지도 학습**: 이미지-텍스트 대응 관계가 명확하지 않은 "bag-level" 데이터셋을 사용하여 VLM을 훈련하는 효과적인 방법을 개발했습니다. 각 카테고리 내에서 이미지와 텍스트를 무작위로 페어링하고, 카테고리 수준의 대조 손실(contrastive loss)을 사용하여 강력한 성능 향상을 달성합니다.
* **다양한 정보원 평가**: 시각적 특징뿐만 아니라 서식지(habitat), 지리적 위치(geographic region), 분류학적 구조(taxonomic structure) 등 다양한 차원의 LLM 생성 설명을 체계적으로 평가하여, 지리적 선행 정보가 시각적 정보만큼 효과적이며 상호 보완적임을 밝혔습니다.
* **제로샷 분류 성능 향상**: 12개 데이터셋에서 평균 4-5%의 제로샷 분류 정확도 향상을 보였으며, NeWT [40] 데이터셋에서는 4.1%의 상대적 오류 감소를 달성했습니다.
* **벤치마크 공개**: 제로샷 인식 연구를 위한 14개 데이터셋 벤치마크(AdaptCLIPZS)를 공개했습니다.

## 📎 Related Works

* **Zero-shot Image Classification using VLMs**: CLIP [29], ALIGN [11], FLAVA [33] 등 VLM은 이미지와 텍스트 간의 공유 임베딩을 학습하여 제로샷 분류에 뛰어난 성능을 보입니다. 그러나 미세한 속성 인식에는 한계가 있습니다.
* **Generating Better Prompts**: CoOp [50], CoCoOp [49], CHiLS [24], CuPL [28] 등은 프롬프트 튜닝이나 LLM 질의를 통해 클래스 설명을 개선하여 제로샷 성능을 높이려 했습니다. 본 연구는 CLIP이 미세한 속성 인코딩에 어려움을 겪는다는 점을 지적하며 VLM 미세 조정의 필요성을 강조합니다.
* **Fine-tuning VLMs**: CLIP Adapter [8], Maniparambil et al. [19], WiSE-FT [43], LaFTer [21] 등은 VLM 미세 조정을 통해 Few-shot 분류나 도메인 이동에 대한 강건성을 개선하려 했습니다. GIST [15]와 I2MVFormer [22]는 GPT를 활용하여 세분화된 도메인 텍스트를 생성하지만, 본 연구는 무작위 페어링 전략을 사용하고 더 큰 데이터셋과 다양한 정보원을 탐색한다는 점에서 차별화됩니다.

## 🛠️ Methodology

본 연구 프레임워크는 세 가지 주요 단계로 구성됩니다:

1. **텍스트 설명 생성 (Dataset Generation)**:
    * **LLM 프롬프트**: 각 클래스에 대해 LLM (gpt-4-0613)에 질의하여 시각적 속성, 서식지, 지리적 위치 정보를 생성합니다.
    * **시각적 속성**: "[class]를 사진만으로 다른 [domain]과 구별하는 데 사용할 수 있는 특징은 무엇인가요? 고유하게 식별할 수 있는 모든 속성을 "[domain] with [characteristic]" 형식으로 exhaustive list로 제공해주세요."
    * **서식지 및 지리적 위치**: "[class]는 어디에서 찾을 수 있나요? [domain]을 식별하는 데 사용할 수 있는 서식지 및 지리적 위치 정보 목록을 제공해주세요."
    * 생성된 텍스트는 "a photo of a [class] [domain] with [characteristic]" 형태로 변환되어 각 카테고리 $k$에 대한 텍스트 집합 $Y_k$를 구성합니다.

2. **VLM 미세 조정 (VLM Fine-Tuning)**:
    * **"Bag-level" 지도 학습**: 훈련 세트의 각 카테고리 $k$에 대해 이미지 집합 $X_k$와 텍스트 집합 $Y_k$가 주어집니다.
    * **무작위 페어링**: 훈련 중에는 각 이미지 $x_i$에 대해 해당 카테고리에서 무작위로 샘플링된 텍스트 $y_i$를 페어링합니다.
    * **카테고리 수준 대조 손실**: CLIP의 배치 수준 손실을 수정하여 동일한 카테고리에 속하는 모든 이미지-텍스트 쌍 간의 유사성을 최대화하도록 합니다. 이미지에 대한 손실 함수는 다음과 같습니다:
        $$ L_{\text{image}} = - \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|G_i|} \sum_{j \in G_i} \log \frac{\exp(S_{i,j}/\tau)}{\sum_{r=1}^{N} \exp(S_{i,r}/\tau)} $$
        여기서 $S_{i,j}$는 이미지 $x_i$와 텍스트 $y_j$ 간의 유사도 점수이며, $G_i$는 쌍 $(x_i, y_i)$와 동일한 클래스에 속하는 배치 내의 모든 인덱스 집합입니다. 텍스트에 대한 $L_{\text{text}}$도 유사하게 정의됩니다.
    * **EMA (Exponential Moving Average)**: 과적합 방지를 위해 모멘텀 인코더($\theta_{\text{EMA}}, \phi_{\text{EMA}}$)를 사용하여 인코더 가중치($\theta, \phi$)를 업데이트합니다.
    * **하이퍼파라미터**: 각 데이터셋에 대해 15 epochs (iNaturalist는 5 epochs) 동안 훈련하며, 학습 가능한 온도 매개변수 $\tau$를 사용합니다.

3. **제로샷 분류 평가 (Evaluation for Zero-shot Classification)**:
    * **LLM 생성 테스트 텍스트**: 보지 못한 클래스($k \in K_{\text{test}}$)에 대해 LLM을 사용하여 텍스트 $Y_k$를 생성합니다.
    * **유사도 집계**: 주어진 이미지 $x$에 대해 모든 테스트 클래스 텍스트와의 유사도 점수 $S_k^m$을 계산합니다.
    * **클래스 예측**: 각 클래스에 대한 텍스트 확률의 평균을 사용하여 최종 클래스를 예측합니다.
        $$ \text{argmax}_k \frac{1}{l_k} \sum_{m=1}^{l_k} \frac{\exp(S_k^m)}{\sum_{p \in K_{\text{test}}} \sum_{q=1}^{l_p} \exp(S_p^q)} $$

## 📊 Results

* **Fine-grained 도메인 성능 향상**: CUB, Stanford Cars, FGVC Aircrafts, Flowers 102, Food 101 등 세분화된 도메인에서 CLIP [29] 대비 현저한 제로샷 분류 정확도 향상을 보였습니다 (예: CUB 50.5% → 53.34%).
* **다양한 정보원 효과**: CUB 데이터셋에서 시각적 정보만 사용하는 것보다 서식지(habitat) 정보만 사용하는 것이 더 좋은 성능을 보였으며 (53.34% vs 53.69%), 시각+분류학적+서식지 정보를 모두 조합했을 때 가장 높은 성능(54.23%)을 달성했습니다. 이는 지리적 선행 정보가 시각적 정보와 상호 보완적임을 시사합니다.
* **도메인 전이 성능**: 훈련 데이터셋과 테스트 데이터셋의 도메인이 다를 때도 강력한 성능 향상을 보였습니다. 예를 들어, iNaturalist에서 새 클래스를 모두 제거한 후 CUB 데이터셋에 테스트했을 때도 CLIP + A (52.83%) 대비 53.89%로 성능이 향상되었습니다.
* **다양한 LLM 텍스트에 대한 강건성**: GPT4로 훈련된 모델이 GPT3.5 turbo 및 LLaMA2-7B가 생성한 텍스트로 평가될 때도 사전 훈련된 CLIP보다 일관되게 높은 성능을 보였습니다.
* **Novel Tasks 평가**: NeWT [40] 벤치마크의 다양한 이진 분류 작업(연령, 속성, 건강 상태 등)에서 baseline CLIP 대비 4.1%의 상대적 오류 감소를 달성했습니다 (MAP 60.25% → 61.90%).

## 🧠 Insights & Discussion

* **세분화된 속성 학습의 중요성**: LLM이 생성한 풍부하고 세분화된 텍스트 설명을 사용하여 VLM을 미세 조정함으로써, 모델이 단순히 카테고리 이름을 넘어 시각적 속성을 더 잘 이해하고 연관시키는 능력을 습득했음을 시사합니다.
* **LLM의 효율적인 데이터 생성 능력**: LLM은 대규모 전문가 레이블링 없이도 세분화된 도메인에 대한 고품질의 카테고리 수준 설명을 생성할 수 있어, 이미지-텍스트 데이터셋 구축의 비용과 시간을 크게 절감합니다.
* **다중 모달리티 정보의 보완성**: 시각적 정보 외에 서식지나 지리적 위치와 같은 비시각적 속성이 특히 자연 도메인의 제로샷 분류에서 시각적 정보와 동등하게 효과적이고 상호 보완적이라는 점이 중요한 통찰입니다.
* **한계**: LLM이 생성한 텍스트의 정확성을 대규모 데이터셋에 대해 수동으로 검증하기 어렵다는 한계가 있습니다. 하지만 인간이 생성한 텍스트에 대한 개선된 성능과 검증된 평가 세트에서의 결과는 모델이 다소 노이즈가 있는 훈련 데이터에서도 의미 있는 정보를 학습할 수 있음을 보여줍니다.

## 📌 TL;DR

본 논문은 세분화된 도메인에서 VLM의 제한적인 제로샷 분류 성능을 개선하기 위해 LLM이 생성한 카테고리별 텍스트 설명을 활용하는 방법을 제안합니다. 이미지와 텍스트의 "bag-level" 무작위 페어링과 수정된 대조 손실 함수를 사용한 VLM 미세 조정을 통해, 모델은 시각적, 서식지, 지리적 정보 등 다양한 속성을 학습합니다. 결과적으로 CUB, Flowers102 등 12개 데이터셋에서 평균 4-5%의 정확도 향상과 NeWT 벤치마크에서 4.1%의 오류 감소를 달성하며, LLM 기반 데이터 확장의 효율성과 다중 모달리티 정보의 보완성을 입증합니다.
