# Cross-modal Representation Learning for Zero-shot Action Recognition

Chung-Ching Lin, Kevin Lin, Linjie Li, Lijuan Wang, Zicheng Liu

---

## 🧩 Problem to Solve

기존 제로샷 액션 인식(ZSAR) 연구는 다음과 같은 한계를 가집니다.

* **시각 특징 추출의 한계**: 사전 학습된 액션 인식 모델에서 시각 특징을 추출할 경우, 보지 못한 클래스에 대한 사전 지식을 획득할 수 있어 ZSAR의 엄격한 설정을 위반할 수 있습니다. 또한, 이러한 특징들이 공정한 표현 학습에 충분하지 않을 수 있습니다.
* **의미 정보의 불완전성**: 수동으로 레이블링된 액션 클래스의 의미 정보는 종종 부정확하거나 불완전합니다. 이는 의미적 공백(semantic gap) 문제를 야기합니다.
* **정보 손실 및 허브니스 문제**: 시각 및 의미 공간을 매핑하거나 투영하는 과정에서 시각적 판별력이 줄어들어 지식 전달 효율성이 저하됩니다. 고차원 임베딩 공간에서는 특정 프로토타입이 많은 관련 없는 인스턴스의 가장 가까운 이웃이 되는 허브니스(hubness) 문제가 발생합니다.

이 논문은 시각적 판별력을 유지하면서 시각 및 의미 공간을 연결하여 효과적인 지식 전달을 가능하게 하는 ZSAR 프레임워크를 제안합니다.

## ✨ Key Contributions

* **교차 모달 프레임워크**: 시각 및 의미 공간을 연결하면서도 효과적인 지식 전달을 위해 시각적 판별력을 유지하는 교차 모달 트랜스포머 기반 프레임워크를 제안합니다. 단일 모델을 Kinetics 데이터셋에 학습시켜 UCF101, HMDB51, ActivityNet 벤치마크에서 새로운 SOTA(State-Of-The-Art) ZSAR 성능을 달성했습니다.
* **간단하면서도 효과적인 의미 전달 방식**: 미확인(unseen) 클래스의 시각 프로토타입을 합성하기 위한 의미 전달 방식을 개발했습니다. 이를 통해 ZSAR이 시각 공간에서 실현되어 정보 손실과 허브니스 문제를 완화합니다.
* **모델의 세 가지 장점**:
  * **종단 간 학습 가능(End-to-end trainable)**: 전체 파이프라인이 단일 아키텍처 내에서 학습됩니다.
  * **정확도-복잡도 균형**: 합리적인 계산 복잡도로 우수한 성능을 달성합니다.
  * **유연성**: 다양한 특징 인코더 백본 활용이 유연하며, 사전 학습된 모델과 협력하여 일반화 능력을 향상시킬 수 있습니다.

## 📎 Related Works

* **시각 및 의미 연관 학습**:
  * **간접 의미-시각 매핑**: 시각 및 의미 임베딩을 공통 중간 공간에 투영하여 ZSAR을 수행하는 방식([Wang and Chen, 2017], [Chen and Huang, 2021]).
  * **시각 특징을 의미 공간에 투영**: 시각 특징을 의미 임베딩 공간에 직접 투영하여 분류하는 방식([Brattoli et al., 2020]).
  * **의미 특징을 시각 공간에 투영**: 허브니스 문제를 완화하기 위해 의미 임베딩에서 시각 임베딩으로 역매핑하는 방식([Zhang and Peng, 2018], [Mandal et al., 2019]). 본 논문은 기존 연구와 달리 학습된 시각 표현을 활용하여 미확인 시각 프로토타입을 구성합니다.
* **트랜스포머 아키텍처**: 이미지 인식([Dosovitskiy et al., 2020]), 객체 탐지([Carion et al., 2020]), 시각-언어 태스크([Sun et al., 2019]) 등 다양한 분야에서 우수한 성능을 보였습니다. CLIP([Radford et al., 2021])은 대규모 데이터셋으로 학습되어 뛰어난 제로샷 전이 능력을 보이지만, 본 연구는 제한된 데이터셋으로 학습된 모델의 확장성에 초점을 맞춥니다.

## 🛠️ Methodology

제안된 **ResT (ResNet-Transformer)** 모델은 비디오 데이터와 텍스트 레이블을 공동으로 인코딩하여 ZSAR을 수행하는 교차 모달 트랜스포머 기반 프레임워크입니다.

### 1. ResT 모델 구성

ResT는 프레임 레벨 특징 인코더 $F$와 교차 모달 트랜스포머 $T$로 구성됩니다.

* **특징 인코더 $F$**: 바닐라 2D ResNet을 사용하여 시각 특징을 추출합니다. ZSAR 설정을 보존하기 위해 사전 학습 없이 무작위 가중치로 초기화됩니다.
* **트랜스포머 $T$**: $F$의 출력 특징을 입력으로 받으며, 역시 무작위 가중치로 초기화됩니다.

### 2. 모델 입력

ResT는 비디오-텍스트 쌍 $(O, Y)$를 입력으로 받습니다.

* 비디오 $O$는 샘플링된 프레임 시퀀스 $\{F_1, F_2, ..., F_T\}$입니다.
* 각 프레임 $F_i$는 $F$에 의해 시각 특징 $A_i \in \mathbb{R}^{D_v}$로 추출됩니다.
* 클래스 레이블 텍스트 $Y$는 단어 임베딩 시퀀스 $w = [W_1, ..., W_{L_w}]$로 변환됩니다.
* 트랜스포머 입력은 $z_0 = [\text{CLS}, r, w]$ 형태이며, 여기서 $r = [A_1, ..., A_T]$입니다.

### 3. 학습 태스크

ResT는 두 가지 주요 태스크를 공동으로 학습합니다.

#### a. 시각 표현 학습 (주요 태스크)

* **목표**: 의미적 단서 없이 판별력 있는 시각 표현 $G = \text{ResT}(z_0)$를 학습합니다.
* **모달리티-특정 주의 (Modality-specific attention)**: 모든 시각 특징 토큰은 양방향 주의(bidirectional attention)를 통해 서로에게만 주의를 기울입니다 ($M(A_k, A_j) = 1$). 시각 특징은 단어 토큰에 주의를 기울이지 않습니다 ($M(A_k, W_j) = 0$). 이는 모델이 시각 표현 학습에 전념하도록 하고, 추론 시 텍스트 입력이 없을 때 예상치 못한 동작을 방지합니다.
* **분류**: 특수 토큰 $[\text{CLS}]$의 출력 $I_0^L$에 1-히든 레이어 MLP를 추가하여 비디오 클래스를 예측하고 Softmax 교차 엔트로피 손실 $L_{CLS}$를 사용합니다. $I_0^L$은 최종 비디오 표현 $G$로 사용됩니다.

#### b. 시각-의미 연관 학습 (보조 태스크)

* **목표**: 시각적 단서를 사용하여 마스킹된 언어 모델링을 수행하여 시각 및 의미 개념을 정렬합니다.
* **교차 모달 주의 (Cross-modal attention)**: 단어 토큰은 모든 시각 및 단어 토큰(자신보다 왼쪽에 있는)에 주의를 기울입니다.
* **마스킹된 토큰 손실 (Masked Token Loss, MTL)**: BERT의 MLM과 유사하게, 입력 단어 토큰의 15%를 마스킹하고 주변 단어 및 모든 시각 특징 토큰 $r$을 기반으로 마스킹된 토큰 $W_u$를 예측합니다. 음의 로그 가능도 손실 $L_{MLM}$을 사용합니다.

### 4. 학습 과정

* 최종 학습 목표는 두 손실의 합입니다: $\mathcal{L} = \mathcal{L}_{CLS} + \lambda_{MLM} \cdot \mathcal{L}_{MLM}$.
* **워밍업(Warm-up)**: 먼저 $F$만 150 에포크 동안 SGD로 학습합니다.
* **공동 학습(Joint-training)**: 그 다음 전체 파이프라인을 50 에포크 동안 AdamW로 공동 학습합니다.

### 5. 제로샷 액션 인식 (추론)

학습 후, ResT는 학습된 시각 공간에서 ZSAR을 수행합니다.

#### a. 기지(seen) 클래스 시각 프로토타입 생성

각 기지 클래스 $S_k$에 대해 시각 프로토타입 $i_k$를 해당 클래스의 모든 비디오 시각 표현 평균으로 생성합니다:
$$i_k = \frac{1}{\#S_k} \sum_{j=1}^{\#S_k} G_{S,k}^{(j)}$$

#### b. 의미적 관련성 전달 (미지(unseen) 시각 프로토타입 합성)

* **개념**: 기지 클래스와 미지 클래스 레이블 간의 의미적 관련성을 활용하여 미지 클래스의 시각 프로토타입을 합성합니다.
* **의미 관련성 행렬 $S$**: Word2Vec으로 계산된 미지 클래스 $b_U^i$와 기지 클래스 $b_S^j$의 의미 임베딩 간의 코사인 유사도를 측정하여 행렬 $S \in \mathbb{R}^{W \times K}$를 구성합니다.
* **부분 그래프 선택**: 미지 클래스 노드 각각에 대해, 기지 클래스 노드에 대한 부분 그래프를 구성하는 최적의 이진 인접 행렬 $\hat{A}$를 찾기 위해 정수 계획법(Integer Programming, IP)을 사용합니다. 이 과정에는 다음과 같은 제약 조건이 포함됩니다:
    1. $A_{i,j} \le 1$ (인덱스 함수 $1_{C}$에 의해 정의되며, $b_U^i$의 $K$-최근접 이웃에 속하는 $b_S^j$만 고려)
    2. $A_{i,j} \le \tau_A(i,j)$ (합성된 프로토타입의 판별성을 높이기 위한 상대 거리 제약)
    3. $\sum_{j} A_{i,j} \le d$ (연결 수 제한)
* **미지 시각 프로토타입 합성**: IP 솔루션 $\hat{A}$를 사용하여 미지 시각 프로토타입 $\hat{i}_U^i$를 계산합니다.
$$\hat{i}_{U}^{i} = \frac{1}{\sum_{j=1}^{K} \hat{A}_{i,j} \cdot S_{i,j}} \sum_{j=1}^{K} \hat{A}_{i,j} \cdot S_{i,j} \cdot i_{S}^{j}$$

#### c. 분류

새로운 미지 비디오 $E_U$에 대해 시각 표현 $G_U$를 추출한 후, 합성된 미지 시각 프로토타입 $\hat{i}_U^i$들과의 코사인 거리 기반 최근접 이웃 검색을 통해 레이블을 할당합니다: $H^* = \arg \min_{i} \text{dist}(G_U, \hat{i}_{U}^{i})$.

## 📊 Results

* **SOTA 성능 달성**: Kinetics 데이터셋에 한 번만 학습된 ResT 모델은 UCF101, HMDB51, ActivityNet 벤치마크 데이터셋에서 엄격한 제로샷 설정(보조 데이터셋 사전 학습 없음, 중복 클래스 제거) 하에 최신 ZSAR 방법론들을 크게 능가했습니다.
  * UCF101 (0/50): ResT18 54.7%, ResT101 58.7% Top-1 정확도.
  * HMDB51 (0/50): ResT18 39.3%, ResT101 41.1% Top-1 정확도.
  * 특히, 0/100 전체 데이터셋 테스트에서 E2ER(2+1)D 대비 UCF101에서 5.9%p의 상당한 정확도 향상을 보였습니다.
* **시각적 판별력 확인**: t-SNE 시각화 결과, 동일 클래스의 테스트 샘플들이 서로 가까이 위치하여 모델이 판별력 있는 시각 표현을 학습했음을 보여줍니다. 합성된 미지 프로토타입이 해당 인스턴스들의 중심에서 약간 벗어나더라도, 다양한 미지 클래스를 분류하기에 충분히 판별적입니다.
* **어블레이션 스터디 (Ablation Study)**:
  * **트랜스포머 아키텍처의 중요성**: RNN/LSTM보다 트랜스포머 아키텍처가 월등히 우수한 성능을 보였습니다.
  * **CLS 및 MLM 컴포넌트의 영향**: CLS(분류)와 MLM(마스킹된 언어 모델링) 및 의미 전달 방식의 조합이 가장 높은 성능을 달성했습니다. MLM은 시각-의미 개념을 정렬하는 데 필수적인 역할을 합니다.
  * **주의 메커니즘**: CLS 태스크에서 단일 모달리티 주의(시각 토큰이 시각 토큰에만 주의)를 사용하는 것이 성능 저하 없이 시각 표현 학습에 집중하게 합니다.
  * **백본 네트워크**: 더 깊은 네트워크(ResT101)와 적은 프레임을 사용하여 학습할 경우 계산 비용과 정확도 사이의 더 나은 트레이드오프를 제공합니다.
  * **ZSAR 수행 공간**: 의미 공간에서 분류하는 것보다 시각 공간에서 분류하는 것이 정보 손실을 줄이고 판별력을 보존하여 일관되게 더 높은 정확도를 얻었습니다.
  * **제한 조건의 중요성**: 의미 관련성 전달 시 적용된 제약 조건들은 허브니스 문제를 완화하고 분류 정확도를 향상시키는 데 기여합니다.
  * **일반화 능력**: ResT는 C3D 모델과 같은 사전 학습된 모델과 협력하거나 객체 특징을 입력으로 받아 종단 간 학습을 수행하여 성능을 더욱 향상시킬 수 있는 유연성을 보여주었습니다.

## 🧠 Insights & Discussion

* **시각적 판별력의 핵심 역할**: 본 연구의 결과는 ZSAR 성공의 핵심 요소가 시각 임베딩 공간에서 판별 능력을 유지하는 것임을 시사합니다. 제안된 교차 모달 프레임워크는 이를 효과적으로 달성합니다.
* **모델의 강점**: ResT는 종단 간 학습 가능하며, 효율적인 계산으로 높은 정확도를 달성하고, 다양한 백본 및 사전 학습 모델과의 유연한 통합이 가능합니다.
* **한계점**:
  * **미묘한 동작 혼동**: "웃음", "미소", "씹기"와 같이 미묘한 근육 움직임을 포함하는 유사한 동작들을 혼동하는 경향이 있습니다.
  * **희귀 객체 기반 동작 합성 실패**: "훌라후프"와 같이 주 객체의 명사로만 이름 지어졌거나, "daf 연주하기"와 같이 일반 동사와 희귀 객체의 명사로 이름 지어진 클래스에 대한 합성 실패가 발생할 수 있습니다. 이는 분포 외(out-of-distribution) 객체에 대한 관련성을 찾기 어렵기 때문입니다.
  * 이러한 한계는 순수 제로샷 설정을 완화하고 ImageNet과 같은 추가 데이터셋으로 사전 학습을 수행하면 개선될 수 있습니다.
* **허브니스 문제 완화**: 의미 관련성 전달 방식은 학습된 시각 공간에서 직접 프로토타입을 합성함으로써 정보 손실과 허브니스 문제를 효과적으로 완화합니다.

## 📌 TL;DR

본 논문은 기존 ZSAR의 한계(사전 학습된 시각 특징의 부적합성, 불완전한 의미 레이블, 정보 손실, 허브니스 문제)를 극복하기 위해 **ResT(ResNet-Transformer)**라는 새로운 교차 모달 트랜스포머 기반 프레임워크를 제안합니다. ResT는 시각적 판별력을 유지하며 시각 표현 학습과 시각-의미 연관 학습을 종단 간 공동으로 수행합니다. 추론 시에는 학습된 시각 공간에서 의미적 관련성 기반의 부분 그래프 선택을 통해 미지 클래스의 시각 프로토타입을 합성하여 분류합니다. 엄격한 제로샷 설정(추가 데이터셋 사전 학습 없음, 중복 클래스 제거) 하에 UCF101, HMDB51, ActivityNet에서 SOTA 성능을 달성하여, ZSAR에서 시각 임베딩 공간의 판별력 유지와 효과적인 의미 전달의 중요성을 입증했습니다.
