# Zero-Shot Learning by Convex Combination of Semantic Embeddings

Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S. Corrado, Jeffrey Dean

## 🧩 Problem to Solve

기존의 객체 인식 시스템은 학습을 위해 방대한 양의 레이블링된 훈련 데이터셋을 필요로 합니다. 그러나 새로운 객체 범주가 지속적으로 등장함에 따라, 수천 개의 속성을 수천 개의 객체 클래스에 수동으로 레이블링하는 것은 비용이 많이 들고 시간이 소요되며, 확장성이 매우 낮습니다. 이러한 한계를 극복하기 위해 **Zero-Shot Learning (ZSL)**은 이전에 학습되지 않은 객체 범주의 이미지를 올바르게 분류하는 능력을 목표로 합니다. 기존 ZSL 접근 방식은 일반적으로 이미지를 연속적인 의미 임베딩 공간으로 매핑하기 위한 회귀 모델을 학습하지만, 이 방식이 기존 N-way 분류기를 사용하여 더 직접적으로 ZSL을 수행할 수 있는지에 대한 의문이 있었습니다.

## ✨ Key Contributions

- **새로운 이미지 임베딩 시스템 제안**: 기존의 N-way 이미지 분류기(예: Softmax 출력)와 단어 임베딩 모델(클래스 레이블 포함)을 결합하여 이미지를 의미 임베딩 공간으로 매핑하는 간단하고 직접적인 방법인 **ConSE(Convex combination of Semantic Embeddings)**를 제안합니다.
- **추가 훈련 불필요**: 이 방법은 이미지를 의미 임베딩 공간으로 매핑하기 위해 추가적인 훈련 과정이 필요하지 않습니다. 기존 분류기의 예측 확률을 클래스 레이블 임베딩 벡터의 볼록 결합에 대한 가중치로 사용합니다.
- **최첨단 성능 능가**: ImageNet zero-shot learning 태스크에서 기존의 복잡한 이미지 임베딩 기법(예: DeViSE)보다 뛰어난 성능을 보여줍니다.
- **단순성과 일반화 능력**: 단순한 모델 구조를 통해 복잡한 회귀 모델에서 발생할 수 있는 과적합 문제를 피하고, 이전에 보지 못한(unseen) 객체 범주에 대해 더 안정적으로 일반화됩니다.

## 📎 Related Works

- **Zero-Shot Learning 및 One-Shot Learning**: 레이블이 없는 새로운 범주를 인식하는 ZSL [10, 14]과 적은 수의 예시로 학습하는 One-Shot Learning [13, 5, 1, 8]과 밀접하게 관련되어 있습니다.
- **속성 기반 접근 방식**: 사람에 의해 레이블링된 시각적 속성 [4, 9]을 사용하여 보지 못한 객체 범주를 감지합니다. 각 클래스 레이블은 속성 벡터로 표현되지만, 대규모 태스크에서는 확장성 문제가 있습니다.
- **신경 언어 모델**: 비지도 학습으로 텍스트 코퍼스에서 단어 임베딩 벡터를 학습하는 방식 [2]을 활용합니다. Frome et al. [6]과 Socher et al. [18]은 이러한 단어 임베딩을 사용하여 객체 클래스 레이블의 텍스트 이름을 연속적인 의미 공간에 임베딩했습니다. 본 논문 또한 skip-gram 모델 [12]을 사용합니다.
- **Deep Visual-Semantic Embedding (DeViSE)** [6]: 컨볼루션 신경망의 마지막 Softmax 계층을 선형 변환 계층으로 대체하고 랭킹 목적 함수를 사용하여 훈련하여 이미지 입력을 올바른 레이블의 연속 임베딩 벡터에 가깝게 매핑합니다. ConSE는 이 모델과 비교됩니다.

## 🛠️ Methodology

ConSE(Convex combination of Semantic Embeddings)는 다음과 같은 방식으로 작동합니다:

1. **사전 훈련된 분류기 활용**: 훈련 데이터셋 $D_0$에서 이미지 $x$가 클래스 레이블 $y \in Y_0$에 속할 확률 $p_0(y|x)$를 추정하도록 사전 훈련된 확률적 N-way 분류기 $p_0$를 사용합니다.
2. **의미 임베딩 벡터 정의**: 각 클래스 레이블 $y$는 의미 임베딩 벡터 $s(y) \in S \equiv \mathbb{R}^q$와 연결됩니다. 이 벡터는 독립적인 자연어 처리 태스크(예: skip-gram 모델 [12]로 Wikipedia 코퍼스에서 학습)를 통해 얻어집니다.
3. **이미지 임베딩 생성 (볼록 결합)**: 입력 이미지 $x$에 대해 분류기 $p_0$의 상위 $T$개 예측 $\hat{y}_0(x, t)$ ($t=1, \dots, T$)와 해당 예측 확률 $p_0(\hat{y}_0(x, t)|x)$를 사용하여 의미 임베딩 벡터 $f(x)$를 생성합니다. 이는 상위 예측 레이블들의 의미 임베딩 벡터들을 확률로 가중치 부여한 볼록 결합입니다.
   $$
   f(x) = \frac{1}{Z} \sum_{t=1}^{T} p(\hat{y}_0(x, t)|x) \cdot s(\hat{y}_0(x, t))
   $$
   여기서 $Z = \sum_{t=1}^{T} p(\hat{y}_0(x, t)|x)$는 정규화 계수이며, $T$는 고려할 임베딩 벡터의 최대 개수를 제어하는 하이퍼파라미터입니다.
   - 예를 들어, 분류기가 이미지에 '사자'와 '호랑이'가 있을 확률을 각각 0.6, 0.4로 예측하면, $f(x)$는 $0.6 \cdot s(\text{lion}) + 0.4 \cdot s(\text{tiger})$가 되어 의미 공간에서 사자와 호랑이 중간 지점에 위치하게 됩니다.
4. **Zero-Shot 분류**: 생성된 이미지 임베딩 $f(x)$를 사용하여 테스트 레이블 집합 $Y_1$에서 가장 관련성 높은 클래스 레이블을 찾습니다. 코사인 유사도를 사용하여 $f(x)$와 테스트 레이블 임베딩 $s(y')$ 간의 유사도를 측정하고 가장 높은 유사도를 가진 레이블을 예측합니다.
   $$
   \hat{y}_1(x,1) \equiv \arg\max_{y' \in Y_1} \cos(f(x), s(y'))
   $$
   이 과정은 $k$-최근접 이웃(k-NN) 검색과 유사하며, 의미 공간에서 레이블 외삽(label extrapolation)을 수행합니다.

## 📊 Results

- **ImageNet Zero-Shot Learning**: ConSE는 ImageNet 데이터셋에서 DeViSE [6]와 비교되었습니다. 두 모델 모두 동일한 skipgram 텍스트 모델(500차원 단어 임베딩)과 ImageNet 2012 1K 데이터셋으로 훈련된 컨볼루션 신경망 [7]을 사용했습니다.
- **성능 지표**: "flat" hit@k 및 "hierarchical" precision@k 지표를 사용했습니다.
- **DeViSE 대비 우위**: ConSE는 모든 데이터셋(2-hops, 3-hops, ImageNet 2011 21K) 및 $k$ 값에서 DeViSE보다 일관되게 우수한 zero-shot 성능을 보였습니다. 특히, ConSE(10) 모델이 최적의 성능을 나타냈습니다.
  - 예를 들어, 2-hops 데이터셋(1,589개 미학습 객체)에서 ConSE(10)는 hit@1 9.4%, hit@5 24.7%를 달성하여 DeViSE보다 높은 수치를 기록했습니다.
- **정성적 결과**: ConSE(10)가 예측한 레이블은 DeViSE보다 일반적으로 더 일관적이고 이상치(outliers)가 적은 것으로 나타났습니다 (예: DeViSE는 "Steller sea lion"에 대해 "flip-flop"을 예측).
- **훈련 클래스 포함 시 성능 하락**: zero-shot 클래스 분류 시 훈련 레이블을 후보군에 포함하면 성능이 크게 하락했습니다. 이는 모델이 훈련 레이블에 더 편향되어 있기 때문입니다.
- **일반 분류 태스크**: 기존 1000개 클래스 학습 태스크에서는 DeViSE가 ConSE보다 약간 더 나은 성능을 보였지만, zero-shot 클래스에 대한 일반화 능력은 ConSE가 더 우수했습니다.

## 🧠 Insights & Discussion

- **단순함의 효율성**: ConSE의 핵심은 매우 단순한 접근 방식에 있습니다. 분류기 점수를 단어 벡터의 볼록 결합에 대한 가중치로 사용하는 것은 N-way 분류 시스템을 이미지 임베딩 시스템으로 재구성하는 가장 직접적인 방법 중 하나입니다.
- **기존 시스템의 강점 활용**: 이 방법의 성공은 최첨단 이미지 분류기(예: CNN)와 최첨단 텍스트 임베딩 시스템(예: skip-gram)의 내재된 강점을 효과적으로 활용하는 능력에 있습니다.
- **일반화 능력 우위**: DeViSE 모델이 훈련 레이블 임베딩 예측에 특화된 복잡한 비선형 임베딩 함수를 학습하여 과적합될 수 있는 반면, ConSE는 더 단순한 임베딩 모델을 통해 이전에 보지 못한 zero-shot 클래스에 더 안정적으로 일반화됩니다.
- **확장성 및 유연성**: ConSE 프레임워크는 특정 시각 및 텍스트 모델의 세부 사항에 의존하지 않고, 다른 시각 객체 분류 시스템이나 다양한 텍스트 표현에도 적용 가능합니다.
- **신뢰도 표현 가능성**: ConSE가 이미지에 할당하는 벡터의 노름(norm)은 모델의 임베딩에 대한 암묵적인 신뢰도 표현으로 활용될 수 있습니다. 분류기가 확신하지 못하는 레이블 할당은 더 낮은 점수를 받고, 이는 볼록 결합 벡터의 크기를 줄여 신뢰도 신호로 사용될 수 있습니다.

## 📌 TL;DR

이 논문은 추가 훈련 없이 기존 N-way 이미지 분류기와 단어 임베딩 모델을 결합하여 Zero-Shot Learning을 위한 **ConSE(Convex combination of Semantic Embeddings)**라는 간단한 이미지 임베딩 시스템을 제안합니다. ConSE는 분류기의 상위 예측 확률을 사용하여 해당 클래스 레이블의 의미 임베딩 벡터들을 볼록 결합함으로써 이미지를 의미 공간에 매핑합니다. ImageNet zero-shot 태스크에서 이전에 보지 못한 클래스에 대해 최첨단 방법인 DeViSE보다 우수한 성능과 더 나은 일반화 능력을 보였으며, 이는 모델의 단순성이 복잡한 과적합 문제를 피하는 데 도움이 되었음을 시사합니다.
