# Context-aware Feature Generation for Zero-shot Semantic Segmentation

Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, Liqing Zhang

## 🧩 Problem to Solve

기존의 시맨틱 분할(Semantic Segmentation) 모델은 픽셀 단위의 조밀한(dense) 주석(annotation)에 크게 의존하여 주석 비용이 높습니다. 이 문제를 해결하기 위해, 본 논문은 주석 없이 미분류(unseen) 객체를 분할하는 도전적인 과제인 **제로샷 시맨틱 분할(Zero-shot Semantic Segmentation, ZS3)**에 초점을 맞춥니다. 기존 ZS3 방법론인 ZS3Net은 다음과 같은 한계점을 가집니다:

1. **모드 붕괴(Mode Collapse) 문제**: 의미 단어 임베딩(semantic word embeddings)과 무작위 노이즈만으로 특징을 생성하여, 한정된 다양성의 특징만 생성하고 모드 붕괴 현상을 겪습니다.
2. **제한된 컨텍스트 정보 활용**: 객체 수준의 공간적 관계만 고려하며, 미분류 카테고리의 관계 그래프(relational graphs)는 일반적으로 접근하기 어렵습니다. 픽셀 단위의 미묘한 컨텍스트 정보를 충분히 활용하지 못합니다.

## ✨ Key Contributions

* 픽셀 단위 컨텍스트 정보를 활용하여 다양하고 컨텍스트를 인지하는 특징을 생성하는 새로운 특징 생성기(feature generator)를 제안했습니다. 이를 통해 제로샷 시맨틱 분할에서 모드 붕괴 문제를 완화하고 성능을 향상시켰습니다.
* 시맨틱 분할 네트워크와 특징 생성 네트워크를 효과적으로 통합하는 프레임워크를 구축했습니다.
* 다양한 스케일의 컨텍스트 정보를 적응적으로 가중치 부여하는 새로운 컨텍스트 셀렉터(Context Selector)를 포함하는 컨텍스트 모듈(Contextual Module, CM)을 설계했습니다.
* Pascal-Context, COCO-stuff, Pascal-VOC 3가지 벤치마크 데이터셋에서 최첨단(state-of-the-art) 성능을 달성하여 제안 방법의 효과를 입증했습니다.

## 📎 Related Works

* **시맨틱 분할**: FCN, Deeplab, PSPNet, U-Net 등과 같이 확장된 수용장(receptive field)과 인코더-디코더 구조를 활용하여 픽셀을 분류합니다. 컨텍스트 정보 활용의 중요성이 강조되지만, 이 모델들은 모든 카테고리에 대한 주석을 필요로 합니다.
* **제로샷 학습(Zero-shot Learning, ZSL)**: 시각적 특징과 의미 임베딩 간의 매핑을 학습하거나 미분류 카테고리를 위한 시각적 특징을 합성하는 방식으로, 학습 데이터에 없는 카테고리를 인식합니다. 초기 ZSL 방법론은 이미지 특징을 생성했지만, 픽셀 단위 컨텍스트는 고려하지 않았습니다.
* **제로샷 시맨틱 분할**: SPNet[43]은 시맨틱 투영 레이어와 보정(calibration)을 통해 지식을 전달하며, ZS3Net[3]은 픽셀 단위 특징을 생성하여 분류기를 미세 조정(finetune)합니다. 본 논문은 ZS3Net에서 영감을 받았으나, 픽셀 단위 컨텍스트 정보 활용과 네트워크 통합에서 차별점을 둡니다.

## 🛠️ Methodology

본 논문은 DeepLabv2를 기반으로 제안된 **CaGNet (Context-aware Feature Generation Network)**은 다음과 같은 구성요소로 이루어집니다:

1. **개요**:
    * 백본(Backbone) $E$, 컨텍스트 모듈(Contextual Module, CM), 특징 생성기(Generator) $G$, 판별기(Discriminator) $D$, 분류기(Classifier) $C$로 구성됩니다.
    * $E$는 실제 특징 맵(real feature map)을 출력하고, $CM$은 이를 받아 픽셀 단위 컨텍스트 정보를 포착합니다.
    * $G$는 의미 단어 임베딩 맵과 컨텍스트 잠재 코드 맵을 입력으로 받아 가짜(fake) 특징을 생성합니다.
    * $C$와 $D$는 실제/가짜 특징을 입력으로 받아 분할 및 판별 결과를 출력하며, $C$는 $1 \times 1$ 컨볼루션 레이어를 공유합니다.
2. **컨텍스트 모듈 (CM)**:
    * **다중 스케일 컨텍스트 맵**: $E$의 출력 특징 맵 $F_n$을 입력으로 받아 픽셀 단위 컨텍스트 정보를 수집합니다. 공간 해상도 손실 없이 수용장을 확장하기 위해 세 개의 연속적인 확장된 컨볼루션(dilated convolution) 레이어를 사용하여 $\hat{F}^0_n, \hat{F}^1_n, \hat{F}^2_n$와 같은 다중 스케일 컨텍스트 맵을 생성합니다.
    * **컨텍스트 셀렉터**: 연결된 컨텍스트 맵 $[\hat{F}^0_n, \hat{F}^1_n, \hat{F}^2_n]$을 $3 \times 3$ 컨볼루션 레이어를 통해 3채널 스케일 가중치 맵 $A_n$으로 변환합니다. $A_n$은 각 픽셀에 대해 다른 스케일의 컨텍스트 정보를 적응적으로 가중치 부여합니다.
    * **컨텍스트 잠재 코드**: 가중치가 부여된 컨텍스트 맵에 $1 \times 1$ 컨볼루션 레이어를 적용하여 $\mu_Z$와 $\sigma_Z$를 출력합니다. 픽셀별 컨텍스트 잠재 코드 $z_{n,i}$는 가우시안 분포 $N(\mu_{z_{n,i}}, \sigma_{z_{n,i}})$에서 샘플링되며, KL-발산 손실 $L_{KL}$을 통해 단위 가우시안 분포 $N(0,1)$을 따르도록 강제합니다.
    * $CM$은 $X_n = F_n + F_n \odot \phi(Z_n)$와 같은 새로운 특징 맵을 출력합니다.
3. **컨텍스트 인지 특징 생성기**:
    * $G$는 픽셀 단위 카테고리 임베딩 $w_{s_{n,i}}$와 $CM$에서 생성된 $z_{n,i}$를 입력으로 받아 가짜 픽셀 특징 $\tilde{x}_{s_{n,i}} = G(z_{n,i}, w_{s_{n,i}})$를 생성합니다.
    * **손실 함수**:
        * 재구성 손실 $L_{REC} = \sum_{n,i} ||x^s_{n,i} - \tilde{x}^s_{n,i}||^2_2$: 생성된 특징이 실제 특징을 잘 재구성하도록 합니다.
        * 분류 손실 $L_{CLS} = -\sum_{n,i} y^s_{n,i} \log(C(x^s_{n,i}))$: 실제 특징에 대한 분류 정확도를 높입니다.
        * 적대적 손실 $L_{ADV} = \sum_{n,i} (D(x^s_{n,i}))^2 + (1-D(G(z_{n,i}, w^s_{n,i})))^2$: 생성된 특징이 실제처럼 보이도록 $G$를 훈련하고, $D$는 실제와 가짜를 구별합니다.
4. **최적화**:
    * **훈련 (Training)**: $E, CM, G, D, C$ 모든 모듈을 분류 카테고리(seen categories)의 주석을 사용하여 함께 훈련합니다. 전체 목적 함수는 $L_{CLS} + L_{ADV} + \lambda_1 L_{REC} + \lambda_2 L_{KL}$입니다.
    * **미세 조정 (Finetuning)**: $N(0,1)$에서 샘플링된 잠재 코드와 분류/미분류 카테고리(seen/unseen categories)의 의미 임베딩을 사용하여 가짜 특징을 생성합니다. 이 단계에서는 $E$와 $CM$을 고정하고 $G, D, C$만 업데이트합니다.

## 📊 Results

* **성능 비교**: Pascal-Context, COCO-stuff, Pascal-VOC 세 데이터셋 모두에서 CaGNet은 SPNet 및 ZS3Net보다 미분류 카테고리 및 전체 성능(hIoU, mIoU)에서 **상당한 개선**을 보였습니다. 특히, hIoU와 mIoU 지표에서 큰 폭의 향상을 달성했습니다.
* **모듈별 검증 (Ablation Study)**:
  * $CM$이 단독으로 시맨틱 분할 성능에 미치는 영향은 미미하지만, $G$와 $D$와 결합될 때 미분류 카테고리 성능을 크게 향상시킵니다.
  * $CM$의 다중 스케일 접근 방식과 컨텍스트 셀렉터는 컨텍스트 정보를 효과적으로 집계하고 활용하는 데 중요한 역할을 합니다.
  * $L_{KL}$, $L_{ADV}$, $L_{REC}$ 모든 손실 항이 모델 성능에 기여함을 확인했습니다.
* **하이퍼파라미터 분석**: 미세 조정 단계에서 분류:미분류 픽셀의 생성 비율(feature generating ratio)을 $1:1$로 설정하는 것이 최적의 성능을 보임을 확인했습니다.
* **정성적 분석 (Qualitative Analyses)**:
  * 미분류 객체(예: 기차, TV 모니터, 화분, 양)에 대한 시맨틱 분할 결과가 기존 방법보다 더 정확합니다.
  * $CM$을 사용하면 분류/미분류 카테고리 모두에 대해 더 나은 특징 재구성 품질을 보여주며, 이는 더 나은 생성 품질을 의미합니다.
  * 컨텍스트 셀렉터는 픽셀의 위치와 객체 크기에 따라 적절한 스케일의 컨텍스트(예: 식별 가능한 지역이나 작은 객체는 작은 스케일, 다른 픽셀은 중간/큰 스케일)를 적응적으로 선택함을 시각적으로 증명했습니다.

## 🧠 Insights & Discussion

* 픽셀 단위 컨텍스트 정보는 제로샷 시맨틱 분할에서 특징 다양성과 정확도를 높이는 데 결정적인 역할을 합니다. 이는 기존 ZS3Net이 겪었던 모드 붕괴 문제를 효과적으로 완화합니다.
* 컨텍스트 모듈을 통해 얻은 컨텍스트 잠재 코드는 확률적 샘플링(stochastic sampling)을 가능하게 하여, 미분류 카테고리에 대한 명시적인 관계 그래프 없이도 특징을 생성할 수 있게 합니다. 이는 ZS3Net(GC)의 한계를 극복합니다.
* 시맨틱 분할 네트워크와 특징 생성 네트워크의 통합은 실제 특징과 생성된 특징 간의 일관성을 유지하며 모델의 일반화 능력을 향상시킵니다.
* 컨텍스트 셀렉터는 픽셀마다 다른 컨텍스트 스케일의 중요도를 학습하여 특징 생성의 정밀도를 높이는 데 기여합니다.
* 본 연구는 여전히 분류 카테고리에서 학습된 구조에 암묵적으로 의존하며, 미분류 카테고리의 성능이 분류 카테고리보다 낮은 한계가 있습니다. 미세 조정 시 생성 비율 등 하이퍼파라미터 튜닝이 필요합니다.

## 📌 TL;DR

CaGNet은 픽셀 단위 주석의 부담을 줄이기 위해 제로샷 시맨틱 분할 문제를 해결합니다. 픽셀 단위 컨텍스트 정보를 활용하는 컨텍스트 모듈과 특징 생성 네트워크를 통합하여, 기존 방법의 모드 붕괴와 제한된 컨텍스트 활용 문제를 개선합니다. CaGNet은 다양하고 컨텍스트를 인지하는 특징을 생성하여 3가지 벤치마크 데이터셋에서 최첨단 성능을 달성했으며, 특히 미분류 객체 분할에서 큰 향상을 보였습니다.
