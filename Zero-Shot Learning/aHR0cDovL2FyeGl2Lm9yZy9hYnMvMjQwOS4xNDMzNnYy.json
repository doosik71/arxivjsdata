{
  "title": "Zero-Shot Skeleton-based Action Recognition with Dual Visual-Text\n  Alignment",
  "authors": "Jidong Kuang, Hongsong Wang, Chaolei Han, Yang Zhang, Jie Gui",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.14336v2",
  "abstract": "Zero-shot action recognition, which addresses the issue of scalability and\ngeneralization in action recognition and allows the models to adapt to new and\nunseen actions dynamically, is an important research topic in computer vision\ncommunities. The key to zero-shot action recognition lies in aligning visual\nfeatures with semantic vectors representing action categories. Most existing\nmethods either directly project visual features onto the semantic space of text\ncategory or learn a shared embedding space between the two modalities. However,\na direct projection cannot accurately align the two modalities, and learning\nrobust and discriminative embedding space between visual and text\nrepresentations is often difficult. To address these issues, we introduce Dual\nVisual-Text Alignment (DVTA) for skeleton-based zero-shot action recognition.\nThe DVTA consists of two alignment modules--Direct Alignment (DA) and Augmented\nAlignment (AA)--along with a designed Semantic Description Enhancement (SDE).\nThe DA module maps the skeleton features to the semantic space through a\nspecially designed visual projector, followed by the SDE, which is based on\ncross-attention to enhance the connection between skeleton and text, thereby\nreducing the gap between modalities. The AA module further strengthens the\nlearning of the embedding space by utilizing deep metric learning to learn the\nsimilarity between skeleton and text. Our approach achieves state-of-the-art\nperformances on several popular zero-shot skeleton-based action recognition\nbenchmarks. The code is available at: https://github.com/jidongkuang/DVTA.",
  "citation": 1
}