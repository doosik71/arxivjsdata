{
  "title": "A Generalized Zero-Shot Quantization of Deep Convolutional Neural\n  Networks via Learned Weights Statistics",
  "authors": "Prasen Kumar Sharma, Arun Abraham, Vikram Nelvoy Rajendiran",
  "year": 2021,
  "url": "http://arxiv.org/abs/2112.02834v2",
  "abstract": "Quantizing the floating-point weights and activations of deep convolutional\nneural networks to fixed-point representation yields reduced memory footprints\nand inference time. Recently, efforts have been afoot towards zero-shot\nquantization that does not require original unlabelled training samples of a\ngiven task. These best-published works heavily rely on the learned batch\nnormalization (BN) parameters to infer the range of the activations for\nquantization. In particular, these methods are built upon either empirical\nestimation framework or the data distillation approach, for computing the range\nof the activations. However, the performance of such schemes severely degrades\nwhen presented with a network that does not accommodate BN layers. In this line\nof thought, we propose a generalized zero-shot quantization (GZSQ) framework\nthat neither requires original data nor relies on BN layer statistics. We have\nutilized the data distillation approach and leveraged only the pre-trained\nweights of the model to estimate enriched data for range calibration of the\nactivations. To the best of our knowledge, this is the first work that utilizes\nthe distribution of the pretrained weights to assist the process of zero-shot\nquantization. The proposed scheme has significantly outperformed the existing\nzero-shot works, e.g., an improvement of ~ 33% in classification accuracy for\nMobileNetV2 and several other models that are w & w/o BN layers, for a variety\nof tasks. We have also demonstrated the efficacy of the proposed work across\nmultiple open-source quantization frameworks. Importantly, our work is the\nfirst attempt towards the post-training zero-shot quantization of futuristic\nunnormalized deep neural networks.",
  "citation": 8
}