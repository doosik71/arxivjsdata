{
  "title": "A Meta-Learning Framework for Generalized Zero-Shot Learning",
  "authors": "Vinay Kumar Verma, Dhanajit Brahma, Piyush Rai",
  "year": 2019,
  "url": "http://arxiv.org/abs/1909.04344v1",
  "abstract": "Learning to classify unseen class samples at test time is popularly referred\nto as zero-shot learning (ZSL). If test samples can be from training (seen) as\nwell as unseen classes, it is a more challenging problem due to the existence\nof strong bias towards seen classes. This problem is generally known as\n\\emph{generalized} zero-shot learning (GZSL). Thanks to the recent advances in\ngenerative models such as VAEs and GANs, sample synthesis based approaches have\ngained considerable attention for solving this problem. These approaches are\nable to handle the problem of class bias by synthesizing unseen class samples.\nHowever, these ZSL/GZSL models suffer due to the following key limitations:\n$(i)$ Their training stage learns a class-conditioned generator using only\n\\emph{seen} class data and the training stage does not \\emph{explicitly} learn\nto generate the unseen class samples; $(ii)$ They do not learn a generic\noptimal parameter which can easily generalize for both seen and unseen class\ngeneration; and $(iii)$ If we only have access to a very few samples per seen\nclass, these models tend to perform poorly. In this paper, we propose a\nmeta-learning based generative model that naturally handles these limitations.\nThe proposed model is based on integrating model-agnostic meta learning with a\nWasserstein GAN (WGAN) to handle $(i)$ and $(iii)$, and uses a novel task\ndistribution to handle $(ii)$. Our proposed model yields significant\nimprovements on standard ZSL as well as more challenging GZSL setting. In ZSL\nsetting, our model yields 4.5\\%, 6.0\\%, 9.8\\%, and 27.9\\% relative improvements\nover the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets,\nrespectively.",
  "citation": 9
}