# Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement

Shen Zheng, Gaurav Gupta

## 🧩 Problem to Solve

저조도 이미지 및 비디오는 인간의 시각적 인식을 저해할 뿐만 아니라 컴퓨터 비전 알고리즘(예: 실시간 객체 감지, 분할)의 성능을 심각하게 저하시킵니다. 이러한 이미지들은 주로 노출 부족, 높은 ISO 노이즈, 저하된 특징 및 대비 문제를 겪습니다. 기존의 이미지 향상 방법들은 다음과 같은 한계를 가집니다:

* **전통적인 방법:** 수동으로 설정된 사전 조건(priors)에 의존하며, 복잡한 최적화 과정으로 인해 실시간 적용이 어렵습니다.
* **지도 학습 기반 방법:** 저조도/정상 조도 쌍(paired data)으로 구성된 고비용의 훈련 데이터셋이 필요합니다.
* **비지도 학습 기반 방법:** 쌍으로 구성되지 않은 데이터셋(unpaired data)으로 훈련되지만, 데이터 편향으로 인해 일반화 능력이 제한적입니다.
* **제로샷 학습 기반 방법:** 쌍으로 구성된/구성되지 않은 데이터셋 모두 없이 훈련 가능하지만, 이미지의 핵심적인 의미론적 정보(semantic information)를 무시하여 최적화되지 않은 결과물을 생성합니다.

## ✨ Key Contributions

본 논문은 이러한 한계를 해결하기 위해 의미론적 정보를 활용하는 새로운 제로샷 저조도 이미지/비디오 향상 네트워크(SGZ)를 제안하며, 주요 기여는 다음과 같습니다:

* **의미론적 정보 융합 제로샷 네트워크 제안:** 쌍 이미지, 비쌍 데이터셋 또는 분할 레이블 없이 고수준 의미론적 정보를 저수준 이미지 향상에 융합한 최초의 제로샷 저조도 이미지 향상 네트워크를 제안했습니다.
* **경량 특징 추출 네트워크 개발:** 저조도 이미지의 픽셀별 조명 부족 정도를 효율적으로 추정하는 경량의 합성곱 신경망(Enhancement Factor Extraction, EFE)을 개발했습니다.
* **반복적 이미지 향상 전략 설계:** 다양한 조명 조건의 이미지에 대한 모델의 일반화 능력을 높이기 위해 5가지 비참조(non-reference) 손실 함수를 포함하는 반복적 이미지 향상(Recurrent Image Enhancement, RIE) 전략을 설계했습니다.
* **우수한 성능 및 효율성 입증:** 광범위한 실험을 통해 제안된 모델이 정성적 및 정량적 지표 모두에서 최첨단(SOTA) 성능을 능가함을 입증했습니다. 특히, 1200x900 크기의 이미지 1000장을 1초 이내에 처리할 수 있어 저조도 비디오 향상에 이상적입니다.

## 📎 Related Works

* **전통적인 저조도 이미지 향상 방법:**
  * **히스토그램 평활화(HE-based):** BPDHE (밝기 보존), WTHE (가중치 및 임계값 기반 대비 향상).
  * **Retinex 이론 기반:** NPE (비균일, 자연성 보존), PIE (조명 및 반사율 동시 추정), LIME (조명 맵 추정).
* **심층 학습 기반 저조도 이미지 향상 방법:**
  * **지도 학습:** LLNet (노이즈에 강한 오토인코더), Retinex (분해 네트워크 및 조명 조정 네트워크), KinD (반사율의 품질 저하 제거).
  * **비지도 학습:** EnlightenGAN (쌍 데이터 없이 훈련, 어텐션 기반 멀티스케일 판별자).
  * **제로샷 학습:** Zero-DCE (경량 네트워크로 조명 향상 곡선 근사).
* **본 연구의 차별점:** 기존의 심층 학습 기반 방법들과 달리, 본 모델은 분할 레이블 없이 사전 훈련된 분할 네트워크를 활용하여 고수준 의미론적 정보를 사용하여 계산 복잡성을 크게 증가시키지 않으면서 핵심적인 의미론적 정보를 보존합니다.

## 🛠️ Methodology

제안된 SGZ 모델은 세 단계 네트워크로 구성됩니다:

1. **Enhancement Factor Extraction (EFE) 네트워크:**
    * U-Net 아키텍처에서 영감을 받았으며, 대칭적인 스킵 연결을 가진 완전 합성곱 신경망입니다.
    * $3 \times 3$ 깊이별 분리 합성곱(depthwise separable convolution)과 ReLU 활성화 함수로 구성된 블록을 사용합니다.
    * 입력 이미지의 픽셀별 조명 부족 정도를 나타내는 '향상 계수' $x_r$을 Tanh 활성화 함수를 통해 출력합니다.
    * 배치 정규화나 업/다운샘플링을 사용하지 않아 공간적 일관성(spatial coherence)을 보존합니다.
2. **Recurrent Image Enhancement (RIE) 네트워크:**
    * 이전 단계의 출력 $x_{t-1}$과 EFE에서 추출된 향상 계수 $x_r$을 입력으로 사용하여 저조도 이미지를 점진적으로 향상시킵니다.
    * 반복적 향상 과정은 다음 수식을 따릅니다: $$x_t = x_{t-1} + x_r * (x_{t-1}^2 - x_{t-1})$$ 여기서 $x_t$는 $t$번째 반복 단계의 출력이며, $Order=2$는 가장 견고한 향상을 제공하는 것으로 결정되었습니다.
3. **Unsupervised Semantic Segmentation (USS) 네트워크:**
    * 집중적인 향상 과정 동안 의미론적 정보를 보존하는 것을 목표로 합니다.
    * RIE에서 향상된 이미지를 입력으로 받아 의미론적 분할을 수행하며, 분할 손실(segmentation loss)을 계산합니다.
    * ResNet-50과 ImageNet 가중치로 초기화된 하향식 경로(bottom-up pathway, 훈련 중 파라미터 고정)와 가우시안 초기화된 상향식 경로(top-down pathway)를 포함합니다.
    * 특징 피라미드 네트워크(FPN) 구조를 사용하여 고수준 의미론적 특징을 고해상도 공간 정보로 변환합니다.

**손실 함수:** 총 5가지 비참조 손실 함수를 사용합니다.

* **공간적 일관성 손실 ($L_{spa}$):** 저조도 이미지와 향상된 이미지 간의 공간적 일관성을 유지합니다. 인접 픽셀뿐만 아니라 비인접 픽셀의 차이도 고려합니다:
    $$L_{spa} = \frac{1}{A} \sum_{i=1}^{A} \left[ \sum_{j \in \phi(i)} (|(Y_i - Y_j)| - |(I_i - I_j)|)^2 + \alpha \sum_{k \in \psi(i)} (|(Y_i - Y_k)| - |(I_i - I_k)|)^2 \right]$$
    여기서 $Y$와 $I$는 향상된 이미지와 저조도 이미지의 로컬 영역 평균 픽셀 값이며, $A$는 로컬 영역의 한 변 길이(4로 설정), $\phi(i)$는 인접 이웃, $\psi(i)$는 비인접 이웃입니다.
* **RGB 손실 ($L_{rgb}$):** Charbonnier 손실을 사용하여 색상 채널 간의 관계를 강화하여 색상 부정확성을 줄입니다:
    $$L_{rgb} = \sum_{\forall(i,j) \in \zeta} \sqrt{((Y_i) - (Y_j))^2 + \epsilon^2}, \quad \zeta=\{(R,G),(R,B),(G,B)\}$$
* **밝기 손실 ($L_{bri}$):** 이미지의 과/부족 노출을 제한하기 위해 특정 영역의 평균 픽셀 값과 사전 정의된 노출 수준 $E$ (0.60으로 설정) 간의 $L1$ 차이를 측정합니다:
    $$L_{bri} = \frac{1}{A} \sum_{a=1}^{A} |Y_a - E|$$
* **전체 변화(Total Variation) 손실 ($L_{tv}$):** 이미지의 노이즈를 줄이고 부드러움을 증가시키며, 채널 간(R, G, B) 관계를 추가로 고려하여 색상 밝기를 향상시킵니다:
    $$L_{tv} = \frac{1}{CHW} \sum_{c=1}^{C} \sum_{h=1}^{H} \sum_{w=1}^{W} \sqrt{(\nabla_x Y_{c,h,w})^2 + (\nabla_y Y_{c,h,w})^2}$$
* **의미론적 손실 ($L_{sem}$):** 향상 과정 동안 이미지의 의미론적 정보를 유지합니다. 분할 레이블 없이 사전 초기화된 모델과 Focal Loss를 기반으로 합니다:
    $$L_{sem} = \frac{1}{HW} \sum_{1 \le i \le H, 1 \le j \le W} -\beta(1-p_{i,j})^\gamma \log p_{i,j}$$
    여기서 $p_{i,j}$는 픽셀에 대한 분할 네트워크의 예상 클래스 확률입니다.
* **총 손실 ($L_{total}$):** 위 다섯 가지 손실 함수의 가중 합으로, EFE 네트워크의 파라미터를 업데이트하는 데 사용됩니다:
    $$L_{total} = \lambda_{spa} * L_{spa} + \lambda_{rgb} * L_{rgb} + \lambda_{bri} * L_{bri} + \lambda_{tv} * L_{tv} + \lambda_{sem} * L_{sem}$$

## 📊 Results

* **정량적 비교:**
  * NPE, LIME, MEF, DICM, VV, LOL, DarkBDD, DarkCityScape 등 다양한 벤치마크 데이터셋에서 이전 SOTA 모델들을 능가하는 성능을 보였습니다.
  * 평균 UNIQUE 지표에서 가장 높은 점수를, 평균 BRISQUE 지표에서 두 번째로 높은 점수를 기록했습니다.
  * 특히 어려운 DarkCityScape 데이터셋에서 PSNR, SSIM, MSE 모두에서 최고의 성능을 달성했습니다.
* **모델 효율성:**
  * 제시된 모델은 계산적으로 가장 효율적이며, $1200 \times 900$ 크기의 이미지 한 장을 처리하는 데 0.001초밖에 걸리지 않습니다 (초당 1000장 처리).
  * 가장 적은 파라미터 수(0.011M)와 FLOPs(0.12B)를 가져 저조도 비디오 향상 및 모바일 기기에 적합합니다.
* **정성적 비교:**
  * 어두운 영역을 크게 개선하고, 색상 균형과 이미지 대비를 유지하며, 충분한 얼굴 디테일과 자연스러운 노출을 제공합니다.
* **고수준 비전 작업에 대한 이점:**
  * **객체 감지(Object Detection):** DarkBDD 데이터셋에서 Yolov3를 사용했을 때, 다른 모델들에 비해 가장 많은 수의 차량을 감지하는 데 도움을 줍니다.
  * **의미론적 분할(Semantic Segmentation):** DarkCityScape 데이터셋에서 PSPNet을 사용했을 때, 지면 진실(groundtruth)에 가장 가까운 결과를 보였으며, mIOU 및 mPA에서 최고의 점수를 달성했습니다.
* **저조도 비디오 향상:** 야간 항공 비디오 사용자 연구에서 비교 대상 모델들 중 가장 높은 점수를 받았습니다.

## 🧠 Insights & Discussion

* **연구의 의의:** 제안된 의미론적 가이드 제로샷 접근 방식은 기존 저조도 이미지 향상 방법의 한계를 극복하는 효과적인 해결책을 제시합니다. 특히, 별도의 레이블링 비용 없이 고수준 의미론적 정보를 통합함으로써 이미지 품질을 향상시키고, 일반화 능력을 강화합니다. 이는 실시간 응용 분야에서 특히 중요한 모델의 효율성을 유지하면서 달성되었습니다.
* **고수준 작업에 미치는 영향:** 향상된 저조도 이미지가 객체 감지 및 의미론적 분할과 같은 다운스트림 컴퓨터 비전 작업의 성능을 크게 향상시킬 수 있음을 입증했습니다. 이는 단순히 시각적 품질을 개선하는 것을 넘어, 실제 응용 시나리오에서 시스템의 견고성을 높이는 데 기여합니다.
* **한계점:** 본 모델은 강한 모션 블러(motion blur)나 거울 반사와 같은 특정 유형의 이미지 저하에는 여전히 취약하며, 이러한 경우에 대한 추가적인 연구가 필요합니다 (Figure 11에 실패 사례 제시).

## 📌 TL;DR

**문제:** 기존 저조도 이미지 향상 방법들은 쌍 데이터 부족, 데이터셋 편향, 또는 의미론적 정보 무시로 인해 최적화되지 않은 결과를 초래했습니다.
**방법:** 본 논문은 '의미론적 가이드 제로샷(Semantic-Guided Zero-Shot, SGZ)' 네트워크를 제안합니다. 이 네트워크는 세 단계로 구성됩니다: 1) 픽셀별 향상 계수를 추출하는 EFE (Enhancement Factor Extraction), 2) 추출된 계수를 바탕으로 이미지를 점진적으로 밝게 하는 RIE (Recurrent Image Enhancement), 3) 향상 과정에서 의미론적 정보를 보존하는 USS (Unsupervised Semantic Segmentation). SGZ는 쌍 이미지, 비쌍 데이터셋, 분할 레이블 없이 훈련됩니다.
**발견:** SGZ는 시각적 품질, 효율성(1000 FPS) 측면에서 최첨단 성능을 달성하며, 객체 감지 및 의미론적 분할과 같은 고수준 컴퓨터 비전 작업의 성능을 크게 향상시켰습니다.
