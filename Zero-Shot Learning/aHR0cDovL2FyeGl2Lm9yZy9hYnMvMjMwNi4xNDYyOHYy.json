{
  "title": "An Integral Projection-based Semantic Autoencoder for Zero-Shot Learning",
  "authors": "William Heyden, Habib Ullah, M. Salman Siddiqui, Fadi Al Machot",
  "year": 2023,
  "url": "http://arxiv.org/abs/2306.14628v2",
  "abstract": "Zero-shot Learning (ZSL) classification categorizes or predicts classes\n(labels) that are not included in the training set (unseen classes). Recent\nworks proposed different semantic autoencoder (SAE) models where the encoder\nembeds a visual feature vector space into the semantic space and the decoder\nreconstructs the original visual feature space. The objective is to learn the\nembedding by leveraging a source data distribution, which can be applied\neffectively to a different but related target data distribution. Such\nembedding-based methods are prone to domain shift problems and are vulnerable\nto biases. We propose an integral projection-based semantic autoencoder\n(IP-SAE) where an encoder projects a visual feature space concatenated with the\nsemantic space into a latent representation space. We force the decoder to\nreconstruct the visual-semantic data space. Due to this constraint, the\nvisual-semantic projection function preserves the discriminatory data included\ninside the original visual feature space. The enriched projection forces a more\nprecise reconstitution of the visual feature space invariant to the domain\nmanifold. Consequently, the learned projection function is less domain-specific\nand alleviates the domain shift problem. Our proposed IP-SAE model consolidates\na symmetric transformation function for embedding and projection, and thus, it\nprovides transparency for interpreting generative applications in ZSL.\nTherefore, in addition to outperforming state-of-the-art methods considering\nfour benchmark datasets, our analytical approach allows us to investigate\ndistinct characteristics of generative-based methods in the unique context of\nzero-shot inference.",
  "citation": 7
}