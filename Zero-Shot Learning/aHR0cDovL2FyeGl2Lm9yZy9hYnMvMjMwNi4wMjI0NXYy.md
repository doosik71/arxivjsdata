# SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model

Dingyuan Zhang, Dingkang Liang, Hongcheng Yang, Zhikang Zou, Xiaoqing Ye, Zhe Liu, Xiang Bai

## 🧩 Problem to Solve

최근 2D 비전 작업에서 강력한 제로샷(zero-shot) 및 퓨샷(few-shot) 능력을 보여준 Segment Anything Model (SAM)과 같은 기반 모델(foundation models)이 3D 비전 작업, 특히 3D 객체 감지(object detection)에도 적용될 수 있는지 탐구하는 것이 주된 연구 문제입니다.

핵심 과제는 다음과 같습니다:

1. **데이터 도메인 차이**: SAM은 밀집된 픽셀로 구성된 2D 자연 이미지로 학습된 반면, 3D 객체 감지는 3D 공간에 불균일하게 분포된 희소한 LiDAR 포인트 클라우드를 입력으로 받습니다.
2. **출력 형식 차이**: SAM은 2D 세그멘테이션 마스크를 출력하지만, 3D 객체 감지기는 3D 바운딩 박스를 출력해야 합니다.
3. **제한된 3D 인식 능력**: SAM은 2D 이미지로만 학습되었기 때문에 본질적으로 3D 인식 능력이 제한적입니다.

이러한 문제들로 인해 2D 기반 모델을 3D 객체 감지에 직접 적용하는 것은 어렵습니다.

## ✨ Key Contributions

* **SAM 기반 제로샷 3D 객체 감지 탐색**: 3D 학습 없이 SAM 단독으로 제로샷 3D 객체 감지 가능성을 최초로 탐색하여, 2D 기반 모델이 3D 비전 작업에서 활용될 수 있음을 보여주었습니다.
* **BEV 기반 SAM 활용 파이프라인 제안**: LiDAR 포인트를 2D 이미지 형태의 BEV(Bird's Eye View) 맵으로 변환하여 SAM의 강력한 2D 세그멘테이션 능력을 3D 객체 감지로 확장하는 SAM3D 파이프라인을 제안했습니다. 이 방법은 다른 사전 학습된 3D 모델에 의존하지 않습니다.
* **효과적인 전처리 및 후처리 모듈 개발**: 희소한 LiDAR 신호와 SAM 훈련 데이터 간의 도메인 격차를 줄이기 위해 LiDAR-to-BEV 투영, BEV 후처리(형태학적 팽창), 마스크 후처리(영역 및 종횡비 필터링), 그리고 마스크에서 3D 바운딩 박스를 예측하는 Mask2Box 모듈을 포함한 전체 파이프라인을 설계했습니다.
* **Waymo Open Dataset을 통한 잠재력 입증**: 대규모 자율주행 데이터셋인 Waymo Open Dataset에서 SAM3D의 유효성을 검증하며, SAM의 3D 객체 감지 적용 가능성에 대한 긍정적인 신호를 제시했습니다.

## 📎 Related Works

* **SAM을 이용한 2D 작업**: SAM [11]은 유연한 프롬프트 지원, 모호성 인식, 방대한 훈련 데이터를 통해 강력한 일반화 능력을 보여주며 의료 영상 처리(SAMPolyp [25], Deng et al. [6], He et al. [7]), 위장 객체 분할(SAMCOD [20]) 등 다양한 2D 다운스트림 작업에서 활용되었습니다.
* **SAM을 이용한 3D 작업**: SAM을 사전 학습된 3D 모델과 결합하려는 시도들이 있었습니다. SA3D [2]는 NeRF(Neural Radiance Field)를 활용하여 3D 객체를 분할하고, Anything-3D [15]는 BLIP과 SAM을 결합하여 단일 뷰 3D 재구성을 수행했습니다. 3D-Box-Segment-Anything [3]은 SAM과 사전 학습된 3D 감지기 VoxelNeXt [4]를 사용하여 대화형 3D 감지 및 라벨링을 수행했습니다. 본 연구는 이들과 달리 SAM 단독으로 3D 객체 감지를 수행합니다.
* **3D 객체 감지**: 3D 비전의 핵심 과제로, SECOND [22], PointPillars [12], PointRCNN [18], PVRCNN [16], VoxelNeXt [4], QTNet [9], ViT-WSS3D [24] 등 많은 연구가 진행되었지만, 대부분 3D 주석 기반이며, 실용적 가치가 큰 제로샷 3D 객체 감지 설정은 잘 탐구되지 않았습니다.

## 🛠️ Methodology

SAM3D는 LiDAR 포인트 클라우드를 입력으로 받아 3D 바운딩 박스를 예측하는 5단계 파이프라인을 따릅니다.

1. **LiDAR-to-BEV 투영 (LiDAR-to-BEV Projection)**:
    * 희소한 LiDAR 포인트 $P = \{(x_i, y_i, z_i)\}_{i=1}^{N_p}$를 $H \times W \times 3$ 크기의 BEV 이미지 $I$로 변환합니다.
    * 각 포인트의 BEV 이미지 그리드 좌표 $(cx_i, cy_i)$는 다음과 같이 계산됩니다:
        $$cx_i = \lfloor(U_x - x_i)/s_x\rfloor$$
        $$cy_i = \lfloor(U_y - y_i)/s_y\rfloor$$
        여기서 $s_x, s_y$는 필러 크기, $U_x, U_y$는 좌표 상한입니다.
    * LiDAR 포인트의 반사 강도 $r_i$를 정규화([0,1] 범위)한 후 사전 정의된 팔레트 함수 $\text{Palette} : \mathbb{R} \to \mathbb{R}^3$를 사용하여 RGB 벡터 $c_i$로 변환하고 해당 그리드 픽셀에 채워 넣어 BEV 이미지의 판별력을 높입니다: $I[cx_i, cy_i, :] = c_i$.
    * 포인트가 투영되지 않은 그리드는 0으로 채웁니다.

2. **BEV 후처리 (BEV Post-processing)**:
    * SAM이 학습된 "밀집된" 자연 이미지와의 도메인 격차를 줄이기 위해, 희소한 BEV 이미지에 형태학적 팽창(morphology dilation, 최대 풀링으로 해석)을 적용합니다:
        $$I' = \text{MaxPool2D}(I)$$
    * 이는 SAM이 세그멘테이션을 더 쉽게 수행하도록 돕습니다.

3. **SAM을 이용한 세그멘테이션 (Segmentation with SAM)**:
    * 후처리된 BEV 이미지 $I'$를 SAM의 입력으로 사용합니다.
    * 이미지 전체를 $32 \times 32$ 메쉬 그리드 프롬프트(점 프롬프트)로 덮어 가능한 많은 전경 객체를 분할합니다.
    * 빈 공간에 해당하는 프롬프트는 제거하여 세그멘테이션 속도를 5배 가량 가속화합니다.
    * 이 단계에서 $N_m$개의 세그멘테이션 마스크 $M = \{m_i \in \mathbb{R}^{H \times W}\}_{i=1}^{N_m}$를 얻습니다.

4. **마스크 후처리 (Mask Post-processing)**:
    * SAM에서 생성된 노이즈가 많은 마스크를 필터링합니다.
    * 자율주행 환경에서 차량의 일반적인 크기와 모양을 고려하여, 영역 임계값 $[T_a^l, T_a^h]$과 종횡비 임계값 $[T_r^l, T_r^h]$을 사용하여 불필요한 마스크(오탐지)를 제거하고 최종적으로 $N_o$개의 고품질 전경 마스크 $M' = \{m_i \in \mathbb{R}^{H \times W}\}_{i=1}^{N_o}$를 얻습니다.

5. **Mask2Box**:
    * 최종 전경 마스크 $M'$로부터 3D 바운딩 박스 $B^{3D}$를 예측합니다.
    * **수평 속성(Horizontal attributes)**: 2D 마스크의 최소 바운딩 박스 $B^{2D}$로부터 2D 중심 $(x_i^{2D}, y_i^{2D})$, 크기 $(dx_i^{2D}, dy_i^{2D})$, 회전 각도 $\theta_i^{2D}$를 추출하고, 이를 BEV 이미지 스케일에 맞춰 3D 수평 속성 $(x_i^{3D}, y_i^{3D}, dx_i^{3D}, dy_i^{3D}, \theta_i^{3D})$으로 변환합니다.
        $$x_i^{3D} = U_x - (x_i^{2D} + 0.5) \times s_x$$
        $$y_i^{3D} = U_y - (y_i^{2D} + 0.5) \times s_y$$
        $$dx_i^{3D} = dx_i^{2D} \times s_x$$
        $$dy_i^{3D} = dy_i^{2D} \times s_y$$
        $$\theta_i^{3D} = \theta_i^{2D}$$
    * **수직 속성(Vertical attributes)**: 해당 2D 바운딩 박스 영역 내에 투영되는 LiDAR 포인트들의 z-좌표 $Z_i = \{z_j | (x_j, y_j, z_j) \text{ inside } B_i^{3D}\}$를 사용하여 3D 높이 $dz_i^{3D}$와 3D 중심 $z_i^{3D}$를 계산합니다.
        $$dz_i^{3D} = \max(Z_i) - \min(Z_i)$$
        $$z_i^{3D} = \min(Z_i) + \frac{dz_i^{3D}}{2}$$

## 📊 Results

* **평가 데이터셋**: Waymo Open Dataset의 검증 세트에서 VEHICLE 클래스에 대해 30미터 이내 거리의 mAP 및 mAPH를 측정했습니다.
* **SAM 버전별 성능**: SAM의 ViT-H 버전이 ViT-B, ViT-L보다 가장 좋은 성능(AP 19.51, APH 13.30)을 보였지만, ViT-L과 ViT-H 간의 차이는 미미하여 더 큰 모델 용량이 반드시 병목 현상은 아님을 시사했습니다.
* **필러(Pillar) 크기 영향**: 0.1m 필러 크기에서 최적의 성능을 달성했습니다. 너무 큰 필러(예: 0.2m, 0.4m)는 이산화 오류가 커서 객체 구분이 어렵고, 너무 작은 필러는 LiDAR 신호의 희소성으로 인해 하나의 객체가 여러 부분으로 분할되는 경향이 있었습니다.
* **BEV 이미지 타입의 중요성**: LiDAR 강도를 RGB 팔레트에 매핑한 BEV 이미지가 이진(binary) 또는 회색조 강도(intensity) BEV 이미지에 비해 AP가 크게 향상되었습니다 (예: Binary: AP 0.94 vs. Intensity + Palette: AP 19.51). 이는 BEV 이미지의 판별력이 SAM의 세그멘테이션 정확도에 크게 기여함을 보여줍니다.
* **후처리 모듈의 효과 (Ablation Study)**:
  * BEV 후처리(형태학적 팽창)가 없을 경우 AP는 약 8%, APH는 약 5% 감소하여 중요한 성능 기여를 확인했습니다.
  * 마스크 후처리(영역 및 종횡비 필터링) 또한 모두 필수적이며, 둘 중 하나라도 제거하면 성능이 크게 저하되었습니다.
* **정성적 결과 및 실패 사례**: SAM3D는 BEV 이미지에서 명확하게 구별되는 객체에 대해 합리적인 3D 바운딩 박스를 생성하지만, 객체가 너무 가깝거나(중복 마스크), 배경이 전경처럼 보이거나(오탐지), LiDAR 신호의 희소성/가려짐/절단으로 인해 객체가 부분적으로 활성화될 경우(미탐지) 실패 사례가 발생했습니다.
* **완전 지도 학습 모델과의 비교**: SAM3D는 완전 지도 학습 3D 감지기(예: CenterFormer의 AP 91.11, APH 90.58)에 비해 AP 19.51, APH 13.30으로 여전히 상당한 성능 차이를 보였습니다. 특히 APH 차이가 더 큰데, 이는 SAM이 방향 인식을 하지 못하고, 2D 마스크로부터 최소 회전 바운딩 박스를 추정하는 방식의 한계 때문입니다.

## 🧠 Insights & Discussion

* **시사점**: 본 연구는 2D 이미지로만 훈련된 SAM이 3D 주석 없이도 BEV 표현을 통해 제로샷 3D 객체 감지 작업에 활용될 수 있음을 성공적으로 입증했습니다. 이는 SAM과 같은 시각 기반 모델이 3D 비전 작업에서 가지는 엄청난 잠재력을 보여주는 초기 시도입니다.
* **한계 및 향후 연구 방향**:
  * **장면 표현의 한계**: BEV 이미지는 주로 실외 장면에 적합하므로, 실내 장면에 대한 일반화를 위해 더 나은 장면 표현 방법이 필요합니다.
  * **원거리 및 가려진 객체 성능**: LiDAR 신호의 희소성, 가려짐, 절단으로 인해 원거리 객체에 대한 미탐지(false negatives)가 많습니다. 다른 모달리티(예: 카메라 이미지)의 정보 통합이 도움이 될 수 있습니다.
  * **추론 속도**: 현재 2 FPS로 제한된 추론 속도를 개선하기 위해 모델 압축 및 증류(distillation) 기술이 필요합니다.
  * **다중 클래스 감지**: SAM은 의미론적 라벨 출력을 제공하지 않아 현재 다중 클래스 감지를 지원하지 않습니다. 3D 비전-언어 모델(예: CLIP Goes 3D, CrowdCLIP)을 활용하여 제로샷 분류를 도입할 수 있습니다.
  * **방향 인식 부족**: SAM은 방향 인식이 부족하여 APH 성능에 큰 영향을 미칩니다.
* **결론**: SAM3D는 기반 모델을 3D 작업에 활용하는 가능성과 기회를 제시합니다. 향후 퓨샷 학습, 모델 증류, 프롬프트 엔지니어링과 같은 기술을 통해 2D 및 3D 데이터 규모의 큰 차이에도 불구하고 3D 비전 작업을 더 효과적으로 해결할 수 있을 것으로 기대됩니다.

## 📌 TL;DR

**문제**: 2D 이미지로만 학습된 SAM의 제로샷 능력을 3D 객체 감지에 적용하는 것은 2D-3D 데이터 도메인 및 출력 형식의 큰 차이로 인해 도전적입니다. **방법**: SAM3D는 LiDAR 포인트 클라우드를 2D BEV 이미지로 변환하고, 형태학적 팽창으로 후처리하여 SAM의 입력으로 사용합니다. SAM이 생성한 마스크는 영역 및 종횡비 기반 필터링을 거쳐 노이즈를 제거한 후, 2D 마스크에서 수평 3D 속성을, LiDAR 포인트에서 수직 3D 속성을 추출하여 최종 3D 바운딩 박스를 예측합니다. **발견**: Waymo Open Dataset에서 SAM3D는 유망한 제로샷 3D 객체 감지 성능을 보였으며, BEV 표현 방식과 전처리/후처리 단계가 성능에 필수적임을 입증했습니다. 비록 완전 지도 학습 모델에는 미치지 못하지만, 2D 기반 모델이 3D 데이터에 대한 명시적인 훈련 없이 3D 작업에 활용될 수 있음을 성공적으로 보여주었습니다.
