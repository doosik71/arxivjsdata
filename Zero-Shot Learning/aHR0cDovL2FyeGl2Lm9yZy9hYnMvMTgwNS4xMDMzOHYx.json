{
  "title": "Zero-Shot Dual Machine Translation",
  "authors": "Lierni Sestorain, Massimiliano Ciaramita, Christian Buck, Thomas Hofmann",
  "year": 2018,
  "url": "http://arxiv.org/abs/1805.10338v1",
  "abstract": "Neural Machine Translation (NMT) systems rely on large amounts of parallel\ndata. This is a major challenge for low-resource languages. Building on recent\nwork on unsupervised and semi-supervised methods, we present an approach that\ncombines zero-shot and dual learning. The latter relies on reinforcement\nlearning, to exploit the duality of the machine translation task, and requires\nonly monolingual data for the target language pair. Experiments show that a\nzero-shot dual system, trained on English-French and English-Spanish,\noutperforms by large margins a standard NMT system in zero-shot translation\nperformance on Spanish-French (both directions). The zero-shot dual method\napproaches the performance, within 2.2 BLEU points, of a comparable supervised\nsetting. Our method can obtain improvements also on the setting where a small\namount of parallel data for the zero-shot language pair is available. Adding\nRussian, to extend our experiments to jointly modeling 6 zero-shot translation\ndirections, all directions improve between 4 and 15 BLEU points, again,\nreaching performance near that of the supervised setting.",
  "citation": 27
}