# Zero-Shot Text Classification with Self-Training

Ariel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz, Liat Ein-Dor, Noam Slonim

## 🧩 Problem to Solve

최근 사전 훈련된 대규모 언어 모델(Large Language Models, LLMs)은 제로샷 텍스트 분류에 대한 관심을 증가시켰습니다. 특히, 자연어 추론(Natural Language Inference, NLI) 데이터셋에 미세 조정된 모델들은 유망한 결과와 손쉬운 활용성 덕분에 널리 채택되고 있습니다. 그러나 이러한 모델들은 대상 작업에 직접 노출된 적이 없어 성능 불안정성 문제를 야기할 수 있습니다. 본 논문은 레이블링된 데이터, 도메인 전문 지식, 또는 시행착오 없이 클래스 이름과 레이블 없는 데이터셋만을 사용하여 이 간극을 메우는 간단한 자기 훈련(self-training) 방법을 제안합니다.

## ✨ Key Contributions

* **간단한 자기 훈련 방법 제안**: NLI 기반 제로샷 분류기를 대상 텍스트 분류 작업에 적용하기 위한 플러그 앤 플레이(plug-and-play) 방식의 자기 훈련 방법론을 제안합니다. 이는 레이블 없는 데이터셋과 클래스 이름만 요구합니다.
* **상당한 성능 향상 입증**: 가장 확신하는 예측으로 제로샷 분류기를 미세 조정하는 것이 광범위한 텍스트 분류 작업에서 상당한 성능 향상(평균 9.8~13.4% 정확도 향상)을 가져옴을 보여줍니다.
* **정보성 향상을 위한 토큰 마스킹**: 유사성 기반 토큰 마스킹(token masking) 휴리스틱을 도입하여 의사 레이블(pseudo-label)이 부여된 예시의 정보성을 높이고 모델 학습을 더 효과적으로 만듭니다.
* **음성 예시 선택 전략 분석**: 자기 훈련 시 음성 예시를 생성하는 다양한 전략(`Contrast-random`, `Contrast-closest`, `Contrast-furthest`, `Contrast-all`)을 비교하고, `Contrast-random`이 계산 효율성과 성능 간에 좋은 균형을 제공함을 발견합니다.
* **교차 작업 효과 연구**: 자기 훈련의 교차 작업(cross-task) 효과를 분석하여, 자기 훈련이 관련 작업에서는 도움이 되지만, 다른 특성을 가진 작업에서는 성능 저하를 일으킬 수 있음을 보여줍니다.
* **최초의 연구**: 일반적인 제로샷 모델 맥락에서 자기 훈련을 탐구하는 첫 번째 연구입니다.

## 📎 Related Works

* **NLI 기반 제로샷 분류**: Yin et al. (2019)은 텍스트 분류 작업을 텍스트 함의(textual entailment) 문제로 재구성하여 NLI 훈련 모델을 제로샷 분류기로 활용하는 방법을 제안했습니다.
* **자기 훈련(Self-training)**: Scudder (1965)에 의해 시작된 반지도 학습(semi-supervised learning) 방법으로, 모델의 자체 예측을 의사 레이블로 사용하여 훈련 데이터를 확장합니다. Triguero et al. (2015)은 다양한 변형을 제시했습니다.
* **통합 작업 형식(Unified Task Formats)**: Wei et al. (2022), Zhong et al. (2021), Bragg et al. (2021), Sanh et al. (2022) 등은 다양한 NLP 작업을 통합된 형식으로 매핑하여 "메타 튜닝(meta-tuning)"을 통해 보이지 않는 작업에 대한 제로샷 성능을 달성하는 방법을 탐구했습니다.
* **일관성 훈련(Consistency Training)**: Zhou et al. (2022)은 프롬프트 기반 제로샷 학습자(T0)를 개선하기 위해 레이블 없는 텍스트에 프롬프트 일관성 손실(prompt consistency loss)을 적용하여 모델이 유사한 프롬프트에 대해 일관된 예측을 하도록 훈련했습니다.
* **엔트로피 최소화(Entropy Minimization)**: Lee et al. (2013), Grandvalet and Bengio (2004)는 자기 훈련이 엔트로피 최소화와 유사하게 모델의 결정 경계를 수정하여 더 자신감 있는 예측을 유도한다고 설명했습니다.

## 🛠️ Methodology

1. **NLI 기반 제로샷 분류 모델 초기화**:
    * MNLI 데이터셋에 훈련된 `roberta-large-mnli`, `deberta-large-mnli-zero-cls`, `bart-large-mnli`와 같은 기성(off-the-shelf) NLI 모델 $M$을 사용합니다.
    * 입력 텍스트 $t$를 전제(premise)로 사용하고, 각 클래스 $c$에 대해 "This example is $c$"와 같은 템플릿으로 가설(hypothesis)을 구성합니다.
    * 모델은 텍스트 $t$에 의해 가장 강하게 함의(entail)되는 가설의 클래스를 예측합니다.
2. **의사 레이블링된 예시 선택 (Selecting Pseudo-labeled Examples)**:
    * 레이블 없는 데이터셋 $U$에서 자기 훈련에 사용할 예시를 선택합니다.
    * **긍정 예시 선택**:
        * 각 $u \in U$와 $c \in C$에 대해 함의 점수(confidence score) $S_{uc}$를 얻습니다.
        * `Best-versus-Second-Best` 접근 방식을 사용하여 가장 확신하는 예측을 선택합니다. 즉, $c$가 가장 높은 함의 점수를 얻었을 때, $c$의 점수와 두 번째로 높은 클래스 $c'$의 점수 차이 $\delta_{uc} = S_{uc} - S_{uc'}$가 가장 큰 예시들을 우선으로 선택합니다.
        * 각 클래스에 대해 $\delta_{uc}$를 기준으로 상위 $n$개의 예시를 긍정 예시로 선택합니다. 이들은 `entail` 의사 레이블이 할당된 (전제 $u$, 가설 "This example is $c$") 쌍이 됩니다.
    * **음성 예시 선택**: `contradict` 의사 레이블을 가진 예시를 생성하기 위해 네 가지 접근법을 탐색합니다.
        * `Contrast-random`: `entail` 쌍의 클래스 $c$를 무작위로 다른 클래스로 대체하여 `contradict` 쌍을 만듭니다.
        * `Contrast-closest`: `entail` 쌍의 클래스 $c$를 두 번째로 높은 함의 점수를 받은 클래스 $c'$로 대체하여 `contradict` 쌍을 만듭니다.
        * `Contrast-furthest`: `entail` 쌍의 클래스 $c$를 가장 낮은 함의 점수를 받은 클래스 $c'$로 대체하여 `contradict` 쌍을 만듭니다.
        * `Contrast-all`: `entail` 쌍의 클래스 $c$를 제외한 모든 $C-1$개의 다른 클래스 각각에 대해 `contradict` 쌍을 만듭니다.
3. **토큰 마스킹 (Token Masking) 휴리스틱**:
    * 선택된 예시의 텍스트에서 해당 예시에 할당된 의사 레이블 클래스 이름과 GloVe(Pennington et al., 2014) 유사도가 가장 높은 토큰을 `<UNK>` 토큰으로 마스킹합니다.
    * 이는 모델이 클래스 이름과 직접적으로 관련된 단어에만 의존하는 것을 방지하고, 더 도전적인 학습을 통해 문맥적 신호에 더 의존하도록 유도합니다.
4. **모델 미세 조정**:
    * 생성된 `entail` 및 `contradict` 의사 레이블 예시들을 사용하여 NLI 모델 $M$을 1 epoch 동안 미세 조정합니다. 이 과정은 총 2회 반복됩니다.
    * 학습률 $2 \times 10^{-5}$, 배치 크기 32, AdamW 옵티마이저, 교차 엔트로피 손실을 사용합니다.

## 📊 Results

* **전반적인 성능 향상**: 모든 모델(BART, DeBERTa, RoBERTa)과 8개 데이터셋에 걸쳐 자기 훈련을 통해 제로샷 분류 정확도가 일관되고 유의미하게 향상되었습니다. 평균적으로 약 9.8~13.4%의 정확도 향상을 보였습니다.
* **음성 예시 선택 전략**:
  * `Contrast-furthest`는 모델에 너무 쉬운 음성 예시를 제공하여 가장 낮은 성능을 보였습니다.
  * `Contrast-closest`는 더 어려운 예시를 제공하여 더 나은 결과를 보였지만, 노이즈가 클 가능성도 있었습니다.
  * `Contrast-all`과 `Contrast-random`이 정보성과 노이즈 수준 사이에서 가장 좋은 균형을 이루며 최고의 성능을 보였습니다. 계산 비용을 고려할 때 `Contrast-random`이 선호되었습니다.
* **토큰 마스킹의 효과**: 토큰 마스킹을 적용하는 것이 전반적인 성능 향상(p=$3 \times 10^{-4}$)에 기여했습니다. 특히 RoBERTa 모델과 ISEAR 데이터셋에서 효과가 두드러졌습니다. 이는 선택된 예시의 정보성을 높이는 데 도움이 됩니다.
* **교차 작업 효과**: 자기 훈련의 교차 작업 효과는 이질적이었습니다.
  * 주제별 데이터셋(20 newsgroup, AG’s news 등) 간에는 상호 유익한 효과가 관찰되었습니다.
  * 감정 분류 데이터셋(GoEmotions, ISEAR) 간에도 유사한 긍정적 효과가 있었습니다.
  * 반면, 감성 데이터(Amazon, IMDB)로 자기 훈련하는 것은 감정 데이터셋에 대한 성능을 상당히 저하시켰는데, 이는 특정 도메인 특성과 감정의 미묘한 차이와 대비되는 이진 감성 분류의 차이 때문으로 추정됩니다.
* **레이블 없는 데이터의 양**: 10,000개 정도의 레이블 없는 예시만으로도 자기 훈련의 이점을 얻기에 충분했습니다.

## 🧠 Insights & Discussion

* **실용적 가치**: 본 논문에서 제안하는 자기 훈련 방법은 기존 NLI 기반 제로샷 분류 모델의 성능을 향상시키는 간단하고 실용적인 방법을 제공하며, 이는 레이블 없는 소량의 도메인 데이터만으로도 큰 도움이 될 수 있습니다. 이는 실무자들이 일반 모델을 특정 다운스트림 작업에 맞게 조정하는 데 유용합니다.
* **확장 가능성**: 자기 훈련을 일반적인 제로샷 모델에 적용하는 개념은 NLI 모델이나 분류 작업에만 국한되지 않으며, 다른 유형의 매핑 함수나 "메타 작업(meta-tasks)"을 사용하는 모델에도 적용될 수 있습니다.
* **기존 접근 방식과의 차별점**: 스크래치부터 제로샷 분류기를 구축하는 다른 접근 방식(더 많은 레이블 없는 데이터 필요)과 달리, 이 방법은 기존의 일반 목적 제로샷 분류기에 내재된 지식을 활용하고 그 위에 구축됩니다.
* **미래 연구 방향**:
  * 다양한 매핑 함수 또는 메타 작업을 사용하는 모델에 대한 자기 훈련 적용.
  * 스크래치부터 분류기를 구축하는 반복적인 제로샷 접근 방식과 본 방법의 결합.
  * 교차 작업 적응에 자기 훈련이 유용한 정확한 조건 연구.
  * 다중 데이터셋에 대한 자기 훈련 효과 탐색.
  * 더 다양한 또는 정보성 있는 예시 선택을 위한 대체 전략(예: 엔트로피 기반) 탐구.
  * 다중 레이블 분류 시나리오에 대한 방법론 적용.
* **한계점**:
  * **높은 계산 비용**: 대규모 모델을 사용하므로 10,000개의 레이블 없는 샘플에 대한 추론에도 GPU가 필요하여 실용적인 접근성이 제한될 수 있습니다.
  * **경험적 연구**: 이론적 보장 없이 실험 결과를 보고하므로 예외가 존재할 수 있습니다.
  * **데이터셋의 한계**: 비교적 표준적인 학술 벤치마크에 대한 결과이며, 법률, 의료 등 특정 도메인의 실제 데이터셋에서는 추가적인 도전 과제가 있을 수 있습니다.
  * **단일 레이블 분류**: 각 예시가 정확히 하나의 클래스에 할당되는 시나리오에 초점을 맞추고 있어, 다중 레이블 분류에는 직접적인 적용이 어려울 수 있습니다.
  * **재현성 및 모델 가용성**: 대규모 실험은 재현에 상당한 부담을 주며, 사용된 기성 모델의 향후 공개 호스팅이 보장되지 않습니다.

## 📌 TL;DR

* **문제**: NLI 기반 제로샷 텍스트 분류기는 대상 작업에 대한 직접적인 노출 부족으로 인해 성능이 불안정합니다.
* **방법**: 본 논문은 레이블 없는 데이터셋과 클래스 이름만을 활용하여 제로샷 분류기를 자기 훈련하는 간단한 방법을 제안합니다. 모델이 가장 확신하는 예측을 의사 레이블로 사용하여 모델을 미세 조정하며, 클래스 이름과 가장 유사한 토큰을 마스킹하여 학습 데이터의 정보성을 높이는 휴리스틱을 사용합니다. 음성 예시는 `Contrast-random` 전략을 통해 효과적으로 생성됩니다.
* **발견**: 이 자기 훈련 접근 방식은 다양한 텍스트 분류 작업에서 기존 제로샷 분류기의 정확도를 평균 9.8~13.4%까지 크게 향상시켰습니다. 토큰 마스킹은 성능 향상에 긍정적인 영향을 미쳤으며, 자기 훈련의 교차 작업 효과는 관련 작업 간에는 도움이 되지만, 특성이 다른 작업 간에는 성능을 저하시킬 수 있음을 보여주었습니다.
