{
  "title": "Rethinking Zero-Shot Learning: A Conditional Visual Classification\n  Perspective",
  "authors": "Kai Li, Martin Renqiang Min, Yun Fu",
  "year": 2019,
  "url": "http://arxiv.org/abs/1909.05995v2",
  "abstract": "Zero-shot learning (ZSL) aims to recognize instances of unseen classes solely\nbased on the semantic descriptions of the classes. Existing algorithms usually\nformulate it as a semantic-visual correspondence problem, by learning mappings\nfrom one feature space to the other. Despite being reasonable, previous\napproaches essentially discard the highly precious discriminative power of\nvisual features in an implicit way, and thus produce undesirable results. We\ninstead reformulate ZSL as a conditioned visual classification problem, i.e.,\nclassifying visual features based on the classifiers learned from the semantic\ndescriptions. With this reformulation, we develop algorithms targeting various\nZSL settings: For the conventional setting, we propose to train a deep neural\nnetwork that directly generates visual feature classifiers from the semantic\nattributes with an episode-based training scheme; For the generalized setting,\nwe concatenate the learned highly discriminative classifiers for seen classes\nand the generated classifiers for unseen classes to classify visual features of\nall classes; For the transductive setting, we exploit unlabeled data to\neffectively calibrate the classifier generator using a novel\nlearning-without-forgetting self-training mechanism and guide the process by a\nrobust generalized cross-entropy loss. Extensive experiments show that our\nproposed algorithms significantly outperform state-of-the-art methods by large\nmargins on most benchmark datasets in all the ZSL settings. Our code is\navailable at \\url{https://github.com/kailigo/cvcZSL}",
  "citation": 175
}