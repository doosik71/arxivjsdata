# CLIP model is an Efficient Continual Learner

Vishal Thengane, Salman Khan, Munawar Hayat, Fahad Khan

## 🧩 Problem to Solve

기존의 계속 학습(Continual Learning, CL) 방법론은 새로운 작업을 학습하면서 이전 작업 지식을 잊어버리는 치명적인 망각(catastrophic forgetting) 문제를 해결하기 위해 메모리 리플레이, 지식 증류, 모델 정규화, 동적 네트워크 확장 등 복잡한 기술을 사용합니다. 이러한 방법들은 각 학습 단계마다 재훈련 비용, 전용 메모리 요구 사항, 그리고 특정 설정에 맞는 설계 선택을 필요로 하여 실용성이 제한적입니다. 본 연구는 이러한 복잡한 접근 방식 대신, 학습 및 미세 조정(fine-tuning) 없이도 모든 계속 학습 시나리오에서 효과적인 범용적인 방법을 찾고자 합니다.

## ✨ Key Contributions

* **고정된 CLIP 모델의 파격적인 성능 입증**: 미세 조정 없이 고정된 CLIP(Contrastive Language-Image Pretraining) 모델이 다양한 계속 학습 시나리오(클래스-증분, 도메인-증분, 작업-불가지론적 학습)에서 최신 계속 학습 방법론보다 우수하거나 경쟁력 있는 성능을 달성함을 최초로 입증했습니다.
* **간소화된 계속 학습 접근법 제시**: 재훈련, 전용 메모리 버퍼, 복잡한 하이퍼파라미터 튜닝, 동적 모델 확장 또는 분류 헤드 변경 없이도 계속 학습이 가능함을 보여주었습니다.
* **CL 분야의 새로운 기준선 제안**: CLIP의 제로샷 전이 능력(zero-shot transfer capability)을 활용하여, 향후 계속 학습 연구의 강력하고 단순한 기준선(baseline)을 제시하며, 단편화된 CL 연구 노력을 통합하는 데 기여했습니다.
* **프롬프트 엔지니어링의 영향 분석**: 텍스트 입력에 사용되는 간단한 프롬프트 템플릿이 CLIP 모델의 계속 학습 성능에 미치는 영향을 분석했습니다.

## 📎 Related Works

* **계속 학습(Continual Learning)**:
  * **모델 정규화(Model Regularization)**: 이전 작업에 중요한 파라미터의 유연성을 제한하여 치명적인 망각을 방지합니다 (예: EWC, LwF).
  * **메모리 리플레이(Memory Replay)**: 이전 작업의 샘플을 메모리에 저장하거나 생성하여 재활용합니다 (예: iCaRL, DER).
  * **동적 네트워크 확장(Dynamic Network Expansion)**: 새로운 작업에 맞춰 네트워크 구조를 동적으로 확장하거나 서브 네트워크로 분할합니다 (예: DyTox, PackNet).
* **비전-언어 모델(Vision-language Models)**:
  * 텍스트와 이미지 데이터 간의 상호작용을 가능하게 하여 제로샷 학습, 시각적 접지(visual grounding) 등의 문제를 해결합니다.
  * **CLIP(Contrastive Language-Image Pre-training)**: 이미지와 텍스트를 전용 인코더로 처리하고, 대조 학습(contrastive learning) 목표를 통해 두 모달리티를 공유 임베딩 공간에 정렬시키는 듀얼 스트림 모델입니다. 대규모 데이터셋으로 사전 훈련되어 뛰어난 제로샷 전이 능력을 보여줍니다.
  * **프롬프트 기반 미세 조정(Prompt-based Fine-tuning)**: CLIP과 같은 대규모 사전 훈련 모델을 작은 학습 가능한 프롬프트(prompt)를 통해 새로운 작업에 빠르게 적응시키는 접근 방식이 있지만, 본 연구는 학습 가능한 파라미터 없이 순수한 제로샷 접근 방식을 사용합니다.

## 🛠️ Methodology

본 연구에서는 **사전 훈련된 고정(frozen) CLIP 모델**을 **제로샷(zero-shot) 방식**으로 다양한 계속 학습 시나리오에 적용하는 `Continual-CLIP` 접근법을 제안합니다.

1. **CLIP 모델 개요**:
    * CLIP은 이미지 인코더 ($E_{\text{visual}}$)와 텍스트 인코더 ($E_{\text{text}}$)로 구성됩니다.
    * 이미지 인코더는 이미지($x_{\text{test}}$)를 이미지 임베딩 벡터($v$)로, 텍스트 인코더는 텍스트 입력({$p; y_i$})을 텍스트 임베딩 벡터($t_i$)로 변환합니다.
    * 이 두 인코더는 대규모 이미지-캡션 쌍 데이터셋을 통해 대조 학습 방식으로 훈련되어, 일치하는 이미지-텍스트 쌍의 코사인 유사도를 최대화하고 불일치하는 쌍의 유사도를 최소화하도록 학습됩니다.

2. **제로샷 계속 학습 적용**:
    * **모델 고정**: CLIP 모델($F=\{E_{\text{visual}}, E_{\text{text}}\}$)의 모든 가중치를 고정하여 어떠한 훈련이나 미세 조정도 수행하지 않습니다.
    * **텍스트 프롬프트 생성**: 분류하려는 각 클래스 레이블($y_i$)에 대해 "a photo of a {category}"와 같은 사전 정의된 프롬프트 템플릿($p$)을 사용하여 텍스트 입력을 생성합니다. 예를 들어, '개' 클래스에 대해서는 "a photo of a dog"가 됩니다.
    * **임베딩 추출**:
        * 테스트 이미지($x_{\text{test}}$)를 이미지 인코더에 입력하여 이미지 임베딩($v = E_{\text{visual}}(x_{\text{test}})$)을 얻습니다.
        * 생성된 각 클래스별 텍스트 입력($\{p; y_i\}$)을 텍스트 인코더에 입력하여 텍스트 임베딩($t_i = E_{\text{text}}(\{p; y_i\})$)을 얻습니다.
    * **유사도 기반 분류**: 이미지 임베딩 $v$와 각 텍스트 임베딩 $t_i$ 간의 코사인 유사도($\text{sim}(t_i \cdot v)$)를 계산합니다. 예측 확률은 소프트맥스 함수를 통해 얻습니다:
        $$p(y_i|x_{\text{test}}) = \frac{\exp(\text{sim}(t_i \cdot v))}{\sum_{k=1}^{K}\exp(\text{sim}(t_k \cdot v))}$$
    * **계속 학습 시나리오 평가**: 모델은 현재 작업 $t$를 학습하고, 지금까지 관찰된 모든 작업에 대해 평가됩니다. CIFAR100 및 TinyImageNet과 같은 저해상도 이미지에는 "a bad photo of a {category}" 프롬프트를, ImageNet-100/1K와 같은 고해상도 이미지에는 "a good photo of a {category}" 프롬프트를 사용했습니다.

## 📊 Results

본 연구는 다양한 벤치마크 데이터셋(CIFAR-100, ImageNet-100/1K, TinyImageNet, CORe50, CLEAR-10/100, Gaussian scheduled CIFAR-100)과 13가지 학습 작업 구성에서 Continual-CLIP의 성능을 평가했습니다.

* **클래스-증분 학습(Class-incremental Setting)**:
  * **CIFAR-100**: 10, 20, 50단계 설정에서 Continual-CLIP은 평균 정확도와 최종 정확도 모두에서 DyTox 및 DER과 같은 최신 SOTA 방법과 비교하여 매우 경쟁력 있거나 더 우수한 성능을 달성했습니다. 특히, 20단계에서 75.95%, 50단계에서 76.49%의 평균 정확도를 기록했으며, 최종 정확도는 모든 경우에 66.72%로 유지되었습니다.
  * **ImageNet-100 & 1K**: 표준 설정(ImageNet-100-B0)에서 평균 정확도는 두 번째로 좋은 방법보다 7.84%p 높았고, ImageNet-1K에서는 4.63%p 높았습니다. 최종 정확도에서도 각각 7.71%p, 7.70%p의 상당한 개선을 보였습니다.
  * **TinyImageNet**: 100개의 기본 클래스가 있는 TinyImageNet 데이터셋에서 Continual-CLIP은 모든 3가지 설정(5, 10, 20단계)에서 평균 정확도에서 두 번째로 좋은 DyTox보다 평균 20.30%p 더 높은 성능을 보였습니다.

* **도메인-증분 학습(Domain-incremental Setting)**:
  * **CORe50**: Continual-CLIP은 84.73%의 테스트 정확도를 달성하여 GDM(74.87%) 및 Cumulative(65.15%)와 같은 기존 방법들을 크게 능가했습니다.
  * **CLEAR-10 & 100**: CVPR 2022 CLEAR 챌린지 우승팀과 비교했을 때, Continual-CLIP은 CLEAR-10에서 93.79%, CLEAR-100에서 93.63%의 전반적인 정확도를 기록하며 경쟁력 있는 성능을 보여주었습니다.

* **작업-불가지론적 학습(Task-agnostic Setting)**:
  * **Gaussian scheduled CIFAR-100**: Continual-CLIP은 기존 최상위 방법인 Encoders and Ensemble(39.0%)을 크게 능가하는 66.72%의 테스트 정확도를 달성했습니다.

* **텍스트 프롬프트 분석**:
  * **클래스 이름의 영향**: ImageNet-1K 데이터셋에서 Radford et al. (2021)이 큐레이팅한 클래스 이름이 ImageNet 원본 레이블이나 WordNet의 첫 번째 동의어보다 더 나은 성능(평균 정확도 74.81%)을 보였습니다. 이는 큐레이팅된 이름이 클래스 간 명확한 구분을 제공하기 때문입니다.
  * **프롬프트 엔지니어링 기술**: ImageNet-100에서 Decision-based pooling(82.24%)과 Embedding pooling(84.85%)은 단일 프롬프트보다 더 높은 정확도를 보여주었지만, 추가 계산과 도메인 전문가 지식을 요구합니다.

## 🧠 Insights & Discussion

* **단순함의 힘**: 본 연구는 복잡한 재훈련, 메모리 버퍼, 모델 확장 없이 고정된 CLIP 모델이 계속 학습 시나리오에서 SOTA 성능을 능가할 수 있음을 보여주며, 계속 학습 연구의 패러다임을 바꿀 수 있는 강력하고 단순한 기준선을 제시합니다.
* **CLIP의 일반화 능력**: 대규모 사전 훈련을 통해 얻은 CLIP의 일반화된 비전-언어 표현이 데이터 분포 변화에 강건하게 대응하며, 치명적인 망각 문제를 효과적으로 완화한다는 것을 입증합니다.
* **분절된 CL 연구 통합**: 특정 설정에 맞춰 개발된 기존 방법론들과 달리, Continual-CLIP은 클래스-증분, 도메인-증분, 작업-불가지론적 학습 등 다양한 시나리오에 범용적으로 적용 가능하여, 계속 학습 연구의 단편화를 해소하는 데 기여합니다.
* **제한 사항 및 향후 연구**:
  * **정보 유출 가능성**: CLIP의 사전 훈련 과정에서 평가에 사용된 일부 클래스를 이미 접했을 가능성이 있습니다.
  * **의미론적 유사성으로 인한 혼란**: CIFAR-100 혼동 행렬에서 볼 수 있듯이, 모델이 의미론적으로 유사한 클래스(예: 'mouse'와 'shrew')를 혼동하는 경향이 있습니다.
  * **향후 연구**: CLIP의 제로샷 전이 능력 위에 빠른 적응 방법론을 추가하여 계속 학습 성능을 더욱 향상시키는 방향으로 연구를 확장할 수 있습니다. 이는 비전-언어 기반 모델을 활용한 계속 학습 연구의 새로운 지평을 열 것입니다.

## 📌 TL;DR

기존 계속 학습(Continual Learning) 방법론의 복잡성(재훈련, 메모리, 특정 설정 의존)을 극복하고자, 본 논문은 **사전 훈련된 고정된 CLIP 모델**이 미세 조정 없이도 (제로샷) 다양한 계속 학습 시나리오(클래스-증분, 도메인-증분, 작업-불가지론적)에서 최신 기술을 능가하는 놀라운 성능을 보여준다는 것을 입증합니다. 이는 CLIP의 일반화된 비전-언어 표현 능력 덕분이며, 재훈련, 메모리, 복잡한 하이퍼파라미터 튜닝 없이 작동하는 **강력하고 단순한 계속 학습 기준선**으로서 CLIP의 활용을 제안합니다.
