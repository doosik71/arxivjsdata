{
  "title": "ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback",
  "authors": "Jiacheng Ye, Jiahui Gao, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong",
  "year": 2022,
  "url": "http://arxiv.org/abs/2210.12329v1",
  "abstract": "Recently, dataset-generation-based zero-shot learning has shown promising\nresults by training a task-specific model with a dataset synthesized from large\npre-trained language models (PLMs). The final task-specific model often\nachieves compatible or even better performance than PLMs under the zero-shot\nsetting, with orders of magnitude fewer parameters. However, synthetic datasets\nhave their drawbacks. They have long been suffering from low-quality issues\n(e.g., low informativeness and redundancy). This explains why the massive\nsynthetic data does not lead to better performance -- a scenario we would\nexpect in the human-labeled data. To improve the quality of dataset synthesis,\nwe propose a progressive zero-shot dataset generation framework, ProGen, which\nleverages the feedback from the task-specific model to guide the generation of\nnew training data via in-context examples. Extensive experiments on five text\nclassification datasets demonstrate the effectiveness of the proposed approach.\nWe also show ProGen achieves on-par or superior performance with only 1\\%\nsynthetic dataset size compared to baseline methods without in-context\nfeedback.",
  "citation": 85
}