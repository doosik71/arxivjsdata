# Attribute-Guided Network for Cross-Modal Zero-Shot Hashing

Zhong Ji, Yuxin Sun, Yunlong Yu, Yanwei Pang, Jungong Han

## 🧩 Problem to Solve

이 논문은 **크로스모달 제로샷 해싱 (Cross-Modal Zero-Shot Hashing, CMZSH)**이라는 새로운 문제를 해결하고자 합니다. 기존의 제로샷 해싱(Zero-Shot Hashing, ZSH) 연구는 주로 이미지-기반 이미지 검색(Image-Based Image Retrieval, IBIR)과 같은 단일 모달리티 검색에 초점을 맞추었습니다. 하지만 실제 애플리케이션에서는 텍스트로 이미지를 검색하는 텍스트-기반 이미지 검색(Text-Based Image Retrieval, TBIR)과 같은 크로스모달 검색이 더욱 일반적입니다.

CMZSH를 달성하기 위한 주요 과제는 다음과 같습니다:

1. **모달리티 이질성 (Modality heterogeneity)**: 이미지와 텍스트 같은 다른 모달리티 간의 의미론적 관계를 해시 코드에 보존해야 합니다.
2. **카테고리 마이그레이션 (Category migration)**: 학습 시 보지 못한(unseen) 카테고리의 인스턴스를 처리할 수 있는 제로샷 학습(ZSL) 능력이 필요합니다.
3. **의미론적 유사성 보존 (Semantic similarity preservation)**: 고차원 특징의 의미론적 유사성을 저차원 이진 해시 코드에서도 유지해야 합니다.

## ✨ Key Contributions

- **CMZSH 문제 해결**: 제로샷 환경에서 크로스모달 검색(CMZSH) 문제를 다루는 최초의 딥 해싱 신경망인 AgNet을 제안합니다. 이는 IBIR뿐만 아니라 TBIR도 수행할 수 있습니다.
- **AgNet 프레임워크 개발**: 클래스 수준 속성 정보를 활용하여 Attribute-Guided Network (AgNet) 프레임워크를 제안합니다. 이 프레임워크는 두 가지 다른 모달리티(시각, 텍스트)를 공통 속성 공간으로 매핑하여, 보지 못한 카테고리와 보아온 카테고리, 그리고 시각 및 텍스트 모달리티 사이의 간극을 연결하는 허브 역할을 합니다.
- **효과적인 해시 코드 생성 전략**: 동일한 네트워크 내에서 속성 정보를 활용하여 이미지와 텍스트에 대한 개별 해시 코드를 생성하는 효과적인 전략을 설계합니다. 이 전략은 카테고리 유사성 및 속성 유사성을 보존함으로써 해시 코드 생성을 유도합니다.
- **우수한 성능 입증**: 세 가지 벤치마크 데이터셋(AwA, SUN, ImageNet)에서 IBIR 및 TBIR 작업을 모두 포함한 광범위한 실험을 통해 제안된 AgNet이 경쟁력 있는 성능을 달성함을 입증합니다.

## 📎 Related Works

- **크로스모달 해싱 (Cross-Modal Hashing, CMH)**:
  - **비지도 방식**: Collective Matrix Factorization Hashing (CMFH) [16], 다중 모달리티의 융합 유사성을 이용한 CMH [17] 등이 있습니다.
  - **지도 방식**: Semantic Correlation Maximization Hashing (SCMH) [18], KL-발산 최소화를 통한 해시 코드 생성 [19], Graph-regularized Supervised Matrix Factorization Hashing (SMFH) [20] 등이 있습니다. 딥러닝 기반으로는 DCMH [21] 및 Pairwise Relationship Guided Deep Hashing (PRDH) [22]가 있습니다. 본 논문의 CMZSH는 지도 CMH의 아이디어를 따르지만, 보지 못한 카테고리를 처리해야 하는 추가적인 제로샷 문제를 해결해야 합니다.
- **제로샷 해싱 (Zero-Shot Hashing, ZSH)**:
  - **선구 연구**: Transferring Supervised Knowledge (TSK) [12]는 word2vec [23]을 사용하여 의미론적 임베딩 공간을 통해 지식을 이전했습니다.
  - **속성 정보 활용**: Xu et al. [24]은 의미론적으로 풍부한 속성 정보를 전이 가능한 지식으로 채택했습니다.
  - **딥 ZSH**: Similarity Transfer Network (SitNet) [13]은 멀티태스크 아키텍처를 사용하여 시각적 개념과 의미론적 표현의 감독 지식을 동시에 활용하며, 중심 정규화 손실을 통해 인트라-개념 유사성과 인터-개념 거리를 보존합니다.
  - **단점**: 기존 ZSH 방법들은 주로 IBIR 작업에만 초점을 맞춰 텍스트를 해시 코드로 인코딩할 수 없어 TBIR에는 적용하기 어렵습니다.

## 🛠️ Methodology

제안된 AgNet 프레임워크는 세 가지 주요 구성 요소로 이루어져 있습니다: V2A Net, T2A Net, A2H Net.

1. **문제 정의**: 학습 데이터셋 $D_{tr} = \{d_i = (x_i, z_i, y_i), i=1,...,N\}$는 $s$개의 보아온 카테고리 $S$에서 제공됩니다. 여기서 $x_i \in \mathbb{R}^l$는 시각적 표현, $z_i \in \mathbb{R}^k$는 텍스트 의미론적 표현, $y_i \in \{0,1\}^s$는 원-핫 인코딩된 레이블 벡터입니다. 각 인스턴스는 이진 속성 벡터 $a_i \in \{0,1\}^d$로 주석이 달려 있습니다. 목표는 보지 못한 카테고리 $U$에 대해 해시 함수를 학습하는 것입니다. 이 논문에서는 속성을 매개 공간으로 활용합니다.

2. **네트워크 아키텍처 (AgNet)**:

   - **V2A Net (Visual to Attribute Network)**: 파인튜닝된 GoogleNet [32]의 시각적 특징을 입력으로 받아 3개의 완전 연결(FC) 레이어로 구성된 딥 신경망을 통해 속성 벡터 $\hat{a}_i^{(v)} \in \mathbb{R}^d$로 임베딩합니다.
   - **T2A Net (Text to Attribute Network)**: word2vec 모델 [23]로 표현된 텍스트 입력(카테고리 이름)을 받아 2개의 FC 레이어로 구성된 신경망을 통해 속성 벡터 $\hat{a}_i^{(t)} \in \mathbb{R}^d$로 임베딩합니다.
   - **A2H Net (Attribute to Hash Network)**: V2A Net과 T2A Net에서 예측된 속성 벡터를 입력으로 받아, 단일 3-레이어 신경망을 통해 시각적 해시 코드 $P^{*i} \in \mathbb{R}^c$와 텍스트 해시 코드 $Q^{*i} \in \mathbb{R}^c$를 모두 생성합니다. (Table II에 상세 구성)

3. **목표 함수**:

   - **속성 예측 손실 ($L_{att}$)**: V2A Net 및 T2A Net을 훈련시키기 위한 교차 엔트로피 손실 함수입니다. 예측된 속성 벡터 $\hat{a}_i$가 실제 속성 벡터 $a_i$의 분포에 가깝도록 합니다.
     $$L_{att} = -\frac{1}{N}\sum_{i=1}^{N} [a_i^T \log (\hat{a}_i) + (1-a_i)^T \log(1-\hat{a}_i)]$$
   - **카테고리 유사성 손실 ($L_{cs}$)**: 동일한 카테고리 내에서 다른 모달리티의 속성 벡터가 유사한 해시 코드를 생성하도록 합니다.
     $$\Theta_{ij} = \frac{1}{2} P_i^{*T} Q_j^{*}$$
     $$L_{cs} = -\sum_{i,j=1}^{N} \left(S_{ij}^{(c)}\Theta_{ij} - \log(1+e^{\Theta_{ij}})\right)$$
     여기서 $S_{ij}^{(c)}=1$은 $y_i=y_j$일 때, 아니면 $0$입니다.
   - **속성 유사성 손실 ($L_{as}$)**: 단일 모달리티 내에서 해시 코드가 더 잘 구별되도록 속성 유사성 $S_{ij}^{(att)}$를 사용하여 시각적 해시 코드 생성을 안내합니다.
     $$\phi_{ij} = \frac{1}{2} P_i^{*T} P_j^{*}$$
     $$S_{ij}^{(att)} = \cos(a_i, a_j) - S_{ij}^{(c)}$$
     $$L_{as} = \sum_{i,j=1}^{N} \sigma(\phi_{ij} S_{ij}^{(att)})$$
     여기서 $\sigma(\bullet)$는 시그모이드 함수입니다.
   - **정규화 손실 ($L_{reg}$)**: $P$를 이진 해시 코드 $B$에 가깝게 만들고, 각 해시 코드 비트가 균형을 이루도록 합니다.
     $$L_{reg} = \|B-P\|_F^2 + \|P^T 1\|_F^2$$
     여기서 $B = \text{sign}(P)$입니다.
   - **전체 A2H Net 목표 함수 ($L_{A2H}$)**: 위의 손실들을 결합합니다.
     $$L_{A2H} = L_{cs} + \lambda L_{as} + \eta L_{reg}$$
     여기서 $\lambda$와 $\eta$는 트레이드오프 파라미터입니다.

4. **최적화**: AgNet은 두 단계로 훈련됩니다. 먼저, V2A Net과 T2A Net은 교차 엔트로피 함수로 개별적으로 학습됩니다. 그 다음, 두 모달리티에서 예측된 속성 벡터를 사용하여 A2H Net을 Eq. (5)에 따라 훈련합니다. 역전파 알고리즘과 미니 배치 확률적 경사 하강법(SGD)을 사용하여 최적화됩니다.

## 📊 Results

세 가지 벤치마크 데이터셋(AwA, SUN, ImageNet)에서 AgNet의 성능을 평가했습니다.

- **크로스모달 제로샷 해싱 (CMZSH) 결과 (TBIR)**:

  - AgNet은 모든 데이터셋에서 SCMH, SMFH, DCMH, FSH와 같은 최신 CMH 방법들보다 일관되게 우수한 성능을 달성했습니다. 특히 AwA 데이터셋에서 48비트 해시 코드 사용 시 58.1%의 mAP를 기록하며, 차순위 SMFH보다 35.2%p 높은 성능을 보였습니다.
  - 대부분의 경우 해시 코드 길이가 길어질수록 mAP 성능이 향상되었습니다.
  - SUN 데이터셋에서의 성능은 AwA보다 낮았는데, 이는 SUN이 세밀한(fine-grained) 데이터셋으로 카테고리 내 다양성이 적어 해시 코드의 식별력이 떨어지기 때문으로 분석되었습니다.

- **단일 모달 제로샷 해싱 (IBIR) 결과**:

  - AgNet은 AwA 및 ImageNet 데이터셋에서 IMH, ITQ, SDH, TSK, SitNet과 같은 기존 ZSH 방법들보다 뛰어난 성능을 보였습니다.
  - AwA 데이터셋에서 32비트 사용 시 AgNet은 19.1%의 mAP를 얻어 차순위 SitNet보다 9.1%p 높은 성능을 기록했습니다.
  - ImageNet 데이터셋에서도 모든 코드 길이에서 AgNet이 다른 비교 방법들을 크게 능가했습니다.

- **속성의 영향**:

  - **속성 공간 크기**: 속성 수가 증가할수록 mAP 성능이 향상되었으며, 특정 수(예: AwA에서 20개에서 30개 사이) 이상에서는 성능 향상이 포화 상태에 도달했습니다.
  - **속성 예측 정확도**: 속성 예측 정확도가 높을수록 AgNet의 성능이 좋았습니다. AwA의 속성 예측 정확도가 SUN보다 높았고, 이는 AwA에서 AgNet이 더 좋은 성능을 보이는 이유를 설명합니다.

- **시각화**:
  - **혼동 행렬**: AwA 데이터셋에서 시각적 인스턴스와 텍스트 인스턴스 간의 인접 관계를 시각화한 결과, 대부분의 인스턴스가 동일 카테고리 내에서 잘 정렬되어 있었으나, "물범(seal)"과 "혹등고래(humpback whale)"와 같이 시각적으로 유사한 카테고리에서는 일부 혼동이 관찰되었습니다.
  - **t-SNE 시각화**: t-SNE [38]를 사용하여 속성 예측 및 A2H Net의 최종 레이어 출력을 시각화한 결과, 속성 공간의 유사성 관계가 해시 공간에서 잘 보존되었음을 보여주어 A2H Net의 효과를 입증했습니다.

## 🧠 Insights & Discussion

AgNet은 속성 정보를 활용하여 크로스모달 제로샷 해싱이라는 도전적인 문제를 효과적으로 해결했습니다. 특히, 보지 못한 카테고리에 대한 지식 전이와 모달리티 간의 이질성 문제를 동시에 해결하는 데 속성 공간이 중요한 역할을 함을 입증했습니다. AgNet의 단일 네트워크 구조는 이미지와 텍스트 해시 코드 생성을 효율적으로 통합합니다.

**함의**: 속성은 ZSH, 특히 크로스모달 환경에서 강력하고 효과적인 지식 전이 메커니즘으로 작용합니다.

**한계 및 향후 연구**:

- 시각적으로 유사하거나 의미론적으로 유사한 카테고리에서 해시 코드 생성 시 발생하는 혼동(예: 물범과 혹등고래)은 향후 개선이 필요합니다.
- 속성 주석(annotation)은 사전 지식을 요구하므로, 미래에는 클릭-스루 데이터(click-through data)와 같은 다른 의미론적 데이터를 활용하여 공통 공간을 형성하는 연구가 필요합니다. 이는 명시적인 속성 주석에 대한 의존도를 줄이는 방향이 될 수 있습니다.

## 📌 TL;DR

이 논문은 보지 못한 카테고리에 대한 크로스모달 검색(CMZSH) 문제를 해결하기 위해 **Attribute-Guided Network (AgNet)**를 제안합니다. AgNet은 이미지와 텍스트 데이터를 공유 속성 공간으로 매핑한 후, 카테고리 및 속성 유사성 손실을 활용하여 해시 코드를 생성합니다. 실험 결과, AgNet은 기존 크로스모달 및 제로샷 해싱 방법들보다 이미지-기반 및 텍스트-기반 이미지 검색 모두에서 우수한 성능을 보여, 속성 기반 지식 전이의 효과를 입증했습니다.
