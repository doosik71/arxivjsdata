{
  "title": "FADE: Few-shot/zero-shot Anomaly Detection Engine using Large\n  Vision-Language Model",
  "authors": "Yuanwei Li, Elizaveta Ivanova, Martins Bruveris",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.00556v1",
  "abstract": "Automatic image anomaly detection is important for quality inspection in the\nmanufacturing industry. The usual unsupervised anomaly detection approach is to\ntrain a model for each object class using a dataset of normal samples. However,\na more realistic problem is zero-/few-shot anomaly detection where zero or only\na few normal samples are available. This makes the training of object-specific\nmodels challenging. Recently, large foundation vision-language models have\nshown strong zero-shot performance in various downstream tasks. While these\nmodels have learned complex relationships between vision and language, they are\nnot specifically designed for the tasks of anomaly detection. In this paper, we\npropose the Few-shot/zero-shot Anomaly Detection Engine (FADE) which leverages\nthe vision-language CLIP model and adjusts it for the purpose of industrial\nanomaly detection. Specifically, we improve language-guided anomaly\nsegmentation 1) by adapting CLIP to extract multi-scale image patch embeddings\nthat are better aligned with language and 2) by automatically generating an\nensemble of text prompts related to industrial anomaly detection. 3) We use\nadditional vision-based guidance from the query and reference images to further\nimprove both zero-shot and few-shot anomaly detection. On the MVTec-AD (and\nVisA) dataset, FADE outperforms other state-of-the-art methods in anomaly\nsegmentation with pixel-AUROC of 89.6% (91.5%) in zero-shot and 95.4% (97.5%)\nin 1-normal-shot. Code is available at https://github.com/BMVC-FADE/BMVC-FADE.",
  "citation": 6
}