{
  "title": "Bi-Adversarial Auto-Encoder for Zero-Shot Learning",
  "authors": "Yunlong Yu, Zhong Ji, Yanwei Pang, Jichang Guo, Zhongfei Zhang, Fei Wu",
  "year": 2018,
  "url": "http://arxiv.org/abs/1811.08103v1",
  "abstract": "Existing generative Zero-Shot Learning (ZSL) methods only consider the\nunidirectional alignment from the class semantics to the visual features while\nignoring the alignment from the visual features to the class semantics, which\nfails to construct the visual-semantic interactions well. In this paper, we\npropose to synthesize visual features based on an auto-encoder framework paired\nwith bi-adversarial networks respectively for visual and semantic modalities to\nreinforce the visual-semantic interactions with a bi-directional alignment,\nwhich ensures the synthesized visual features to fit the real visual\ndistribution and to be highly related to the semantics. The encoder aims at\nsynthesizing real-like visual features while the decoder forces both the real\nand the synthesized visual features to be more related to the class semantics.\nTo further capture the discriminative information of the synthesized visual\nfeatures, both the real and synthesized visual features are forced to be\nclassified into the correct classes via a classification network. Experimental\nresults on four benchmark datasets show that the proposed approach is\nparticularly competitive on both the traditional ZSL and the generalized ZSL\ntasks.",
  "citation": 6
}