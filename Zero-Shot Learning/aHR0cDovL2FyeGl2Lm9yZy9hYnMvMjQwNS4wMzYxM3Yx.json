{
  "title": "Dual Relation Mining Network for Zero-Shot Learning",
  "authors": "Jinwei Han, Yingguo Gao, Zhiwen Lin, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia",
  "year": 2024,
  "url": "http://arxiv.org/abs/2405.03613v1",
  "abstract": "Zero-shot learning (ZSL) aims to recognize novel classes through transferring\nshared semantic knowledge (e.g., attributes) from seen classes to unseen\nclasses. Recently, attention-based methods have exhibited significant progress\nwhich align visual features and attributes via a spatial attention mechanism.\nHowever, these methods only explore visual-semantic relationship in the spatial\ndimension, which can lead to classification ambiguity when different attributes\nshare similar attention regions, and semantic relationship between attributes\nis rarely discussed. To alleviate the above problems, we propose a Dual\nRelation Mining Network (DRMN) to enable more effective visual-semantic\ninteractions and learn semantic relationship among attributes for knowledge\ntransfer. Specifically, we introduce a Dual Attention Block (DAB) for\nvisual-semantic relationship mining, which enriches visual information by\nmulti-level feature fusion and conducts spatial attention for visual to\nsemantic embedding. Moreover, an attribute-guided channel attention is utilized\nto decouple entangled semantic features. For semantic relationship modeling, we\nutilize a Semantic Interaction Transformer (SIT) to enhance the generalization\nof attribute representations among images. Additionally, a global\nclassification branch is introduced as a complement to human-defined semantic\nattributes, and we then combine the results with attribute-based\nclassification. Extensive experiments demonstrate that the proposed DRMN leads\nto new state-of-the-art performances on three standard ZSL benchmarks, i.e.,\nCUB, SUN, and AwA2.",
  "citation": 2
}