{
  "title": "Visual and Semantic Prompt Collaboration for Generalized Zero-Shot\n  Learning",
  "authors": "Huajie Jiang, Zhengxian Li, Xiaohan Yu, Yongli Hu, Baocai Yin, Jian Yang, Yuankai Qi",
  "year": 2025,
  "url": "http://arxiv.org/abs/2503.23030v1",
  "abstract": "Generalized zero-shot learning aims to recognize both seen and unseen classes\nwith the help of semantic information that is shared among different classes.\nIt inevitably requires consistent visual-semantic alignment. Existing\napproaches fine-tune the visual backbone by seen-class data to obtain\nsemantic-related visual features, which may cause overfitting on seen classes\nwith a limited number of training images. This paper proposes a novel visual\nand semantic prompt collaboration framework, which utilizes prompt tuning\ntechniques for efficient feature adaptation. Specifically, we design a visual\nprompt to integrate the visual information for discriminative feature learning\nand a semantic prompt to integrate the semantic formation for visualsemantic\nalignment. To achieve effective prompt information integration, we further\ndesign a weak prompt fusion mechanism for the shallow layers and a strong\nprompt fusion mechanism for the deep layers in the network. Through the\ncollaboration of visual and semantic prompts, we can obtain discriminative\nsemantic-related features for generalized zero-shot image recognition.\nExtensive experiments demonstrate that our framework consistently achieves\nfavorable performance in both conventional zero-shot learning and generalized\nzero-shot learning benchmarks compared to other state-of-the-art methods.",
  "citation": 1
}