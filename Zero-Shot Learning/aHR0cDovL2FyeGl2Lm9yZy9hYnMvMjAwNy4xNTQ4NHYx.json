{
  "title": "Learning from Few Samples: A Survey",
  "authors": "Nihar Bendre, Hugo Terashima Mar√≠n, Peyman Najafirad",
  "year": 2020,
  "url": "http://arxiv.org/abs/2007.15484v1",
  "abstract": "Deep neural networks have been able to outperform humans in some cases like\nimage recognition and image classification. However, with the emergence of\nvarious novel categories, the ability to continuously widen the learning\ncapability of such networks from limited samples, still remains a challenge.\nTechniques like Meta-Learning and/or few-shot learning showed promising\nresults, where they can learn or generalize to a novel category/task based on\nprior knowledge. In this paper, we perform a study of the existing few-shot\nmeta-learning techniques in the computer vision domain based on their method\nand evaluation metrics. We provide a taxonomy for the techniques and categorize\nthem as data-augmentation, embedding, optimization and semantics based learning\nfor few-shot, one-shot and zero-shot settings. We then describe the seminal\nwork done in each category and discuss their approach towards solving the\npredicament of learning from few samples. Lastly we provide a comparison of\nthese techniques on the commonly used benchmark datasets: Omniglot, and\nMiniImagenet, along with a discussion towards the future direction of improving\nthe performance of these techniques towards the final goal of outperforming\nhumans.",
  "citation": 82
}