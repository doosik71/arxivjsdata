{
  "title": "Visual Semantic Segmentation Based on Few/Zero-Shot Learning: An\n  Overview",
  "authors": "Wenqi Ren, Yang Tang, Qiyu Sun, Chaoqiang Zhao, Qing-Long Han",
  "year": 2022,
  "url": "http://arxiv.org/abs/2211.08352v1",
  "abstract": "Visual semantic segmentation aims at separating a visual sample into diverse\nblocks with specific semantic attributes and identifying the category for each\nblock, and it plays a crucial role in environmental perception. Conventional\nlearning-based visual semantic segmentation approaches count heavily on\nlarge-scale training data with dense annotations and consistently fail to\nestimate accurate semantic labels for unseen categories. This obstruction spurs\na craze for studying visual semantic segmentation with the assistance of\nfew/zero-shot learning. The emergence and rapid progress of few/zero-shot\nvisual semantic segmentation make it possible to learn unseen-category from a\nfew labeled or zero-labeled samples, which advances the extension to practical\napplications. Therefore, this paper focuses on the recently published\nfew/zero-shot visual semantic segmentation methods varying from 2D to 3D space\nand explores the commonalities and discrepancies of technical settlements under\ndifferent segmentation circumstances. Specifically, the preliminaries on\nfew/zero-shot visual semantic segmentation, including the problem definitions,\ntypical datasets, and technical remedies, are briefly reviewed and discussed.\nMoreover, three typical instantiations are involved to uncover the interactions\nof few/zero-shot learning with visual semantic segmentation, including image\nsemantic segmentation, video object segmentation, and 3D segmentation. Finally,\nthe future challenges of few/zero-shot visual semantic segmentation are\ndiscussed.",
  "citation": 67
}