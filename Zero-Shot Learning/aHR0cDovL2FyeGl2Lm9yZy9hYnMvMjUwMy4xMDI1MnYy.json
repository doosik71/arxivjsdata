{
  "title": "SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning",
  "authors": "Zhi Chen, Zecheng Zhao, Jingcai Guo, Jingjing Li, Zi Huang",
  "year": 2025,
  "url": "http://arxiv.org/abs/2503.10252v2",
  "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes without labeled\ntraining examples by leveraging class-level semantic descriptors such as\nattributes. A fundamental challenge in ZSL is semantic misalignment, where\nsemantic-unrelated information involved in visual features introduce ambiguity\nto visual-semantic interaction. Unlike existing methods that suppress\nsemantic-unrelated information post hoc either in the feature space or the\nmodel space, we propose addressing this issue at the input stage, preventing\nsemantic-unrelated patches from propagating through the network. To this end,\nwe introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a\ntransformer-based framework designed to enhance visual-semantic alignment.\nSpecifically, we propose a self-supervised patch selection mechanism that\npreemptively learns to identify semantic-unrelated patches in the input space.\nThis is trained with the supervision from aggregated attention scores across\nall transformer layers, which estimate each patch's semantic score. As removing\nsemantic-unrelated patches from the input sequence may disrupt object\nstructure, we replace them with learnable patch embeddings. With initialization\nfrom word embeddings, we can ensure they remain semantically meaningful\nthroughout feature extraction. Extensive experiments on ZSL benchmarks\ndemonstrate that SVIP achieves state-of-the-art performance results while\nproviding more interpretable and semantically rich feature representations.\nCode is available at https://github.com/uqzhichen/SVIP.",
  "citation": 1
}