{
  "title": "On Zero-Shot Reinforcement Learning",
  "authors": "Scott Jeen",
  "year": 2025,
  "url": "http://arxiv.org/abs/2508.16496v1",
  "abstract": "Modern reinforcement learning (RL) systems capture deep truths about general,\nhuman problem-solving. In domains where new data can be simulated cheaply,\nthese systems uncover sequential decision-making policies that far exceed the\nability of any human. Society faces many problems whose solutions require this\nskill, but they are often in domains where new data cannot be cheaply\nsimulated. In such scenarios, we can learn simulators from existing data, but\nthese will only ever be approximately correct, and can be pathologically\nincorrect when queried outside of their training distribution. As a result, a\nmisalignment between the environments in which we train our agents and the\nreal-world in which we wish to deploy our agents is inevitable. Dealing with\nthis misalignment is the primary concern of zero-shot reinforcement learning, a\nproblem setting where the agent must generalise to a new task or domain with\nzero practice shots. Whilst impressive progress has been made on methods that\nperform zero-shot RL in idealised settings, new work is needed if these results\nare to be replicated in real-world settings. In this thesis, we argue that\ndoing so requires us to navigate (at least) three constraints. First, the data\nquality constraint: real-world datasets are small and homogeneous. Second, the\nobservability constraint: states, dynamics and rewards in the real-world are\noften only partially observed. And third, the data availability constraint: a\npriori access to data cannot always be assumed. This work proposes a suite of\nmethods that perform zero-shot RL subject to these constraints. In a series of\nempirical studies we expose the failings of existing methods, and justify our\ntechniques for remedying them. We believe these designs take us a step closer\nto RL methods that can be deployed to solve real-world problems.",
  "citation": 0
}