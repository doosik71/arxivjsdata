{
  "title": "Learning Graph Structures with Transformer for Multivariate Time Series\n  Anomaly Detection in IoT",
  "authors": "Zekai Chen, Dingshuo Chen, Xiao Zhang, Zixuan Yuan, Xiuzhen Cheng",
  "year": 2021,
  "url": "http://arxiv.org/abs/2104.03466v3",
  "abstract": "Many real-world IoT systems, which include a variety of internet-connected\nsensory devices, produce substantial amounts of multivariate time series data.\nMeanwhile, vital IoT infrastructures like smart power grids and water\ndistribution networks are frequently targeted by cyber-attacks, making anomaly\ndetection an important study topic. Modeling such relatedness is, nevertheless,\nunavoidable for any efficient and effective anomaly detection system, given the\nintricate topological and nonlinear connections that are originally unknown\namong sensors. Furthermore, detecting anomalies in multivariate time series is\ndifficult due to their temporal dependency and stochasticity. This paper\npresented GTA, a new framework for multivariate time series anomaly detection\nthat involves automatically learning a graph structure, graph convolution, and\nmodeling temporal dependency using a Transformer-based architecture. The\nconnection learning policy, which is based on the Gumbel-softmax sampling\napproach to learn bi-directed links among sensors directly, is at the heart of\nlearning graph structure. To describe the anomaly information flow between\nnetwork nodes, we introduced a new graph convolution called Influence\nPropagation convolution. In addition, to tackle the quadratic complexity\nbarrier, we suggested a multi-branch attention mechanism to replace the\noriginal multi-head self-attention method. Extensive experiments on four\npublicly available anomaly detection benchmarks further demonstrate the\nsuperiority of our approach over alternative state-of-the-arts. Codes are\navailable at https://github.com/ZEKAICHEN/GTA.",
  "citation": 582
}