# Learning Graph Structures with Transformer for Multivariate Time Series Anomaly Detection in IoT

Zekai Chen, Student Member, IEEE, Dingshuo Chen, Xiao Zhang, Member, IEEE, Zixuan Yuan, and Xiuzhen Cheng, Fellow, IEEE

## 🧩 Problem to Solve

IoT 시스템에서 생성되는 방대한 다변량 시계열 데이터는 사이버 공격에 취약하여 이상 탐지가 필수적입니다. 그러나 센서들 간의 복잡하고 알려지지 않은 토폴로지적 및 비선형적 연결성, 그리고 시계열 데이터 자체의 시간적 의존성 및 확률론적 특성 때문에 효율적이고 정확한 이상 탐지 시스템을 구축하는 것이 어렵습니다. 기존 방법들은 이러한 토폴로지 구조를 명시적으로 학습하지 못하거나, 센서 수($M$)에 대해 $O(M^2)$의 높은 계산 복잡성을 초래하는 한계가 있었습니다.

## ✨ Key Contributions

- **새롭고 차별화 가능한 연결 학습 정책 제안:** Gumbel-Softmax 샘플링 방식을 기반으로 센서 간의 양방향 그래프 구조를 자동으로 학습하여, 기존 $O(M^2)$ 복잡성을 $O(1)$로 줄이고 숨겨진 의존성을 효율적으로 발견합니다.
- **영향 전파(Influence Propagation, IP) 컨볼루션 도입:** 그래프 노드 간의 이상(anomaly) 정보 흐름 과정을 모델링하기 위한 새로운 그래프 컨볼루션입니다.
- **계층적 Dilated 컨볼루션 통합:** IP 그래프 컨볼루션과 결합하여 다양한 범위의 시간적 패턴을 포착하는 효과적인 계층적 시간 컨텍스트 인코딩 블록을 구성합니다.
- **새로운 멀티-브랜치 어텐션 메커니즘 제안:** 오리지널 Multi-head Self-Attention의 이차 복잡성($O(n^2)$) 문제를 해결하기 위해 고안되었습니다.
- **광범위한 실험을 통한 우수성 입증:** 4가지 공개 이상 탐지 벤치마크 데이터셋에서 제안된 접근 방식이 최첨단 방법들보다 우수함을 보여주었습니다.

## 📎 Related Works

- **단변량 시계열 이상 탐지:** ARIMA, Wavelet Analysis, DeepAnt(CNN), LSTM 기반 Encoder-Decoder.
- **다변량 시계열 이상 탐지:**
  - **일반 모델:** MAD-GAN (GAN + LSTM), Autoencoders, ODCA (상관관계 기반), OmniAnomaly (확률적 RNN + 정규화 흐름).
  - **그래프 기반 모델:** GCNs (그래프 표현 학습), Graph Deviation Network (GDN) (코사인 유사도 기반 그래프 학습, 그래프 어텐션 기반 예측), MTAD-GAT (특징 지향 및 시간 지향 그래프 어텐션 레이어 결합).
- **Transformer 및 Attention 메커니즘:** Transformer의 Multi-head Self-Attention (Vaswani et al.), Multi-branch Attention (Wu et al.), Global-learned Attention (Synthesizer).
- **그래프 신경망:** GNNs (불규칙한 구조 인코딩), GAT (그래프 어텐션), Dynamic Graph CNN.

## 🛠️ Methodology

본 논문은 IoT 다변량 시계열 이상 탐지를 위한 **GTA (Graph Learning with Transformer for Anomaly detection)** 프레임워크를 제안합니다.

1. **Gumbel-Softmax 샘플링 기반 그래프 학습:**

   - 이산적인 샘플링 과정의 비차별성 문제를 해결하기 위해 Gumbel-Softmax 분포를 사용합니다.
   - 각 센서 쌍($i, j$)에 대해 양방향 연결 확률($\pi_{i,j}^0, \pi_{i,j}^1$)을 학습하여 $z_{i,j} \in \{0,1\}^2$와 같은 연결 변수를 샘플링합니다.
   - 이를 통해 고차원 노드 임베딩 간의 내적 없이 숨겨진 센서 연관성을 자동으로 발견하여 계산 복잡성을 $O(M^2)$에서 $O(1)$으로 줄입니다.
   - 각 노드의 이웃 범위를 제한하고 모델 추론 효율성을 높이기 위해 스파스 정규화 손실 $L_s = \sum_{1 \leq i,j \leq M, i \neq j} \log\pi_{i,j}^1$을 추가합니다.
   - 정책 학습의 좋은 시작점을 제공하기 위해 초기에는 모든 노드가 연결된 완전 그래프로 네트워크를 '웜업' 학습합니다.

2. **영향 전파(IP) 그래프 컨볼루션:**

   - 학습된 토폴로지 구조 위에서 이상 영향 전파 과정을 모델링하고 이웃 정보 통합을 통해 각 노드의 표현을 업데이트합니다.
   - $i$-번째 노드의 업데이트된 출력 $x'_i$는 다음과 같이 정의됩니다:
     $$ x'_i = \sum_{j \in N(i)} h*{\Theta}(x_i || x_j - x_i || x_j + x_i) $$
     여기서 $h*{\Theta}$는 MLP이며, $||$는 연결 연산입니다. $x_j - x_i$는 노드 간의 차이를 통해 영향 전파 지연을 명시적으로 모델링하며, $x_j + x_i$는 이상으로 인한 다른 노드에 대한 일반화된 영향을 학습하기 위한 스케일 벤치마크 역할을 합니다.

3. **계층적 Dilated 컨볼루션:**

   - IP 그래프 컨볼루션과 다양한 레벨의 Dilated 컨볼루션을 결합하여 계층적 시간 컨텍스트 인코딩 블록을 구성합니다.
   - Dilated 컨볼루션은 서로 다른 Dilated 크기(dilation rates)를 설정하여 다양한 범위의 시간적 패턴을 포착하며 긴 시퀀스를 처리할 수 있습니다.
   - 각 Dilated 컨볼루션 레이어의 출력은 그래프 컨볼루션 모듈로 전달되어 시간적 및 공간적(토폴로지적) 의존성을 모두 포괄적으로 학습합니다.

4. **더 효율적인 멀티-브랜치 Transformer:**

   - 기존 Transformer의 Self-Attention 메커니즘이 시퀀스 길이에 대해 $O(n^2)$의 계산 복잡성을 가지는 문제를 해결합니다.
   - 입력 시퀀스를 임베딩 채널을 따라 여러 브랜치로 분할합니다.
   - 한 브랜치는 기존 Multi-Head Attention을 사용하여 쌍별 토큰 상호작용을 캡처하고, 다른 브랜치는 글로벌 학습 어텐션($S \in \mathbb{R}^{m \times m}$과 같은 학습 가능한 전역 호환성 함수)을 사용하여 전역 컨텍스트를 캡처합니다.
   - 이러한 '브랜치별 혼합' 전략은 계산 효율성과 모델 성능 간의 더 나은 균형을 제공합니다.

5. **이상 스코어링:**
   - 단일 스텝 예측 전략을 사용하여 다음 타임스탬프의 값을 예측합니다.
   - 예측된 출력 $\hat{Y}$와 실제 관측값 $Y$ 간의 평균 제곱 오차(MSE)를 손실 함수로 사용합니다.
   - 이상 스코어 $\hat{y}^{(t)}$는 각 타임스탬프 $t$에서 예측값과 관측값 간의 편차 수준을 측정하여 계산됩니다:
     $$ \hat{y}^{(t)} = \sum\_{i=1}^{M} ||Y^{(t)}\_i - \hat{Y}^{(t)}\_i||^2_2 $$
   - 스코어가 특정 임계값을 초과하면 해당 타임스탬프를 이상으로 분류하며, 최적의 F1-score 또는 Recall을 위한 그리드 서치를 통해 임계값을 결정합니다.

## 📊 Results

- **SWaT 및 WADI 데이터셋 (사이버 공격):**
  - GTA는 SWaT에서 0.91, WADI에서 0.84의 최고 F1-score를 달성하며 모든 최첨단 방법들을 크게 능가했습니다. 이는 두 번째로 우수한 GDN 대비 SWaT에서 12.35%, WADI에서 47.37%의 인상적인 F1-score 향상을 의미합니다.
  - 심층 학습 기반 방법들이 PCA, KNN과 같은 전통적인 비지도 학습 방법들보다 전반적으로 우수한 성능을 보였습니다.
  - Transformer 기반 아키텍처는 RNN 기반 모델보다 장거리 의존성 포착 능력이 뛰어남을 입증했습니다.
- **SMAP 및 MSL 데이터셋 (NASA 위성 및 로버):**
  - SWaT 및 WADI 데이터셋만큼 인상적인 F1-score 향상을 보이지는 않았습니다. 이는 SMAP 및 MSL 데이터셋의 특징들 간 내부 의존성이 SWaT 및 WADI와 같은 산업 제어 시스템의 강한 토폴로지 구조보다 약하기 때문인 것으로 분석됩니다. 이는 GTA의 그래프 학습 전략이 강한 토폴로지 구조를 가진 데이터셋에서 더 효과적임을 시사합니다.
- **제거 연구 (Ablation Studies):**
  - 그래프 학습 모듈 제거(`w/o Graph`) 시 F1-score가 SWaT에서 0.91에서 0.75로 크게 감소하여 토폴로지 구조 모델링의 중요성을 확인했습니다.
  - 학습된 연결 정책을 정적 완전 그래프로 대체(`w/o LP`)했을 때도 성능 저하가 발생하여 제안된 정책의 효과를 입증했습니다.
  - Transformer 기반 아키텍처를 GRU 기반 RNN으로 대체(`w/o Attn`)했을 때 F1-score가 SWaT에서 0.91에서 0.71로 크게 감소하여 Transformer의 우수성을 재확인했습니다.
  - 이는 GTA의 모든 구성 요소가 다변량 시계열 이상 탐지에서 필수적임을 의미합니다.
- **사례 연구 (WADI):**
  - GTA의 그래프 학습 정책은 WADI 공격 시나리오에서 센서들 간의 실제 토폴로지적 상호작용을 거의 정확하게 묘사하는 부분 그래프를 학습했습니다.
  - 모델은 직접 공격받지 않은 센서들도 학습된 그래프를 통한 영향 전파 덕분에 비정상적인 예측을 생성하여 이상 탐지에 기여함을 보여주었습니다.

## 🧠 Insights & Discussion

- **그래프 학습의 중요성:** IoT 산업 제어 시스템과 같이 센서 간에 강한 상호 의존성이 있는 시스템에서 복잡하고 숨겨진 토폴로지 관계를 명시적으로 학습하는 것은 효과적인 다변량 시계열 이상 탐지에 매우 중요합니다.
- **계산 효율성:** Gumbel-Softmax 기반의 연결 학습 정책은 기존 그래프 학습 방법(예: 코사인 유사도 기반의 Top-K 접근 방식)의 이차 복잡성 병목 현상을 효과적으로 피합니다. 또한, 멀티-브랜치 어텐션 메커니즘은 Transformer의 효율성을 더욱 향상시킵니다.
- **시계열 및 공간 모델링의 조화:** 계층적 Dilated 컨볼루션(시간적)과 영향 전파 그래프 컨볼루션(공간적/토폴로지적)의 결합은 다변량 시계열 데이터의 두 가지 핵심 측면을 효과적으로 포착합니다.
- **Transformer의 장점:** 제안된 멀티-브랜치 변형을 포함한 Transformer의 Self-Attention 메커니즘은 시계열 데이터의 장거리 의존성을 포착하는 데 순환 신경망보다 우수하여 더 나은 예측 및 이상 탐지 성능으로 이어집니다.
- **한계 및 향후 연구:** 그래프 학습의 효과는 강한 토폴로지 구조를 가진 데이터셋에서 더 두드러지게 나타납니다. 향후 연구로는 모바일 IoT 시나리오를 위한 온라인 학습 전략과의 결합을 탐색할 계획입니다.

## 📌 TL;DR

IoT 환경의 다변량 시계열 이상 탐지에서 센서 간의 숨겨진 복잡한 토폴로지 구조와 시계열 특성을 모델링하는 문제를 해결하기 위해, 본 논문은 **GTA (Graph Learning with Transformer for Anomaly detection)** 프레임워크를 제안합니다. GTA는 Gumbel-Softmax 샘플링 기반의 **연결 학습 정책**을 통해 센서 간 그래프 구조를 자동으로 학습하여 $O(M^2)$ 복잡성을 $O(1)$로 줄입니다. 또한, **영향 전파(IP) 그래프 컨볼루션**과 **계층적 Dilated 컨볼루션**을 결합하여 시계열 및 공간적 의존성을 효과적으로 포착하며, Transformer의 효율적인 **멀티-브랜치 어텐션** 메커니즘으로 장거리 의존성 모델링의 계산 병목 현상을 해결합니다. 실험 결과, GTA는 SWaT 및 WADI와 같은 사이버 공격 데이터셋에서 기존 최첨단 모델들을 F1-score 기준으로 최대 47.37%까지 능가하는 우수한 성능을 보여주며, 제안된 각 구성 요소의 중요성을 입증했습니다. 특히 강한 토폴로지 구조를 가진 데이터셋에서 효과적임을 확인했습니다.
