{
  "title": "SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene\n  Reconstruction by Neural Radiance Field (NeRF)",
  "authors": "Ange Lou, Yamin Li, Xing Yao, Yike Zhang, Jack Noble",
  "year": 2023,
  "url": "http://arxiv.org/abs/2308.11774v2",
  "abstract": "The accurate reconstruction of surgical scenes from surgical videos is\ncritical for various applications, including intraoperative navigation and\nimage-guided robotic surgery automation. However, previous approaches, mainly\nrelying on depth estimation, have limited effectiveness in reconstructing\nsurgical scenes with moving surgical tools. To address this limitation and\nprovide accurate 3D position prediction for surgical tools in all frames, we\npropose a novel approach called SAMSNeRF that combines Segment Anything Model\n(SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates\naccurate segmentation masks of surgical tools using SAM, which guides the\nrefinement of the dynamic surgical scene reconstruction by NeRF. Our\nexperimental results on public endoscopy surgical videos demonstrate that our\napproach successfully reconstructs high-fidelity dynamic surgical scenes and\naccurately reflects the spatial information of surgical tools. Our proposed\napproach can significantly enhance surgical navigation and automation by\nproviding surgeons with accurate 3D position information of surgical tools\nduring surgery.The source code will be released soon."
}