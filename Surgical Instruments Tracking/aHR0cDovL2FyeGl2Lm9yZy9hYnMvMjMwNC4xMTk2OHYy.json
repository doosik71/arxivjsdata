{
  "title": "Track Anything: Segment Anything Meets Videos",
  "authors": "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng",
  "year": 2023,
  "url": "http://arxiv.org/abs/2304.11968v2",
  "abstract": "Recently, the Segment Anything Model (SAM) gains lots of attention rapidly\ndue to its impressive segmentation performance on images. Regarding its strong\nability on image segmentation and high interactivity with different prompts, we\nfound that it performs poorly on consistent segmentation in videos. Therefore,\nin this report, we propose Track Anything Model (TAM), which achieves\nhigh-performance interactive tracking and segmentation in videos. To be\ndetailed, given a video sequence, only with very little human participation,\ni.e., several clicks, people can track anything they are interested in, and get\nsatisfactory results in one-pass inference. Without additional training, such\nan interactive design performs impressively on video object tracking and\nsegmentation. All resources are available on\n{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitate\nrelated research.",
  "citation": 298
}