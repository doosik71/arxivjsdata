{
  "url": "http://arxiv.org/abs/2409.19821v2",
  "title": "Tracking Everything in Robotic-Assisted Surgery",
  "authors": "Bohan Zhan, Wang Zhao, Yi Fang, Bo Du, Francisco Vasconcelos, Danail Stoyanov, Daniel S. Elson, Baoru Huang",
  "year": 2024,
  "abstract": "Accurate tracking of tissues and instruments in videos is crucial for\nRobotic-Assisted Minimally Invasive Surgery (RAMIS), as it enables the robot to\ncomprehend the surgical scene with precise locations and interactions of\ntissues and tools. Traditional keypoint-based sparse tracking is limited by\nfeatured points, while flow-based dense two-view matching suffers from\nlong-term drifts. Recently, the Tracking Any Point (TAP) algorithm was proposed\nto overcome these limitations and achieve dense accurate long-term tracking.\nHowever, its efficacy in surgical scenarios remains untested, largely due to\nthe lack of a comprehensive surgical tracking dataset for evaluation. To\naddress this gap, we introduce a new annotated surgical tracking dataset for\nbenchmarking tracking methods for surgical scenarios, comprising real-world\nsurgical videos with complex tissue and instrument motions. We extensively\nevaluate state-of-the-art (SOTA) TAP-based algorithms on this dataset and\nreveal their limitations in challenging surgical scenarios, including fast\ninstrument motion, severe occlusions, and motion blur, etc. Furthermore, we\npropose a new tracking method, namely SurgMotion, to solve the challenges and\nfurther improve the tracking performance. Our proposed method outperforms most\nTAP-based algorithms in surgical instruments tracking, and especially\ndemonstrates significant improvements over baselines in challenging medical\nvideos. Our code and dataset are available at\nhttps://github.com/zhanbh1019/SurgicalMotion.",
  "citation": 6
}