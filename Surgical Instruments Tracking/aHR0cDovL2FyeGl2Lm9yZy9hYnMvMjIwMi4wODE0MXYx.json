{
  "title": "FUN-SIS: a Fully UNsupervised approach for Surgical Instrument\n  Segmentation",
  "authors": "Luca Sestini, Benoit Rosa, Elena De Momi, Giancarlo Ferrigno, Nicolas Padoy",
  "year": 2022,
  "url": "http://arxiv.org/abs/2202.08141v1",
  "abstract": "Automatic surgical instrument segmentation of endoscopic images is a crucial\nbuilding block of many computer-assistance applications for minimally invasive\nsurgery. So far, state-of-the-art approaches completely rely on the\navailability of a ground-truth supervision signal, obtained via manual\nannotation, thus expensive to collect at large scale. In this paper, we present\nFUN-SIS, a Fully-UNsupervised approach for binary Surgical Instrument\nSegmentation. FUN-SIS trains a per-frame segmentation model on completely\nunlabelled endoscopic videos, by solely relying on implicit motion information\nand instrument shape-priors. We define shape-priors as realistic segmentation\nmasks of the instruments, not necessarily coming from the same dataset/domain\nas the videos. The shape-priors can be collected in various and convenient\nways, such as recycling existing annotations from other datasets. We leverage\nthem as part of a novel generative-adversarial approach, allowing to perform\nunsupervised instrument segmentation of optical-flow images during training. We\nthen use the obtained instrument masks as pseudo-labels in order to train a\nper-frame segmentation model; to this aim, we develop a\nlearning-from-noisy-labels architecture, designed to extract a clean\nsupervision signal from these pseudo-labels, leveraging their peculiar noise\nproperties. We validate the proposed contributions on three surgical datasets,\nincluding the MICCAI 2017 EndoVis Robotic Instrument Segmentation Challenge\ndataset. The obtained fully-unsupervised results for surgical instrument\nsegmentation are almost on par with the ones of fully-supervised\nstate-of-the-art approaches. This suggests the tremendous potential of the\nproposed method to leverage the great amount of unlabelled data produced in the\ncontext of minimally invasive surgery.",
  "citation": 39
}