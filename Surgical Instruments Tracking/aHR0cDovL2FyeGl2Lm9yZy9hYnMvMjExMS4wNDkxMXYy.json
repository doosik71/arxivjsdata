{
  "title": "Real-time Instance Segmentation of Surgical Instruments using Attention\n  and Multi-scale Feature Fusion",
  "authors": "Juan Carlos Angeles-Ceron, Gilberto Ochoa-Ruiz, Leonardo Chang, Sharib Ali",
  "year": 2021,
  "url": "http://arxiv.org/abs/2111.04911v2",
  "abstract": "Precise instrument segmentation aid surgeons to navigate the body more easily\nand increase patient safety. While accurate tracking of surgical instruments in\nreal-time plays a crucial role in minimally invasive computer-assisted\nsurgeries, it is a challenging task to achieve, mainly due to 1) complex\nsurgical environment, and 2) model design with both optimal accuracy and speed.\nDeep learning gives us the opportunity to learn complex environment from large\nsurgery scene environments and placements of these instruments in real world\nscenarios. The Robust Medical Instrument Segmentation 2019 challenge\n(ROBUST-MIS) provides more than 10,000 frames with surgical tools in different\nclinical settings. In this paper, we use a light-weight single stage instance\nsegmentation model complemented with a convolutional block attention module for\nachieving both faster and accurate inference. We further improve accuracy\nthrough data augmentation and optimal anchor localisation strategies. To our\nknowledge, this is the first work that explicitly focuses on both real-time\nperformance and improved accuracy. Our approach out-performed top team\nperformances in the ROBUST-MIS challenge with over 44% improvement on both\narea-based metric MI_DSC and distance-based metric MI_NSD. We also demonstrate\nreal-time performance (> 60 frames-per-second) with different but competitive\nvariants of our final approach.",
  "citation": 48
}