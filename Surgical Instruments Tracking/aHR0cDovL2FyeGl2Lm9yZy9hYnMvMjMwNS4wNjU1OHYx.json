{
  "title": "Segment and Track Anything",
  "authors": "Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, Yi Yang",
  "year": 2023,
  "url": "http://arxiv.org/abs/2305.06558v1",
  "abstract": "This report presents a framework called Segment And Track Anything (SAMTrack)\nthat allows users to precisely and effectively segment and track any object in\na video. Additionally, SAM-Track employs multimodal interaction methods that\nenable users to select multiple objects in videos for tracking, corresponding\nto their specific requirements. These interaction methods comprise click,\nstroke, and text, each possessing unique benefits and capable of being employed\nin combination. As a result, SAM-Track can be used across an array of fields,\nranging from drone technology, autonomous driving, medical imaging, augmented\nreality, to biological analysis. SAM-Track amalgamates Segment Anything Model\n(SAM), an interactive key-frame segmentation model, with our proposed AOT-based\ntracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022\nchallenge, to facilitate object tracking in video. In addition, SAM-Track\nincorporates Grounding-DINO, which enables the framework to support text-based\ninteraction. We have demonstrated the remarkable capabilities of SAM-Track on\nDAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in\ndiverse applications. The project page is available at:\nhttps://github.com/z-x-yang/Segment-and-Track-Anything."
}