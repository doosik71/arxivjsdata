{
  "title": "Performance and Non-adversarial Robustness of the Segment Anything Model\n  2 in Surgical Video Segmentation",
  "authors": "Yiqing Shen, Hao Ding, Xinyuan Shao, Mathias Unberath",
  "year": 2024,
  "url": "http://arxiv.org/abs/2408.04098v2",
  "abstract": "Fully supervised deep learning (DL) models for surgical video segmentation\nhave been shown to struggle with non-adversarial, real-world corruptions of\nimage quality including smoke, bleeding, and low illumination. Foundation\nmodels for image segmentation, such as the segment anything model (SAM) that\nfocuses on interactive prompt-based segmentation, move away from semantic\nclasses and thus can be trained on larger and more diverse data, which offers\noutstanding zero-shot generalization with appropriate user prompts. Recently,\nbuilding upon this success, SAM-2 has been proposed to further extend the\nzero-shot interactive segmentation capabilities from independent frame-by-frame\nto video segmentation. In this paper, we present a first experimental study\nevaluating SAM-2's performance on surgical video data. Leveraging the\nSegSTRONG-C MICCAI EndoVIS 2024 sub-challenge dataset, we assess SAM-2's\neffectiveness on uncorrupted endoscopic sequences and evaluate its\nnon-adversarial robustness on videos with corrupted image quality simulating\nsmoke, bleeding, and low brightness conditions under various prompt strategies.\nOur experiments demonstrate that SAM-2, in zero-shot manner, can achieve\ncompetitive or even superior performance compared to fully-supervised deep\nlearning models on surgical video data, including under non-adversarial\ncorruptions of image quality. Additionally, SAM-2 consistently outperforms the\noriginal SAM and its medical variants across all conditions. Finally,\nframe-sparse prompting can consistently outperform frame-wise prompting for\nSAM-2, suggesting that allowing SAM-2 to leverage its temporal modeling\ncapabilities leads to more coherent and accurate segmentation compared to\nfrequent prompting."
}