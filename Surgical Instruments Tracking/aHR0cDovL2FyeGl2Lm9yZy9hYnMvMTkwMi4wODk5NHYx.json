{
  "url": "http://arxiv.org/abs/1902.08994v1",
  "title": "U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic\n  and Instance Segmentation of Surgical Instrument",
  "authors": "S. M. Kamrul Hasan, Cristian A. Linte",
  "year": 2019,
  "abstract": "Conventional therapy approaches limit surgeons' dexterity control due to\nlimited field-of-view. With the advent of robot-assisted surgery, there has\nbeen a paradigm shift in medical technology for minimally invasive surgery.\nHowever, it is very challenging to track the position of the surgical\ninstruments in a surgical scene, and accurate detection & identification of\nsurgical tools is paramount. Deep learning-based semantic segmentation in\nframes of surgery videos has the potential to facilitate this task. In this\nwork, we modify the U-Net architecture named U-NetPlus, by introducing a\npre-trained encoder and re-design the decoder part, by replacing the transposed\nconvolution operation with an upsampling operation based on nearest-neighbor\n(NN) interpolation. To further improve performance, we also employ a very fast\nand flexible data augmentation technique. We trained the framework on 8 x 225\nframe sequences of robotic surgical videos, available through the MICCAI 2017\nEndoVis Challenge dataset and tested it on 8 x 75 frame and 2 x 300 frame\nvideos. Using our U-NetPlus architecture, we report a 90.20% DICE for binary\nsegmentation, 76.26% DICE for instrument part segmentation, and 46.07% for\ninstrument type (i.e., all instruments) segmentation, outperforming the results\nof previous techniques implemented and tested on these data."
}