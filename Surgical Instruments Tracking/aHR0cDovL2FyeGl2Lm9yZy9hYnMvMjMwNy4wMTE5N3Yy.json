{
  "title": "Segment Anything Meets Point Tracking",
  "authors": "Frano Rajiƒç, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu",
  "year": 2023,
  "url": "http://arxiv.org/abs/2307.01197v2",
  "abstract": "The Segment Anything Model (SAM) has established itself as a powerful\nzero-shot image segmentation model, enabled by efficient point-centric\nannotation and prompt-based models. While click and brush interactions are both\nwell explored in interactive image segmentation, the existing methods on videos\nfocus on mask annotation and propagation. This paper presents SAM-PT, a novel\nmethod for point-centric interactive video segmentation, empowered by SAM and\nlong-term point tracking. SAM-PT leverages robust and sparse point selection\nand propagation techniques for mask generation. Compared to traditional\nobject-centric mask propagation strategies, we uniquely use point propagation\nto exploit local structure information agnostic to object semantics. We\nhighlight the merits of point-based tracking through direct evaluation on the\nzero-shot open-world Unidentified Video Objects (UVO) benchmark. Our\nexperiments on popular video object segmentation and multi-object segmentation\ntracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a\npoint-based segmentation tracker yields better zero-shot performance and\nefficient interactions. We release our code that integrates different point\ntrackers and video segmentation benchmarks at https://github.com/SysCV/sam-pt."
}