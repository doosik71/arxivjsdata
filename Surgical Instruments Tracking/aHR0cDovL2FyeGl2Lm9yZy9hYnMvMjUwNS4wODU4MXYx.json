{
  "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
  "authors": "Haofeng Liu, Mingqi Gao, Xuxiao Luo, Ziyue Wang, Guanyi Qin, Junde Wu, Yueming Jin",
  "year": 2025,
  "url": "http://arxiv.org/abs/2505.08581v1",
  "abstract": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2."
}