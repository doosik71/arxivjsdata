{
  "url": "http://arxiv.org/abs/2007.11319v2",
  "title": "Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary\n  Supervised Deep Adversarial Learning",
  "authors": "Mobarakol Islam, Daniel A. Atputharuban, Ravikiran Ramesh, Hongliang Ren",
  "year": 2020,
  "abstract": "Robot-assisted surgery is an emerging technology which has undergone rapid\ngrowth with the development of robotics and imaging systems. Innovations in\nvision, haptics and accurate movements of robot arms have enabled surgeons to\nperform precise minimally invasive surgeries. Real-time semantic segmentation\nof the robotic instruments and tissues is a crucial step in robot-assisted\nsurgery. Accurate and efficient segmentation of the surgical scene not only\naids in the identification and tracking of instruments but also provided\ncontextual information about the different tissues and instruments being\noperated with. For this purpose, we have developed a light-weight cascaded\nconvolutional neural network (CNN) to segment the surgical instruments from\nhigh-resolution videos obtained from a commercial robotic system. We propose a\nmulti-resolution feature fusion module (MFF) to fuse the feature maps of\ndifferent dimensions and channels from the auxiliary and main branch. We also\nintroduce a novel way of combining auxiliary loss and adversarial loss to\nregularize the segmentation model. Auxiliary loss helps the model to learn\nlow-resolution features, and adversarial loss improves the segmentation\nprediction by learning higher order structural information. The model also\nconsists of a light-weight spatial pyramid pooling (SPP) unit to aggregate rich\ncontextual information in the intermediate stage. We show that our model\nsurpasses existing algorithms for pixel-wise segmentation of surgical\ninstruments in both prediction accuracy and segmentation time of\nhigh-resolution videos."
}