# Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning

Alexey A. Shvets, Alexander Rakhlin, Alexandr A. Kalinin, and Vladimir I. Iglovikov

## 🧩 Problem to Solve

로봇 보조 수술에서 수술 도구의 정확한 위치를 감지하여 추적 및 자세 추정에 활용하는 것은 매우 중요합니다. 이를 위해서는 수술 영상에서 도구를 픽셀 단위로 정확하게 분할하는 것이 필수적입니다. 이 문제는 그림자 및 반사와 같은 조명 변화, 혈액 및 카메라 렌즈 김서림과 같은 시각적 가림, 그리고 배경 조직의 복잡하고 동적인 특성으로 인해 어렵습니다. 본 논문은 이러한 도구-배경 이진 분할 문제와 다양한 도구 또는 도구 부위를 식별하는 다중 클래스 분할 문제를 해결하고자 합니다.

## ✨ Key Contributions

- 로봇 수술 도구 분할을 위한 딥러닝 기반의 새로운 접근 방식을 제안합니다.
- 다양한 새로운 딥 신경망 아키텍처를 활용하여 이진 분할 및 다중 클래스 분할 문제 모두에서 최첨단(state-of-the-art) 성능을 달성했습니다.
- MICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation에서 상위권 결과를 획득했습니다.
- 제안된 솔루션의 소스 코드를 공개하여 연구 커뮤니티에 기여했습니다.

## 📎 Related Works

- 로봇 도구 감지 및 추적을 위한 기존의 비전 기반 방법들.
- 색상 및/또는 텍스처 특징을 사용한 이진 또는 인스턴스 분할에 적용된 고전적인 머신러닝 알고리즘들.
- 다양한 도구 또는 그 부위를 구별하는 의미론적 분할에 초점을 맞춘 초기 연구들.
- 의료 영상 분야에서 컨볼루션 신경망(CNN)의 성공적인 적용 사례들 (예: 유방암 조직 분석, 뼈 질환 예측 등).
- 로봇 도구 분할에 적용된 이전 딥러닝 기반 연구들 (이진 분할에서 경쟁력 있는 성능, 다중 클래스 분할에서 유망한 결과).
- U-Net, TernausNet, LinkNet과 같은 현대적인 인코더-디코더 기반 아키텍처들.

## 🛠️ Methodology

- **데이터셋**:
  - 다빈치 Xi 수술 시스템에서 얻은 8개의 225프레임 고해상도 스테레오 카메라 영상 시퀀스 사용 (돼지 수술).
  - 해상도는 1920x1080 픽셀 RGB이며, 1280x1024 픽셀 카메라 이미지를 추출하기 위해 크롭됩니다.
  - 왼쪽 프레임에 대해서만 수동으로 라벨링된 Ground Truth가 제공됩니다. 도구의 관절 부분(경직 샤프트, 관절 손목, 집게)과 배경이 10, 20, 30, 40, 0의 수치 값으로 인코딩됩니다.
  - 도구 유형 라벨(왼쪽/오른쪽 ProGrasp 겸자, 단극성 구부러진 가위, 큰 바늘 드라이버, 기타)도 포함됩니다.
- **네트워크 아키텍처**:
  - **U-Net**: 컨텍스트를 포착하는 수축 경로와 정밀한 위치화를 가능하게 하는 대칭적 확장 경로로 구성된 인코더-디코더 아키텍처. 스킵 연결을 통해 고해상도 특징이 전달됩니다. 원본 U-Net의 약간 수정된 버전을 사용했습니다.
  - **TernausNet**: ImageNet으로 사전 훈련된 VGG11 또는 VGG16 네트워크를 인코더로 사용하는 U-Net 계열 아키텍처. VGG16이 더 좋은 성능을 보였습니다.
  - **LinkNet**: ImageNet으로 사전 훈련된 ResNet34 기반 인코더를 사용하는 모델. 각 디코더 블록은 해당 인코더 블록으로부터 정보를 추가받습니다.
- **훈련**:
  - **평가 지표**: Jaccard 지수 (Intersection Over Union, IoU)를 사용합니다.
    $$J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}$$
    픽셀 기반으로 재정의하면:
    $$J = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{y_i \hat{y_i}}{y_i + \hat{y_i} - y_i \hat{y_i}}\right)$$
    여기서 $y_i$는 실제 라벨, $\hat{y_i}$는 픽셀 $i$에 대한 예측 확률입니다.
  - **손실 함수**: Jaccard 지수와 일반적인 분류 손실 함수 $H$ (이진 분할의 경우 이진 교차 엔트로피, 다중 클래스 분할의 경우 범주형 교차 엔트로피)를 결합한 일반화된 손실 함수 $L = H - \log J$를 사용합니다.
  - **출력 처리**: 모델의 출력은 각 픽셀이 관심 영역 또는 클래스에 속할 확률을 나타내는 이미지입니다. 이진 분할의 경우, 검증 데이터셋을 통해 0.3의 임계값을 사용하여 픽셀 확률을 0 또는 255로 이진화합니다. 다중 클래스 분할의 경우, 각 클래스에 다른 정수 값을 설정합니다.

## 📊 Results

- **이진 분할**:
  - TernausNet-16 모델이 IoU = 0.836, Dice = 0.901로 가장 우수한 성능을 보였습니다. 이는 현재까지 보고된 결과 중 최고 수준입니다.
- **다중 클래스 분할 (도구 부위)**:
  - TernausNet-16 모델이 IoU = 0.655, Dice = 0.760으로 가장 우수한 성능을 보였습니다.
- **다중 클래스 분할 (도구 유형)**:
  - TernausNet-16 모델이 IoU = 0.346, Dice = 0.459로 성능이 상대적으로 낮게 나왔습니다. 이는 데이터셋 크기가 작고 일부 클래스가 훈련 데이터셋에 적게 나타나기 때문으로 분석됩니다.
- **추론 시간**:
  - LinkNet-34는 더 가벼운 인코더 덕분에 가장 빠른 모델이었으며, 1280x1024 픽셀 이미지에 대해 약 7ms의 추론 시간을 보였습니다 (TernausNet보다 두 배 이상 빠름).
  - 성능 측정은 NVIDIA GTX 1080 Ti GPU를 사용했습니다.

## 🧠 Insights & Discussion

- TernausNet-16은 이진 분할 및 도구 부위의 다중 클래스 분할에서 탁월한 성능을 입증하며, 현재까지 최고 수준의 결과를 제공합니다.
- 도구 유형별 다중 클래스 분할의 낮은 성능은 데이터셋의 크기 부족과 특정 도구 유형의 샘플 부족에서 기인하며, 이는 더 큰 데이터셋으로 개선될 수 있음을 시사합니다.
- LinkNet-34는 TernausNet보다 훨씬 빠른 추론 속도를 제공하여, 실시간 애플리케이션에서 속도와 정확도 간의 트레이드오프를 고려할 때 유용한 대안이 될 수 있습니다.
- 제안된 모델들은 전체 이미지 해상도에서 효율적인 분석을 수행하는 종단 간 파이프라인을 구성합니다.
- 이러한 정확하고 견고한 수술 도구 분할 방법은 실시간 수술 도구 위치 감지를 위한 견고한 기반이 될 수 있으며, 이는 다시 수술 장면에서의 추적 및 자세 추정에 활용될 수 있습니다.

## 📌 TL;DR

**Problem:** 로봇 보조 수술 영상에서 수술 도구를 픽셀 단위로 정확하게 분할해야 합니다.
**Method:** U-Net 기반으로, ImageNet으로 사전 훈련된 VGG (TernausNet) 및 ResNet (LinkNet) 인코더를 활용한 딥러닝 모델을 제안합니다. Jaccard 지수와 교차 엔트로피를 결합한 손실 함수로 훈련했습니다.
**Findings:** TernausNet-16이 이진 분할 및 도구 부위 다중 클래스 분할에서 최첨단 성능을 달성했습니다. LinkNet-34는 TernausNet보다 두 배 이상 빠른 추론 속도를 보여 실시간 적용 가능성을 제시했습니다. 도구 유형별 다중 클래스 분할은 데이터셋 크기 제약으로 인해 성능이 상대적으로 낮았습니다.
