# Metric and Kernel Learning using a Linear Transformation

Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon

## 🧩 Problem to Solve

많은 기계 학습 알고리즘(예: 준지도 클러스터링, K-최근접 이웃 분류)은 객체 간의 유사성 또는 거리를 계산해야 합니다. 유클리드 거리나 코사인 유사도와 같은 일반적인 거리 함수가 사용되지만, 모든 문제에 적합하지는 않습니다.

최근에는 데이터 객체를 비교하는 방법을 학습하는 데 많은 노력이 집중되었습니다. 특히, **마할라노비스 거리(Mahalanobis distance)**는 데이터에 선형 변환을 적용한 후 유클리드 거리를 계산하는 방식으로, 뛰어난 일반화 특성을 보입니다. 그러나 마할라노비스 거리는 두 가지 주요 단점을 가집니다:

1. **높은 차원 데이터 문제**: 데이터 차원에 따라 매개변수 수가 기하급수적으로 증가하여 고차원 데이터에 대한 거리 함수 학습이 어렵습니다.
2. **비선형 결정 경계 문제**: 데이터 세트가 비선형 결정 경계를 가질 경우 선형 변환 학습만으로는 불충분합니다.

후자의 단점을 해결하기 위해 **커널 학습(Kernel Learning)** 알고리즘이 개발되었지만, 많은 기존 커널 학습 방법은 학습된 커널이 새로운 데이터 포인트에 일반화되지 않는다는 한계(변환적(transductive) 설정)가 있으며, 대규모 데이터 세트에 적용하기에는 계산 비용이 높습니다.

이 논문은 고차원 데이터에서 메트릭 학습의 효율성을 높이고, 새로운 데이터 포인트에도 일반화될 수 있는 방법을 모색하여 기존의 한계를 극복하는 것을 목표로 합니다.

## ✨ Key Contributions

- **선형 변환 기반 메트릭 학습의 커널화**: 메트릭 학습을 입력 데이터의 선형 변환 학습 문제로 재정의하고, 이를 임의의 고차원 공간에서 메트릭(또는 커널 함수)을 학습하기 위해 효율적으로 커널화될 수 있음을 보여줍니다. 특히, LogDet(로그 디터미넌트) 발산 기반 프레임워크가 효과적입니다.
- **LogDet 발산의 효율성 및 확장성**: LogDet 발산은 양의 정부호 행렬에 대해서만 정의되어 최적화가 단순하며, 제안된 최적화 알고리즘은 수백만 개의 데이터 객체에 이르는 매우 큰 데이터 세트에도 확장 가능합니다.
- **일반적인 볼록 손실 함수의 커널화**: LogDet 정식화의 커널화 결과를 다른 광범위한 볼록 손실 함수로 확장하여, 학습된 커널 함수를 계산하고 평가할 수 있는 조건을 제시함으로써 메트릭 학습의 잠재적 적용 분야를 크게 확장합니다.
- **일반화 능력(Out-of-sample Extension)**: 기존 변환적 커널 학습 방법과 달리, 제안된 방법은 학습된 메트릭/커널이 새로운(훈련 세트에 없는) 데이터 포인트에 대해서도 일반화될 수 있도록 합니다. 이는 행렬 $W$가 무한 차원일 수 있지만, 제약된 훈련 데이터 포인트로 완전히 표현될 수 있음을 통해 이루어집니다.
- **대규모 데이터 세트를 위한 구조화된 메트릭 학습**: "항등(identity) 행렬 더하기 저랭크(low-rank)" 구조의 마할라노비스 거리(또는 커널) 함수를 학습하는 방법을 제안하여, 매개변수 수를 데이터 차원 또는 훈련 세트 크기에 선형적으로 비례하게 줄여 고차원 및 대규모 데이터에 효율적으로 확장합니다.
- **실험적 검증**: 컴퓨터 비전 및 텍스트 마이닝 분야의 대규모 실제 문제에 적용하여 기존 최첨단 기술보다 개선된 성능을 보여줍니다.

## 📎 Related Works

- **마할라노비스 거리 학습**:
  - **초기 연구**: Xing et al. [XNJR02]은 유사성 및 비유사성 제약 조건 하에서 마할라노비스 거리 학습을 위한 반정부호 계획법(SDP)을 제안했으나, 최적화가 느리고 성능이 좋지 않았습니다.
  - **대규모 마진 설정**: Weinberger et al. (LMNN) [WBS05]은 K-NN 분류를 위해 대규모 마진 설정으로 메트릭 학습을 정식화하고 부분 기울기 하강법과 교대 투영법으로 해결했습니다.
  - **클래스 붕괴**: Globerson and Roweis (MCML) [GR05]는 클래스 내 거리를 최소화하고 클래스 간 거리를 최대화하여 선형 변환을 학습했습니다.
  - **온라인 학습 및 커널 공간 확장**: Shalev-Shwartz et al. [SSSN04]은 온라인 메트릭 학습을 다루었고, 커널 공간에서 마할라노비스 거리 학습의 첫 시연을 제공했지만, 매개변수 업데이트에 입방 시간이 필요하여 비효율적이었습니다.
- **비선형 변환 기반 메트릭 학습**: NCA [GRHS04], CNN 기반 방법 [CHL05], 리만 메트릭 학습 [Leb06] 등이 있지만, 최적화가 비볼록하거나 계산 복잡성이 높았습니다.
- **커널 학습**:
  - **변환적 설정**: 많은 연구가 새로운 데이터 포인트에 일반화되지 않는 변환적 설정에 국한되었습니다 [KT03, KSD06, TRW05].
  - **일반화 가능한 커널**: 다중 커널 학습 [LCB$^+$04] 및 하이퍼커널 [OSW03]에 대한 연구가 있었지만, 종종 SDP를 사용하므로 대규모 학습 문제에는 비실용적이었습니다.
  - **최근 연구**: Chatpatanasiri et al. [CKTK08]은 LMNN, NCA와 같은 마할라노비스 거리 학습 알고리즘의 커널화를 다루었는데, 이는 본 논문의 일반적인 커널화 결과와 보완적입니다.
- **본 논문의 확장**: [DKJ$^+$07], [KSD06], [DD08]의 기존 연구를 확장하여 더 광범위한 볼록 손실 함수의 커널화를 특징짓고, LogDet 손실에 대한 더 자세한 분석 및 대규모 커널 학습에 대한 실험 결과를 포함합니다.

## 🛠️ Methodology

1. **마할라노비스 거리 및 매개변수화된 커널**:

   - 두 점 $x_i, x_j$ 사이의 제곱 마할라노비스 거리는 $d_W(x_i, x_j) = (x_i - x_j)^T W (x_i - x_j)$로 정의되며, 여기서 $W$는 양의 정부호 행렬입니다.
   - 이는 데이터에 선형 변환 $G$ ($W=G^T G$)을 적용하고 변환된 공간에서 유클리드 거리를 측정하는 것으로 볼 수 있습니다.
   - 비선형 문제를 다루기 위해 커널 공간으로 확장하여, 매핑 $\phi$를 적용한 후의 마할라노비스 거리는 $d_W(\phi(x_i), \phi(x_j)) = (\phi(x_i) - \phi(x_j))^T W (\phi(x_i) - \phi(x_j))$가 됩니다.
   - 이는 입력 커널 함수 $\kappa_0(x,y) = \phi(x)^T \phi(y)$가 주어졌을 때, 매개변수화된 커널 함수 $\kappa(x,y) = \phi(x)^T W \phi(y)$를 학습하는 문제와 동등합니다.
   - 학습은 주어진 유사성/비유사성 제약 조건(예: 같은 클래스 점은 유사, 다른 클래스 점은 비유사)을 기반으로 합니다.

2. **LogDet 메트릭 학습**:

   - 두 양의 정부호 행렬 $W, W_0$ 간의 LogDet 발산은 $D_{\ell d}(W, W_0) = \text{tr}(W W_0^{-1}) - \log \det(W W_0^{-1}) - d$로 정의됩니다.
   - 제약 조건을 만족하면서 $W_0=I$에 가장 가까운 $W$를 찾는 최적화 문제를 제안합니다:
     $$ \min*{W \succ 0} D*{\ell d}(W, I) \\ \text{s.t. } d_W(x_i, x_j) \le u, \quad (i, j) \in S \\ d_W(x_i, x_j) \ge \ell, \quad (i, j) \in D $$
   - LogDet 발산은 확장 가능하고 효율적으로 커널화될 수 있어 선택되었습니다.

3. **문제의 커널화 (핵심 정리)**:

   - LogDet 메트릭 학습 문제(차원 $d \times d$ 행렬 $W$ 최적화)와 커널 학습 문제(훈련 데이터 포인트 수 $n \times n$ 커널 행렬 $K$ 최적화) 간의 동등성을 증명합니다.
   - **정리 3.1**은 최적 해 $W^*$와 $K^*$가 $K^* = X^T W^* X$ 및 $W^* = I + X M X^T$ ($M=K_0^{-1}(K^* - K_0)K_0^{-1}$, $K_0 = X^T X$)로 관련되어 있음을 보여줍니다. 이 동등성은 아웃-오브-샘플(out-of-sample) 일반화를 가능하게 합니다.
   - 이는 브레그만(Bregman) 행렬 발산의 일반적인 이중 문제(dual problem)를 통해 증명됩니다.

4. **새로운 포인트로의 일반화**:

   - 훈련 세트에서 학습된 커널 행렬 $K^*$를 사용하여, 훈련 세트에 없는 두 새로운 포인트 $z_1, z_2$ 간의 학습된 거리를 계산할 수 있습니다.
   - $\phi(z_1)^T W \phi(z_2) = \kappa_0(z_1, z_2) + k_1^T M k_2$ ($k_i = [\kappa_0(z_i, x_1), ..., \kappa_0(z_i, x_n)]^T$) 공식으로 임의의 데이터 객체에 대한 커널화된 거리를 평가할 수 있습니다.

5. **커널 학습 알고리즘 (Algorithm 1)**:

   - LogDet 발산 기반 커널 학습 문제를 해결하기 위해 브레그만 투영(Bregman projections) 기술을 사용합니다 (슬랙 변수 포함).
   - 각 반복에서 제약 조건 $(i, j)$를 선택하고, $K$에 랭크-1(rank-one) 업데이트를 적용합니다: $K \leftarrow K + \beta K(e_i - e_j)(e_i - e_j)^T K$.
   - 이 업데이트는 $O(n^2)$ 시간 복잡도를 가지며, 양의 정부호성을 유지하고 고유 벡터 계산이나 SDP를 필요로 하지 않아 효율적입니다.

6. **대규모 데이터 세트를 위한 메트릭/커널 학습 (Identity Plus Low-Rank, IPLR)**:

   - 매개변수 수를 줄이기 위해 학습된 마할라노비스 거리 행렬을 $W = I_d + U L U^T$ 형태로 제한합니다 (여기서 $L \in S_{k \times k}^+$이고 $k \ll \min(n, d)$).
   - 이 문제는 $k \times k$ 행렬 $F = I_k + L$을 최적화하는 문제로 변환되며, $O(k^2)$ 계산 단계로 해결될 수 있습니다.
   - 이를 통해 $O(\min(n, d)k)$ 단계로 $W^*$ 또는 $K^*$를 저장하고 거리를 계산할 수 있어 대규모 고차원 데이터에 효율적입니다.

7. **다른 볼록 손실 함수를 이용한 커널화**:
   - 손실 함수가 $\text{tr}(f(W))$ 형태이고, $f$가 특정 조건을 만족하는 경우 (볼록성, 효율적인 서브-기울기 계산, $f(\eta)=0$ 만족), 메트릭 학습 문제 (4.1)가 커널화 가능함을 보입니다.
   - 이 증명은 최적의 $W^*$가 $W^* = \eta I_d + X S^* X^T$ 형태를 가지며, 문제를 $n \times n$ 행렬 $S^*$에 대한 최적화 문제로 재정의할 수 있음을 기반으로 합니다.
   - von Neumann 발산, 제곱 프로베니우스(Frobenius) 발산, 반정부호 계획법(SDPs) 등의 특수 케이스도 이 일반 프레임워크 내에서 커널화될 수 있음을 증명합니다.

## 📊 Results

- **저차원 데이터 세트 (UCI, Clarify 시스템)**:
  - UCI 벤치마크 데이터 세트에서 LogDet Linear (선형 커널) 및 LogDet Gaussian (가우시안 커널)은 기존 메트릭 학습 방법(Euclidean, Inverse Covariance, MCML, LMNN)보다 대부분의 경우 더 높은 분류 정확도를 달성했습니다.
  - Clarify 시스템의 소프트웨어 지원 문제에서 LogDet Linear는 Latex 데이터 세트에서 상당한 개선을 보였습니다.
  - LogDet Linear는 다른 알고리즘에 비해 학습 속도가 현저히 빨랐습니다.
  - 준지도 클러스터링(semi-supervised clustering)에서도 LogDet Linear는 HMRF-KMeans보다 우수한 성능을 보였습니다.
- **객체 인식 (Caltech-101 데이터 세트)**:
  - 고차원 이미지 데이터에 적용되어 최첨단 객체 인식 방법과 비교되었습니다.
  - 학습된 CORR 커널을 사용했을 때 $T=15$에서 61.0%, $T=30$에서 69.6%의 정확도를 달성하며, 기존 단일 커널 분류기 방법들을 능가했습니다. 학습된 SUM 커널은 $T=15$에서 73.7% 정확도를 달성했습니다.
  - 학습된 커널은 1-NN 분류 정확도를 비학습 기준 커널에 비해 크게 향상시켰으며, 심지어 SVM과 함께 사용된 기준 커널보다도 우수했습니다.
- **텍스트 분류 (CMU Newsgroups, Classic3)**:
  - 선형 커널과 클래스 레이블에서 구성된 기저 벡터를 사용하여 텍스트 데이터에 적용되었습니다.
  - 기저 크기가 증가함에 따라 LogDet Linear는 기준 유클리드 거리 및 잠재 의미 분석(LSA)보다 훨씬 높은 정확도를 달성했으며, LMNN과 경쟁력 있는 성능을 보였습니다.

## 🧠 Insights & Discussion

- **LogDet 발산의 효율성**: LogDet 발산은 메트릭 학습을 위한 유용한 손실 함수이며, 커널 공간에서 선형 변환을 효율적으로 학습할 수 있도록 합니다. 이는 기존 커널 학습 방법의 한계였던 새로운 데이터 포인트로의 일반화 문제를 해결하고, 브레그만 투영 기반 알고리즘을 통해 대규모 데이터 세트에서도 효율적으로 작동합니다.
- **일반화 및 확장성**: 본 논문에서 제시된 프레임워크는 $W$ 행렬이 무한 차원일 수 있더라도, 제약된 훈련 데이터 포인트로 완전히 표현될 수 있음을 통해 학습된 커널이 새로운 데이터 포인트에도 일반화될 수 있게 합니다. 또한 "항등 더하기 저랭크(IPLR)" 메트릭 학습을 통해 매개변수 수를 데이터 차원 또는 훈련 세트 크기에 선형적으로 비례하게 줄여, 고차원 및 대규모 데이터 문제를 효과적으로 해결합니다.
- **광범위한 적용 가능성**: LogDet 발산 외에도 다양한 볼록 손실 함수에 대한 커널화 가능성을 증명함으로써, 본 프레임워크가 메트릭 학습의 광범위한 응용 분야에 적용될 수 있음을 보여줍니다. 이는 기존의 많은 메트릭 학습 알고리즘을 커널 공간으로 확장할 수 있는 이론적 기반을 제공합니다.
- **실험적 우월성**: 저차원 UCI 데이터 세트부터 고차원 컴퓨터 비전 및 텍스트 마이닝 문제에 이르기까지 다양한 분야에서 기존 최첨단 기술에 비해 일관된 성능 향상을 입증하여, 제안된 방법의 실용적 가치와 효과를 강력하게 지지합니다.
- **향후 연구 방향**:
  - IPLR 방법에서 기저(basis) 선택의 중요성을 강조하며, 더 신중한 기저 선택이 정확도를 향상시킬 수 있는 잠재적 연구 분야임을 시사합니다.
  - 더 큰 데이터 세트를 위한 온라인 학습 방법 및 LogDet 발산을 활용한 다중 지역 메트릭 학습에 대한 연구가 유망한 방향으로 제시됩니다.

## 📌 TL;DR

- **문제**: 기존 메트릭 학습(마할라노비스 거리)은 고차원 데이터 및 비선형 결정 경계에 취약하며, 많은 커널 학습 방법은 새로운 데이터에 일반화되지 않는 한계를 가집니다.
- **방법**: LogDet 발산을 활용한 메트릭 학습을 제안하며, 이 방법이 고차원 특성 공간에서 선형 변환(또는 등가 커널)을 효율적으로 "커널화"하여 새로운 데이터에도 일반화될 수 있음을 보였습니다. 대규모 데이터 세트의 경우, 매개변수 수를 선형적으로 확장 가능한 "항등 더하기 저랭크" 근사를 도입했습니다. 또한, 광범위한 볼록 손실 함수에 대한 커널화 가능성을 이론적으로 증명했습니다.
- **발견**: LogDet 기반 접근 방식은 저차원 UCI 데이터, 고차원 객체 인식(Caltech-101), 텍스트 분류(Newsgroups, Classic3) 등 다양한 벤치마크 문제에서 기존 메트릭 학습 방법보다 분류 정확도를 크게 향상시켰으며, 계산적으로도 효율적이고 새로운 데이터 포인트에 효과적으로 일반화됩니다.
