{
  "title": "Deep Metric Learning with Hierarchical Triplet Loss",
  "authors": "Weifeng Ge, Weilin Huang, Dengke Dong, Matthew R. Scott",
  "year": 2018,
  "url": "http://arxiv.org/abs/1810.06951v1",
  "abstract": "We present a novel hierarchical triplet loss (HTL) capable of automatically\ncollecting informative training samples (triplets) via a defined hierarchical\ntree that encodes global context information. This allows us to cope with the\nmain limitation of random sampling in training a conventional triplet loss,\nwhich is a central issue for deep metric learning. Our main contributions are\ntwo-fold. (i) we construct a hierarchical class-level tree where neighboring\nclasses are merged recursively. The hierarchical structure naturally captures\nthe intrinsic data distribution over the whole database. (ii) we formulate the\nproblem of triplet collection by introducing a new violate margin, which is\ncomputed dynamically based on the designed hierarchical tree. This allows it to\nautomatically select meaningful hard samples with the guide of global context.\nIt encourages the model to learn more discriminative features from visual\nsimilar classes, leading to faster convergence and better performance. Our\nmethod is evaluated on the tasks of image retrieval and face recognition, where\nit outperforms the standard triplet loss substantially by 1%-18%. It achieves\nnew state-of-the-art performance on a number of benchmarks, with much fewer\nlearning iterations.",
  "citation": 528
}