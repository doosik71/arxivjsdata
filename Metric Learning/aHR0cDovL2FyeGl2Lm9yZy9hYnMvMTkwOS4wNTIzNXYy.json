{
  "title": "SoftTriple Loss: Deep Metric Learning Without Triplet Sampling",
  "authors": "Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, Rong Jin",
  "year": 2019,
  "url": "http://arxiv.org/abs/1909.05235v2",
  "abstract": "Distance metric learning (DML) is to learn the embeddings where examples from\nthe same class are closer than examples from different classes. It can be cast\nas an optimization problem with triplet constraints. Due to the vast number of\ntriplet constraints, a sampling strategy is essential for DML. With the\ntremendous success of deep learning in classifications, it has been applied for\nDML. When learning embeddings with deep neural networks (DNNs), only a\nmini-batch of data is available at each iteration. The set of triplet\nconstraints has to be sampled within the mini-batch. Since a mini-batch cannot\ncapture the neighbors in the original set well, it makes the learned embeddings\nsub-optimal. On the contrary, optimizing SoftMax loss, which is a\nclassification loss, with DNN shows a superior performance in certain DML\ntasks. It inspires us to investigate the formulation of SoftMax. Our analysis\nshows that SoftMax loss is equivalent to a smoothed triplet loss where each\nclass has a single center. In real-world data, one class can contain several\nlocal clusters rather than a single one, e.g., birds of different poses.\nTherefore, we propose the SoftTriple loss to extend the SoftMax loss with\nmultiple centers for each class. Compared with conventional deep metric\nlearning algorithms, optimizing SoftTriple loss can learn the embeddings\nwithout the sampling phase by mildly increasing the size of the last fully\nconnected layer. Experiments on the benchmark fine-grained data sets\ndemonstrate the effectiveness of the proposed loss function. Code is available\nat https://github.com/idstcv/SoftTriple",
  "citation": 518
}