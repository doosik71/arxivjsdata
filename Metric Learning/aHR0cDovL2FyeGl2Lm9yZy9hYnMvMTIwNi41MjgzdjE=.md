# Bayesian Active Distance Metric Learning

Liu Yang, Rong Jin, Rahul Sukthankar

## 🧩 Problem to Solve

기존의 거리 측정 학습(Distance Metric Learning, DML) 방법론은 주로 다음과 같은 두 가지 주요 문제에 직면해 있었습니다:

- **신뢰성 부족**: 대부분의 알고리즘은 거리 측정에 대한 "점 추정(point estimation)"만을 제공하며, 이는 훈련 예제 수가 적을 때 모델의 불확실성을 충분히 고려하지 못하여 신뢰성이 떨어질 수 있습니다.
- **레이블링 비효율성**: 훈련 예제를 무작위로 선택하는 경향이 있어, 수동 레이블링에 필요한 노력이 제한적일 때 가장 정보 가치가 높은 예제를 효율적으로 식별하지 못하여 학습 과정이 비효율적입니다.

이 논문은 적은 수의 레이블된 데이터로도 신뢰할 수 있는 거리 측정을 학습하고, 레이블링 효율성을 극대화하기 위해 가장 유익한 훈련 예제를 능동적으로 선택하는 방법을 제시하는 것을 목표로 합니다.

## ✨ Key Contributions

- **베이즈(Bayesian) DML 프레임워크 제안**: 레이블된 쌍별 제약 조건(pairwise constraints)으로부터 거리 측정에 대한 사후 분포(posterior distribution)를 추정하는 베이즈 프레임워크를 제시하여, 거리 측정의 불확실성을 모델링합니다.
- **효율적인 베이즈 DML 알고리즘 개발**: 제안된 베이즈 접근 방식을 위해 변분 방법(variational method)에 기반한 효율적인 알고리즘을 설계하여 복잡한 사후 분포 추정 문제를 해결합니다.
- **능동적 DML(Active DML)로 확장**: 베이즈 프레임워크를 활용하여 능동 거리 측정 학습을 수행합니다. 이는 상대적 거리에 대한 불확실성이 가장 큰 레이블되지 않은 예제 쌍을 선별하여 레이블링 효율성을 극대화하는 방식입니다.
- **우수한 성능 입증**: 분류 태스크에 대한 실험을 통해, 제안된 프레임워크가 비베이즈 방식 및 최신 DML 알고리즘보다 더 높은 분류 정확도를 달성하고, 더 정보 가치가 높은 훈련 예제를 식별함을 보여줍니다.

## 📎 Related Works

- **거리 측정 학습 (Distance Metric Learning)**:
  - Xing et al. (2003): 제약된 볼록 계획법(constrained convex programming)을 사용하여 같은 클래스 내 데이터 포인트 간 거리를 최소화하고 다른 클래스는 분리합니다.
  - Kwok & Tsang (2003): 커널(kernels)을 도입하여 비선형 DML로 확장했습니다.
  - Bar-Hillel et al. (2003) (RCA), Hoi et al. (2006) (DCA, Kernel DCA): 동등 제약(equivalence constraints)으로부터 전역 선형 변환을 학습합니다.
  - Goldberger et al. (2005) (NCA), Weinberger et al. (2006) (LMNN): 최신접 이웃(Nearest Neighbor) 분류기를 확장하여 거리 측정을 학습하는 최신 기법입니다.
  - **한계점**: 대부분의 기존 DML 알고리즘은 거리 측정의 점 추정만을 제공하며, 훈련 예제가 적을 때 신뢰성이 떨어지고, 무작위 샘플링을 가정하여 비효율적이라는 본 연구의 문제점을 공유합니다.
- **능동 학습 (Active Learning)**:
  - 주로 분류 문제에 초점을 맞추며, 가장 불확실한 예제를 선택하여 레이블링 효율을 높이는 것이 핵심입니다.
  - 불확실성 측정 방법: 앙상블 모델의 불일치(Seung et al., 1992), 결정 경계와의 거리(Tong & Koller, 2000b), 피셔 정보 행렬(MacKay, 1992) 등이 사용되었습니다.
  - 베이즈 분석을 능동 학습에 적용한 연구도 존재합니다(Tong & Koller, 2000a; Jin & Si, 2004).
  - 본 연구와 유사하게 정보성 높은 예제 쌍 선택에 중점을 둔 연구(X. Zhu & Ghahramani, 2003; Sugato Basu & Mooney, 2004)도 있으나, 이는 서수 회귀(ordinal regression) 또는 데이터 클러스터링을 목표로 합니다.

## 🛠️ Methodology

### 3.1 베이즈 프레임워크 (The Bayesian Framework)

- **유사성 확률 정의**: 두 데이터 포인트 $x_i$와 $x_j$가 주어진 거리 측정 $A$ 및 임계값 $\mu$ 하에 동등($y_{i,j}=+1$) 또는 비동등($y_{i,j}=-1$) 제약 조건을 형성할 확률은 로지스틱 함수 형태로 정의됩니다:
  $$
  Pr(y_{i,j}|x_i,x_j,A, \mu) = \frac{1}{1 + \exp(y_{i,j}(\|x_i-x_j\|_{A}^{2}-\mu))} \quad (1)
  $$
  여기서 $\|x_i-x_j\|_{A}^{2} = (x_i-x_j)^{T}A(x_i-x_j)$는 마할라노비스 거리 제곱입니다.
- **전체 우도 함수(Likelihood Function)**: 모든 레이블된 동등 제약 조건 $S$와 비동등 제약 조건 $D$에 대한 전체 우도는 개별 확률의 곱으로 표현됩니다 (식 2 참조).
- **사전 분포(Priors)**: 거리 측정 행렬 $A$에 대해 Wishart 분포를, 임계값 $\mu$에 대해 Gamma 분포를 사전 분포로 설정합니다.
- **사후 분포 추정**: 베이즈 정리를 통해 $Pr(A, \mu|S,D)$를 계산하며, 이 적분은 계산적으로 복잡합니다.

### 3.2 효율적인 알고리즘 (An Efficient Algorithm)

- **3.2.1 고유값 근사 (Eigen Approximation)**:
  - 계산 복잡성을 줄이기 위해, 거리 측정 $A$를 데이터의 상위 $K$개 고유 벡터 $v_l$의 선형 조합 $A = \sum_{l=1}^{K} \gamma_l v_l v_{l}^{T}$으로 근사합니다. 여기서 $\gamma_l \geq 0$은 조합 가중치입니다.
  - 이 근사를 통해 $Pr(y_{i,j}|x_i,x_j)$는 $\gamma = (\mu, \gamma_1, ..., \gamma_K)$와 $\omega_{i,j}$의 내적 형태로 다시 쓸 수 있습니다 (식 6 참조).
  - $A$와 $\mu$의 사전 분포를 $\gamma$에 대한 단일 Gaussian 분포 $N(\gamma; \gamma_0 1_{K+1}, \delta^{-1} I_{K+1})$로 완화하여 근사합니다.
- **3.2.2 변분 근사 (Variational Approximation)**:
  - $\gamma$의 사후 분포를 추정하기 위해 변분 방법(variational method)을 사용합니다. 이는 증거 함수 $\log Pr(S,D)$의 하한을 최대화하는 방식으로 $\gamma$의 사후 분포를 Gaussian 분포 $\phi(\gamma) \sim N(\gamma; \mu_{\gamma}, \Sigma_{\gamma})$로 근사합니다.
  - EM(Expectation-Maximization)과 유사한 반복 과정을 통해 $\mu_{\gamma}$와 $\Sigma_{\gamma}$ (식 9, 10 참조) 및 변분 파라미터 $\xi_{s}^{i,j}, \xi_{d}^{i,j}$ (식 11 참조)를 번갈아 업데이트합니다.

### 4. 베이즈 능동 거리 측정 학습 (Bayesian Active Distance Metric Learning)

- **정보성 쌍 선택**: 능동 학습의 "불확실성 원리"에 따라, 두 데이터 포인트가 서로 가까운지 여부를 결정하는 데 가장 큰 불확실성을 가진 예제 쌍을 선택합니다. 이 불확실성은 엔트로피 함수 $H_{i,j}$ (식 12)로 측정됩니다.
- **$Pr(\pm|x_i,x_j)$ 효율적인 계산**:
  - $\gamma$의 전체 분포를 고려하여 조건부 확률 $Pr(\pm|x_i,x_j)$를 계산해야 합니다.
  - 이는 Laplacian 근사(Laplacian approximation)를 통해 효율적으로 수행됩니다. $\mu_{\gamma}$ 근처에서 $l_{i,j}^{\pm}(\gamma)$를 Taylor 확장으로 근사하고, 이를 통해 $\gamma$의 평균($\mu_{\gamma}$)과 공분산($\Sigma_{\gamma}$)을 모두 고려한 $Pr(\pm|x_i,x_j)$ 값을 계산합니다. 이 방법은 각 후보 쌍에 대한 비용이 많이 드는 최적화 문제를 피하면서 확률을 효과적으로 추정합니다.

## 📊 Results

- **데이터셋**: COREL 이미지 데이터베이스 (5개 카테고리, 500개 이미지)와 Spoken Letter Recognition (ISOLET, 10개 클래스, 1000개 예제) 두 가지 데이터셋을 사용했습니다.
- **평가 방법**: 1NN(Nearest Neighbor) 분류기의 분류 정확도를 사용하여 학습된 거리 측정의 품질을 평가했습니다.
- **적은 훈련 예제에서의 DML 성능**:
  - **BAYES vs. MLE**: 베이즈 방식(BAYES)은 최대 우도 추정(MLE) 방식보다 모든 경우에서 1NN 분류기의 정확도를 유의미하게 향상시켰습니다. 특히, 훈련 예제 수가 10개로 적을 때 MLE가 유클리드 거리보다 성능이 떨어지는 반면, BAYES는 우수한 성능을 보여 베이즈 접근 방식의 중요성을 강조했습니다.
  - **BAYES vs. 최신 DML**: BAYES는 NCA(Neighborhood Component Analysis) 및 LMNN(Maximum Margin Nearest Neighbor Classifier)과 같은 최신 DML 알고리즘보다 모든 설정에서 약간 더 나은 성능을 보였습니다.
- **능동적 DML 성능**:
  - **능동 학습의 효율성**: MLE+ACT (능동 학습)는 MLE+RAND (무작위 선택)보다 모든 반복에서 1NN 분류 정확도를 유의미하게 향상시켜, 능동 학습이 정보성 높은 예제 쌍을 식별하는 데 더 효과적임을 입증했습니다.
  - **베이즈 능동 학습의 우수성**: BAYES+ACT ($\gamma$의 평균만 활용)와 비교하여 BAYES+VAR ($\gamma$의 평균과 공분산 모두 활용)은 1NN 분류 정확도 향상에 더 효과적임을 보여주었습니다. 특히 COREL 데이터셋에서 BAYES+VAR이 BAYES+ACT를 크게 능가했습니다. ISOLET 데이터셋에서도 초기 훈련 쌍 수가 10개, 20개일 때 BAYES+VAR이 BAYES+ACT보다 명확한 이점을 보였습니다. 이는 $\gamma$의 불확실성(공분산)을 고려하는 것이 능동 학습 성능에 중요함을 시사합니다.

## 🧠 Insights & Discussion

- **베이즈 프레임워크의 장점**: 이 연구는 베이즈 프레임워크를 DML에 적용함으로써, 기존 점 추정 방식의 한계를 극복하고 거리 측정의 사후 분포를 추정하여 모델의 불확실성을 정량화할 수 있음을 보여주었습니다. 이는 특히 훈련 데이터가 부족한 시나리오에서 학습된 거리 측정의 신뢰성을 크게 향상시킵니다.
- **능동 학습의 효과**: 사후 분포에서 파생된 불확실성 측정 지표를 활용하여 가장 유익한 예제 쌍을 능동적으로 선택하는 전략은 레이블링 노력을 최소화하면서도 모델 성능을 극대화하는 데 매우 효과적입니다. 이는 실제 애플리케이션에서 레이블링 예산이 제한적일 때 중요한 의미를 가집니다.
- **효율적인 근사 방법**: 변분 방법과 라플라스 근사 기법을 사용하여 복잡한 베이즈 추론을 효율적으로 수행할 수 있음을 입증했으며, 이는 베이즈 DML의 실용성을 높이는 데 기여합니다.
- **향후 연구**: ISOLET 데이터셋에서 초기 훈련 쌍 수가 많아질수록 BAYES+VAR과 BAYES+ACT의 성능 차이가 줄어드는 현상에 대한 추가 조사가 필요하다고 언급되어, 데이터 양이 충분할 때 공분산 정보의 상대적 중요도 변화에 대한 탐색 가능성을 제시했습니다.

## 📌 TL;DR

이 논문은 적은 훈련 데이터로 인한 거리 측정 학습(DML)의 신뢰성 부족과 레이블링 비효율성 문제를 해결하기 위해 **베이즈(Bayesian) DML 프레임워크**를 제안합니다. 이 프레임워크는 거리 측정의 **사후 분포**를 추정하여 모델의 불확실성을 정량화하며, 변분 방법 기반의 효율적인 알고리즘으로 계산됩니다. 나아가, 이 베이즈 프레임워크를 활용하여 **능동적 DML**을 구현, 상대적 거리에 대한 불확실성이 가장 큰 레이블되지 않은 예제 쌍을 능동적으로 선택함으로써 레이블링 효율성을 극대화합니다. 실험 결과, 제안된 베이즈 DML은 적은 훈련 데이터 환경에서 기존 최신 DML 방법보다 뛰어난 분류 정확도를 달성했으며, 특히 거리 측정의 평균과 공분산 모두를 활용하는 능동 학습 방식(BAYES+VAR)이 가장 효과적임을 입증했습니다.
