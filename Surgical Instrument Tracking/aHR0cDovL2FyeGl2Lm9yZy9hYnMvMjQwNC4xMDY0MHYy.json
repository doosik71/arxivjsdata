{
  "title": "Adapting SAM for Surgical Instrument Tracking and Segmentation in\n  Endoscopic Submucosal Dissection Videos",
  "authors": "Jieming Yu, Long Bai, Guankun Wang, An Wang, Xiaoxiao Yang, Huxin Gao, Hongliang Ren",
  "year": 2024,
  "url": "http://arxiv.org/abs/2404.10640v2",
  "abstract": "The precise tracking and segmentation of surgical instruments have led to a\nremarkable enhancement in the efficiency of surgical procedures. However, the\nchallenge lies in achieving accurate segmentation of surgical instruments while\nminimizing the need for manual annotation and reducing the time required for\nthe segmentation process. To tackle this, we propose a novel framework for\nsurgical instrument segmentation and tracking. Specifically, with a tiny subset\nof frames for segmentation, we ensure accurate segmentation across the entire\nsurgical video. Our method adopts a two-stage approach to efficiently segment\nvideos. Initially, we utilize the Segment-Anything (SAM) model, which has been\nfine-tuned using the Low-Rank Adaptation (LoRA) on the EndoVis17 Dataset. The\nfine-tuned SAM model is applied to segment the initial frames of the video\naccurately. Subsequently, we deploy the XMem++ tracking algorithm to follow the\nannotated frames, thereby facilitating the segmentation of the entire video\nsequence. This workflow enables us to precisely segment and track objects\nwithin the video. Through extensive evaluation of the in-distribution dataset\n(EndoVis17) and the out-of-distribution datasets (EndoVis18 & the endoscopic\nsubmucosal dissection surgery (ESD) dataset), our framework demonstrates\nexceptional accuracy and robustness, thus showcasing its potential to advance\nthe automated robotic-assisted surgery.",
  "citation": 5
}