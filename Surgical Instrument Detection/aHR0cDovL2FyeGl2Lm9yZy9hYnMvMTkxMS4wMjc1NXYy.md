# Automatic Tip Detection of Surgical Instruments in Biportal Endoscopic Spine Surgery

Sue Min Cho, Young-Gon Kim, Jinhoon Jeong, Ho-jin Lee, Namkug Kim

## 🧩 Problem to Solve

일반적인 내시경 수술에서 외과의는 한 손으로 내시경을 잡고 다른 한 손으로 수술 기구를 조작해야 하므로, 수술 중 양손 사용의 제약이 있습니다. 보조자가 내시경을 잡아주는 경우 추가 비용이 발생하고 의사소통 문제가 발생할 수 있습니다. 로봇 내시경을 도입하면 외과의의 한 손을 자유롭게 하여 양손 조작과 더 정교한 기술 사용이 가능해지지만, 이를 위해서는 로봇이 내시경을 자율적으로 제어할 수 있는 정교한 시각 지능이 필수적입니다. 특히 양방향 내시경 척추 수술(Biportal Endoscopic Spine Surgery, BESS)의 경우, 수술 기구 끝단(tip)의 자동 감지 및 정확한 위치 파악 기술이 로봇 내시경 컨트롤러의 입력으로 활용되기 위해 필요합니다.

## ✨ Key Contributions

* BESS 비디오에서 수술 기구 끝단을 자동으로 감지하고 위치를 파악하는 딥러닝 기반 방법을 제안했습니다.
* RetinaNet과 YOLOv2, 두 가지 최첨단 객체 탐지 모델의 성능을 비교 분석하여 BESS 환경에서 기구 끝단 감지에 대한 적합성을 평가했습니다.
* 바운딩 박스의 마진 크기에 따른 최적의 성능을 체계적으로 분석하여, RetinaNet의 경우 150픽셀 마진에서 가장 높은 F1-score (0.846)와 완벽한 재현율(1.000)을 달성함을 확인했습니다.
* 개발된 방법의 전반적인 견고성(robustness)을 검증하기 위해 9-겹 교차 검증을 수행하여 안정적인 감지 성능을 입증했습니다.
* BESS와 같이 수술 기구 추적 연구가 부족했던 특정 내시경 수술 분야에 대한 새로운 시각 지능 솔루션을 제시했습니다.
* 이러한 기구 끝단 감지 기술이 로봇 내시경의 실시간 제어 입력으로 활용될 수 있는 잠재력을 보여주었습니다.

## 📎 Related Works

* **전통적인 비전 기반 방법:**
  * Doignon C. et al. [8]: 색상 속성을 활용하여 수술 기구의 회색 영역을 감지하고 위치를 파악했습니다.
  * Wolf R. et al. [9]: 기구의 기하학적 모델과 확률적 응축 알고리즘을 사용한 3D 기구 추적 방법을 제안했습니다.
  * Uecker D. et al. [10]: 로봇 보조 복강경 수술에서 자동 이미지 분석 및 로봇 시각 서보 제어를 기반으로 한 기구 추적을 시도했습니다.
  * 한계: 이러한 방법들은 수작업으로 정의된 특징에 의존하며, 실시간 추론이 어렵고, 지저분하거나 가려진 이미지에 대한 확장성이 부족합니다.
* **딥러닝 기반 방법:**
  * EndoNet [13]: 복강경 비디오에서 도구 존재 감지 및 단계 인식을 위한 최초의 CNN 기반 모델입니다.
  * Du X. et al. [14]: 완전 합성곱 탐지-회귀 네트워크를 사용하여 다중 기구의 관절형 2D 포즈 추정을 수행했습니다.
  * Zhao Z. et al. [15]: 공간 변환 네트워크 및 시공간 컨텍스트를 기반으로 한 수술 기구의 실시간 자동 추적 방법을 제시했습니다.
  * Vardazaryan A. et al. [16] 및 Nwoye et al. [17]: 명시적인 공간 주석 없이 도구를 위치 파악하는 약한 지도 학습(weakly supervised learning) 접근법을 제안했습니다.
  * LapTool-Net [18]: 순환 합성곱 신경망(RCNN) 기반의 수술 도구 문맥 감지기를 개발했습니다.
  * 이전 연구들은 주로 m2cai16-tool, RMIT, EndoVis, Cholec80과 같은 공개 데이터셋과 망막 미세수술, 담낭절제술, 복강경 수술과 같은 분야에 집중했습니다. 본 연구는 BESS 분야에서의 기구 끝단 감지라는 새로운 영역을 다룹니다.

## 🛠️ Methodology

1. **데이터셋 구축 및 주석:**
    * 총 9개의 BESS 비디오에서 300프레임마다 정지 이미지(640x480 해상도)를 추출하여 2310개의 이미지를 확보했습니다.
    * 각 이미지에는 전문가가 수술 기구 끝단(x, y) 좌표를 주석 처리했습니다. 기구 끝단이 가려진 경우, 가장 가까운 명확하게 보이는 지점을 끝단으로 정의했습니다.
    * 데이터셋은 비디오 단위로 훈련, 검증, 테스트 세트로 7:1:1 비율로 분할했습니다 (훈련 7개, 검증 1개, 테스트 1개 비디오).
2. **모델 학습 및 바운딩 박스 설정:**
    * **RetinaNet [20]**과 **YOLOv2 [21]** 두 가지 딥러닝 객체 탐지 모델을 사용했습니다.
    * 주석 처리된 기구 끝단 좌표를 중심으로 다양한 마진 크기 ($M$)의 정사각형 바운딩 박스($M \times M$)를 생성했습니다. 실험에서는 $M$ 값을 50, 100, 150, 200 픽셀로 설정했습니다.
    * **RetinaNet:** ImageNet 사전 훈련된 ResNet50을 백본으로 사용하고, Smooth L1 loss와 Focal loss를 적용했습니다. Adam 옵티마이저를 사용했습니다.
    * **YOLOv2:** Darknet19 사전 훈련된 모델을 미세 조정(fine-tune)했으며, Sum-squared error (분류, 위치, 신뢰도 손실 포함)를 손실 함수로 사용하고 Adam 옵티마이저를 적용했습니다.
3. **추론 및 결과 처리:**
    * 각 이미지에 하나의 기구만 존재한다는 특성을 고려하여, 신뢰도가 0.1을 초과하는 예측 중 가장 높은 신뢰도를 가진 하나의 바운딩 박스만 최종 결과로 출력하도록 알고리즘을 수정했습니다.
4. **성능 측정:**
    * 예측된 바운딩 박스의 중간점(midpoint)을 '예측된 기구 끝단'으로 간주했습니다.
    * 모델 간 공정한 비교를 위해 ground truth 끝단과 예측된 끝단에 대해 고정된 박스 크기(194x192 픽셀)를 적용했습니다.
    * IoU (Intersection over Union)가 0.5를 초과하는 경우를 "성공적인 예측"으로 정의하여 재현율(Recall), 정밀도(Precision), F1-score를 계산했습니다.
5. **9-겹 교차 검증:**
    * 더 나은 성능을 보인 RetinaNet 모델과 최적 마진 크기를 사용하여 9-겹 교차 검증을 수행하여 전반적인 모델의 견고성과 일반화 성능을 평가했습니다.

## 📊 Results

1. **RetinaNet 성능:**
    * 평균 훈련 시간: 2.5시간. 평균 추론 시간: $13.21 \pm 0.63$ fps.
    * **150 픽셀 마진에서 최적 성능:** 재현율(Recall) 1.000, 정밀도(Precision) 0.733, F1-score 0.846을 달성했습니다.
    * 이 최적의 경우, 176개의 True Positives, 64개의 False Positives, 0개의 False Negatives를 기록했습니다.
    * 시각적 분석 결과, 기구가 명확하게 보이고 이미지 중앙에 위치할 때 끝단 감지가 우수했습니다. 기구가 멀리 있거나 그림자, 움직임 흐림, 폐색이 발생할 때 감지 난이도가 증가했습니다.
2. **YOLOv2 성능:**
    * 평균 훈련 시간: 3.5시간. 평균 추론 시간: 71.43 fps.
    * **150 픽셀 마진에서 최적 성능:** F1-score 0.835 (재현율 0.864, 정밀도 0.808)를 달성했습니다.
    * 이 경우, 172개의 True Positives, 41개의 False Positives, 27개의 False Negatives가 감지되었습니다.
    * 예측된 끝단과 ground truth 끝단 간의 평균 거리 오차는 $37.48 \pm 40.33$ 픽셀이었습니다.
3. **RetinaNet vs. YOLOv2 비교:**
    * **RetinaNet**은 더 높은 F1-score (0.846 vs 0.835)와 0개의 False Negatives를 기록하며 **더 나은 감지 정확도**를 보였습니다.
    * **YOLOv2**는 71.43 fps로 RetinaNet (13.21 fps)에 비해 훨씬 **빠른 추론 속도**를 제공했습니다.
4. **9-겹 교차 검증 (RetinaNet, 150 픽셀 마진):**
    * 평균 재현율: $1.000 \pm 0.000$
    * 평균 정밀도: $0.767 \pm 0.033$
    * 평균 F1-score: $0.868 \pm 0.022$
    * 만족스러운 평균 성능과 낮은 표준 편차를 보여 모델의 높은 견고성을 입증했습니다.

## 🧠 Insights & Discussion

* **최적 마진 크기의 중요성:** 실험 결과, 수술 기구 끝단 감지에는 최적의 바운딩 박스 마진 크기가 존재함을 확인했습니다. 마진이 너무 작으면 핵심 정보 손실로 이어지고, 너무 크면 불필요한 배경 정보(장애물)를 학습하여 오탐(false positive)을 유발할 수 있습니다. 이 최적 마진 크기는 사용된 데이터셋의 특성에 따라 달라질 수 있으며, 향후 추가 검증이 필요합니다.
* **모델 선택의 딜레마 (정확도 vs. 속도):** RetinaNet은 더 높은 F1-score와 완벽한 재현율을 달성하여 감지 정확도 측면에서 우수했습니다. 그러나 추론 속도는 13.21 fps로 실시간 적용을 위해서는 2-3프레임마다 추론해야 할 수 있습니다. 반면 YOLOv2는 71.43 fps로 훨씬 빠르지만, RetinaNet에 비해 F1-score가 약간 낮고 False Negatives가 발생했습니다. 즉각적인 실시간 감지가 필수적인 상황에서는 YOLOv2가 더 적합할 수 있으며, YOLOv2의 성능 정확도를 높이는 연구가 필요합니다.
* **데이터셋의 한계 및 미래 방향:**
  * **정지 이미지 사용:** 본 연구는 정지 이미지를 기반으로 했지만, 궁극적인 목표는 비디오 적용이므로 모델을 비디오 데이터에 테스트하는 것이 중요합니다.
  * **기구의 유무:** 훈련 시에는 기구가 보이는 이미지만 사용했으나, 실제 BESS 비디오에는 기구가 없는 순간도 있으므로 테스트 단계에서 이를 고려해야 합니다.
  * **단일 기구:** 현재 BESS는 한 번에 하나의 기구만 사용하지만, 로봇 내시경이 도입되면 양손 BESS가 가능해져 여러 기구 끝단을 동시에 감지하는 방법 개발이 필요합니다.
* **끝단 위치의 영향:** 시각적 분석과 좌표 산점도($\text{scatterplot}$)를 통해 기구 끝단이 이미지 중앙에 가까울수록 감지 성능이 우수한 경향을 발견했습니다. 이는 훈련 데이터셋의 대부분의 끝단 좌표가 중앙에 집중되어 있기 때문으로 보입니다. 데이터 증강 또는 데이터 큐레이션을 통해 이미지 주변부에 위치한 끝단 사례를 훈련 세트에 추가함으로써 모델의 견고성을 더욱 향상시킬 수 있을 것입니다.
* **활용 가능성:** 본 연구에서 제안된 자동 기구 끝단 감지 방법은 다양한 유형의 내시경 수술에 확장 적용될 수 있으며, 로봇 내시경 시스템의 핵심 제어 입력으로 활용되어 수술의 정밀도와 효율성을 높이는 데 크게 기여할 수 있습니다.

## 📌 TL;DR

본 연구는 양방향 내시경 척추 수술(BESS)에서 수술 기구 끝단 자동 감지 기술을 개발하여 로봇 내시경 제어를 위한 시각 지능을 제공하는 것을 목표로 했습니다. RetinaNet과 YOLOv2 모델을 다양한 바운딩 박스 마진 크기로 훈련 및 비교한 결과, RetinaNet이 150픽셀 마진에서 F1-score 0.846, 재현율 1.000으로 더 높은 감지 정확도를 보였습니다. 9-겹 교차 검증을 통해 모델의 견고성을 입증했으며, 이 기술은 로봇 내시경의 핵심 제어 입력으로 활용되어 수술 효율성 증대에 기여할 수 있습니다.
