# Text Promptable Surgical Instrument Segmentation with Vision-Language Models

Zijian Zhou, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Miaojing Shi

## 🧩 Problem to Solve

최소 침습 수술(Minimally Invasive Surgeries, MIS)은 환자 불편 감소와 회복 시간 단축 등 여러 이점에도 불구하고, 제한된 시야와 간접적인 내시경 시야로 인해 수술 도구 자동 인식이 어렵습니다. 특히 다음과 같은 문제에 직면해 있습니다:

1. **수술 도구 다양성:** 새로운 수술 도구의 등장으로 기존 모델은 새로운 도구 유형에 맞게 재라벨링 및 재훈련이 필요하여 실용화에 큰 걸림돌이 됩니다.
2. **도구 간 구별의 어려움:** 유사한 외형을 가진 도구들이 많고, 수술 환경의 열악한 영상 조건으로 인해 도구 종류와 정밀한 경계를 구별하기 어렵습니다.

## ✨ Key Contributions

* 수술 도구 분할을 위한 최초의 텍스트 프롬프트 기반 접근 방식을 제안합니다. 이로써 모델의 일반화 능력을 향상시키고 새로운 도구 유형에 대한 적응성을 높였습니다.
* 사전 훈련된 비전-언어 모델(Vision-Language Model)을 백본으로 활용하고, 주의(attention) 기반 및 컨볼루션(convolution) 기반 프롬프트 스키마로 구성된 새로운 텍스트 프롬프트 가능한 마스크 디코더를 설계했습니다.
* 새로운 "프롬프트 혼합(Mixture of Prompts, MoP)" 메커니즘을 도입하여 각 수술 도구에 대한 여러 텍스트 프롬프트를 활용하여 분할 성능을 향상시켰습니다.
* "어려운 도구 영역 강화(Hard Instrument Area Reinforcement)" 모듈을 도입하여 이미지 특징 이해와 분할 정밀도를 개선했습니다.
* 다양한 수술 도구 분할 데이터셋에서 우수한 성능과 유망한 일반화 능력을 입증했습니다.

## 📎 Related Works

* **수술 도구 분할:** 전통적인 비전 기반 모델(TernausNet, ISINet, MF-TapNet 등)과 트랜스포머 기반 모델(TraSeTR, MATIS)이 존재합니다. 이들은 주석이 달린 훈련 데이터의 범주적 이해에 의존하여 새로운 범주가 도입될 때 재훈련이 필요하다는 한계가 있습니다.
* **비전-언어 모델:** CLIP과 같은 대규모 사전 훈련 모델이 이미지와 텍스트 모달리티를 정렬하여 제로샷(zero-shot) 분류 등 다양한 비전 작업에서 뛰어난 성능을 보였습니다.
* **텍스트 프롬프트 가능한 분할 (Referring Expression Segmentation):** 자연어 표현을 프롬프트로 사용하여 이미지 분할을 수행하는 작업입니다. 초기 CNN/RNN 기반 접근 방식에서 CLIPSeg, CRIS와 같은 트랜스포머 기반 모델로 발전했습니다. 그러나 이러한 기존 방법들은 수술 도구 분할과 같은 특정 작업에 최적화되어 있지 않습니다.

## 🛠️ Methodology

본 논문은 사전 훈련된 비전-언어 모델(CLIP)을 기반으로 수술 도구 분할을 위한 텍스트 프롬프트 가능한 접근 방식을 제안하며, 네 가지 주요 모듈로 구성됩니다.

1. **이미지 및 텍스트 인코더 (Image and Text Encoders)**
    * **백본:** CLIP의 사전 훈련된 Vision Transformer (ViT) 이미지 인코더와 Transformer 텍스트 인코더를 활용합니다.
    * **이미지 인코더:** ViT 기반 이미지 인코더를 미세 조정하며, 다중 스케일 특징 추출을 위해 4, 8, 12번째 레이어의 출력을 특징 피라미드 네트워크(FPN)로 융합하는 **다중 스케일 특징 증강(Multi-scale Feature Augmentation, MSFA)**을 도입하여 시각 특징 $F_I$를 얻습니다.
    * **텍스트 인코더:** CLIP의 텍스트 인코더는 고정(frozen)하며, `[CLS]` 토큰에서 전역 텍스트 특징 $F_T$를 추출합니다.

2. **텍스트 프롬프트 가능한 마스크 디코더 (Text Promptable Mask Decoder)**
    $F_I$와 $F_T$를 활용하여 점수 맵(score map) $S$를 디코딩합니다.
    * **주의 기반 프롬프트 (Attention-based Prompting):** $F_I$에 자기 주의(self-attention)를 적용한 후, $F_I$와 $F_T$ 간의 교차 주의(cross-attention)를 통해 도구 영역을 전역적으로 탐색합니다. 세 개의 디코딩 블록으로 구성됩니다.
    * **컨볼루션 기반 프롬프트 (Convolution-based Prompting):** 주의 기반으로 얻은 특징 $F_I^A$를 국부적으로 정제합니다. $F_T$를 완전 연결(FC) 레이어를 통해 컨볼루션 커널 가중치 $w$와 바이어스 $b$로 변환한 후, 이를 사용하여 $\tilde{F}_I^A$ (재구성된 $F_I^A$)에 컨볼루션을 적용하여 점수 맵 $S$를 생성합니다.
        $$S = \text{Sigmoid}(\text{Conv}(\tilde{F}_I^A | w, b))$$

3. **프롬프트 혼합 메커니즘 (Mixture of Prompts, MoP)**
    다양한 텍스트 프롬프트의 강점을 활용하여 분할 성능을 높입니다.
    * **프롬프트 획득:**
        1. 도구 클래스 이름 (예: "bipolar forceps").
        2. 템플릿 기반 프롬프트 (예: "the surgical instrument area represented by the [class name]").
        3. GPT-4가 생성한 상세 설명.
    * **프롬프트 예측 융합:** 각 프롬프트는 개별적인 점수 맵 $S_i$를 생성합니다. 시각 특징 $F_I$와 텍스트 특징 $F_T$를 입력으로 받는 **시각-텍스트 게이팅 네트워크(Visual-Textual Gating Network)**는 픽셀 단위의 가중치 맵을 예측하고, 이를 소프트맥스 정규화하여 $S_i$들을 가중 합산하여 최종 점수 맵 $S_{et}$를 얻습니다.

4. **어려운 도구 영역 강화 모듈 (Hard Instrument Area Reinforcement, HIAR)**
    복잡한 수술 환경에서 모델이 도구의 정확한 범주와 경계를 구별하지 못하는 문제점을 개선합니다.
    * **어려운 영역 마이닝 (Hard Area Mining, HAM):** 예측 마스크 $M_{et}$와 실제 마스크 $M_{gt}$를 비교하여 잘못 예측된 영역을 "어려운 도구 영역"으로 정의합니다. 이 영역에 대해 마스킹 비율 $r_t$ (실험적으로 $0.25$)에 맞춰 마스킹된 이미지를 생성합니다.
    * **강화:** 마스킹된 이미지를 *공유 이미지 인코더*에 입력하고, MAE(Masked AutoEncoder)와 유사한 디코더 구조를 사용하여 마스킹되지 않은 영역으로부터 전체 이미지를 재구성합니다. 이는 모델이 어려운 영역의 미묘한 특징에 집중하도록 돕습니다 (훈련 시에만 사용).

5. **모델 훈련 (Model Training)**
    텍스트 인코더는 고정하고 이미지 인코더는 미세 조정합니다. 총 손실 함수 $L = L_{seg} + \lambda L_{rec}$를 사용합니다.
    * $L_{seg}$: 분할 마스크 $M_{et}$에 대한 이진 교차 엔트로피 손실.
        $$L_{seg} = -\sum_j (M_{gt,j} \log M_{et,j} + (1-M_{gt,j}) \log(1-M_{et,j}))$$
    * $L_{rec}$: 재구성된 이미지 $I_{rec}$와 원본 이미지 $I$ 간의 픽셀 단위 L2 손실.
        $$L_{rec} = \sum_j \|I_{rec,j} - I_j\|^2$$
    * 손실 가중치 $\lambda = 0.5$입니다.

## 📊 Results

* **최고 성능 달성:** EndoVis2017 및 EndoVis2018 데이터셋에서 기존 감독 학습 방법들과 기존 텍스트 프롬프트 가능한 자연 이미지 분할 방법(CRIS, CLIPSeg)을 월등히 능가하는 성능을 보였습니다. 예를 들어, EndoVis2017에서 Ch\_IoU +7.36%, ISI\_IoU +5.84%, mc\_IoU +9.67%의 개선을 달성했습니다.
* **강력한 일반화 능력 (교차 데이터셋):** EndoVis2018로 훈련하고 EndoVis2017로 테스트하는 등 교차 데이터셋 환경에서도 경쟁력 있는 결과를 얻어, 도구 카테고리가 완전히 겹치지 않음에도 불구하고 강력한 일반화 가능성을 입증했습니다.
* **SAM과의 비교:** 커뮤니티 기반의 SAM 변형(lang-segment-anything)은 의료 프롬프트에서 낮은 성능(예: EndoVis2018에서 Ch\_IoU 17.77%)을 보이며, 본 제안 방법이 의료 도메인에 특화된 성능 우위를 가짐을 시사합니다.
* **모듈의 효과 검증 (Ablation Study):**
  * 다중 스케일 특징 증강(MSFA)과 `[CLS]` 토큰 사용이 성능 향상에 기여했습니다.
  * 주의 기반 및 컨볼루션 기반 프롬프트 스키마를 함께 사용했을 때 분할 성능이 크게 향상되었습니다.
  * 프롬프트 혼합 메커니즘(MoP)과 픽셀 단위 가중치 맵이 효과적이며, GPT-4 생성 프롬프트가 가장 좋은 성능을 보였습니다.
  * 어려운 도구 영역 강화(HIAR) 모듈, 특히 어려운 영역 마이닝(HAM)이 분할 정확도를 크게 개선했습니다.
* **계산 효율성:** CRIS, CLIPSeg와 같은 다른 텍스트 프롬프트 가능한 접근 방식과 유사한 FLOPs 및 FPS를 보여 실시간 임상 적용에 적합함을 입증했습니다.

## 🧠 Insights & Discussion

* **새로운 패러다임 제시:** 본 연구는 수술 도구 분할 분야에 텍스트 프롬프트 가능한 접근 방식을 처음 도입하여, 새로운 도구 유형에 대한 적응성과 도구 간 구별 문제를 효과적으로 해결할 수 있는 잠재력을 보여주었습니다.
* **비전-언어 모델의 강력한 활용:** CLIP과 같은 사전 훈련된 비전-언어 모델을 활용함으로써, 수술 이미지 데이터의 부족을 보완하고 강력한 시각 및 텍스트 특징 정렬을 가능하게 했습니다.
* **맞춤형 모듈의 효과:** 수술 도메인에 특화된 마스크 디코더, 프롬프트 혼합 메커니즘, 어려운 도구 영역 강화 모듈이 모델 성능 향상에 결정적인 기여를 했음이 입증되었습니다.
* **실용적 적용 가능성:** 교차 데이터셋 실험에서 보여준 뛰어난 일반화 능력은 실제 로봇 보조 수술 환경에서 새로운 도구나 미세한 시각적 차이에 대응하는 모델의 실용적 잠재력을 강조합니다.
* **한계 및 향후 연구:** 모델의 "어려운 영역" 정의가 때로는 외과의사의 직관과 다를 수 있습니다 (예: 도구 끝 부분과 샤프트). 향후 연구에서는 도구의 각 부분 간의 관계 모델링을 통해 분할 성능을 더욱 향상시킬 수 있습니다.
* **사회적 영향:** 이 기술은 수술 안전성과 정밀도를 높이고, 외과의사의 부담을 줄이며, 신입 외과의사 훈련을 돕고, AI 기반 스마트 수술 도구 개발에 기여하여 궁극적으로 환자 치료의 질을 향상시킬 수 있습니다.

## 📌 TL;DR

수술 도구의 다양성과 구별의 어려움을 해결하기 위해 본 논문은 비전-언어 모델(VLM) 기반의 새로운 텍스트 프롬프트 가능한 수술 도구 분할 방법을 제안합니다. 이 방법은 CLIP을 백본으로 사용하고, 주의(attention) 및 컨볼루션(convolution) 기반의 맞춤형 마스크 디코더, 다양한 텍스트 프롬프트를 활용하는 프롬프트 혼합(MoP) 메커니즘, 그리고 어려운 영역에 대한 정밀도를 높이는 어려운 도구 영역 강화(HIAR) 모듈로 구성됩니다. 제안된 모델은 여러 데이터셋에서 최고 수준의 성능과 강력한 일반화 능력을 입증하여 로봇 보조 수술 분야에 상당한 실용적 잠재력을 제공합니다.
