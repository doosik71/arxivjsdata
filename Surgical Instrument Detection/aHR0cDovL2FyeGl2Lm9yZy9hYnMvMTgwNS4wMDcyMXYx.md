# Joint Surgical Gesture and Task Classification with Multi-Task and Multimodal Learning

Duygu Sarikaya, Khurshid A. Guru, Jason J. Corso

## 🧩 Problem to Solve

로봇 보조 수술(RAS) 비디오에서 수술 활동을 자동으로 이해하는 것은 중요한 연구 분야입니다. 본 논문은 특히 두 가지 문제를 동시에 해결하고자 합니다:

1. **낮은 수준의 수술 제스처(low-level surgical gestures) 분류:** 다양한 수술 작업에 걸쳐 반복되는 일반적인 제스처를 정확하게 인식하는 것이 어렵습니다. 기존 연구는 특정 작업 내의 제스처 인식에 집중하는 경향이 있었습니다.
2. **수술 작업(surgical tasks) 분류:** 제스처와 함께 비디오 내에서 수행되는 더 높은 수준의 수술 작업을 분류해야 합니다.

이러한 낮은 수준의 제스처와 수술 작업을 시각적 특징과 동작 신호 간의 복잡한 관계를 활용하여 동시에 정확하게 분류하는 통합적인 아키텍처를 개발하는 것이 목표입니다.

## ✨ Key Contributions

* **다중 모드 및 다중 작업 학습 아키텍처 제안:** 시각적 특징(RGB 프레임)과 동작 신호(옵티컬 플로우)의 두 가지 모달리티를 활용하고, 낮은 수준의 제스처 분류와 수술 작업 분류라는 두 가지 작업을 동시에 수행하는 새로운 엔드-투-엔드 신경망 아키텍처를 제안합니다.
* **LSTM 기반 시공간 동역학 학습:** Long Short-Term Memory(LSTM) 네트워크를 기반으로 하여 비디오 시퀀스의 시간적 동역학을 효과적으로 학습합니다.
* **상호 보완적인 모달리티 활용:** 수술 작업은 시각적 특징(장면 및 객체)으로, 낮은 수준 제스처는 동작 신호로 더 잘 모델링될 수 있다는 가설을 제시하고, 이 두 모달리티가 상호 보완적으로 작용함을 실험으로 입증했습니다.
* **"일반적인" 낮은 수준 제스처 인식에 초점:** 특정 작업에만 국한되지 않고, 여러 다른 수술 작업 및 환경에서 재발하는 일반적인 14가지 낮은 수준의 제스처를 인식하는 데 중점을 두어 기존 벤치마크 연구와 차별화됩니다.
* **기존 방식 대비 22% 성능 향상:** 단일 모달리티 및 단일 작업 모델과 비교하여 평균 정밀도(Average Precision, AP)에서 약 22%의 상당한 성능 향상을 달성했습니다.

## 📎 Related Works

* **초기 수술 활동 인식 연구:** SVM (LDA) [4,5], GMMs [6], HMM (Hidden Markov Models) [7,8] 등 다양한 통계적 모델링 기법들이 사용되었습니다. HMM은 시퀀스의 확률 분포를 모델링하는 데 자주 활용되었습니다.
* **심층 신경망 도입:** 최근에는 CNN (Convolutional Neural Networks) 및 RNN (Recurrent Neural Networks) [9,10,11]이 수술 활동 인식에 적용되기 시작했습니다.
* **JIGSAWS 데이터셋 벤치마크 연구:** Ahmidi et al. [12]은 JIGSAWS 데이터셋에서 BoF, LDS, GMM-HMM 등을 사용하여 제스처 인식 벤치마크를 수행했습니다. DiPietro et al. [10]은 JIGSAWS 데이터셋의 운동학적 데이터(kinematic data)를 사용하여 RNN을 훈련했습니다.
* **수술 단계 분류:** Cadène et al. [9]은 딥 잔여 네트워크로 시각적 특징을 추출하고 HMM으로 수술 단계 전환을 모델링했습니다. Twinanda et al. [11]은 CNN으로 시각적 특징을 추출한 후 SVM, HMM, LSTM을 사용하여 수술 단계를 분류했습니다.
* **다중 작업 학습(Multi-task Learning):** 여러 관련 작업을 동시에 학습하여 성능을 향상시키는 접근 방식 [20,21,22]과 공유 및 작업별 표현의 조합을 활용하는 방법 [23]이 최근 연구에서 뛰어난 성능을 보였습니다.
* **LRCN (Long-term Recurrent Convolutional Networks):** Donahue et al. [17]이 제안한 모델로, CNN으로 시각적 특징을 추출하고 LSTM으로 시간적 동역학을 학습합니다. 본 논문의 아키텍처는 LRCN의 원리를 기반으로 다중 모달리티 및 다중 작업 학습을 확장합니다.

## 🛠️ Methodology

본 논문은 LRCN(Long-term Recurrent Convolutional Networks)의 원리를 기반으로 하는 다중 모드 및 다중 작업 학습 아키텍처를 제안합니다.

1. **입력 데이터 준비:**
    * **모달리티:** RGB 비디오 프레임(시각적 특징)과 동일한 프레임의 옵티컬 플로우(동작 신호)를 RGB 표현으로 변환하여 두 가지 모달리티 입력으로 사용합니다.
    * **데이터셋:** JIGSAWS 데이터셋을 활용하며, 3가지 수술 작업(Suturing, Needle Passing, Knot Tying)과 14가지 낮은 수준의 제스처 레이블을 사용합니다.
    * **전처리:** 비디오를 제스처 클립으로 자르고, 초당 8프레임을 추출하며, 640x480으로 크기를 조정합니다. 옵티컬 플로우는 Thomas et al. [25]의 방법을 사용하여 계산됩니다.

2. **병렬 CNN 특징 추출:**
    * 두 개의 독립적인 CNN 스트림을 사용하여 RGB 프레임과 옵티컬 플로우 입력 각각에서 고급 특징을 병렬로 추출합니다.
    * CNN 아키텍처는 Zeiler and Fergus [26] 및 AlexNet [27]과 유사하게 5개의 컨볼루션 레이어를 가집니다.

3. **특징 융합(Fusion):**
    * 각 CNN 스트림의 5번째 컨볼루션 레이어 활성화 후, 두 모달리티의 특징들을 연결(concatenate)합니다.
    * 연결된 특징에 추가 컨볼루션 레이어를 적용하여 차원을 줄이고, 이어서 완전 컨볼루션(fully convolutional) 레이어를 통과시킵니다.

4. **LSTM 기반 시퀀스 학습:**
    * 융합된 특징은 시퀀스 클립 마커와 함께 LSTM 레이어의 입력으로 사용됩니다. LSTM은 시퀀스 데이터의 시간적 동역학 및 장기 의존성을 학습합니다.

5. **다중 작업 출력 레이어:**
    * LSTM의 출력은 두 개의 독립적인 예측 헤드로 분기됩니다.
    * 첫 번째 헤드는 3가지 수술 작업 레이블(Knot Tying, Suturing, Needle Passing)을 분류합니다.
    * 두 번째 헤드는 14가지 낮은 수준의 제스처 레이블(<G1, G2, ..., G14>)을 분류합니다.
    * 두 작업 모두 동일한 가중치로 손실 함수에 기여합니다.

6. **학습 전략:**
    * **사전 훈련:** CNN은 1.2M 이미지 ILSVRC-2012 데이터셋 [28]에서 사전 훈련된 모델의 가중치를 전이 학습하여 초기화하고 과적합을 방지합니다.
    * **단계별 훈련:**
        1. 개별 프레임 기반 CNN 모델(RGB 및 플로우 각각)을 40k 반복 학습합니다.
        2. 개별 LSTM 모델(RGB 및 플로우 각각)을 이전 단계에서 전이된 가중치로 60k 반복 학습합니다.
        3. 최종적으로, 두 개의 별도 LSTM 모델에서 컨볼루션 및 완전 연결 레이어의 가중치를 결합하여 전체 통합 모델을 엔드-투-엔드로 90k 반복 학습합니다.
    * **최적화:** 확률적 경사 하강법(SGD)을 사용하며, 학습률은 0.001로 시작하여 20k 반복마다 0.1씩 감소시킵니다. LSTM 모델에는 그래디언트 클리핑(임계값 15)을 적용합니다.
    * **데이터 증강:** 과적합을 방지하기 위해 프레임 자르기, 미러링, 크롭 등의 데이터 증강 기법을 사용합니다.

## 📊 Results

* **성능 우월성:** 제안된 다중 모드 및 다중 작업 접근 방식은 수술 작업과 낮은 수준 제스처를 각각 시각적 단서와 동작 단서에 대해 개별적으로 분류하는 기존 아키텍처보다 우수함을 입증했습니다.
* **AP(Average Precision) 향상:** 6-분할 실험에서 기존 접근 방식이 29.13%의 MAP(Mean Average Precision)를 달성한 반면, 본 아키텍처는 3가지 작업과 14가지 제스처 레이블에 대해 50.83%의 MAP를 달성했습니다. 이는 21.7% (약 22%)의 상당한 개선을 의미합니다.
* **일관된 개선:** 6가지 모든 분할 실험에서 18%에서 26% 범위의 견고한 개선(중앙값 22.5%)을 보였습니다.
* **데이터셋:** JIGSAWS 데이터셋의 1200개 제스처 비디오 세그먼트(약 42,000 프레임)로 훈련하고, 422개 세그먼트(약 14,500 프레임)로 테스트했습니다.
* **기존 벤치마크와의 차이점:** JIGSAWS 벤치마크 연구들이 특정 수술 작업 내의 제스처에 초점을 맞춘 반면 (예: Suturing 작업의 10개 제스처), 본 연구는 전체 14개 제스처 어휘를 사용하여 여러 수술 작업에 걸쳐 나타나는 공통 제스처를 인식했습니다.

## 🧠 Insights & Discussion

* **모달리티의 상호 보완성:** 시각적 특징(RGB)과 동작 신호(옵티컬 플로우)의 결합은 수술 활동 이해에 매우 효과적입니다. 특히, 수술 작업은 장면과 객체에 의해 정의되는 시각적 특징으로, 낮은 수준 제스처는 객체 및 장면에 독립적인 동작 신호로 더 잘 모델링될 수 있다는 가설이 실험적으로 지지되었습니다.
* **다중 작업 학습의 시너지 효과:** 다중 작업 학습은 낮은 수준 제스처와 수술 작업 간의 공동 관계를 활용하고, 공유 및 작업별 표현을 결합함으로써 단일 작업 모델보다 훨씬 뛰어난 성능을 보여주었습니다. 이는 두 작업이 서로에게 유용한 정보를 제공하며 학습을 강화한다는 것을 의미합니다.
* **일반적인 제스처 인식의 중요성:** 본 연구는 특정 작업에 국한되지 않고 다양한 작업에서 재현되는 일반적인 낮은 수준 제스처를 인식하는 데 성공했습니다. 이는 실제 수술 환경에서 유연하고 일반화 가능한 시스템을 구축하는 데 중요한 진전입니다.
* **잠재적 응용:** 이러한 통찰력은 효과적인 기술 습득, 객관적인 기술 평가, 실시간 피드백, 그리고 인간-로봇 협업 수술 등 다양한 분야에 적용될 수 있습니다.
* **한계점:** 현재 구현은 GPU 및 병렬화를 완전히 활용하지 못하여 훈련 및 테스트 시간이 더 효율적일 수 있습니다. 또한, 하이퍼파라미터는 실험적 관찰을 기반으로 설정되었으며, 그리드 검색 등을 통한 추가 최적화 가능성이 있습니다. 비디오 클립 길이 선정에도 개선의 여지가 있습니다.

## 📌 TL;DR

본 논문은 로봇 보조 수술(RAS) 비디오에서 낮은 수준의 수술 제스처와 수술 작업을 동시에 분류하는 새로운 다중 모드 및 다중 작업 학습 아키텍처를 제안합니다. 시각적 특징(RGB)과 동작 신호(옵티컬 플로우)를 병렬 CNN 스트림으로 추출하고 LSTM을 통해 시간적 동역학을 학습하여 융합된 특징을 사용합니다. JIGSAWS 데이터셋을 활용한 실험에서, 제안된 모델은 기존 단일 모달리티/단일 작업 접근 방식보다 MAP에서 22%p 높은 50.83%의 성능을 달성하며, 두 모달리티와 작업 간의 상호 보완적인 관계가 복잡한 수술 활동 이해에 중요함을 입증했습니다.
