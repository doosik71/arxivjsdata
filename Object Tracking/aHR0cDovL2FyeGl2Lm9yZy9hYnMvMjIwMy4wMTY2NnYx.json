{
  "title": "Correlation-Aware Deep Tracking",
  "authors": "Fei Xie, Chunyu Wang, Guangting Wang, Yue Cao, Wankou Yang, Wenjun Zeng",
  "year": 2022,
  "url": "http://arxiv.org/abs/2203.01666v1",
  "abstract": "Robustness and discrimination power are two fundamental requirements in\nvisual object tracking. In most tracking paradigms, we find that the features\nextracted by the popular Siamese-like networks cannot fully discriminatively\nmodel the tracked targets and distractor objects, hindering them from\nsimultaneously meeting these two requirements. While most methods focus on\ndesigning robust correlation operations, we propose a novel target-dependent\nfeature network inspired by the self-/cross-attention scheme. In contrast to\nthe Siamese-like feature extraction, our network deeply embeds cross-image\nfeature correlation in multiple layers of the feature network. By extensively\nmatching the features of the two images through multiple layers, it is able to\nsuppress non-target features, resulting in instance-varying feature extraction.\nThe output features of the search image can be directly used for predicting\ntarget locations without extra correlation step. Moreover, our model can be\nflexibly pre-trained on abundant unpaired images, leading to notably faster\nconvergence than the existing methods. Extensive experiments show our method\nachieves the state-of-the-art results while running at real-time. Our feature\nnetworks also can be applied to existing tracking pipelines seamlessly to raise\nthe tracking performance. Code will be available."
}