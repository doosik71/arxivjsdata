{
  "title": "Dynamic Template Tracking and Recognition",
  "authors": "Rizwan Chaudhry, Gregory Hager, Rene Vidal",
  "year": 2012,
  "url": "http://arxiv.org/abs/1204.4476v1",
  "abstract": "In this paper we address the problem of tracking non-rigid objects whose\nlocal appearance and motion changes as a function of time. This class of\nobjects includes dynamic textures such as steam, fire, smoke, water, etc., as\nwell as articulated objects such as humans performing various actions. We model\nthe temporal evolution of the object's appearance/motion using a Linear\nDynamical System (LDS). We learn such models from sample videos and use them as\ndynamic templates for tracking objects in novel videos. We pose the problem of\ntracking a dynamic non-rigid object in the current frame as a maximum\na-posteriori estimate of the location of the object and the latent state of the\ndynamical system, given the current image features and the best estimate of the\nstate in the previous frame. The advantage of our approach is that we can\nspecify a-priori the type of texture to be tracked in the scene by using\npreviously trained models for the dynamics of these textures. Our framework\nnaturally generalizes common tracking methods such as SSD and kernel-based\ntracking from static templates to dynamic templates. We test our algorithm on\nsynthetic as well as real examples of dynamic textures and show that our simple\ndynamics-based trackers perform at par if not better than the state-of-the-art.\nSince our approach is general and applicable to any image feature, we also\napply it to the problem of human action tracking and build action-specific\noptical flow trackers that perform better than the state-of-the-art when\ntracking a human performing a particular action. Finally, since our approach is\ngenerative, we can use a-priori trained trackers for different texture or\naction classes to simultaneously track and recognize the texture or action in\nthe video."
}