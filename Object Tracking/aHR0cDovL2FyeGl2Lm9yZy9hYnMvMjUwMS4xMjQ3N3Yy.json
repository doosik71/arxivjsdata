{
  "title": "Slot-BERT: Self-supervised Object Discovery in Surgical Video",
  "authors": "Guiqiu Liao, Matjaz Jogan, Marcel Hussing, Kenta Nakahashi, Kazuhiro Yasufuku, Amin Madani, Eric Eaton, Daniel A. Hashimoto",
  "year": 2025,
  "url": "http://arxiv.org/abs/2501.12477v2",
  "abstract": "Object-centric slot attention is a powerful framework for unsupervised\nlearning of structured and explainable representations that can support\nreasoning about objects and actions, including in surgical videos. While\nconventional object-centric methods for videos leverage recurrent processing to\nachieve efficiency, they often struggle with maintaining long-range temporal\ncoherence required for long videos in surgical applications. On the other hand,\nfully parallel processing of entire videos enhances temporal consistency but\nintroduces significant computational overhead, making it impractical for\nimplementation on hardware in medical facilities. We present Slot-BERT, a\nbidirectional long-range model that learns object-centric representations in a\nlatent space while ensuring robust temporal coherence. Slot-BERT scales object\ndiscovery seamlessly to long videos of unconstrained lengths. A novel slot\ncontrastive loss further reduces redundancy and improves the representation\ndisentanglement by enhancing slot orthogonality. We evaluate Slot-BERT on\nreal-world surgical video datasets from abdominal, cholecystectomy, and\nthoracic procedures. Our method surpasses state-of-the-art object-centric\napproaches under unsupervised training achieving superior performance across\ndiverse domains. We also demonstrate efficient zero-shot domain adaptation to\ndata from diverse surgical specialties and databases."
}