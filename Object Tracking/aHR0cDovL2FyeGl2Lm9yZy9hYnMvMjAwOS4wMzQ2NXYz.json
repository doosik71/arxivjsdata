{
  "url": "http://arxiv.org/abs/2009.03465v3",
  "title": "LaSOT: A High-quality Large-scale Single Object Tracking Benchmark",
  "authors": "Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu,  Harshit, Mingzhen Huang, Juehuan Liu, Yong Xu, Chunyuan Liao, Lin Yuan, Haibin Ling",
  "year": 2020,
  "abstract": "Despite great recent advances in visual tracking, its further development,\nincluding both algorithm design and evaluation, is limited due to lack of\ndedicated large-scale benchmarks. To address this problem, we present LaSOT, a\nhigh-quality Large-scale Single Object Tracking benchmark. LaSOT contains a\ndiverse selection of 85 object classes, and offers 1,550 totaling more than\n3.87 million frames. Each video frame is carefully and manually annotated with\na bounding box. This makes LaSOT, to our knowledge, the largest densely\nannotated tracking benchmark. Our goal in releasing LaSOT is to provide a\ndedicated high quality platform for both training and evaluation of trackers.\nThe average video length of LaSOT is around 2,500 frames, where each video\ncontains various challenge factors that exist in real world video footage,such\nas the targets disappearing and re-appearing. These longer video lengths allow\nfor the assessment of long-term trackers. To take advantage of the close\nconnection between visual appearance and natural language, we provide language\nspecification for each video in LaSOT. We believe such additions will allow for\nfuture research to use linguistic features to improve tracking. Two protocols,\nfull-overlap and one-shot, are designated for flexible assessment of trackers.\nWe extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis,\nand results reveal that there still exists significant room for improvement.\nThe complete benchmark, tracking results as well as analysis are available at\nhttp://vision.cs.stonybrook.edu/~lasot/."
}