{
  "url": "http://arxiv.org/abs/2503.00516v1",
  "title": "Two-stream Beats One-stream: Asymmetric Siamese Network for Efficient\n  Visual Tracking",
  "authors": "Jiawen Zhu, Huayi Tang, Xin Chen, Xinying Wang, Dong Wang, Huchuan Lu",
  "year": 2025,
  "abstract": "Efficient tracking has garnered attention for its ability to operate on\nresource-constrained platforms for real-world deployment beyond desktop GPUs.\nCurrent efficient trackers mainly follow precision-oriented trackers, adopting\na one-stream framework with lightweight modules. However, blindly adhering to\nthe one-stream paradigm may not be optimal, as incorporating template\ncomputation in every frame leads to redundancy, and pervasive semantic\ninteraction between template and search region places stress on edge devices.\nIn this work, we propose a novel asymmetric Siamese tracker named\n\\textbf{AsymTrack} for efficient tracking. AsymTrack disentangles template and\nsearch streams into separate branches, with template computing only once during\ninitialization to generate modulation signals. Building on this architecture,\nwe devise an efficient template modulation mechanism to unidirectional inject\ncrucial cues into the search features, and design an object perception\nenhancement module that integrates abstract semantics and local details to\novercome the limited representation in lightweight tracker. Extensive\nexperiments demonstrate that AsymTrack offers superior speed-precision\ntrade-offs across different platforms compared to the current\nstate-of-the-arts. For instance, AsymTrack-T achieves 60.8\\% AUC on LaSOT and\n224/81/84 FPS on GPU/CPU/AGX, surpassing HiT-Tiny by 6.0\\% AUC with higher\nspeeds. The code is available at https://github.com/jiawen-zhu/AsymTrack."
}