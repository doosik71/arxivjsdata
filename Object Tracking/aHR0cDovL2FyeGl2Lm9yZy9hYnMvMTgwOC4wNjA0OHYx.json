{
  "title": "Distractor-aware Siamese Networks for Visual Object Tracking",
  "authors": "Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, Weiming Hu",
  "year": 2018,
  "url": "http://arxiv.org/abs/1808.06048v1",
  "abstract": "Recently, Siamese networks have drawn great attention in visual tracking\ncommunity because of their balanced accuracy and speed. However, features used\nin most Siamese tracking approaches can only discriminate foreground from the\nnon-semantic backgrounds. The semantic backgrounds are always considered as\ndistractors, which hinders the robustness of Siamese trackers. In this paper,\nwe focus on learning distractor-aware Siamese networks for accurate and\nlong-term tracking. To this end, features used in traditional Siamese trackers\nare analyzed at first. We observe that the imbalanced distribution of training\ndata makes the learned features less discriminative. During the off-line\ntraining phase, an effective sampling strategy is introduced to control this\ndistribution and make the model focus on the semantic distractors. During\ninference, a novel distractor-aware module is designed to perform incremental\nlearning, which can effectively transfer the general embedding to the current\nvideo domain. In addition, we extend the proposed approach for long-term\ntracking by introducing a simple yet effective local-to-global search region\nstrategy. Extensive experiments on benchmarks show that our approach\nsignificantly outperforms the state-of-the-arts, yielding 9.6% relative gain in\nVOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker\ncan perform at 160 FPS on short-term benchmarks and 110 FPS on long-term\nbenchmarks."
}