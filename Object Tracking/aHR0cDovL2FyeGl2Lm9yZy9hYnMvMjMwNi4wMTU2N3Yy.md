# Segment Anything in High Quality

Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu

## 🧩 Problem to Solve

최근 출시된 Segment Anything Model (SAM)은 강력한 제로샷(zero-shot) 기능과 유연한 프롬프트 기능을 통해 대규모 이미지 분할(segmentation) 분야에서 큰 발전을 이루었습니다. 그러나 SAM은 11억 개의 마스크로 훈련되었음에도 불구하고, 특히 복잡한 구조를 가진 객체의 경우 마스크 예측 품질이 낮고, 경계선이 거칠며, 얇은 객체 구조를 놓치거나 어려운 경우에 잘못된 예측 또는 깨진 마스크를 생성하는 문제점을 가지고 있습니다. 이러한 한계는 자동 주석(annotation) 및 이미지/비디오 편집과 같이 매우 정확한 마스크가 필수적인 애플리케이션에서의 적용 가능성을 심각하게 제한합니다.

## ✨ Key Contributions

- **고품질 분할 모델 제안:** SAM의 원래 프롬프트 가능 디자인, 효율성 및 제로샷 일반화 능력을 유지하면서 모든 객체를 정확하게 분할할 수 있는 HQ-SAM을 제안합니다.
- **최소한의 파라미터 추가:** SAM의 사전 학습된 모델 가중치를 재사용하고 보존하며, 0.5% 미만의 최소한의 추가 파라미터와 연산만 도입합니다.
- **고품질 출력 토큰(High-Quality Output Token) 설계:** 학습 가능한 HQ-Output Token을 설계하여 SAM의 마스크 디코더에 주입하고, 고품질 마스크 예측을 담당하게 합니다.
- **글로벌-로컬 특징 융합(Global-local Feature Fusion):** 마스크 디코더 특징뿐만 아니라, SAM의 ViT 인코더의 초기 및 최종 계층 특징을 융합하여 마스크 세부 정보를 개선합니다.
- **HQSeg-44K 데이터셋 구축:** HQ-SAM 훈련을 위해 여러 소스에서 수집한 4만 4천 개의 정밀 마스크(fine-grained masks)로 구성된 HQSeg-44K 데이터셋을 구축했습니다.
- **효율적인 훈련:** 도입된 학습 가능한 파라미터는 8개의 GPU에서 4시간 만에 훈련될 수 있을 정도로 효율적입니다.
- **광범위한 평가:** 10개의 다양한 분할 데이터셋(이 중 8개는 제로샷 전이 프로토콜로 평가)에서 HQ-SAM의 효과를 입증합니다.

## 📎 Related Works

- **고품질 분할 (High-quality Segmentation):** 기존의 고품질 분할 연구는 대부분 특정 분할 작업(예: 이미지/비디오 인스턴스 분할, 시맨틱 분할, 파노프틱 분할)에 대해 닫힌 세계(close-world) 패러다임으로 훈련됩니다. 일부는 CRF(Conditional Random Fields) [23]와 같은 그래픽 모델이나 영역 성장(region growing) [10]을 사용한 후처리(post-refinement)에 중점을 둡니다. 그러나 이러한 방법은 상위 수준의 시맨틱 컨텍스트를 완전히 활용하지 못하거나 과적합되기 쉽습니다. HQ-SAM은 사전 학습된 SAM의 이미지 인코더와 마스크 디코더를 재사용하여 직접 새로운 고품질 마스크를 예측하며, 제로샷 일반화 능력을 보존합니다.
- **파운데이션 모델의 파인튜닝 및 프롬프트 튜닝 (Fine-tuning and Prompt Tuning for Foundation Models):** NLP 커뮤니티에서 시작된 파운데이션 모델(Foundation Models) [2, 1]은 강력한 제로샷 일반화 능력을 보여줍니다. CLIP [36]과 같은 비전 기반 파운데이션 모델에서는 사전 학습된 모델을 고정하고 학습 가능한 파라미터를 가진 프롬프트(prompt)를 사용하여 하류(downstream) 작업에 적용하는 방식이 연구되었습니다. HQ-SAM은 기존 프롬프트 기반 또는 파인튜닝 작업과 달리, 고품질 분할을 위해 SAM의 최소한의 적응에 중점을 둡니다.

## 🛠️ Methodology

HQ-SAM은 SAM의 고품질 제로샷 분할 기능을 업그레이드하기 위해 두 가지 중요한 변경 사항만 도입합니다. SAM의 사전 학습된 모델 가중치를 최대한 재사용하여 제로샷 전이 능력과 효율성을 유지합니다.

1. **SAM 아키텍처 (Preliminaries: SAM):**

   - **이미지 인코더:** 무거운 ViT 기반 백본으로 이미지 특징(feature)을 추출하여 $64 \times 64$ 크기의 이미지 임베딩(embedding)을 생성합니다.
   - **프롬프트 인코더:** 입력 포인트, 바운딩 박스 또는 코스 마스크로부터 상호작용적인 위치 정보를 인코딩하여 마스크 디코더에 제공합니다.
   - **마스크 디코더:** 2계층 트랜스포머(transformer) 기반 디코더로, 추출된 이미지 임베딩과 연결된 출력 토큰(output token) 및 프롬프트 토큰을 입력으로 받아 최종 마스크를 예측합니다.

2. **HQ-SAM의 핵심 구성 요소:**

   - **고품질 출력 토큰 (High-Quality Output Token):**

     - SAM의 마스크 디코더에 새로운 학습 가능한 HQ-Output Token($1 \times 256$ 크기)을 SAM의 원래 출력 토큰 및 프롬프트 토큰과 함께 입력으로 연결합니다.
     - HQ-Output Token은 마스크 디코더의 두 계층을 통과하며 전역 이미지 컨텍스트, 프롬프트 토큰의 기하학적/유형 정보, 다른 출력 토큰의 숨겨진 마스크 정보에 접근합니다.
     - 마지막으로, 업데이트된 HQ-Output Token으로부터 동적 컨볼루션 커널(dynamic convolutional kernels)을 생성하는 새로운 3계층 MLP(Multi-Layer Perceptron)를 추가하여 융합된 HQ-Feature와 공간적 점 단위 곱셈(spatially point-wise product)을 수행하여 고품질 마스크를 생성합니다.
     - 훈련 중에는 HQ-Output Token과 이와 관련된 MLP만 학습되고, 사전 학습된 SAM의 모든 파라미터는 고정됩니다.

   - **글로벌-로컬 특징 융합 (Global-local Feature Fusion):**
     - 정확한 분할을 위해 풍부한 전역 시맨틱 컨텍스트와 로컬 경계 세부 정보를 모두 포함하는 특징을 사용합니다.
     - 다음 세 가지 유형의 특징을 추출하고 융합하여 새로운 HQ-Features를 구성합니다.
       1. SAM ViT 인코더의 **초기 계층 로컬 특징**($64 \times 64$, 이미지 경계/세부 정보 포착).
       2. SAM ViT 인코더의 **최종 계층 전역 특징**($64 \times 64$, 전역 이미지 컨텍스트 정보 포함).
       3. SAM 마스크 디코더의 **마스크 특징**($256 \times 256$, 강력한 마스크 형상 정보 포함).
     - 초기 계층 및 최종 계층 인코더 특징을 전치 컨볼루션(transposed convolution)으로 $256 \times 256$ 크기로 업샘플링한 다음, 간단한 컨볼루션 처리 후 세 가지 유형의 특징을 요소별(element-wise)로 합산합니다.

3. **훈련 및 추론 (Training and Inference):**
   - **훈련 데이터:** SA-1B 대신 44,320개의 매우 정확한 마스크 주석을 포함하는 새로운 훈련 데이터셋인 **HQSeg-44K**를 구성했습니다. 이는 기존의 6개 데이터셋(DIS, ThinObject-5K, FSS-1000, ECSSD, MSRA-10K, DUT-OMRON)을 병합한 것입니다.
   - **훈련 과정:** 사전 학습된 SAM 모델의 파라미터는 고정하고, 제안된 HQ-SAM의 학습 가능한 파라미터(HQ-Output Token, 관련 3계층 MLP, 3개의 간단한 컨볼루션)만 업데이트합니다. 바운딩 박스, 랜덤 샘플링된 포인트, 거친 마스크 등 다양한 프롬프트 유형을 샘플링하여 훈련합니다. 학습률 0.001, 12 에포크(epoch), 8개의 Nvidia GeForce RTX 3090 GPU를 사용하여 총 4시간 소요됩니다.
   - **추론 과정:** SAM과 동일한 추론 파이프라인을 따르지만, HQ-Output Token에서 예측된 마스크를 고품질 마스크 예측으로 사용합니다. 추론 시 SAM 마스크(Output Token에 의해)와 HQ-Output Token에 의해 예측된 마스크의 로짓(logits)을 합산하여 $256 \times 256$ 해상도에서 마스크 보정(correction)을 수행한 후, 원래 해상도인 $1024 \times 1024$로 업샘플링합니다.

## 📊 Results

HQ-SAM은 SAM에 비해 분할 품질을 크게 향상시키면서도 SAM의 제로샷 능력과 효율성을 유지함을 입증했습니다.

- **정량적 평가:**

  - **마스크 품질 향상:** DIS 벤치마크에서 mBIoU(mean Boundary IoU)가 SAM의 52.8에서 70.4로 크게 개선되었으며, COIFT 및 HRSOD와 같은 고품질 데이터셋에서도 일관된 성능 향상을 보였습니다 (Table 2).
  - **다양한 백본에서의 일관된 성능:** ViT-B, ViT-L, ViT-H 및 TinyViT 등 다양한 ViT 백본을 사용하여 HQ-SAM이 SAM보다 일관되게 뛰어난 성능을 보임을 확인했습니다 (Table 10). 특히 COCO AP$_{B}$에서 ViT-L 기반 SAM의 33.3에서 34.4로, ViT-B 기반 SAM의 28.2에서 31.3으로 개선되었습니다.
  - **효율성:** HQ-SAM은 SAM에 비해 파라미터 증가가 0.5% 미만이며, 추론 시간은 SAM의 96% 수준을 유지하여 경량성과 효율성을 입증했습니다 (Table 1).
  - **제로샷 성능:** COCO [31], UVO [42], LVIS [14], HQ-YTVIS [20], BIG [6] 등 8개의 제로샷 전이 프로토콜 벤치마크를 포함한 10개 데이터셋에서 SAM을 능가했습니다. 특히 UVO에서 AP$^{\text{strict}}_{B}$가 1.3 증가하고, HQ-YTVIS 비디오 분할에서 Tube Boundary AP$_{B}$가 3.8 포인트 증가했습니다 (Table 5, 8).
  - **다양한 프롬프트에 대한 강건성:** GT 박스 및 코스 마스크 프롬프트 모두에서 SAM을 일관되게 능가했으며 (Table 6), 노이즈가 있는 박스 프롬프트에 대해서도 SAM보다 훨씬 강건한 분할 결과를 보였습니다 (Table 13).
  - **높은 BIoU 임계값에서의 우수성:** 엄격한 IoU 임계값($0.9$)에서 SAM과 HQ-SAM 간의 성능 격차가 크게 증가하여 HQ-SAM이 매우 정확한 마스크 예측에 강점을 가짐을 보여줍니다 (Figure 4).

- **정성적 평가:**
  - HQ-SAM은 SAM보다 훨씬 더 세부적인 결과를 생성하며 매우 정확한 경계를 가집니다 (Figure 1, 7, 9, 10, 12).
  - SAM이 얇은 구조를 잘못 해석하거나 깨진 마스크를 생성하는 경우(예: 연줄) HQ-SAM은 오류를 수정하고 깨진 부분을 채웁니다 (Figure 1).
  - HQ-Output Token의 교차-어텐션 맵(cross-attention map)은 SAM의 원래 토큰이 놓쳤던 경계 및 얇은 구조 영역에 집중함을 시각적으로 보여줍니다 (Figure 6).

## 🧠 Insights & Discussion

HQ-SAM은 SAM의 강력한 제로샷 일반화 능력을 유지하면서도 마스크 품질 문제를 효과적으로 해결합니다. 이는 SAM의 핵심 아키텍처를 최소한으로 수정하고, 학습 가능한 HQ-Output Token과 전역-로컬 특징 융합을 통해 고품질 특징을 활용하는 전략 덕분입니다. HQ-SAM은 SAM의 파라미터를 고정함으로써 과적합 및 치명적인 망각(catastrophic forgetting)을 방지하며, 데이터 효율적이고 계산 비용이 저렴한 훈련이 가능합니다. 이 연구는 SAM과 같은 파운데이션 분할 모델을 데이터 효율적이고 계산 비용이 저렴한 방식으로 활용하고 확장하는 방법에 대한 중요한 통찰력을 제공합니다.

제한점으로는 HQ-SAM이 여전히 SAM의 무거운 ViT 인코더를 공유하므로, MobileSAM 백본을 사용하지 않는 한 비디오 처리에서 실시간 속도를 달성하기 어렵습니다. 또한, 매우 어두운 환경이나 극도로 작은 금속 막대와 같은 일부 극한의 경우에서는 HQ-SAM도 완전히 정확한 마스크 예측을 달성하지 못하는 실패 사례가 발생할 수 있습니다.

## 📌 TL;DR

SAM은 섬세한 객체 분할에 한계가 있어 거친 마스크를 생성하는 문제가 있습니다. HQ-SAM은 SAM의 마스크 디코더에 학습 가능한 HQ-Output Token을 도입하고, 이미지 인코더의 초기/최종 특징과 마스크 디코더 특징을 융합하는 글로벌-로컬 특징 융합을 통해 고품질 분할을 수행합니다. 4만 4천 개의 정밀 마스크로 구성된 HQSeg-44K 데이터셋으로 SAM 모델을 고정한 채 HQ-SAM 부분만 4시간 훈련하여, SAM의 제로샷 일반화 능력과 효율성을 유지하면서도 마스크 품질을 획기적으로 향상시켰습니다.
