{
  "title": "Object-Adaptive LSTM Network for Real-time Visual Tracking with\n  Adversarial Data Augmentation",
  "authors": "Yihan Du, Yan Yan, Si Chen, Yang Hua",
  "year": 2020,
  "url": "http://arxiv.org/abs/2002.02598v1",
  "abstract": "In recent years, deep learning based visual tracking methods have obtained\ngreat success owing to the powerful feature representation ability of\nConvolutional Neural Networks (CNNs). Among these methods, classification-based\ntracking methods exhibit excellent performance while their speeds are heavily\nlimited by the expensive computation for massive proposal feature extraction.\nIn contrast, matching-based tracking methods (such as Siamese networks) possess\nremarkable speed superiority. However, the absence of online updating renders\nthese methods unadaptable to significant object appearance variations. In this\npaper, we propose a novel real-time visual tracking method, which adopts an\nobject-adaptive LSTM network to effectively capture the video sequential\ndependencies and adaptively learn the object appearance variations. For high\ncomputational efficiency, we also present a fast proposal selection strategy,\nwhich utilizes the matching-based tracking method to pre-estimate dense\nproposals and selects high-quality ones to feed to the LSTM network for\nclassification. This strategy efficiently filters out some irrelevant proposals\nand avoids the redundant computation for feature extraction, which enables our\nmethod to operate faster than conventional classification-based tracking\nmethods. In addition, to handle the problems of sample inadequacy and class\nimbalance during online tracking, we adopt a data augmentation technique based\non the Generative Adversarial Network (GAN) to facilitate the training of the\nLSTM network. Extensive experiments on four visual tracking benchmarks\ndemonstrate the state-of-the-art performance of our method in terms of both\ntracking accuracy and speed, which exhibits great potentials of recurrent\nstructures for visual tracking."
}