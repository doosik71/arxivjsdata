{
  "title": "Multi-Granularity Language-Guided Training for Multi-Object Tracking",
  "authors": "Yuhao Li, Jiale Cao, Muzammal Naseer, Yu Zhu, Jinqiu Sun, Yanning Zhang, Fahad Shahbaz Khan",
  "year": 2024,
  "url": "http://arxiv.org/abs/2406.04844v2",
  "abstract": "Most existing multi-object tracking methods typically learn visual tracking\nfeatures via maximizing dis-similarities of different instances and minimizing\nsimilarities of the same instance. While such a feature learning scheme\nachieves promising performance, learning discriminative features solely based\non visual information is challenging especially in case of environmental\ninterference such as occlusion, blur and domain variance. In this work, we\nargue that multi-modal language-driven features provide complementary\ninformation to classical visual features, thereby aiding in improving the\nrobustness to such environmental interference. To this end, we propose a new\nmulti-object tracking framework, named LG-MOT, that explicitly leverages\nlanguage information at different levels of granularity (scene-and\ninstance-level) and combines it with standard visual features to obtain\ndiscriminative representations. To develop LG-MOT, we annotate existing MOT\ndatasets with scene-and instance-level language descriptions. We then encode\nboth instance-and scene-level language information into high-dimensional\nembeddings, which are utilized to guide the visual features during training. At\ninference, our LG-MOT uses the standard visual features without relying on\nannotated language descriptions. Extensive experiments on three benchmarks,\nMOT17, DanceTrack and SportsMOT, reveal the merits of the proposed\ncontributions leading to state-of-the-art performance. On the DanceTrack test\nset, our LG-MOT achieves an absolute gain of 2.2\\% in terms of target object\nassociation (IDF1 score), compared to the baseline using only visual features.\nFurther, our LG-MOT exhibits strong cross-domain generalizability. The dataset\nand code will be available at https://github.com/WesLee88524/LG-MOT."
}