{
  "title": "TAO: A Large-Scale Benchmark for Tracking Any Object",
  "authors": "Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, Deva Ramanan",
  "year": 2020,
  "url": "http://arxiv.org/abs/2005.10356v1",
  "abstract": "For many years, multi-object tracking benchmarks have focused on a handful of\ncategories. Motivated primarily by surveillance and self-driving applications,\nthese datasets provide tracks for people, vehicles, and animals, ignoring the\nvast majority of objects in the world. By contrast, in the related field of\nobject detection, the introduction of large-scale, diverse datasets (e.g.,\nCOCO) have fostered significant progress in developing highly robust solutions.\nTo bridge this gap, we introduce a similarly diverse dataset for Tracking Any\nObject (TAO). It consists of 2,907 high resolution videos, captured in diverse\nenvironments, which are half a minute long on average. Importantly, we adopt a\nbottom-up approach for discovering a large vocabulary of 833 categories, an\norder of magnitude more than prior tracking benchmarks. To this end, we ask\nannotators to label objects that move at any point in the video, and give names\nto them post factum. Our vocabulary is both significantly larger and\nqualitatively different from existing tracking datasets. To ensure scalability\nof annotation, we employ a federated approach that focuses manual effort on\nlabeling tracks for those relevant objects in a video (e.g., those that move).\nWe perform an extensive evaluation of state-of-the-art trackers and make a\nnumber of important discoveries regarding large-vocabulary tracking in an\nopen-world. In particular, we show that existing single- and multi-object\ntrackers struggle when applied to this scenario in the wild, and that\ndetection-based, multi-object trackers are in fact competitive with\nuser-initialized ones. We hope that our dataset and analysis will boost further\nprogress in the tracking community."
}