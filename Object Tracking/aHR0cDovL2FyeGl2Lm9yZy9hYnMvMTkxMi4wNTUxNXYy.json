{
  "title": "SiamMan: Siamese Motion-aware Network for Visual Tracking",
  "authors": "Wenzhang Zhou, Longyin Wen, Libo Zhang, Dawei Du, Tiejian Luo, Yanjun Wu",
  "year": 2019,
  "url": "http://arxiv.org/abs/1912.05515v2",
  "abstract": "In this paper, we present a novel siamese motion-aware network (SiamMan) for\nvisual tracking, which consists of the siamese feature extraction subnetwork,\nfollowed by the classification, regression, and localization branches in\nparallel. The classification branch is used to distinguish the foreground from\nbackground, and the regression branch is adopt to regress the bounding box of\ntarget. To reduce the impact of manually designed anchor boxes to adapt to\ndifferent target motion patterns, we design the localization branch, which aims\nto coarsely localize the target to help the regression branch to generate\naccurate results. Meanwhile, we introduce the global context module into the\nlocalization branch to capture long-range dependency for more robustness in\nlarge displacement of target. In addition, we design a multi-scale learnable\nattention module to guide these three branches to exploit discriminative\nfeatures for better performance. The whole network is trained offline in an\nend-to-end fashion with large-scale image pairs using the standard SGD\nalgorithm with back-propagation. Extensive experiments on five challenging\nbenchmarks, i.e., VOT2016, VOT2018, OTB100, UAV123 and LTB35, demonstrate that\nSiamMan achieves leading accuracy with high efficiency. Code can be found at\nhttps://isrc.iscas.ac.cn/gitlab/research/siamman."
}