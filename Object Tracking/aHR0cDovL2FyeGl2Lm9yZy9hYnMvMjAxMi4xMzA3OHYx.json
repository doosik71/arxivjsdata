{
  "title": "Rotation Equivariant Siamese Networks for Tracking",
  "authors": "Deepak K. Gupta, Devanshu Arya, Efstratios Gavves",
  "year": 2020,
  "url": "http://arxiv.org/abs/2012.13078v1",
  "abstract": "Rotation is among the long prevailing, yet still unresolved, hard challenges\nencountered in visual object tracking. The existing deep learning-based\ntracking algorithms use regular CNNs that are inherently translation\nequivariant, but not designed to tackle rotations. In this paper, we first\ndemonstrate that in the presence of rotation instances in videos, the\nperformance of existing trackers is severely affected. To circumvent the\nadverse effect of rotations, we present rotation-equivariant Siamese networks\n(RE-SiamNets), built through the use of group-equivariant convolutional layers\ncomprising steerable filters. SiamNets allow estimating the change in\norientation of the object in an unsupervised manner, thereby facilitating its\nuse in relative 2D pose estimation as well. We further show that this change in\norientation can be used to impose an additional motion constraint in Siamese\ntracking through imposing restriction on the change in orientation between two\nconsecutive frames. For benchmarking, we present Rotation Tracking Benchmark\n(RTB), a dataset comprising a set of videos with rotation instances. Through\nexperiments on two popular Siamese architectures, we show that RE-SiamNets\nhandle the problem of rotation very well and out-perform their regular\ncounterparts. Further, RE-SiamNets can accurately estimate the relative change\nin pose of the target in an unsupervised fashion, namely the in-plane rotation\nthe target has sustained with respect to the reference frame."
}