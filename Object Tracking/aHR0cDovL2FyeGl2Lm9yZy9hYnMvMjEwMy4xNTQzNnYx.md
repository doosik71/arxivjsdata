# Transformer Tracking
Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, Huchuan Lu

## 🧩 Problem to Solve
기존의 시아미즈(Siamese) 기반 객체 추적기들은 템플릿(template)과 탐색 영역(search region) 간의 특징을 융합하기 위해 상관(correlation) 연산을 핵심적으로 사용합니다. 그러나 상관 연산은 지역적인 선형 매칭 프로세스이므로, 의미론적 정보 손실을 야기하고 지역 최적점에 쉽게 빠질 수 있어 높은 정확도를 달성하는 데 한계가 있었습니다. 이 논문은 "상관보다 더 나은 특징 융합 방법이 있는가?"라는 질문에 답하고자 합니다.

## ✨ Key Contributions
*   상관 연산 없이 오직 어텐션(attention)만을 사용하여 템플릿과 탐색 영역 특징을 융합하는 새로운 **트랜스포머 기반 추적(TransT)** 프레임워크를 제안했습니다.
*   셀프 어텐션 기반의 **Ego-Context Augment (ECA) 모듈**과 크로스 어텐션 기반의 **Cross-Feature Augment (CFA) 모듈**로 구성된 독창적인 특징 융합 네트워크를 개발했습니다. 이 네트워크는 유용한 정보에 적응적으로 집중하고 장거리 특징 연관성을 확립하여 추적기의 분류 및 회귀 성능을 향상시킵니다.
*   대규모 LaSOT, TrackingNet, GOT-10k 벤치마크를 포함한 다양한 도전적인 데이터셋에서 **최첨단(state-of-the-art) 성능**을 달성했으며, 약 50fps의 **실시간 속도**로 동작합니다.

## 📎 Related Works
*   **시각 객체 추적**: SiamFC, SiamRPN, ATOM, DiMP와 같은 시아미즈 기반 추적기들은 상관 연산을 통해 템플릿 정보를 ROI(관심 영역)에 통합했습니다. 그러나 이러한 방법들은 전역 컨텍스트를 충분히 활용하지 못하고, 의미론적 정보 손실을 겪는다는 한계가 있었습니다.
*   **트랜스포머 및 어텐션**: Vaswani et al. [38]에 의해 처음 도입된 트랜스포머는 어텐션 메커니즘을 통해 입력 시퀀스의 전역 정보를 파악하여, NLP 및 음성 처리 분야에서 RNN을 대체했습니다. 최근 DETR [4]이 객체 탐지에 트랜스포머를 적용하여 성공을 거두었으며, 본 논문은 이러한 성공에 영감을 받아 추적 분야에 트랜스포머의 핵심 아이디어를 도입했습니다.
*   **추적 분야의 어텐션 메커니즘**: ACF [6], MLT [7], SiamAttn [46], CGACD [13] 등 일부 선행 연구들이 추적 성능 향상을 위해 어텐션 메커니즘을 도입했지만, 이들은 여전히 템플릿과 탐색 영역 특징 융합 시 상관 연산에 크게 의존했습니다. TransT는 상관 연산 없이 어텐션 기반 네트워크를 사용하여 특징을 직접 융합한다는 점에서 차별화됩니다.

## 🛠️ Methodology
TransT는 세 가지 주요 구성 요소로 이루어져 있습니다:

1.  **특징 추출 백본(Feature Extraction Backbone)**:
    *   입력으로 템플릿 이미지 패치($z \in \mathbb{R}^{3 \times H_{z0} \times W_{z0}}$)와 탐색 영역 이미지 패치($x \in \mathbb{R}^{3 \times H_{x0} \times W_{x0}}$)를 사용합니다.
    *   ResNet50 [18]의 수정된 버전을 백본으로 사용하며, 마지막 스테이지를 제거하고 4번째 스테이지의 출력을 사용합니다. 더 큰 특징 해상도를 얻기 위해 4번째 스테이지의 다운샘플링 유닛의 컨볼루션 스트라이드를 2에서 1로 변경하고, 수용 필드(receptive field)를 늘리기 위해 3x3 컨볼루션을 스트라이드 2의 딜레이션 컨볼루션(dilation convolution)으로 수정합니다.
    *   이를 통해 템플릿 특징 맵 $f_z \in \mathbb{R}^{C \times H_z \times W_z}$와 탐색 영역 특징 맵 $f_x \in \mathbb{R}^{C \times H_x \times W_x}$를 얻습니다 (여기서 $C=1024$).

2.  **특징 융합 네트워크(Feature Fusion Network)**:
    *   $f_z$와 $f_x$의 채널 차원을 $1 \times 1$ 컨볼루션을 통해 $d=256$으로 줄이고, 공간 차원을 평탄화하여 $f_{z1} \in \mathbb{R}^{d \times H_z W_z}$ 및 $f_{x1} \in \mathbb{R}^{d \times H_x W_x}$ 벡터 셋을 생성합니다.
    *   이 벡터들은 특징 융합 네트워크의 입력으로 사용됩니다.
    *   **Ego-Context Augment (ECA) 모듈**: 멀티 헤드 셀프 어텐션(multi-head self-attention)을 사용하여 특징 표현을 강화하고 유용한 의미론적 컨텍스트를 적응적으로 통합합니다. 위치 정보 구분을 위해 공간 위치 인코딩($P_x$)이 추가됩니다:
        $$X_{EC} = X + \text{MultiHead}(X+P_x, X+P_x, X)$$
    *   **Cross-Feature Augment (CFA) 모듈**: 멀티 헤드 크로스 어텐션(multi-head cross-attention)을 사용하여 두 입력($X_q$ 및 $X_{kv}$)으로부터 특징 벡터를 융합합니다. ECA와 유사하게 공간 위치 인코딩($P_q, P_{kv}$)이 사용되며, 모델의 피팅 능력을 향상시키기 위해 FFN(Feed-Forward Network) 모듈이 포함됩니다:
        $$\tilde{X}_{CF} = X_q + \text{MultiHead} (X_q+P_q, X_{kv}+P_{kv}, X_{kv})$$
        $$X_{CF} = \tilde{X}_{CF} + \text{FFN}(\tilde{X}_{CF})$$
    *   두 개의 ECA와 두 개의 CFA가 하나의 융합 레이어를 형성하며, 이 융합 레이어는 $N=4$번 반복됩니다. 마지막에는 추가적인 CFA 모듈이 두 브랜치의 특징 맵을 융합하여 최종 특징 맵 $f \in \mathbb{R}^{d \times H_x W_x}$를 출력합니다.

3.  **예측 헤드 네트워크(Prediction Head Network)**:
    *   분류(전경/배경) 및 회귀(정규화된 바운딩 박스 좌표) 브랜치로 구성된 3계층 퍼셉트론(perceptron)입니다.
    *   앵커 포인트나 앵커 박스에 대한 사전 지식 없이 정규화된 좌표를 직접 예측하여 추적 프레임워크를 간결하게 만듭니다.
    *   **학습 손실**: 분류를 위해 이진 교차 엔트로피 손실($L_{cls}$)을 사용하고, 음성 샘플의 손실에는 1/16의 가중치를 적용합니다. 회귀를 위해 $L_1$-norm 손실($L_1$)과 일반화된 IoU 손실($L_{GIoU}$)의 선형 조합을 사용합니다 ($L_{reg} = \lambda_G L_{GIoU} + \lambda_1 L_1$).
    *   **온라인 추적**: 예측 헤드 출력에 Hanning window 페널티($w=0.49$)를 적용하여 후처리한 후 가장 높은 신뢰도 점수를 가진 박스를 추적 결과로 선택합니다.

## 📊 Results
*   **대규모 데이터셋 (LaSOT, TrackingNet, GOT-10k)**: TransT는 AUC, Precision (P, P$_{Norm}$), AO, SR$_{0.5}$, SR$_{0.75}$ 등 모든 주요 평가 지표에서 최첨단 성능을 달성했습니다. 특히 LaSOT에서 64.9% AUC로 이전 최고 기록을 능가했습니다. TransT-GOT(GOT-10k 학습 데이터만 사용)도 GOT-10k에서 67.1% AO를 기록하며 SOTA를 달성했습니다.
*   **속성별 평가 (LaSOT)**: 다양한 추적 도전 속성(예: 폐색, 유사 객체 간섭, 모션 블러) 전반에 걸쳐 경쟁 추적기들보다 훨씬 우수한 성능을 보였습니다.
*   **소규모 데이터셋 (NFS, OTB2015, UAV123)**: 이들 데이터셋에서도 최첨단 또는 이에 필적하는 성능을 유지했습니다.
*   **실시간 성능**: GPU에서 약 50fps로 동작하여 실시간 추적 요건을 충족합니다.
*   **어블레이션 연구(Ablation Study)**:
    *   **후처리**: Hanning window 페널티와 같은 간단한 후처리 단계가 성능을 더욱 향상시켰지만, 후처리 없이도 TransT는 여전히 SOTA 성능을 유지하여 제안된 융합 네트워크의 견고함을 입증했습니다.
    *   **오리지널 트랜스포머와의 비교**: TransT에 사용된 맞춤형 ECA/CFA 모듈이 오리지널 트랜스포머 아키텍처를 직접 사용한 경우보다 훨씬 우수한 성능을 보였습니다.
    *   **상관 연산과의 비교**: CFA 모듈을 상관 연산으로 대체했을 때 성능이 크게 저하되었으며, ECA와 CFA 모듈이 모두 성능에 중요한 기여를 한다는 것이 입증되었습니다. 이는 상관 연산에 비해 어텐션 기반 융합 방식이 전역적인 컨텍스트와 풍부한 의미 정보를 포착하는 데 우월함을 보여줍니다.

## 🧠 Insights & Discussion
*   **어텐션의 우월성**: 본 연구의 핵심은 트랜스포머의 어텐션 메커니즘이 기존 상관 연산보다 훨씬 효과적인 특징 융합 방법이라는 것을 보여준 것입니다. 상관 연산은 지역적이고 선형적이어서 의미론적 정보를 손실하고 지역 최적점에 빠지기 쉬웠던 반면, 어텐션은 장거리 특징 연관성을 확립하고 유용한 전역 정보(예: 표적 경계, 유사한 방해물과의 구별)에 적응적으로 집중할 수 있습니다.
*   **풍부한 의미 정보**: 어텐션 기반 융합 네트워크는 단순히 유사성 맵이 아닌, 풍부한 의미 정보를 담은 특징을 출력하여 분류 및 회귀 성능을 직접적으로 향상시킵니다.
*   **어텐션 맵 시각화**: 어텐션 모듈이 작동하는 방식을 시각적으로 보여줍니다. 어텐션은 처음에 환경과 다른 객체를 구분하고, 템플릿 정보가 주어지면 핵심적인 식별 특징(예: 개미 꼬리의 붉은 점)에 집중합니다. 레이어가 깊어질수록 표적의 위치와 경계 정보를 정교하게 다듬어 회귀를 위한 "특징 라이브러리" 역할을 수행하는 것을 알 수 있습니다.
*   **추적에 최적화된 설계**: 오리지널 트랜스포머의 인코더-디코더 구조를 그대로 따르지 않고, 템플릿과 탐색 영역 특징 융합에 특화된 ECA/CFA 아키텍처를 설계한 것이 시각 객체 추적에 더 적합했습니다.
*   **간결성과 효율성**: 제안된 프레임워크는 간결하면서도 견고하며, 높은 정확도를 유지하면서 실시간 속도를 만족시키는 효율적인 솔루션을 제공합니다.

## 📌 TL;DR
*   **문제**: 기존 객체 추적기는 상관(correlation) 연산에 의존하여 특징을 융합하는데, 이는 지역적 선형 매칭으로 의미론적 정보 손실 및 지역 최적화 문제를 야기했습니다.
*   **제안 방법**: 이 논문은 상관 연산을 대체하는 새로운 트랜스포머 기반의 특징 융합 네트워크(TransT)를 제안합니다. 이 네트워크는 셀프 어텐션 기반의 ECA(Ego-Context Augment) 모듈과 크로스 어텐션 기반의 CFA(Cross-Feature Augment) 모듈을 사용하여 템플릿과 탐색 영역 특징을 효과적으로 융합합니다.
*   **핵심 결과**: TransT는 상관 연산 없이 오직 어텐션만으로 장거리 특징 연관성을 확립하고 풍부한 의미론적 정보를 추출합니다. 이를 통해 LaSOT, TrackingNet, GOT-10k 등 대규모 벤치마크에서 최첨단 성능을 달성했으며, 약 50fps의 실시간 속도로 동작합니다. 어텐션 시각화는 모델이 유용한 전역 정보를 자동으로 찾아내는 것을 보여주었습니다.