{
  "title": "Associate Everything Detected: Facilitating Tracking-by-Detection to the\n  Unknown",
  "authors": "Zimeng Fang, Chao Liang, Xue Zhou, Shuyuan Zhu, Xi Li",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.09293v2",
  "abstract": "Multi-object tracking (MOT) emerges as a pivotal and highly promising branch\nin the field of computer vision. Classical closed-vocabulary MOT (CV-MOT)\nmethods aim to track objects of predefined categories. Recently, some\nopen-vocabulary MOT (OV-MOT) methods have successfully addressed the problem of\ntracking unknown categories. However, we found that the CV-MOT and OV-MOT\nmethods each struggle to excel in the tasks of the other. In this paper, we\npresent a unified framework, Associate Everything Detected (AED), that\nsimultaneously tackles CV-MOT and OV-MOT by integrating with any off-the-shelf\ndetector and supports unknown categories. Different from existing\ntracking-by-detection MOT methods, AED gets rid of prior knowledge (e.g. motion\ncues) and relies solely on highly robust feature learning to handle complex\ntrajectories in OV-MOT tasks while keeping excellent performance in CV-MOT\ntasks. Specifically, we model the association task as a similarity decoding\nproblem and propose a sim-decoder with an association-centric learning\nmechanism. The sim-decoder calculates similarities in three aspects: spatial,\ntemporal, and cross-clip. Subsequently, association-centric learning leverages\nthese threefold similarities to ensure that the extracted features are\nappropriate for continuous tracking and robust enough to generalize to unknown\ncategories. Compared with existing powerful OV-MOT and CV-MOT methods, AED\nachieves superior performance on TAO, SportsMOT, and DanceTrack without any\nprior knowledge. Our code is available at https://github.com/balabooooo/AED."
}