{
  "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with\n  3D-awareness",
  "authors": "Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, Xihui Liu",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.18125v3",
  "abstract": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced\ntheir proficiency in 2D visual understanding tasks, enabling them to\neffectively process and understand images and videos. However, the development\nof LMMs with 3D scene understanding capabilities has been hindered by the lack\nof large-scale 3D vision-language datasets and powerful 3D encoders. In this\npaper, we introduce a simple yet effective framework called LLaVA-3D.\nLeveraging the strong 2D visual understanding priors from LLaVA, our LLaVA-3D\nefficiently adapts LLaVA for 3D scene understanding without compromising 2D\nunderstanding capabilities. To achieve this, we utilize the 3D position\nembeddings to enhance the 2D CLIP Patches with 3D spatial context information\nand construct 3D patches. By integrating the 3D position embeddings into 2D\nLMMs and employing joint 2D and 3D vision-language instruction tuning, we\nestablish a unified architecture for both 2D visual understanding and 3D scene\nunderstanding. In contrast to previous 3D LMMs, LLaVA-3D supports decoding\naccurate 3D spatial perception outputs, e.g., 3D bounding boxes, directly from\nthese 3D patches, without relying on the time-consuming off-the-shelf 3D\nsegmentors. Experimental results show that LLaVA-3D converges 3.5x faster than\nexisting 3D LMMs when trained on 3D vision-language datasets. Moreover,\nLLaVA-3D not only achieves state-of-the-art performance across various 3D tasks\nbut also maintains comparable 2D visual understanding and vision-language\nconversation capabilities with LLaVA."
}