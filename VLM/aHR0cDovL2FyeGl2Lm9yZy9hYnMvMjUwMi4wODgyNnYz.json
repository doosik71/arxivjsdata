{
  "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation",
  "authors": "Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari",
  "year": 2025,
  "url": "http://arxiv.org/abs/2502.08826v3",
  "abstract": "Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey."
}