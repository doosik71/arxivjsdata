{
  "title": "Refusing Safe Prompts for Multi-modal Large Language Models",
  "authors": "Zedian Shao, Hongbin Liu, Yuepeng Hu, Neil Zhenqiang Gong",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.09050v2",
  "abstract": "Multimodal large language models (MLLMs) have become the cornerstone of\ntoday's generative AI ecosystem, sparking intense competition among tech giants\nand startups. In particular, an MLLM generates a text response given a prompt\nconsisting of an image and a question. While state-of-the-art MLLMs use safety\nfilters and alignment techniques to refuse unsafe prompts, in this work, we\nintroduce MLLM-Refusal, the first method that induces refusals for safe\nprompts. In particular, our MLLM-Refusal optimizes a nearly-imperceptible\nrefusal perturbation and adds it to an image, causing target MLLMs to likely\nrefuse a safe prompt containing the perturbed image and a safe question.\nSpecifically, we formulate MLLM-Refusal as a constrained optimization problem\nand propose an algorithm to solve it. Our method offers competitive advantages\nfor MLLM model providers by potentially disrupting user experiences of\ncompeting MLLMs, since competing MLLM's users will receive unexpected refusals\nwhen they unwittingly use these perturbed images in their prompts. We evaluate\nMLLM-Refusal on four MLLMs across four datasets, demonstrating its\neffectiveness in causing competing MLLMs to refuse safe prompts while not\naffecting non-competing MLLMs. Furthermore, we explore three potential\ncountermeasures-adding Gaussian noise, DiffPure, and adversarial training. Our\nresults show that though they can mitigate MLLM-Refusal's effectiveness, they\nalso sacrifice the accuracy and/or efficiency of the competing MLLM. The code\nis available at https://github.com/Sadcardation/MLLM-Refusal."
}