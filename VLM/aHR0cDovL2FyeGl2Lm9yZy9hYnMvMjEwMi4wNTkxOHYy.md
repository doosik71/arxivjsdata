# Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig

## 🧩 Problem to Solve

기존의 시각 및 시각-언어 표현 학습은 ImageNet과 같은 명시적 레이블이 있는 데이터셋이나 Conceptual Captions와 같이 정제된(curated) 데이터셋에 크게 의존합니다. 이러한 데이터셋은 구축 및 정제 과정에 상당한 비용과 전문가 지식을 필요로 하며, 이는 데이터셋의 크기를 제한하여 학습 모델의 확장성을 저해합니다. 자연어 처리(NLP) 분야가 비용이 적게 드는 원시 텍스트 데이터를 활용하는 방향으로 전환된 것과 달리, 시각 분야는 여전히 고비용의 데이터 큐레이션 문제에 직면해 있습니다. 본 논문은 이러한 한계를 극복하고 시각 및 시각-언어 모델의 스케일업을 가능하게 하는 비용 효율적인 대규모 데이터 소스와 학습 방법을 모색합니다.

## ✨ Key Contributions

* **대규모 노이즈 데이터셋 활용**: 10억 개 이상의 이미지-텍스트 쌍으로 구성된 노이즈가 많은 데이터셋을 활용하여 시각 및 시각-언어 표현 학습을 확장했습니다. 이 데이터셋은 복잡한 필터링 및 후처리 단계를 생략하고 최소한의 빈도 기반 필터링만 적용하여 구축되어 비용 효율성을 높였습니다.
* **단순한 이중 인코더 아키텍처**: 이미지 인코더(EfficientNet)와 텍스트 인코더(BERT)로 구성된 단순한 이중 인코더 아키텍처와 대조 손실(contrastive loss)을 사용하여 시각 및 언어 표현을 공동으로 학습했습니다. 이 단순한 구조가 복잡한 교차-어텐션(cross-attention) 모델을 능가하는 성능을 달성했습니다.
* **최첨단(SOTA) 성능 달성**:
  * **이미지-텍스트 검색**: Flickr30K 및 MSCOCO 벤치마크에서 제로샷(zero-shot) 및 미세 조정(fine-tuned) 설정 모두에서 기존 SOTA 모델(CLIP, UNITER 등)을 크게 능가하는 성능을 기록했습니다.
  * **제로샷 이미지 분류**: ImageNet에서 76.4%의 상위-1 정확도를 달성하여 강력한 제로샷 분류 능력을 입증했습니다.
  * **시각 분류 전이 학습**: ImageNet 및 VTAB와 같은 시각 작업에서 강력한 성능을 보였으며, 기존 SOTA 모델과 비견되거나 능가했습니다.
* **교차-모달 검색 및 임베딩 구성 능력**: 텍스트, 이미지, 또는 이미지+텍스트 쿼리를 사용한 교차-모달 검색을 가능하게 하여 임베딩 공간 내에서 의미적 구성 능력(compositionality)을 보여주었습니다.
* **다국어 확장**: 100개 이상의 언어를 지원하는 다국어 ALIGN 모델($\text{ALIGN}_{\text{mling}}$)을 성공적으로 훈련하여 Multi30k 벤치마크에서 SOTA 성능을 달성했습니다.

## 📎 Related Works

* **고품질 시각 표현 학습**: ImageNet, OpenImages, JFT-300M과 같은 대규모 레이블 데이터셋을 통한 지도 학습 사전 훈련과 SimCLR, MoCo 등의 자기 지도 학습 및 NoisyStudent와 같은 반자기 지도 학습 방식이 연구되었습니다.
* **이미지 캡션을 활용한 시각 표현 학습**: Joulin et al. (2015), Li et al. (2017), Desai & Johnson (2020) 등은 이미지에서 캡션을 예측하여 시각 표현을 학습하는 방법을 제시했으나, 주로 소규모 데이터셋에 국한되고 교차-모달 검색에 필요한 시각-언어 표현을 생성하지 못했습니다.
* **시각-언어 표현 학습**:
  * **Visual-Semantic Embeddings (VSE)**: Frome et al. (2013), Faghri et al. (2018) 등의 초기 연구에서 시작되어, 객체 감지기나 다중 어텐션 레이어를 활용하여 개선되었습니다.
  * **교차-모달 어텐션 모델**: UNITER (Chen et al., 2020c), Oscar (Li et al., 2020) 등은 이미지-텍스트 매칭에서 우수한 성능을 보였으나, 연산량이 많아 실시간 시스템에는 비실용적이라는 한계가 있습니다.
* **CLIP (Radford et al., 2021)**: 자연어 감독을 통한 시각 표현 학습에 있어 본 연구와 밀접하게 관련되어 있으며, 유사한 대조 학습 설정을 사용합니다. ALIGN과의 주요 차이점은 훈련 데이터셋 구성 방식에 있습니다 (CLIP은 고주파 시각 개념의 허용 목록을 기반으로 데이터를 큐레이션).

## 🛠️ Methodology

ALIGN(A Large-scale ImAge and Noisy-text embedding) 모델은 대규모 노이즈가 있는 이미지-텍스트 쌍 데이터셋을 활용하여 시각 및 언어 표현을 공동으로 학습하는 이중 인코더(dual-encoder) 아키텍처를 사용합니다.

1. **데이터셋 구축**:
    * Conceptual Captions (Sharma et al., 2018)의 데이터셋 구축 방식을 따르지만, 복잡한 정제 단계를 대부분 완화하여 약 18억 개의 이미지-텍스트 쌍으로 구성된 대규모 노이즈 데이터셋을 생성했습니다.
    * **이미지 필터링**: 음란물 이미지, 짧은 변이 200픽셀 미만, 종횡비 3 초과 이미지 제거; 연관 텍스트가 1000개 이상인 이미지 제거; 다운스트림 평가 데이터셋과의 중복/유사 중복 이미지 제거.
    * **텍스트 필터링**: 10개 이상의 이미지에서 공유되는 텍스트(예: "1920x1080"), 희귀 토큰을 포함하거나 너무 짧거나 긴(3단어 미만 또는 20단어 초과) 텍스트 제거.
2. **이중 인코더 아키텍처**:
    * **이미지 인코더**: EfficientNet (전역 풀링 사용)을 사용합니다.
    * **텍스트 인코더**: BERT ([CLS] 토큰 임베딩 사용)를 사용하며, 훈련 데이터셋에서 10만 개의 워드피스(wordpiece) 어휘를 생성합니다. BERT 인코더 상단에 이미지 인코더의 출력 차원과 일치하는 완전 연결 계층이 추가됩니다.
    * 두 인코더 모두 처음부터 훈련됩니다.
3. **대조 학습 (Contrastive Learning)**:
    * 모델은 정규화된 소프트맥스(softmax) 손실을 통해 최적화됩니다. 훈련 배치 내에서 일치하는 이미지-텍스트 쌍은 긍정(positive) 샘플로, 나머지 모든 무작위 쌍은 부정(negative) 샘플로 간주됩니다.
    * 두 가지 손실 함수를 최소화합니다:
        * 이미지-텍스트 분류 손실 ($L_{i2t}$):
            $$L_{i2t} = -\frac{1}{N} \sum_{i}^{N} \log \frac{\exp(\mathbf{x}_i^\top \mathbf{y}_i / \sigma)}{\sum_{j=1}^{N} \exp(\mathbf{x}_i^\top \mathbf{y}_j / \sigma)}$$
        * 텍스트-이미지 분류 손실 ($L_{t2i}$):
            $$L_{t2i} = -\frac{1}{N} \sum_{i}^{N} \log \frac{\exp(\mathbf{y}_i^\top \mathbf{x}_i / \sigma)}{\sum_{j=1}^{N} \exp(\mathbf{y}_i^\top \mathbf{x}_j / \sigma)}$$
        여기서 $\mathbf{x}_i$는 $i$번째 이미지의 정규화된 임베딩, $\mathbf{y}_j$는 $j$번째 텍스트의 정규화된 임베딩, $N$은 배치 크기, $\sigma$는 로짓(logit) 스케일을 위한 온도(temperature) 변수입니다.
    * 온도 $\sigma$는 다른 모든 파라미터와 함께 학습되며, 효과적인 학습을 위해 컴퓨팅 코어 간에 임베딩을 연결하여 배치 내 부정 샘플의 수를 늘립니다.
4. **전이 학습 (Task Transfer)**: 이미지-텍스트 매칭 및 검색(Flickr30K, MSCOCO, CxC)과 시각 분류(ImageNet 및 그 변형, VTAB, 세분화된 분류 데이터셋) 작업을 대상으로 제로샷 및 미세 조정을 통해 성능을 평가합니다.

## 📊 Results

* **이미지-텍스트 매칭 및 검색**:
  * **Flickr30K 및 MSCOCO**: 제로샷 및 미세 조정 설정 모두에서 모든 측정 항목에서 SOTA 성능을 달성했습니다. 제로샷 이미지 검색에서는 CLIP보다 7% 이상 개선되었습니다.
  * **Crisscrossed Captions (CxC)**: 이미지-텍스트 및 텍스트-이미지 검색에서 R@1 기준으로 각각 22.2%, 20.1% 향상된 SOTA를 기록했습니다.
* **제로샷 시각 분류**:
  * **ImageNet 및 변형**: ImageNet에서 76.4%의 상위-1 정확도를 달성했으며, ImageNet-R에서 92.2%를 기록하며 CLIP보다 뛰어난 견고성을 보여주었습니다.
* **이미지 인코더 전이 학습**:
  * **ImageNet**: 학습된 시각 특징을 고정하고 분류 헤드만 훈련했을 때 85.5%의 상위-1 정확도로 SOTA를 달성했습니다. 모든 레이어를 미세 조정했을 때는 88.64%를 달성했으며, NoisyStudent 및 Meta-Pseudo-Labels와 비교 시, 44%의 FLOPS를 절약했습니다.
  * **VTAB**: 19개 작업에서 BiT-L을 능가하는 79.99%의 평균 정확도를 달성하여 모델의 견고성과 일반화 능력을 입증했습니다.
* **어블레이션 연구**:
  * 모델 성능은 더 큰 이미지 및 텍스트 백본(EfficientNet-L2 + BERT-Large)과 높은 임베딩 차원에서 지속적으로 향상됩니다.
  * 대규모 훈련 데이터셋(ALIGN 전체 데이터)이 모델 스케일업 및 성능 향상에 필수적임을 확인했습니다. 노이즈가 많더라도 데이터의 스케일이 충분하면 정제된 데이터보다 우수한 성능을 달성할 수 있음을 보여주었습니다(4배 크기의 ALIGN 데이터가 CC-3M을 능가).
* **다국어 ALIGN ($\text{ALIGN}_{\text{mling}}$)**:
  * Multi30k 데이터셋에서 M$^3$P를 모든 언어에서 크게 능가하는 제로샷 성능을 보여주었습니다. 프랑스어(fr)의 경우 57.8%의 절대 평균 재현율(mR) 향상을 기록했습니다.

## 🧠 Insights & Discussion

* **스케일의 압도적 효과**: 본 연구는 대규모의 노이즈가 많은 데이터셋이 비싸고 정제된 소규모 데이터셋의 품질 이점을 상쇄하고 능가할 수 있음을 강력히 시사합니다. 이는 시각 및 시각-언어 표현 학습의 패러다임을 비용 효율적인 대규모 웹 데이터 활용으로 전환할 수 있는 잠재력을 제시합니다.
* **단순한 아키텍처의 효율성**: 복잡한 교차-모달 어텐션 메커니즘 없이도 단순한 이중 인코더 아키텍처와 대조 학습만으로 SOTA 성능을 달성한 것은 모델 복잡성보다 데이터 스케일의 영향력이 더 클 수 있음을 보여주며, 이는 실제 애플리케이션에 대한 실용성을 높입니다.
* **강력한 임베딩 구성 능력**: 학습된 이미지 및 텍스트 임베딩이 의미적으로 잘 정렬되어 있을 뿐만 아니라, 임베딩 공간에서 텍스트 임베딩을 추가하거나 빼는 방식으로 이미지+텍스트 쿼리를 구성하여 "빨간색 숲"이나 "자동차 없는 장면"과 같은 새로운 형태의 교차-모달 검색을 가능하게 합니다. 이는 모델의 강력한 일반화 및 구성 능력을 입증합니다.
* **다국어 확장성**: 언어별 필터링 없이 다국어 웹 데이터를 통해 ALIGN 모델을 성공적으로 학습시킬 수 있음을 보여주었으며, 이는 비용 효율적인 방식으로 전 세계 언어에 걸쳐 시각-언어 모델을 확장할 수 있는 유망한 방향을 제시합니다.
* **한계 및 사회적 영향**: 훈련 목표가 교차-모달 매칭에 집중되어 있어 내부-모달 작업(텍스트-텍스트, 이미지-이미지 검색)에서는 상대적으로 성능 향상이 덜했습니다. 또한, 대규모 노이즈가 많은 웹 데이터를 사용하기 때문에 유해하거나 편향된 데이터가 모델에 반영되어 고정관념을 강화하거나 특정 인구 통계학적 그룹에 대한 성능 불균형을 야기할 수 있는 잠재적 위험에 대한 추가적인 분석과 완화 노력이 필요하며, 모델의 악의적인 오용도 금지되어야 합니다.

## 📌 TL;DR

ALIGN은 수동 큐레이션 없이 10억 개 이상의 노이즈가 있는 이미지-텍스트 쌍을 활용하여 시각 및 시각-언어 표현 학습을 스케일업하는 논문입니다. EfficientNet 이미지 인코더와 BERT 텍스트 인코더로 구성된 단순한 이중 인코더를 대조 손실로 학습시켰습니다. 이 모델은 이미지-텍스트 검색, 제로샷 이미지 분류, 시각 분류 전이 학습에서 기존 SOTA 성능을 능가했습니다. 핵심은 데이터의 "노이즈"가 "대규모 스케일"로 상쇄될 수 있다는 것을 보여주며, 임베딩의 구성 능력과 다국어 확장 가능성도 입증했습니다.
