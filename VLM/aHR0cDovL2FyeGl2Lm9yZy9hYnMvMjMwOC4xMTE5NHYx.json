{
  "title": "ViLLA: Fine-Grained Vision-Language Representation Learning from\n  Real-World Data",
  "authors": "Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, Curtis Langlotz",
  "year": 2023,
  "url": "http://arxiv.org/abs/2308.11194v1",
  "abstract": "Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained\non datasets consisting of image-caption pairs obtained from the web. However,\nreal-world multimodal datasets, such as healthcare data, are significantly more\ncomplex: each image (e.g. X-ray) is often paired with text (e.g. physician\nreport) that describes many distinct attributes occurring in fine-grained\nregions of the image. We refer to these samples as exhibiting high pairwise\ncomplexity, since each image-text pair can be decomposed into a large number of\nregion-attribute pairings. The extent to which VLMs can capture fine-grained\nrelationships between image regions and textual attributes when trained on such\ndata has not been previously evaluated. The first key contribution of this work\nis to demonstrate through systematic evaluations that as the pairwise\ncomplexity of the training dataset increases, standard VLMs struggle to learn\nregion-attribute relationships, exhibiting performance degradations of up to\n37% on retrieval tasks. In order to address this issue, we introduce ViLLA as\nour second key contribution. ViLLA, which is trained to capture fine-grained\nregion-attribute relationships from complex datasets, involves two components:\n(a) a lightweight, self-supervised mapping model to decompose image-text\nsamples into region-attribute pairs, and (b) a contrastive VLM to learn\nrepresentations from generated region-attribute pairs. We demonstrate with\nexperiments across four domains (synthetic, product, medical, and natural\nimages) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks,\nsuch as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP\npoints on LVIS) and retrieval (up to 14.2 R-Precision points)."
}