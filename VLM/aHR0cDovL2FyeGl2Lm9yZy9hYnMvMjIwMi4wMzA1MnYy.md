# OFA: 단순한 시퀀스-투-시퀀스 학습 프레임워크를 통한 아키텍처, 태스크 및 양식 통합

Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang

## 🧩 Problem to Solve

이 연구는 인간처럼 다양한 태스크와 양식(modality)을 처리할 수 있는 전능한 모델을 구축하는 것을 목표로 합니다. 기존의 복잡한 태스크/양식별 맞춤화 방식은 다음과 같은 문제점을 가지고 있습니다:

- **태스크별 추가 학습 가능한 구성 요소**: 태스크별 헤드(heads), 어댑터(adapters), 소프트 프롬프트(soft prompts) 등으로 인해 모델 구조가 태스크에 종속되고, 사전 학습(pretraining)과 미세 조정(finetuning) 간 불일치가 발생하며, 미지의 태스크에 대한 제로샷(zero-shot) 지원이 어렵습니다.
- **태스크별 공식화**: 사전 학습, 미세 조정, 제로샷 태스크의 공식화가 달라 '태스크 불가지론(Task-Agnostic)' 원칙에 위배되며, 태스크 수를 확장하기 어렵습니다.
- **양식 표현과 다운스트림 태스크의 얽힘**: 객체 감지기(object detector)와 같은 외부 구성 요소에 의존하여 이미지 입력 특징을 추출하는 방식은 개방형(open-domain) 데이터에 취약합니다.

이러한 문제점을 해결하고, **태스크 불가지론(Task-Agnostic, TA)**, **양식 불가지론(Modality-Agnostic, MA)**, **태스크 포괄성(Task Comprehensiveness, TC)**의 세 가지 핵심 속성을 만족하는 통합 패러다임을 제안하는 것이 주요 문제입니다.

## ✨ Key Contributions

- **OFA(One For All) 제안**: 태스크 불가지론적이고 양식 불가지론적이며 태스크 포괄성을 지원하는 통합 프레임워크인 OFA를 제안합니다. OFA는 단순한 시퀀스-투-시퀀스 학습 프레임워크와 통일된 지시 기반(instruction-based) 태스크 표현을 통해 이미지 생성, 시각적 접지(visual grounding), VQA(Visual Question Answering), 이미지 캡셔닝, 이미지 분류, 언어 모델링 등 다양한 교차-양식 및 단일-양식 태스크를 통합하는 첫 시도입니다.
- **적은 학습 데이터로 높은 성능 달성**: 최근의 SOTA 비전 및 언어 모델들이 훨씬 더 많은 규모의 교차-양식 데이터셋에 의존하는 것과 달리, OFA는 2000만 개의 공개 이미지-텍스트 쌍으로만 사전 학습되었습니다.
- **다양한 교차-양식 태스크에서 SOTA 달성**: 이미지 캡셔닝, VQA, 시각적 추론(visual entailment), 참조 표현 이해(referring expression comprehension) 등 일련의 교차-양식 태스크에서 새로운 최고 성능(SOTA)을 달성했습니다.
- **단일-양식 태스크에서 경쟁력 있는 성능**: 언어 모델(RoBERTa, ELECTRA, DeBERTa) 및 비전 모델(MoCo-v3, BEiT, MAE)의 SOTA와 비교하여, 단일-양식 태스크(자연어 이해 및 생성, 이미지 분류)에서도 매우 경쟁력 있는 성능을 보여주었습니다.
- **제로샷 학습 및 태스크/도메인 전이 능력 입증**: OFA가 제로샷 학습에서 경쟁력 있는 성능을 보이며, 새로운 태스크 지시를 통해 미지의 태스크로 효과적으로 전이되고, 미세 조정 없이도 외부 도메인 정보에 적응할 수 있음을 입증했습니다.

## 📎 Related Works

- **언어 및 비전 사전 학습**: BERT, GPT와 같은 트랜스포머(Transformer) 아키텍처의 발전과 마스크드 언어 모델링(MLM), ViT 기반 생성적 사전 학습(generative pretraining) 등 자기 지도 학습(self-supervised learning) 방법론을 언급합니다.
- **다중 양식 사전 학습**: UNITER, OSCAR, VL-T5, VinVL, ALBEF, SimVLM, Florence 등 다양한 연구들이 마스킹 전략, 인코더-디코더, 패치 프로젝션 등을 활용했음을 설명합니다. 기존 방법들은 주로 단일 양식 데이터에 한정되거나 능력에 제약이 있으며, 사전 학습과 미세 조정 간의 불일치 문제가 있다고 지적합니다.
- **통합 프레임워크**: NLP 분야의 T5(text-to-text transfer)와 같은 통합 모델, 혹은 Perceiver 및 Perceiver IO와 같이 균일한 바이트 시퀀스 표현을 사용하는 모델들을 언급합니다. 하지만 이러한 다중 양식 사전 학습 모델들은 VQA, 이미지 캡셔닝 등의 다운스트림 태스크에서 성능 저하를 겪거나 이미지 생성 능력이 없다는 한계가 있다고 언급합니다.

## 🛠️ Methodology

OFA는 입출력 및 아키텍처, 태스크, 양식을 통합하기 위한 단일 Seq2Seq(Sequence-to-Sequence) 프레임워크로 설계되었습니다.

- **입출력 및 아키텍처 통합**:
  - **입력 표현**:
    - **이미지**: ResNet 모듈을 사용하여 시각적 특징을 추출한 후, GPT 및 BART와 유사하게 바이트 쌍 인코딩(BPE)을 적용합니다.
    - **객체**: 객체 레이블과 바운딩 박스(bounding box)의 연속적인 좌표($x_1, y_1, x_2, y_2$)를 균일하게 이산화된 위치 토큰으로 표현합니다.
    - **텍스트**: 바이트 쌍 인코딩(BPE)을 통해 서브워드(subword) 시퀀스로 변환합니다.
    - **통합 어휘**: 서브워드, 이미지 코드, 위치 토큰을 포함한 모든 언어 및 시각 토큰에 대해 하나의 통합 어휘를 사용합니다. 이미지 코드는 VQGAN(Vector Quantized Generative Adversarial Network)과 같은 이미지 양자화(quantization) 기법을 통해 이산적인 코드 시퀀스로 변환하여, 예를 들어 $256 \times 256$ 이미지를 $16 \times 16$ 길이의 코드 시퀀스로 나타냅니다.
  - **아키텍처**: 사전 학습, 미세 조정, 제로샷 태스크 모두를 위해 트랜스포머 인코더-디코더(Transformer encoder-decoder)를 백본으로 채택합니다.
    - 인코더는 자기 주의(self-attention)와 피드 포워드 네트워크(FFN)로 구성됩니다.
    - 디코더는 자기 주의, FFN, 그리고 인코더 출력과의 연결을 위한 교차 주의(cross-attention)로 구성됩니다.
    - 학습 안정화를 위해 헤드 스케일링, 사후 주의 계층 정규화(post-attention LayerNorm), FFN 첫 계층 후 계층 정규화 등을 적용합니다.
    - 텍스트와 이미지에 대해 각각 두 개의 절대 위치 임베딩을 사용하며, 토큰 임베딩과 패치 임베딩으로부터 위치 상관성을 분리합니다. 텍스트에는 1D 상대 위치 바이어스(relative position bias), 이미지에는 2D 상대 위치 바이어스를 사용합니다.

- **태스크 및 양식 통합 (지시 기반 학습)**:
  - 사전 학습 및 다운스트림 태스크 모두를 지시 기반 시퀀스-투-시퀀스 생성 형태로 공식화하여, 추가 태스크별 계층 없이 다양한 태스크를 지원합니다.
  - **교차-양식 태스크 (5가지)**:
    - **시각적 접지 (Visual Grounding, VG)**: 이미지와 텍스트 지시("Which region does the text $x_t$ describe?")를 입력받아 위치 토큰 $\langle x_1, y_1, x_2, y_2 \rangle$을 생성합니다.
    - **접지 캡셔닝 (Grounded Captioning, GC)**: 이미지와 위치 지시("What does the region describe? region: $\langle x_1, y_1, x_2, y_2 \rangle$")를 입력받아 설명을 생성합니다.
    - **이미지-텍스트 매칭 (Image-Text Matching, ITM)**: 이미지와 텍스트 지시("Does the image describe $x_t$?")를 입력받아 "Yes" 또는 "No"를 생성하여 이미지-텍스트 쌍의 일치 여부를 판별합니다.
    - **이미지 캡셔닝 (Image Captioning, IC)**: 이미지와 지시("What does the image describe?")를 입력받아 캡션을 생성합니다.
    - **시각적 질문 답변 (Visual Question Answering, VQA)**: 이미지와 질문을 입력받아 답변을 생성합니다.
  - **단일-양식 태스크 (3가지)**:
    - **이미지 채우기 (Image Infilling)**: 마스킹된 이미지 입력과 지시("What is the image in the middle part?")를 통해 마스킹된 부분의 희소 코드를 생성합니다.
    - **객체 감지 (Object Detection)**: 이미지와 지시("What are the objects in the image?")를 통해 객체 위치와 레이블 시퀀스를 생성합니다.
    - **텍스트 채우기 (Text Infilling)**: 마스킹된 텍스트를 입력받아 원래 텍스트를 복구합니다 (BART의 디노이징 사전 학습과 유사).

- **사전 학습 데이터셋**: 공개적으로 사용 가능한 총 2000만 개의 이미지-텍스트 쌍 외에, 원본 이미지, 객체 레이블링 이미지, 일반 텍스트 데이터(예: CC12M, COCO, VQAv2, OpenImages, Pile 등)를 통합하여 사전 학습 데이터셋을 구성합니다. 데이터 누출을 방지하기 위해 다운스트림 태스크의 검증 및 테스트 세트에 나타나는 이미지는 필터링합니다.

- **학습 및 추론**:
  - **손실 함수**: 교차 엔트로피 손실 $L = - \sum_{i=1}^{|y|} \log P_{\theta}(y_i | y_{\lt i}, x, s)$로 모델을 최적화합니다.
  - **추론**: 빔 서치(beam search)를 사용하며, 분류 태스크에서는 검색 공간을 유효한 레이블 세트로 제한하는 트라이(Trie) 기반 검색 전략을 도입하여 효율성과 성능을 향상시킵니다.

- **모델 확장**: 33M(Tiny)부터 930M(Huge)에 이르는 5가지 크기의 OFA 모델을 개발하여 스케일링 효과를 탐구합니다.

## 📊 Results

- **교차-양식 태스크**:
  - **VQA 및 시각적 추론(SNLI-VE)**: VQA test-std에서 82.0, SNLI-VE test에서 91.2를 달성하며 새로운 SOTA를 기록했습니다. OFA$_{Large}$는 VLMo, SimVLM과 같은 이전 SOTA 모델들을 능가했습니다.
  - **이미지 캡셔닝 (MSCOCO)**: CIDEr 최적화 시 154.9 CIDEr 점수로 새로운 SOTA를 달성했습니다. 교차 엔트로피 최적화에서도 SimVLM$_{Huge}$보다 약 2점 높은 CIDEr 점수를 기록했습니다.
  - **참조 표현 이해 (RefCOCO, RefCOCO+, RefCOCOg)**: 3개 데이터셋 모두에서 SOTA를 달성했으며, UNICORN 대비 RefCOCO testA에서 3.61점, RefCOCO+ testA에서 6.65점, RefCOCOg test-u에서 4.85점 향상되었습니다.
  - **텍스트-투-이미지 생성**: FID, CLIPSIM, IS 지표에서 DALLE, CogView, NÜWA, GLIDE 등 기존 SOTA 모델들을 더 작은 샘플링 크기로 능가했습니다. 일반 및 반사실적(counterfactual) 쿼리에 대해 더 정교한 디테일과 상상 속 장면을 생성하는 정성적 우수성을 입증했습니다.

- **단일-양식 태스크**:
  - **자연어 이해 (GLUE)**: 모든 태스크에서 다중 양식 사전 학습 모델들을 크게 능가했으며, RoBERTa, ELECTRA, DeBERTa와 같은 SOTA 자연어 모델들과 비견되는 성능을 보였습니다.
  - **자연어 생성 (Gigaword 요약)**: Gigaword 데이터셋에서 새로운 SOTA를 달성했습니다.
  - **이미지 분류 (ImageNet-1K)**: OFA$_{Large}$는 85.6%의 Top-1 정확도를 기록하여 EfficientNet-B7, ViT-L 등을 능가했으며, BEiT-L, MAE-L과 유사한 성능을 보였습니다.

- **제로샷 학습 및 태스크 전이**:
  - **제로샷 GLUE 및 SNLI-VE**: Uni-Perceiver를 일반적으로 능가했지만, 문장 쌍 분류 태스크에서는 낮은 성능을 보였는데, 이는 사전 학습 데이터셋에 문장 쌍 데이터가 부족했기 때문으로 추정됩니다.
  - **미지의 태스크 전이 (접지 질의응답)**: 새로운 태스크 지시를 통해 미세 조정 없이도 만족스러운 성능을 달성하여 강력한 전이성을 입증했습니다.
  - **외부 도메인 전이 (VQA, 시각적 접지)**: 애니메이션이나 합성 이미지 등 외부 도메인 이미지에 대한 VQA 및 시각적 접지에서 미세 조정 없이도 만족스러운 성능을 보였습니다.

- **다중 태스크 사전 학습에 대한 절제 연구(Ablation Study)**:
  - **텍스트 채우기**: 이미지 캡셔닝 및 VQA 성능을 향상시켰으나, 이미지 분류 성능을 저하시켰습니다.
  - **이미지 채우기**: 이미지 분류 및 텍스트-투-이미지 생성 성능을 크게 향상시켰으나, 이미지 캡셔닝 및 VQA 성능을 저하시켰습니다.
  - **객체 감지 및 시각적 접지/접지 캡셔닝**: 이미지 캡셔닝 및 VQA 성능을 향상시켜 비전과 언어 간의 미세한 정렬(fine-grained alignment) 능력을 강화하는 데 중요함을 시사했습니다. 객체 감지는 시각적 이해도 향상에 기여했습니다.

## 🧠 Insights & Discussion

- OFA는 통합된 아키텍처와 지시 기반 학습을 통해 추가 계층 없이 다양한 태스크와 양식에서 포괄적인 능력을 발휘할 수 있음을 입증했습니다.
- 상대적으로 적은 사전 학습 데이터(2000만 개의 이미지-텍스트 쌍)만으로도 교차-양식 태스크에서 SOTA를 달성하고 단일-양식 태스크에서 경쟁력 있는 성능을 보이는 것은 다중 양식 모델의 효율성과 잠재력을 강조합니다.
- 강력한 제로샷 및 도메인 전이 능력은 OFA의 뛰어난 일반화 능력을 보여줍니다. 이는 단일 모델이 복잡한 현실 세계 태스크를 해결하는 데 충분할 수 있다는 가능성을 제시합니다.
- 절제 연구는 각 사전 학습 태스크가 모델의 다양한 능력에 미치는 영향을 밝혔습니다. 예를 들어, 언어 사전 학습은 교차-양식 텍스트 생성에, 이미지 채우기는 이미지 분류 및 생성에, 그리고 영역(region) 중심 태스크는 정교한 비전-언어 정렬에 중요합니다.
- 한계점으로는 문장 쌍 분류 태스크에서의 낮은 제로샷 성능과 지시 설계에 대한 민감성이 지적되었습니다. 이는 향후 연구에서 개선해야 할 과제입니다.

## 📌 TL;DR

OFA는 복잡하고 태스크/양식별 맞춤화된 기존 모델의 한계를 극복하기 위해 제안된 **태스크 불가지론적(Task-Agnostic)**, **양식 불가지론적(Modality-Agnostic)**, **태스크 포괄적(Task Comprehensive)** 통합 시퀀스-투-시퀀스 프레임워크입니다. 이 모델은 지시 기반 학습을 사용하여 VQA, 이미지 캡셔닝, 이미지 생성, 텍스트 요약, 이미지 분류 등 다양한 교차-양식 및 단일-양식 태스크를 추가적인 태스크별 계층 없이 처리합니다. OFA는 상대적으로 적은 2000만 개의 이미지-텍스트 쌍으로 사전 학습되었음에도 불구하고, 여러 교차-양식 태스크에서 새로운 SOTA를 달성하고 단일-양식 태스크에서도 경쟁력 있는 성능을 보여줍니다. 또한, 미지의 태스크와 도메인으로의 강력한 제로샷 및 전이 능력을 입증하여, 미래의 전능한 AI 모델 구축 가능성을 제시합니다.
