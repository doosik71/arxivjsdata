{
  "url": "http://arxiv.org/abs/2204.14198v2",
  "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
  "authors": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",
  "year": 2022,
  "abstract": "Building models that can be rapidly adapted to novel tasks using only a\nhandful of annotated examples is an open challenge for multimodal machine\nlearning research. We introduce Flamingo, a family of Visual Language Models\n(VLM) with this ability. We propose key architectural innovations to: (i)\nbridge powerful pretrained vision-only and language-only models, (ii) handle\nsequences of arbitrarily interleaved visual and textual data, and (iii)\nseamlessly ingest images or videos as inputs. Thanks to their flexibility,\nFlamingo models can be trained on large-scale multimodal web corpora containing\narbitrarily interleaved text and images, which is key to endow them with\nin-context few-shot learning capabilities. We perform a thorough evaluation of\nour models, exploring and measuring their ability to rapidly adapt to a variety\nof image and video tasks. These include open-ended tasks such as visual\nquestion-answering, where the model is prompted with a question which it has to\nanswer; captioning tasks, which evaluate the ability to describe a scene or an\nevent; and close-ended tasks such as multiple-choice visual question-answering.\nFor tasks lying anywhere on this spectrum, a single Flamingo model can achieve\na new state of the art with few-shot learning, simply by prompting the model\nwith task-specific examples. On numerous benchmarks, Flamingo outperforms\nmodels fine-tuned on thousands of times more task-specific data."
}