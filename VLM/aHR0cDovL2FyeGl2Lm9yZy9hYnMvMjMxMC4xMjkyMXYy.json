{
  "url": "http://arxiv.org/abs/2310.12921v2",
  "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement\n  Learning",
  "authors": "Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner",
  "year": 2023,
  "abstract": "Reinforcement learning (RL) requires either manually specifying a reward\nfunction, which is often infeasible, or learning a reward model from a large\namount of human feedback, which is often very expensive. We study a more\nsample-efficient alternative: using pretrained vision-language models (VLMs) as\nzero-shot reward models (RMs) to specify tasks via natural language. We propose\na natural and general approach to using VLMs as reward models, which we call\nVLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn\ncomplex tasks without a manually specified reward function, such as kneeling,\ndoing the splits, and sitting in a lotus position. For each of these tasks, we\nonly provide a single sentence text prompt describing the desired task with\nminimal prompt engineering. We provide videos of the trained agents at:\nhttps://sites.google.com/view/vlm-rm. We can improve performance by providing a\nsecond \"baseline\" prompt and projecting out parts of the CLIP embedding space\nirrelevant to distinguish between goal and baseline. Further, we find a strong\nscaling effect for VLM-RMs: larger VLMs trained with more compute and data are\nbetter reward models. The failure modes of VLM-RMs we encountered are all\nrelated to known capability limitations of current VLMs, such as limited\nspatial reasoning ability or visually unrealistic environments that are far\noff-distribution for the VLM. We find that VLM-RMs are remarkably robust as\nlong as the VLM is large enough. This suggests that future VLMs will become\nmore and more useful reward models for a wide range of RL applications."
}