{
  "url": "http://arxiv.org/abs/2004.06165v5",
  "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
  "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao",
  "year": 2020,
  "abstract": "Large-scale pre-training methods of learning cross-modal representations on\nimage-text pairs are becoming popular for vision-language tasks. While existing\nmethods simply concatenate image region features and text features as input to\nthe model to be pre-trained and use self-attention to learn image-text semantic\nalignments in a brute force manner, in this paper, we propose a new learning\nmethod Oscar (Object-Semantics Aligned Pre-training), which uses object tags\ndetected in images as anchor points to significantly ease the learning of\nalignments. Our method is motivated by the observation that the salient objects\nin an image can be accurately detected, and are often mentioned in the paired\ntext. We pre-train an Oscar model on the public corpus of 6.5 million\ntext-image pairs, and fine-tune it on downstream tasks, creating new\nstate-of-the-arts on six well-established vision-language understanding and\ngeneration tasks."
}