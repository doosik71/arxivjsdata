{
  "title": "Vision-Language Pre-training with Object Contrastive Learning for 3D\n  Scene Understanding",
  "authors": "Taolin Zhang, Sunan He, Dai Tao, Bin Chen, Zhi Wang, Shu-Tao Xia",
  "year": 2023,
  "url": "http://arxiv.org/abs/2305.10714v1",
  "abstract": "In recent years, vision language pre-training frameworks have made\nsignificant progress in natural language processing and computer vision,\nachieving remarkable performance improvement on various downstream tasks.\nHowever, when extended to point cloud data, existing works mainly focus on\nbuilding task-specific models, and fail to extract universal 3D vision-language\nembedding that generalize well. We carefully investigate three common tasks in\nsemantic 3D scene understanding, and derive key insights into the development\nof a pre-training model. Motivated by these observations, we propose a\nvision-language pre-training framework 3DVLP (3D vision-language pre-training\nwith object contrastive learning), which transfers flexibly on 3D\nvision-language downstream tasks. 3DVLP takes visual grounding as the proxy\ntask and introduces Object-level IoU-guided Detection (OID) loss to obtain\nhigh-quality proposals in the scene. Moreover, we design Object-level\nCross-Contrastive alignment (OCC) task and Object-level Self-Contrastive\nlearning (OSC) task to align the objects with descriptions and distinguish\ndifferent objects in the scene, respectively. Extensive experiments verify the\nexcellent performance of 3DVLP on three 3D vision-language tasks, reflecting\nits superiority in semantic 3D scene understanding."
}