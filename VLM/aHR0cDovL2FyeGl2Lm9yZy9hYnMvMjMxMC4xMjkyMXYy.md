# VISION-LANGUAGEMODELS AREZERO-SHOT REWARDMODELS FORREINFORCEMENTLEARNING
Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner

## 🧩 Problem to Solve
강화 학습(RL)에서 복잡한 시각 기반 작업을 수행하기 위한 보상 함수를 지정하는 것은 매우 어려운 문제다. 보상 함수를 수동으로 엔지니어링하는 것은 종종 비현실적이며, 대규모 인간 피드백으로부터 보상 모델을 학습하는 것은 비용이 많이 든다. 기존의 시각-언어 모델(VLM)을 보상으로 활용하려는 시도들은 광범위한 미세 조정이나 복잡하고 임시적인 절차를 요구하여 샘플 효율성이 낮았다. 따라서, 보상 함수를 보다 샘플 효율적이고 자연스러운 방식으로 지정하는 새로운 방법이 필요하다.

## ✨ Key Contributions
-   **VLM-RM (Vision-Language Model as Reward Model) 방법론 제안:** 사전 학습된 VLM을 시각 기반 RL 작업의 보상 모델(RM)로 활용하는 일반적인 제로샷(zero-shot) 접근 방식인 VLM-RM을 제안한다. 특히 CLIP을 VLM으로 사용하여 현재 환경 이미지와 자연어 작업 프롬프트 간의 코사인 유사도를 보상 함수로 사용한다.
-   **목표-기준선 정규화 (Goal-Baseline Regularization) 도입:** 보상 모델의 품질을 향상시키기 위해 "기준선 프롬프트"를 추가로 제공하고, 상태 임베딩을 기준선과 목표 프롬프트 사이의 방향으로 부분적으로 투영하여 관련 없는 정보가 보상에 미치는 영향을 줄인다.
-   **복잡한 휴머노이드 작업 학습 성공:** CartPole 및 MountainCar와 같은 고전적 RL 벤치마크뿐만 아니라, 무릎 꿇기, 연꽃 자세 앉기, 다리 찢기 등 수동 보상 지정이 어려운 MuJoCo 휴머노이드 로봇의 복잡한 작업을 단일 문장 텍스트 프롬프트만으로 성공적으로 학습시켰다.
-   **VLM 모델 규모 스케일링 효과 검증:** VLM-RM의 성능이 기반 VLM 모델의 크기에 강하게 비례함을 발견했다. 더 크고 컴퓨팅/데이터로 더 많이 훈련된 VLM일수록 더 나은 보상 모델이 되며, 특히 가장 큰 공개 CLIP 모델에서만 복잡한 휴머노이드 작업을 성공적으로 수행할 수 있었다.

## 📎 Related Works
-   **파운데이션 모델 및 VLM:** 대규모 데이터로 훈련된 파운데이션 모델(Bommasani et al., 2021)의 일반화 능력을 활용하며, 특히 CLIP(Radford et al., 2021)과 Flamingo(Alayrac et al., 2022)와 같은 VLM을 기반으로 한다.
-   **인간 피드백 기반 강화 학습(RLHF):** RLHF(Christiano et al., 2017)의 높은 비용 문제를 해결하기 위해 사전 학습된 파운데이션 모델 자체를 보상 신호로 활용하는 최근 연구 흐름(Bai et al., 2022)과 맥을 같이 한다.
-   **기존 VLM 기반 보상 연구:**
    -   Mahmoudieh et al. (2022): CLIP 인코더를 로봇 조작 작업의 보상 모델로 사용했으나, CLIP 이미지 인코더의 명시적인 미세 조정이 필요했다.
    -   Cui et al. (2022): CLIP을 사용하여 목표 이미지를 기반으로 로봇 조작 보상을 제공했으나, 자연어 설명으로는 제한적인 성공을 보였다.
    -   Du et al. (2023): Flamingo VLM을 "성공 감지기"로 미세 조정했지만, RL 정책 학습에 직접 사용하지는 않았다.
    -   Fan et al. (2022), Sontakke et al. (2023): 환경별 데이터 요구 또는 비디오 시연에 주로 초점을 맞춘다.
-   **언어 모델 기반 보상 연구:** Xie et al. (2023), Ma et al. (2023)은 주로 구조화된 환경 표현에서 언어 모델을 사용한다.
-   **본 연구의 차별점:** 본 연구는 CLIP의 제로샷 능력을 최대한 활용하여 어떠한 미세 조정 없이도 VLM을 효과적인 보상 모델로 사용하며, 이를 통해 복잡한 시각 기반 RL 작업을 성공적으로 해결한다.

## 🛠️ Methodology
1.  **VLM-RM의 정의:** 보상 함수가 없는 부분 관측 마르코프 의사결정 과정(POMDP)에서, 이미지 관측 $o = \psi(s)$와 자연어 작업 설명 $l$을 사용하여 보상 $R_{\text{VLM}}(s)$를 정의한다.
    $$R_{\text{VLM}}(s) = \text{VLM}(l, \psi(s), c)$$
    (여기서 $c$는 선택적 컨텍스트)
2.  **CLIP 기반 보상 모델:**
    -   실험에서는 VLM으로 CLIP 인코더를 사용한다. CLIP의 언어 인코더 $\text{CLIP}_{\text{L}}$과 이미지 인코더 $\text{CLIP}_{\text{I}}$는 동일한 잠재 공간으로 매핑된다.
    -   보상 함수는 작업 설명 임베딩 $\text{CLIP}_{\text{L}}(l)$과 상태 이미지 임베딩 $\text{CLIP}_{\text{I}}(\psi(s))$ 간의 코사인 유사도로 계산된다:
        $$R_{\text{CLIP}}(s) = \frac{\text{CLIP}_{\text{L}}(l) \cdot \text{CLIP}_{\text{I}}(\psi(s))}{\|\text{CLIP}_{\text{L}}(l)\| \cdot \|\text{CLIP}_{\text{I}}(\psi(s))\|}$$
3.  **목표-기준선 정규화 (Goal-Baseline Regularization):**
    -   보상 모델의 성능 향상을 위해 작업 설명 $l$ 외에 "기준선" 설명 $b$를 추가로 사용한다. (예: $l$= "a humanoid robot kneeling", $b$= "a humanoid robot").
    -   정규화된 보상은 목표 임베딩 $g$, 기준선 임베딩 $b$, 상태 임베딩 $s$를 사용하여 계산된다:
        $$R_{\text{CLIP-Reg}}(s) = 1 - \frac{1}{2} \| \alpha \text{proj}_{L} s + (1-\alpha)s - g \|_2^2$$
        (여기서 $L$은 $b$와 $g$에 의해 형성되는 선이며, $\alpha$는 정규화 강도 파라미터이다. $\alpha=0$일 때 기본적인 $R_{\text{CLIP}}$과 동일하다.)
    -   이 방법은 기준선에서 목표로 향하는 방향을 따라 표현을 투영하여, 작업과 무관한 정보를 제거한다.
4.  **RL 알고리즘과의 통합:**
    -   VLM-RM은 DQN(이산 액션) 또는 SAC(연속 액션)와 같은 표준 RL 알고리즘에서 환경의 보상 신호를 대체한다.
    -   환경과의 상호작용 시 관측($\psi(s)$)을 리플레이 버퍼에 저장하고, 주기적으로 배치된 관측을 CLIP 이미지 인코더를 통해 상태 임베딩으로 변환한다.
    -   미리 한 번 계산된 작업 임베딩과 이 상태 임베딩 간의 코사인 유사도를 계산하여 보상을 얻고, 이 보상을 사용하여 표준 RL 업데이트를 수행한다.

## 📊 Results
-   **고전적 제어 벤치마크:** CartPole 및 MountainCar 환경에서 VLM-RM은 실제 보상 함수와 높은 상관관계를 보였으며, 목표-기준선 정규화 적용 시 상관관계가 더욱 높아졌다. 환경을 더 "포토리얼리스틱"한 텍스처로 렌더링하면 VLM-RM의 성능이 크게 개선되어 성공적인 정책 학습으로 이어졌다. 반면, 추상적인 렌더링에서는 학습에 실패했다.
-   **복잡한 휴머노이드 작업:** MuJoCo Humanoid-v4 환경에서 무릎 꿇기, 연꽃 자세, 다리 찢기, 서기, 팔 올리기와 같은 8가지 복잡한 작업 중 5가지를 단일 텍스트 프롬프트만으로 100% 성공률로 학습했다. 환경 텍스처를 더 사실적으로 변경하는 것이 중요했으며, 카메라 앵글 조정 또한 성능 향상에 기여했다. 실패한 "손 허리에 올리기", "한 발로 서기", "팔짱 끼기" 작업은 CLIP의 미묘한 시각적 차이 구별 한계나 물리 시뮬레이션의 난이도 때문으로 분석된다.
-   **VLM 모델 규모 스케일링 효과:** VLM의 크기가 커질수록 VLM-RM의 품질이 비례하여 향상됨을 확인했다. EPIC 거리(Equivalent Policy-Invariant Comparison) 지표에 따르면 모델 규모와 보상 모델 품질 사이에 로그-선형 관계가 관찰되었다. 특히, "무릎 꿇기" 작업은 가장 큰 CLIP 모델(ViT-bigG-14)을 사용했을 때만 100% 성공률로 학습되었으며, 더 작은 모델에서는 0% 성공률을 기록하여 성능의 급격한 '상전이'를 보여주었다. 목표-기준선 정규화는 모델 크기 전반에 걸쳐 보상 모델을 개선했지만, 작은 모델에서 더 큰 영향을 미쳤다.

## 🧠 Insights & Discussion
-   **강력한 제로샷 보상 모델로서의 VLM:** 본 연구는 사전 학습된 VLM이 복잡한 시각 기반 RL 작업에 대한 보상 함수를 자연어 프롬프트만으로 효과적으로 제공할 수 있음을 입증했다. 이는 수동 보상 엔지니어링의 비효율성과 인간 피드백 수집의 높은 비용 문제를 해결할 잠재력을 가진다.
-   **모델 스케일의 결정적 역할:** VLM-RM의 성능은 기반 VLM의 크기와 역량에 크게 의존한다. 더 크고 강력한 VLM일수록 더 미묘하고 복잡한 작업을 이해하고 적절한 보상 신호를 생성하는 데 뛰어나다. 이는 향후 더 발전된 VLM이 광범위한 RL 애플리케이션에서 보상 모델로서 더욱 유용해질 것임을 강력하게 시사한다.
-   **VLM의 한계와 환경의 현실성:** 현재 CLIP 모델의 실패 모드(예: 미묘한 자세 차이 구별 실패, 제한된 공간 추론 능력)는 기존 VLM의 알려진 한계와 일치한다. 또한, VLM이 훈련된 데이터 분포와 유사한 "포토리얼리스틱"한 환경에서 VLM-RM이 더 잘 작동함이 확인되어, 환경의 시각적 현실성이 VLM 기반 보상의 성공에 중요한 요소임을 보여준다.
-   **향후 연구 방향:** VLM-RM은 다양한 확장 가능성을 가진다. 특정 환경에 대한 VLM 미세 조정, 목표 기반 감독을 넘어선 비디오 인코더를 활용한 보상 지정, 대화형 VLM을 통한 복잡한 작업 지정 등이 고려될 수 있다. 또한, VLM-RM의 강건성과 안전성, 특히 RL 에이전트의 최적화 압력에 대한 강건성을 연구하는 것이 중요하다.

## 📌 TL;DR
이 논문은 사전 학습된 시각-언어 모델(VLM)을 강화 학습(RL)의 제로샷(zero-shot) 보상 모델로 활용하는 'VLM-RM' 방법론을 제안한다. 연구팀은 CLIP 모델과 단일 문장 텍스트 프롬프트만으로 MuJoCo 휴머노이드 로봇이 무릎 꿇기, 연꽃 자세 등 복잡한 작업을 수동 보상 함수나 인간 피드백 없이 성공적으로 학습할 수 있음을 보여준다. 특히, VLM의 크기가 커질수록 보상 모델의 성능이 크게 향상되며, 현재 VLM의 한계(예: 미묘한 공간 추론 부족, 비현실적 환경)가 실패 모드로 작용함을 확인한다. 이는 미래의 대규모 VLM이 다양한 RL 애플리케이션에서 보상 모델로서 더욱 강력하고 유용해질 것임을 시사한다.