# ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision

Wonjae Kim, Bokyung Son, Ildoo Kim

## 🧩 Problem to Solve

기존 Vision-and-Language Pre-training (VLP) 모델들은 시각적 입력에서 특징을 추출하기 위해 **영역 기반 감독(예: 객체 감지)** 및 **무거운 컨볼루션 아키텍처(예: ResNet)**에 크게 의존합니다. 이는 다음 두 가지 문제를 야기합니다:

1. **효율성/속도**: 시각적 특징 추출 과정 자체가 멀티모달 상호작용 단계보다 훨씬 더 많은 계산을 필요로 하여 비효율적입니다.
2. **표현력**: 시각적 임베더의 표현력과 미리 정의된 시각적 어휘에 의해 상한선이 정해집니다.
   이 논문은 이러한 무거운 시각적 임베더의 필요성을 없애고, 시각적 입력을 텍스트 입력과 동일하게 컨볼루션 없이 간결하게 처리하는 VLP 모델을 제시하고자 합니다.

## ✨ Key Contributions

- **가장 단순한 VLP 아키텍처**: ViLT는 시각적 특징 추출 및 처리를 별도의 깊은 시각적 임베더 대신 트랜스포머 모듈에 위임하여 모델의 런타임 및 파라미터 효율성을 크게 향상시켰습니다.
- **경쟁력 있는 성능 달성**: 영역 특징(region features) 또는 깊은 컨볼루션 시각적 임베더 없이도 비전-언어 하류(downstream) 작업에서 처음으로 경쟁력 있는 성능을 달성했습니다.
- **새로운 학습 기법 도입**: VLP 학습 방식에서 전례 없던 `Whole Word Masking`과 `Image Augmentation`이 하류 작업 성능을 더욱 향상시킨다는 것을 경험적으로 입증했습니다.

## 📎 Related Works

- **VLP 모델의 분류**:
  - **Region Feature 기반**: 대부분의 VLP 모델(예: ViLBERT, UNITER, LXMERT, OSCAR, VinVL)은 Faster R-CNN과 같은 객체 탐지기를 사용하여 이미지 특징을 추출합니다. 이 방식은 높은 정확도를 제공하지만 계산 비용이 매우 높습니다.
  - **Grid Feature 기반**: Pixel-BERT와 같은 모델은 ResNet 백본의 출력 그리드 특징을 사용합니다. 이는 영역 특징보다는 빠르지만 여전히 깊은 CNN을 사용합니다.
- **비전-언어 모델의 분류 (Figure 2)**:
  - **(a) VE>TE>MI**: 시각적 임베더가 텍스트 및 모달 상호작용보다 무거운 모델 (예: VSE++, SCAN).
  - **(b) VE=TE>MI**: 시각적 및 텍스트 임베더가 동일한 계산량을 가지지만 모달 상호작용은 얕은 모델 (예: CLIP).
  - **(c) VE>MI>TE**: 대부분의 현대 VLP 모델로, 시각적 임베더가 가장 무겁고 깊은 트랜스포머로 모달 상호작용을 처리합니다.
  - **(d) MI>VE=TE**: **ViLT**에 해당하는 유형으로, 시각적 임베딩이 텍스트 임베딩과 유사하게 가볍고, 대부분의 계산이 모달 상호작용에 집중됩니다.
- **모달리티 상호작용 방식**:
  - **Single-stream**: 이미지와 텍스트 입력을 결합하여 단일 트랜스포머에 입력하는 방식 (예: VisualBERT, UNITER). ViLT가 이 방식을 따릅니다.
  - **Dual-stream**: 두 모달리티를 분리된 스트림으로 처리한 후 교차-어텐션(cross-attention)으로 상호작용하는 방식 (예: ViLBERT, LXMERT).
- **시각적 임베딩 방식**:
  - **Patch Projection**: ViT(Vision Transformer)에서 이미지 분류를 위해 도입된 방식으로, 이미지를 패치로 분할한 후 선형 프로젝션하는 가장 단순한 방법입니다. ViLT는 이 방식을 채택합니다.

## 🛠️ Methodology

- **모델 개요**: ViLT는 시각적 입력 처리를 최소화하고 단일 스트림(single-stream) 접근 방식을 따르는 간결한 VLP 모델입니다.
  - **시각적 임베딩**: 입력 이미지를 $P \times P$ 크기의 패치로 슬라이스하고 선형 프로젝션하여 임베딩합니다. ViLT-B/32는 $32 \times 32$ 패치 크기를 사용하며, ViT-B/32 모델에서 사전 학습된 가중치로 초기화됩니다.
  - **텍스트 임베딩**: 단어 임베딩 및 위치 임베딩을 통해 텍스트 토큰을 임베딩합니다. BERT와 달리 텍스트 임베딩 관련 파라미터는 처음부터 학습됩니다.
  - **모달리티 상호작용**: 텍스트와 이미지 임베딩은 해당 모달-타입 임베딩 벡터와 합산된 후 결합된 시퀀스 $z_0$로 연결됩니다. 이 시퀀스는 $D$ 깊이의 트랜스포머 레이어를 통해 반복적으로 업데이트되어 최종 시퀀스 $z_D$를 생성합니다. 이는 `BERT-base`와 유사한 `Transformer Encoder` 구조를 가집니다.
- **사전 학습 목표**:
  - **Image Text Matching (ITM)**: 이미지-텍스트 쌍이 정렬되었는지(aligned) 여부를 예측합니다. 0.5의 확률로 정렬되지 않은 이미지를 대체하여 학습합니다.
  - **Word Patch Alignment (WPA)**: `Optimal Transport (IPOT)`를 사용하여 텍스트 부분($z_D|_{t}$)과 시각적 부분($z_D|_{v}$) 간의 정렬 점수를 계산하고, 이를 ITM 손실에 추가합니다.
  - **Masked Language Modeling (MLM)**: 마스킹된 텍스트 토큰의 실제 레이블을 예측합니다.
- **추가 학습 기술**:
  - **Whole Word Masking**: MLM 목표에서 전체 단어를 구성하는 모든 서브워드 토큰을 마스킹합니다. 이는 모델이 다른 모달리티로부터의 정보를 활용하도록 장려합니다.
  - **Image Augmentation**: 미세 조정(fine-tuning) 시 `RandAugment`를 적용하여 일반화 성능을 향상시킵니다 (색상 반전 및 컷아웃 제외).

## 📊 Results

- **속도 및 효율성**:
  - ViLT-B/32는 기존의 영역 특징 기반 VLP 모델(예: UNITER-Base)보다 최대 **수십 배(약 900ms vs 15ms)** 빠르며, 그리드 특징 기반 모델(예: Pixel-BERT-R50)보다도 **최소 4배(약 60ms vs 15ms)** 빠릅니다. 이는 시각적 임베더에 대한 계산량이 텍스트 임베더와 비슷한 수준으로 줄었기 때문입니다.
- **분류 작업 (VQAv2, NLVR2)**:
  - **NLVR2**: ViLT-B/32는 RandAugment 및 200K 학습 스텝을 적용했을 때 76.13의 테스트-P 정확도를 기록하며 UNITER-Base(75.80)보다 약간 우수하거나 경쟁력 있는 성능을 보여줍니다.
  - **VQAv2**: VQAv2에서는 다른 상위 VLP 모델에 비해 약간 낮은 성능을 보였는데, 이는 VQA 질문이 객체에 대한 경우가 많아 명시적인 객체 탐지 특징이 더 유리할 수 있음을 시사합니다.
- **검색 작업 (MSCOCO, Flickr30K)**:
  - **Zero-shot 검색**: ViLT-B/32는 ImageBERT보다 더 큰 데이터셋으로 사전 학습했음에도 불구하고 일반적으로 더 나은 성능을 보입니다.
  - **Fine-tuned 검색**: Pixel-BERT-R50보다 현저히 높은 Recall@K 성능을 달성합니다.
- **어블레이션 스터디 (Ablation Study)**:
  - 사전 학습 스텝을 늘리거나, `Whole Word Masking`을 적용하거나, 미세 조정 시 `RandAugment`를 적용하는 것이 성능 향상에 기여합니다.
  - 마스킹된 패치 예측(MPP) 목표는 성능에 기여하지 않는 것으로 나타났습니다.

## 🧠 Insights & Discussion

- **VLP 패러다임 전환**: ViLT는 무거운 컨볼루션 네트워크나 영역 기반 감독 없이도 VLP 모델이 경쟁력 있는 성능을 달성할 수 있음을 입증했습니다. 이는 향후 VLP 연구가 단일 모달 임베더를 강화하는 "군비 경쟁"보다는 트랜스포머 모듈 내에서의 모달리티 상호작용 개선에 집중해야 함을 시사합니다.
- **효율성의 중요성**: 현실 세계 애플리케이션에서는 특징 추출의 속도가 매우 중요하며, ViLT는 이 분야에서 기존 모델 대비 압도적인 우위를 제공합니다.
- **제한 사항 및 향후 연구**:
  - **확장성**: 대규모 ViLT 모델(ViLT-L, ViLT-H)은 더 많은 정렬된 비전-언어 데이터셋과 함께 더 나은 성능을 낼 수 있을 것입니다.
  - **시각적 입력에 대한 마스킹 모델링**: 단순한 마스킹된 패치 예측(MPP)이 효과를 보이지 않았으므로, 영역 감독 없이 시각적 모달리티를 위한 더 정교한 마스킹 목표(예: 교대/동시 클러스터링 기반 방법) 개발이 필요합니다.
  - **증강 전략**: 시각적 및 텍스트 입력에 대한 적절한 증강 전략(예: 가우시안 블러) 탐색이 모델의 일반화 능력을 향상시킬 수 있습니다.

## 📌 TL;DR

기존 VLP 모델들이 시각 특징 추출에 의존하는 비효율성을 해결하기 위해, 본 논문은 컨볼루션이나 영역 기반 감독 없이 이미지 패치를 텍스트 토큰과 유사하게 처리하는 경량화된 ViLT(Vision-and-Language Transformer)를 제안합니다. ViLT는 단순한 선형 패치 프로젝션과 ViT 기반 트랜스포머를 사용하여 기존 모델보다 최대 수십 배 빠르면서도 VQAv2, NLVR2, 이미지-텍스트 검색과 같은 다양한 하류 작업에서 경쟁력 있는 성능을 달성합니다. 이는 VLP 연구의 초점을 무거운 단일 모달 임베더 대신 모달 간 상호작용 강화로 전환해야 함을 시사합니다.
