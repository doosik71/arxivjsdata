{
  "title": "Tarsier: Recipes for Training and Evaluating Large Video Description\n  Models",
  "authors": "Jiawei Wang, Liping Yuan, Yuchen Zhang, Haomiao Sun",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.00634v2",
  "abstract": "Generating fine-grained video descriptions is a fundamental challenge in\nvideo understanding. In this work, we introduce Tarsier, a family of\nlarge-scale video-language models designed to generate high-quality video\ndescriptions. Tarsier employs CLIP-ViT to encode frames separately and then\nuses an LLM to model temporal relationships. Despite its simple architecture,\nwe demonstrate that with a meticulously designed two-stage training procedure,\nthe Tarsier models exhibit substantially stronger video description\ncapabilities than any existing open-source model, showing a $+51.4\\%$ advantage\nin human side-by-side evaluation over the strongest model. Additionally, they\nare comparable to state-of-the-art proprietary models, with a $+12.3\\%$\nadvantage against GPT-4V and a $-6.7\\%$ disadvantage against Gemini 1.5 Pro.\nWhen upgraded to Tarsier2 by building upon SigLIP and Qwen2-7B, it further\nimproves significantly with a $+4.8\\%$ advantage against GPT-4o. Besides video\ndescription, Tarsier proves to be a versatile generalist model, achieving new\nstate-of-the-art results across nine public benchmarks, including multi-choice\nVQA, open-ended VQA, and zero-shot video captioning. Our second contribution is\nthe introduction of a new benchmark -- DREAM-1K\n(https://tarsier-vlm.github.io/) for evaluating video description models,\nconsisting of a new challenging dataset featuring videos from diverse sources\nand varying complexity, along with an automatic method specifically designed to\nassess the quality of fine-grained video descriptions. We make our models and\nevaluation benchmark publicly available at\nhttps://github.com/bytedance/tarsier."
}