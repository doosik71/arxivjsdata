{
  "title": "ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched\n  Visual Descriptions",
  "authors": "Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, Mohamed Elhoseiny",
  "year": 2023,
  "url": "http://arxiv.org/abs/2303.06594v1",
  "abstract": "Asking insightful questions is crucial for acquiring knowledge and expanding\nour understanding of the world. However, the importance of questioning has been\nlargely overlooked in AI research, where models have been primarily developed\nto answer questions. With the recent advancements of large language models\n(LLMs) like ChatGPT, we discover their capability to ask high-quality questions\nwhen provided with a suitable prompt. This discovery presents a new opportunity\nto develop an automatic questioning system. In this paper, we introduce\nChatCaptioner, a novel automatic-questioning method deployed in image\ncaptioning. Here, ChatGPT is prompted to ask a series of informative questions\nabout images to BLIP-2, a strong vision question-answering model. By keeping\nacquiring new visual information from BLIP-2's answers, ChatCaptioner is able\nto generate more enriched image descriptions. We conduct human-subject\nevaluations on common image caption datasets such as COCO, Conceptual Caption,\nand WikiArt, and compare ChatCaptioner with BLIP-2 as well as ground truth. Our\nresults demonstrate that ChatCaptioner's captions are significantly more\ninformative, receiving three times as many votes from human evaluators for\nproviding the most image information. Besides, ChatCaptioner identifies 53%\nmore objects within the image than BLIP-2 alone measured by WordNet synset\nmatching. Code is available at https://github.com/Vision-CAIR/ChatCaptioner"
}