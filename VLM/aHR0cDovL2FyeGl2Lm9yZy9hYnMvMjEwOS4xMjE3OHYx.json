{
  "title": "MLIM: Vision-and-Language Model Pre-training with Masked Language and\n  Image Modeling",
  "authors": "Tarik Arici, Mehmet Saygin Seyfioglu, Tal Neiman, Yi Xu, Son Train, Trishul Chilimbi, Belinda Zeng, Ismail Tutar",
  "year": 2021,
  "url": "http://arxiv.org/abs/2109.12178v1",
  "abstract": "Vision-and-Language Pre-training (VLP) improves model performance for\ndownstream tasks that require image and text inputs. Current VLP approaches\ndiffer on (i) model architecture (especially image embedders), (ii) loss\nfunctions, and (iii) masking policies. Image embedders are either deep models\nlike ResNet or linear projections that directly feed image-pixels into the\ntransformer. Typically, in addition to the Masked Language Modeling (MLM) loss,\nalignment-based objectives are used for cross-modality interaction, and RoI\nfeature regression and classification tasks for Masked Image-Region Modeling\n(MIRM). Both alignment and MIRM objectives mostly do not have ground truth.\nAlignment-based objectives require pairings of image and text and heuristic\nobjective functions. MIRM relies on object detectors. Masking policies either\ndo not take advantage of multi-modality or are strictly coupled with alignments\ngenerated by other models. In this paper, we present Masked Language and Image\nModeling (MLIM) for VLP. MLIM uses two loss functions: Masked Language Modeling\n(MLM) loss and image reconstruction (RECON) loss. We propose Modality Aware\nMasking (MAM) to boost cross-modality interaction and take advantage of MLM and\nRECON losses that separately capture text and image reconstruction quality.\nUsing MLM + RECON tasks coupled with MAM, we present a simplified VLP\nmethodology and show that it has better downstream task performance on a\nproprietary e-commerce multi-modal dataset.",
  "citation": 8
}