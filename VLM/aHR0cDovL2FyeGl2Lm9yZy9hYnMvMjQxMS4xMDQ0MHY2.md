# LLaVA-CoT: Let Vision Language Models Reason Step-by-Step

Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, Li Yuan

## 🧩 Problem to Solve

현재 Vision-Language Models (VLMs)는 복잡한 시각적 질문 응답(VQA) 작업에서 체계적이고 구조화된 추론을 수행하는 데 어려움을 겪습니다. 이들은 종종 문제를 제대로 정리하지 않고 응답을 시작하거나, 논리적인 추론에서 벗어나거나, 오류나 환각(hallucination)을 생성하여 잘못된 추론 경로로 이어지는 경향이 있습니다. 기존의 Chain-of-Thought (CoT) 프롬프팅은 도움이 되지만, 대부분의 VLM은 추론 과정에서 여전히 오류나 환각을 자주 생성합니다.

## ✨ Key Contributions

- **LLaVA-CoT 소개:** 자율적인 다단계 추론(요약, 시각적 해석, 논리적 추론, 결론 생성)을 수행하도록 설계된 VLM인 LLaVA-CoT를 제안했습니다.
- **LLaVA-CoT-100k 데이터셋 구축:** GPT-4o로 생성된 구조화된 추론 주석이 포함된 100k 샘플 데이터셋을 구축하여 모델이 체계적인 다단계 방식으로 질문에 접근하도록 훈련시켰습니다.
- **SWIRES (Stage-wise Retracing Search) 방법론 제안:** 테스트 시 효과적이고 효율적인 스케일링을 가능하게 하는 새로운 단계별 되짚기 검색(retracing search) 방법을 제안하여 모델의 자체 성찰 및 오류 수정 능력을 향상시켰습니다.
- **탁월한 성능 입증:** 100k 훈련 샘플과 테스트 시 스케일링만으로도, LLaVA-CoT는 기본 모델보다 다양한 멀티모달 추론 벤치마크에서 평균 9.4% 향상된 성능을 보였을 뿐만 아니라, Gemini-1.5-pro, GPT-4o-mini, Llama-3.2-90B-Vision-Instruct와 같은 더 크고 심지어 폐쇄형 모델의 성능을 능가했습니다.

## 📎 Related Works

- **대규모 언어 모델을 활용한 시각적 추론:** 기존 VLM은 신경-상징적 접근 방식(VISPROG, Chat-UniVi, VILA 등)을 사용하여 시각적 추론을 명시적으로 모델링했습니다. LLM의 발전과 함께 VLM은 LLM의 고급 추론 능력을 활용하여 시각적 작업을 해석하며, LLM 인스트럭션 튜닝(프롬프트 튜닝, 인컨텍스트 학습, 지도 미세 조정) 또한 추론 능력 향상에 기여합니다.
- **대규모 언어 모델의 CoT (Chain-of-Thought):** CoT 프롬프팅(Wei et al.)은 어려운 질문에 대해 단계별 추론 궤적을 제공합니다. Prism, MSG, Distilling CoT, Visual CoT와 같은 최근 연구들은 CoT 추론이 VLM의 추론 능력을 크게 향상시킨다는 것을 보여주었습니다. 본 논문은 구조화된 CoT를 통해 추론 성능을 더욱 향상시키는 것을 목표로 합니다.
- **테스트 시간 스케일링:** 기존 방법으로는 다수결 투표, best-of-N 검색, 빔 검색 등이 있습니다. 이러한 방법들은 고정된 간격으로 검색을 수행하거나, 전체 응답의 정확도를 평가하기 어렵다는 한계가 있습니다. 본 연구에서는 이를 해결하기 위해 단계별 빔 검색 및 오류 수정 기능이 강화된 SWIRES를 제안합니다.

## 🛠️ Methodology

- **구조화된 사고를 통한 추론 능력 향상:**
  LLaVA-CoT는 답변 생성 과정을 네 가지 구조화된 추론 단계로 분해하며, 각 단계는 전용 태그(`<SUMMARY>`...`</SUMMARY>`, `<CAPTION>`...`</CAPTION>`, `<REASONING>`...`</REASONING>`, `<CONCLUSION>`...`</CONCLUSION>`)로 명시됩니다.
  1. **Summary Stage:** 질문의 핵심 내용과 해결 접근 방식을 요약합니다.
  2. **Caption Stage:** 질문과 관련된 이미지의 시각적 요소를 간결하게 설명합니다.
  3. **Reasoning Stage:** 초기 요약에 기반하여 구조화된 논리적 추론을 수행하여 예비 답변을 도출합니다.
  4. **Conclusion Stage:** 이전 추론을 바탕으로 최종 답변을 통합하여 사용자에게 직접적인 응답을 제공합니다.
     모델은 추가적인 프롬프팅 없이 단일 추론 과정에서 이러한 단계들을 원활하게 전환합니다.
- **데이터 준비 및 모델 훈련:**
  - LLaVA-CoT-100k 데이터셋을 구축하기 위해 ShareGPT4V, ChartQA, A-OKVQA, DocVQA, PISC, CLEVR 등 범용 VQA 데이터셋과 GeoQA+, AI2D, ScienceQA, CLEVR-Math 등 과학 중심 VQA 데이터셋의 샘플을 통합했습니다.
  - GPT-4o를 사용하여 요약, 캡션, 추론, 결론을 포함하는 상세한 추론 프로세스를 생성하고 이를 필터링하여 데이터셋을 완성했습니다.
  - Llama-3.2-11B-Vision-Instruct 모델을 기반 모델로 사용하여 LLaVA-CoT-100k 데이터셋으로 전체 파라미터 미세 조정(Supervised Fine-Tuning, SFT)을 수행했습니다.
- **단계별 되짚기 검색 (SWIRES)을 통한 테스트 시간 스케일링:**
  - **단계별 빔 검색:** 각 추론 단계(요약, 캡션 등)에서 M개의 후보 응답을 생성하고 보상 모델을 사용하여 상위 N개를 선택하여 다음 단계로 진행합니다. 이는 기존 빔 검색의 고정된 검색 간격 문제를 해결합니다.
  - **단계별 되짚기 검색 (SWIRES):** 단계별 빔 검색을 개선한 것으로, 되짚기 메커니즘을 통합합니다.
    - 각 단계에서 M개의 후보 응답을 생성합니다.
    - 생성된 응답 중 하나라도 미리 정의된 보상 임계값($\text{backtrack}_{\text{cutoff}} = \text{reward}_{\text{mean}} + \text{Z} \times \text{reward}_{\text{std}}$)을 초과하는지 확인합니다. 보상 모델로는 InternLM-XComposer2.5-Reward를 사용합니다.
    - 만약 모든 응답이 임계값을 초과하지 않으면, 이전 단계의 출력이 최적이지 않다고 판단하여 이전 단계로 **되짚어 가서** 새로운 응답을 재생성한 다음 현재 단계의 새로운 응답을 다시 생성합니다.
    - 이러한 되짚기 메커니즘은 모델이 자체 답변을 수정할 기회를 제공하여 추론 과정 중 오류 수정을 효과적으로 개선합니다. 되짚기 검색은 캡션 단계부터 적용됩니다 (요약 단계의 품질은 일반적으로 높다고 가정).

## 📊 Results

- **성능 향상:** LLaVA-CoT는 6가지 멀티모달 추론 벤치마크(MMStar, MMBench, MMVet, MathVista, AI2D, HallusionBench)에서 기본 모델인 Llama-3.2-11B-Vision-Instruct 대비 평균 5.8%의 점수 향상을 달성하며 방법론의 효과를 입증했습니다.
- **어블레이션 연구:**
  - **LLaVA-CoT-100k 데이터셋의 효과:** LLaVA-CoT-100k의 다단계 형식은 고급 추론 모델 학습에 필수적이며, 원본 Q&A 쌍으로 직접 훈련한 모델보다 성능이 훨씬 우수합니다.
  - **구조화된 태그의 중요성:** 구조화된 태그를 제거하면 성능이 크게 저하되어, 태그가 추론을 촉진하고 모델 성능을 향상시키는 데 필수적임을 확인했습니다.
  - **추론 집중 영역에서의 성능 향상:** LLaVA-CoT는 MMStar 벤치마크에서 인스턴스 추론, 논리적 추론, 수학, 과학 및 기술과 같이 체계적인 추론이 필요한 작업에서 상당한 개선을 보였습니다.
- **테스트 시간 스케일링 (SWIRES) 효과:**
  - 유사한 계산 제약 조건에서 SWIRES 방법은 단계별 빔 검색 및 best-of-N 검색보다 우수한 성능을 보였습니다.
  - SWIRES는 계산 시간이 증가함에 따라 가장 강력한 스케일링 효과를 나타내며, 다른 방법들이 정체되거나 감소하는 지점에서도 지속적으로 성능이 향상되었습니다.
  - 스케일링을 적용한 LLaVA-CoT는 벤치마크에서 평균 65.5%의 정확도를 달성했으며, 이는 스케일링을 적용하지 않았을 때의 62.4%보다 높은 수치입니다.
- **최신 VLM과의 비교:** LLaVA-CoT(11B)는 Qwen2-VL-7B, Deepseek-VL2, Llama-3.2-90B-Vision-Instruct와 같은 유사하거나 더 큰 규모의 오픈 소스 모델과 GPT-4o-mini, Gemini-1.5-pro와 같은 일부 폐쇄형 모델보다 뛰어난 성능을 보였습니다. 이는 특히 추론 능력에 크게 의존하는 벤치마크에서 LLaVA-CoT의 경쟁력을 입증합니다.

## 🧠 Insights & Discussion

LLaVA-CoT는 모델이 문제를 체계적으로 구성하고, 시각적 정보를 해석하고, 단계별로 추론한 다음 결론을 내리도록 강제함으로써 복잡한 멀티모달 추론 작업에서 기존 CoT 프롬프팅을 능가하는 상당한 성능 향상을 달성합니다. SWIRES는 테스트 시 성능 신뢰성을 높이는 확장 가능하고 효과적인 방법으로, 오류 수정 능력을 강화합니다. 이 연구는 멀티모달 모델의 추론 능력 향상에 대한 새로운 접근 방식을 제시하며, 향후 강화 학습을 통해 복잡한 멀티모달 추론을 더욱 개선할 수 있는 가능성을 열어줍니다. 하지만 LLaVA-CoT는 때때로 되짚기 과정에서 길을 잃거나, 입력 이미지가 과도하게 복잡하여 모델의 시각적 이해 능력을 초과할 경우 환각을 일으킬 수 있다는 한계도 있습니다.

## 📌 TL;DR

- **문제:** 기존 VLM은 복잡한 시각적 질문 응답(VQA)에서 체계적이고 구조화된 추론이 부족하여 오류와 환각을 자주 생성합니다.
- **해결책:** LLaVA-CoT는 요약, 캡션, 추론, 결론의 4단계로 구성된 구조화된 다단계 추론 프로세스를 도입합니다. 이를 위해 GPT-4o로 생성된 구조화된 추론 데이터셋인 LLaVA-CoT-100k를 구축하여 모델을 훈련합니다. 또한, 테스트 시 성능을 향상시키기 위해, 단계별 출력이 최적이지 않을 경우 이전 단계로 되짚어 가서 응답을 재생성하여 오류를 수정하는 **단계별 되짚기 검색 (SWIRES)** 방법을 제안합니다.
- **결과:** 11B 규모의 LLaVA-CoT는 100k 데이터만으로도 기본 모델뿐만 아니라 GPT-4o-mini, Gemini-1.5-pro와 같은 더 크거나 폐쇄형 모델보다 다양한 멀티모달 추론 벤치마크에서 뛰어난 성능을 보이며, 구조화된 추론과 SWIRES의 효과적인 테스트 시간 스케일링 능력을 입증했습니다.
