# Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers

Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu

## 🧩 Problem to Solve

기존 시각-언어(Vision-Language) 모델들은 주로 Faster R-CNN과 같은 객체 탐지 모델에서 추출된 영역 기반(region-based) 시각 특징을 사용했습니다. 이러한 접근 방식은 다음과 같은 한계를 가집니다:

- **정보 손실**: 객체의 모양, 겹치는 객체 간의 공간 관계 등 중요한 시각 정보가 손실됩니다.
- **특정 태스크 의존성**: 영역 특징 추출기는 특정 시각 태스크(예: 객체 탐지)에 맞춰 설계되어, 장면이나 감정과 같은 더 넓은 의미론적 시각 정보가 언어 이해와의 정보 불균형을 유발하고, 시각 표현 능력이 사전 정의된 범주로 제한됩니다.
- **주석 비용**: 바운딩 박스(bounding box) 주석에 대한 높은 비용이 발생합니다.
  이 논문은 이미지 픽셀과 언어 의미 사이의 보다 정확하고 철저한 연결을 직접 학습하여 이러한 한계를 극복하는 것을 목표로 합니다.

## ✨ Key Contributions

- **픽셀-텍스트 레벨 정렬**: CNN 기반 시각 인코더와 딥 멀티모달 트랜스포머(Deep Multi-Modal Transformers)를 결합하여 이미지 픽셀과 텍스트의 의미론적 연결을 직접 학습하는 최초의 엔드-투-엔드(end-to-end) 모델인 Pixel-BERT를 제안합니다.
- **랜덤 픽셀 샘플링 메커니즘**: 시각 표현 학습의 견고성을 향상하고 과적합(overfitting)을 방지하기 위한 랜덤 픽셀 샘플링 메커니즘을 제안합니다.
- **범용적인 사전 학습 모델**: Visual Genome 및 MS-COCO 데이터셋을 활용하여 마스킹 언어 모델링(MLM) 및 이미지-텍스트 매칭(ITM)과 같은 사전 학습 태스크를 통해 범용적인 시각 및 언어 임베딩을 학습합니다.
- **최첨단 성능 달성**: VQA (Visual Question Answering), 이미지-텍스트 검색(Image-Text Retrieval), NLVR$_2$ (Natural Language for Visual Reasoning for Real) 등 다양한 다운스트림 태스크에서 기존 최첨단(SOTA) 모델들을 능가하는 성능을 달성했습니다. 특히 VQA 태스크에서 단일 모델 성능을 이전 SOTA 대비 2.17점 향상시켰습니다.

## 📎 Related Works

- **사전 학습 메커니즘**: 자연어 처리(NLP) 분야에서 자가 지도 학습(self-supervised learning)의 성공에 힘입어, 시각-언어 교차 모달리티 학습(cross-modality learning)에서도 사전 학습이 널리 사용되고 있습니다.
  - **2-스트림 신경망**: Transformer 기반으로 시각 및 언어 정보를 각각 처리한 후 융합합니다 (예: ViLBERT, LXMERT).
  - **1-스트림 신경망**: BERT를 활용하여 감지된 바운딩 박스 특징과 문장 임베딩 특징 간의 양방향 공동 분포를 학습합니다 (예: VisualBERT, UNITER). Pixel-BERT는 시각 임베딩 방식에서 차별점을 두며 1-스트림 그룹에 속합니다.
- **시각 특징 임베딩**:
  - 초기에는 CNN 특징을 직접 사용했으나, Visual Genome 데이터셋과 Bottom-Up and Top-Down Attention 모델 도입 이후 객체 탐지 모델(Faster R-CNN)에서 추출한 영역 기반 특징이 주로 사용되었습니다.
  - 이러한 영역 기반 특징은 시각 의미론이 탐지 범주에 의해 제한된다는 한계가 있습니다. Pixel-BERT는 이러한 한계를 극복하기 위해 픽셀에서 직접 시각 임베딩을 학습합니다.

## 🛠️ Methodology

Pixel-BERT는 CNN 기반 시각 인코더, 워드-레벨 토큰 임베딩 기반 문장 인코더, 그리고 멀티모달 트랜스포머로 구성된 엔드-투-엔드 프레임워크입니다.

1. **문장 특징 임베딩 (Sentence Feature Embedding)**:
   - BERT [9]를 따라 WordPiece 토크나이저를 사용하여 문장을 토큰화하고 각 토큰을 벡터로 임베딩합니다.
   - 위치 임베딩($p_i$)을 추가하여 위치 정보를 인코딩합니다. 최종 언어 표현은 {$\hat{w_1}$, $\hat{w_2}$, ..., $\hat{w_n}$}입니다. 여기서 $\hat{w_i} = LayerNorm(w_i + p_i + s_w)$ 입니다. ($s_w$는 의미 임베딩 벡터)
2. **이미지 특징 임베딩 (Image Feature Embedding)**:
   - 객체 탐지 모델 대신 CNN 시각 백본(예: ResNet 또는 ResNeXt)을 사용하여 이미지 픽셀에서 특징을 추출합니다.
   - 추출된 특징 맵을 공간 차원(spatial dimension)으로 평탄화(flatten)하고, 시각 의미 임베딩 벡터($s_v$)를 더해 시각 임베딩 특징 {$\hat{v_1}$, $\hat{v_2}$, ..., $\hat{v_k}$}를 계산합니다. ($\hat{v_i} = v_i + s_v$)
   - 2x2 맥스 풀링 계층을 추가하여 시각 특징 맵의 공간 차원을 줄입니다.
3. **교차 모달리티 모듈 (Cross-Modality Module)**:
   - 트랜스포머를 사용하여 이미지 픽셀과 언어 토큰 간의 교차 모달리티 어텐션(attention)을 학습합니다.
   - [CLS] 및 [SEP] 토큰을 추가하여 최종 입력 시퀀스는 $\{[CLS], \hat{w_1}, \hat{w_2}, \cdots, \hat{w_n}, [SEP], \hat{v_1}, \hat{v_2}, \cdots, \hat{v_k}\}$ 형태가 됩니다.
   - CNN 백본과 트랜스포머는 단일 모델로 결합되어 엔드-투-엔드로 학습 가능합니다.
4. **사전 학습 (Pre-Training)**:
   - **마스킹 언어 모델링 (MLM)**: 언어 토큰의 15%를 무작위로 마스킹하고, 나머지 토큰과 시각 토큰을 기반으로 마스킹된 토큰을 예측하도록 학습합니다. $L_{MLM}$ 손실을 사용합니다.
   - **이미지-텍스트 매칭 (ITM)**: 이미지-문장 쌍이 일치하는지 여부를 분류하는 태스크입니다. 긍정 샘플(일치하는 쌍)과 부정 샘플(무작위로 섞인 쌍)을 동일하게 사용하여 $L_{ITM}$ 손실로 이진 분류기를 학습합니다.
   - **픽셀 랜덤 샘플링 (Pixel Random Sampling)**: 사전 학습 중에 추출된 픽셀 특징의 일부(예: 100개 픽셀)를 무작위로 샘플링하여 트랜스포머에 입력합니다. 이는 모델의 견고성을 높이고, 불완전한 시각 입력에서 의미론적 지식을 학습하도록 유도하며, 계산 비용을 줄입니다. 이 전략은 사전 학습 단계에서만 적용됩니다.

## 📊 Results

Pixel-BERT는 다양한 다운스트림 태스크에서 기존 최첨단 모델을 뛰어넘는 성능을 달성했습니다.

- **VQA (Visual Question Answering)**:
  - ResNet-50 백본으로 test-dev에서 71.35점을 달성하여 ViLBERT, VisualBERT보다 우수합니다.
  - ResNeXt-152 백본 장착 시 test-dev에서 74.45점, test-std에서 74.55점을 달성하여 모든 기존 모델을 크게 능가하며, 심지어 24-Layer Transformer를 사용하는 UNITER (Large)보다도 높은 점수입니다 (2.17점 향상).
- **NLVR$_2$ (Natural Language for Visual Reasoning for Real)**:
  - dev에서 76.5%, test-P에서 77.2%의 정확도를 달성하여 LXMERT 및 UNITER의 "Pair" 설정보다 우수합니다.
- **이미지-텍스트 검색 (Image-Text Retrieval)**:
  - Flickr30K 및 MS-COCO 데이터셋에서 Unicoder-VL 및 UNITER 대비 뛰어난 성능을 보였습니다.
  - MS-COCO 1K 테스트 세트에서 이미지-텍스트 검색(TR) recall@1에서 최소 0.6점, 텍스트-이미지 검색(IR) recall@1에서 최소 1.9점 향상.
  - 이는 텍스트-이미지 검색 태스크에서 이미지의 전역적 설명에 더 집중하며, Pixel-BERT의 아키텍처가 언어와 이미지 픽셀 간의 어텐션 학습에 유리함을 보여줍니다.
- **어블레이션 연구 (Ablation Study)**: MLM, ITM 사전 학습 태스크와 제안된 픽셀 랜덤 샘플링 메커니즘 각각이 모든 다운스트림 태스크 성능 향상에 크게 기여함을 확인했습니다.

## 🧠 Insights & Discussion

- **픽셀 레벨 정렬의 이점**: Pixel-BERT는 영역 기반 특징의 한계를 넘어 이미지 픽셀과 텍스트를 직접 정렬함으로써 시각 및 언어 임베딩 학습을 향상시킵니다. 이는 객체 모양, 공간 관계, 광범위한 시각 의미(장면, 감정) 등 더 풍부한 시각 정보를 활용할 수 있게 합니다.
- **사전 학습의 중요성**: MLM 및 ITM과 같은 사전 학습 태스크는 시각-언어 간의 강력한 연결을 구축하고 모델의 일반화 능력을 향상시키는 데 필수적임을 재확인했습니다.
- **랜덤 픽셀 샘플링의 효과**: 이 메커니즘은 불완전한 시각 입력으로부터 의미론적 지식을 학습하도록 유도하여 모델의 견고성을 높이고 과적합을 방지하는 효과적인 방법입니다.
- **시각화 결과**: 특정 토큰(예: "dog", "cutting", "room")이 이미지 내의 정확한 영역에 어텐션(attention)을 집중하는 것을 시각적으로 확인하여, 모델이 명시적인 공간 감독(예: 바운딩 박스 주석) 없이도 픽셀 수준에서 교차 모달리티 학습을 통해 시각적 표현을 잘 학습함을 보여줍니다. 이는 교차 모달리티 학습이 시각 정보의 의미론적 이해에 역으로 도움을 줄 수 있다는 가능성을 시사합니다.
- **향후 연구**: 대규모 이미지-문장 쌍 데이터셋(예: Conceptual Caption Dataset)으로의 사전 학습 확장 및 시각 콘텐츠에 대한 자가 지도 태스크(예: 마스킹된 시각 예측) 설계 및 결합 연구가 가능합니다. 현재 Pixel-BERT는 픽셀 재구축의 어려움 때문에 마스킹된 시각 예측 대신 픽셀 랜덤 샘플링을 사용합니다.

## 📌 TL;DR

Pixel-BERT는 기존 영역 기반 특징의 한계를 극복하기 위해 이미지 픽셀과 텍스트를 직접 정렬하는 딥 멀티모달 트랜스포머 기반의 엔드-투-엔드 모델입니다. 랜덤 픽셀 샘플링과 MLM, ITM 사전 학습 태스크를 통해 시각 표현의 견고성을 높이고 범용적인 시각-언어 임베딩을 학습합니다. 그 결과 VQA, NLVR$_2$, 이미지-텍스트 검색 등 여러 다운스트림 태스크에서 최첨단 성능을 달성하며, 픽셀 수준의 교차 모달리티 학습이 시각 정보의 풍부한 의미론적 이해에 크게 기여함을 입증했습니다.
