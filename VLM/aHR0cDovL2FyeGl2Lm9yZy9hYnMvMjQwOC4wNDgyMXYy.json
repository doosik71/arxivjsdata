{
  "title": "VLM-MPC: Vision Language Foundation Model (VLM)-Guided Model Predictive\n  Controller (MPC) for Autonomous Driving",
  "authors": "Keke Long, Haotian Shi, Jiaxi Liu, Xiaopeng Li",
  "year": 2024,
  "url": "http://arxiv.org/abs/2408.04821v2",
  "abstract": "Motivated by the emergent reasoning capabilities of Vision Language Models\n(VLMs) and their potential to improve the comprehensibility of autonomous\ndriving systems, this paper introduces a closed-loop autonomous driving\ncontroller called VLM-MPC, which combines the Model Predictive Controller (MPC)\nwith VLM to evaluate how model-based control could enhance VLM decision-making.\nThe proposed VLM-MPC is structured into two asynchronous components: The upper\nlayer VLM generates driving parameters (e.g., desired speed, desired headway)\nfor lower-level control based on front camera images, ego vehicle state,\ntraffic environment conditions, and reference memory; The lower-level MPC\ncontrols the vehicle in real-time using these parameters, considering engine\nlag and providing state feedback to the entire system. Experiments based on the\nnuScenes dataset validated the effectiveness of the proposed VLM-MPC across\nvarious environments (e.g., night, rain, and intersections). The results\ndemonstrate that the VLM-MPC consistently maintains Post Encroachment Time\n(PET) above safe thresholds, in contrast to some scenarios where the VLM-based\ncontrol posed collision risks. Additionally, the VLM-MPC enhances smoothness\ncompared to the real-world trajectories and VLM-based control. By comparing\nbehaviors under different environmental settings, we highlight the VLM-MPC's\ncapability to understand the environment and make reasoned inferences.\nMoreover, we validate the contributions of two key components, the reference\nmemory and the environment encoder, to the stability of responses through\nablation tests."
}