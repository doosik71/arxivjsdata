# Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio

## 🧩 Problem to Solve
이미지에 대한 자연어 캡션을 자동으로 생성하는 것은 장면 이해의 핵심 목표 중 하나입니다. 기존의 이미지 캡션 생성 모델들은 일반적으로 이미지를 정적인 단일 벡터 표현(예: 합성곱 신경망(ConvNet)의 최상위 계층 피처)으로 압축하여 사용했습니다. 이 방식은 이미지의 풍부하고 상세한 정보를 손실할 수 있으며, 특히 복잡한 장면에서 더욱 풍부하고 설명적인 캡션을 생성하는 데 한계가 있었습니다. 따라서, 모델이 이미지의 중요한 부분에 동적으로 초점을 맞춰 보다 정확하고 상세한 캡션을 생성할 수 있는 메커니즘이 필요합니다.

## ✨ Key Contributions
*   **두 가지 어텐션 기반 이미지 캡션 생성기 도입:**
    *   표준 역전파로 학습 가능한 "Soft" 결정론적 어텐션 메커니즘.
    *   변이 하한(variational lower bound)을 최대화하거나 REINFORCE 알고리즘을 사용하여 학습 가능한 "Hard" 확률론적 어텐션 메커니즘.
*   **시각화 통한 모델 해석 가능성 제시:** 어텐션 맵을 시각화하여 모델이 이미지의 "어디"에 "무엇을" 집중하는지 직관적으로 이해할 수 있음을 보여주었습니다.
*   **최첨단 성능 달성:** Flickr8k, Flickr30k, MS COCO 세 가지 벤치마크 데이터셋에서 어텐션 메커니즘의 유용성을 정량적으로 입증하며 최첨단 성능을 달성했습니다.

## 📎 Related Works
*   **신경망 기반 캡션 생성:** 기계 번역의 인코더-디코더 프레임워크(Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014)에서 영감을 받았습니다.
    *   Kiros et al. (2014a, 2014b): 다중 모달 로그-선형 모델.
    *   Mao et al. (2014): 순환 신경 언어 모델을 사용한 유사한 접근 방식.
    *   Vinyals et al. (2014) 및 Donahue et al. (2014): LSTM RNN을 사용했으며, Vinyals는 이미지를 한 번만 입력으로 사용하는 반면 다른 모델들은 각 시간 단계마다 이미지를 참조했습니다. 이들 작업은 주로 사전 훈련된 ConvNet의 최상위 계층에서 추출된 단일 특징 벡터를 사용했습니다.
*   **세분화된 시각-의미 정렬:**
    *   Karpathy & Li (2014): R-CNN 객체 탐지 결과와 양방향 RNN 출력을 사용하여 공동 임베딩 공간 학습.
    *   Fang et al. (2014): 객체 탐지를 통합한 3단계 파이프라인.
    *   이 논문의 어텐션 프레임워크는 명시적인 객체 탐지기를 사용하지 않고 잠재적 정렬을 처음부터 학습하여 "객체성(objectness)"을 넘어 추상적인 개념에 주목할 수 있도록 합니다.
*   **어텐션 메커니즘 연구:** 시각 관련 작업에 어텐션을 통합하는 이전 연구들(Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2014)과, 특히 기계 번역(Bahdanau et al., 2014) 및 객체 인식(Mnih et al., 2014; Ba et al., 2014)에서 어텐션을 성공적으로 적용한 연구들을 확장했습니다.

## 🛠️ Methodology
본 논문은 시각적 어텐션 메커니즘을 통합한 인코더-디코더 구조의 이미지 캡션 생성 모델을 제안합니다.

1.  **인코더: 합성곱 특징 추출 (Convolutional Feature Extraction)**
    *   원시 이미지를 입력으로 받아 합성곱 신경망(Oxford VGGnet)을 사용하여 `annotation vectors` $a = \{a_1, ..., a_L\}$를 추출합니다. 각 $a_i \in \mathbb{R}^D$는 이미지의 특정 부분에 해당하는 $D$차원 표현입니다.
    *   이전 연구들이 완전 연결 계층의 특징을 사용한 것과 달리, 공간적 정보를 보존하기 위해 `하위 합성곱 계층`의 특징(예: Max Pooling 전의 4번째 합성곱 계층의 $14 \times 14 \times 512$ 특징 맵)을 사용합니다. 이를 통해 디코더가 이미지의 특정 부분에 선택적으로 집중할 수 있습니다.

2.  **디코더: 장단기 메모리 네트워크 (LSTM) with Attention**
    *   LSTM은 `컨텍스트 벡터` $\hat{z}_t$, 이전 은닉 상태 $h_{t-1}$, 그리고 이전에 생성된 단어 $y_{t-1}$에 기반하여 각 시간 단계 $t$에서 하나의 단어 $y_t$를 생성합니다.
    *   컨텍스트 벡터 $\hat{z}_t$는 시간 $t$에 이미지 입력의 관련 부분을 동적으로 표현합니다.
    *   어텐션 모델 $f_{att}(a_i, h_{t-1})$은 각 `annotation vector` $a_i$에 대한 가중치 $\alpha_{ti}$를 계산하며, 이 가중치들은 $\sum_i \alpha_{ti} = 1$을 만족합니다.
        $$ e_{ti} = f_{att}(a_i, h_{t-1}) $$
        $$ \alpha_{ti} = \frac{\exp(e_{ti})}{\sum_{k=1}^L \exp(e_{tk})} $$
    *   계산된 가중치를 사용하여 컨텍스트 벡터 $\hat{z}_t$를 계산합니다.
    *   LSTM의 초기 메모리 상태($c_0$) 및 은닉 상태($h_0$)는 `annotation vector`들의 평균을 두 개의 개별 MLP에 통과시켜 예측됩니다.
    *   딥 출력 계층은 LSTM 상태, 컨텍스트 벡터, 이전 단어를 고려하여 출력 단어의 확률을 계산합니다: $p(y_t|a, y_{1}^{t-1}) \propto \exp(L_o(Ey_{t-1} + L_h h_t + L_z \hat{z}_t))$.

3.  **어텐션 메커니즘 학습: "Hard" vs "Soft" Attention**
    *   **확률론적 "Hard" 어텐션 (Stochastic "Hard" Attention):**
        *   어텐션 위치 $s_t$를 중간 잠재 변수로 간주하고, $\{\alpha_i\}$에 의해 파라미터화된 다항 분포에서 샘플링됩니다.
        *   $\hat{z}_t = \sum_i s_{t,i} a_i$, 여기서 $s_{t,i}$는 하나의 핫(one-hot) 지표 변수입니다.
        *   로그-우도(log-likelihood)의 변이 하한을 최대화하여 학습됩니다. 이는 몬테카를로 샘플링 기반의 REINFORCE 학습 규칙(Williams, 1992)과 동등합니다.
        *   그래디언트 추정량의 분산을 줄이기 위해 이동 평균 기준선과 엔트로피 항이 사용됩니다.
    *   **결정론적 "Soft" 어텐션 (Deterministic "Soft" Attention):**
        *   컨텍스트 벡터 $\hat{z}_t$의 기댓값을 직접 계산합니다: $\hat{z}_t = \sum_{i=1}^L \alpha_{t,i} a_i$.
        *   모델 전체가 미분 가능하므로, 표준 역전파를 사용하여 종단 간(end-to-end)으로 학습할 수 있습니다.
        *   **이중 확률적 어텐션 (Doubly Stochastic Attention):** 정규화 항 $\lambda \sum_i (1 - \sum_t \alpha_{ti})^2$를 도입하여, 모델이 캡션 생성 과정 동안 이미지의 모든 부분에 고르게 주의를 기울이도록 장려합니다. 이는 BLEU 점수를 향상시키고 더 풍부한 캡션을 생성하는 데 기여했습니다.
        *   이전 은닉 상태 $h_{t-1}$로부터 게이팅 스칼라 $\beta_t = \sigma(f_{\beta}(h_{t-1}))$를 예측하여, $\phi(\{a_i\},\{\alpha_i\}) = \beta_t \sum_i^L \alpha_i a_i$로 컨텍스트 벡터를 변형하여 객체에 더 강조를 둡니다.

4.  **훈련 절차 (Training Procedure):**
    *   적응형 학습률 알고리즘(Flickr8k에 RMSProp, Flickr30k/MS COCO에 Adam)을 사용하는 확률적 경사 하강법으로 학습됩니다.
    *   캡션 길이에 따라 미니배치를 구성하여 훈련 속도를 개선했습니다.
    *   규칙화 전략으로 드롭아웃(dropout)과 BLEU 점수 기반 조기 종료(early stopping)를 사용했습니다.

## 📊 Results
*   **정량적 결과:**
    *   Flickr8k, Flickr30k, MS COCO 세 가지 벤치마크 데이터셋에서 BLEU 및 METEOR 지표에서 최첨단 성능을 달성했습니다.
    *   특히 "Soft" 어텐션과 "Hard" 어텐션 모델은 BLEU-1부터 BLEU-4까지의 모든 BLEU 스코어와 METEOR에서 이전 모델(Google NIC, Log Bilinear 등)을 꾸준히 능가했습니다.
    *   MS COCO 데이터셋에서 "Soft" 어텐션 모델은 BLEU-4 25.0을 달성했으며, "Hard" 어텐션 모델은 METEOR 23.90으로 가장 높은 성능을 보여주었습니다. 이는 이중 확률적 정규화 및 하위 계층 특징 표현과 관련이 있는 것으로 추정됩니다.
    *   앙상블 모델을 사용하지 않고 단일 모델로 이러한 성능을 달성했습니다.
*   **정성적 결과:**
    *   어텐션 구성 요소를 시각화함으로써 모델의 출력에 대한 해석 가능성이 높아졌습니다. 모델이 각 단어를 생성할 때 이미지의 어떤 부분에 "시선"을 고정하는지 명확하게 보여줍니다(그림 2, 3, 5 참조).
    *   학습된 어텐션 정렬은 인간의 직관과 매우 잘 일치하며, 이를 통해 모델이 왜 특정 단어를 생성했는지, 또는 실수가 발생한 원인이 무엇인지 직관적으로 파악할 수 있습니다.
    *   이중 확률적 어텐션(Soft 어텐션 모델의 정규화)은 더 풍부하고 설명적인 캡션 생성에 기여했습니다.

## 🧠 Insights & Discussion
*   **높은 해석 가능성:** 어텐션 메커니즘은 이미지 캡션 생성 과정에 대한 전례 없는 수준의 투명성을 제공합니다. 모델이 어떤 단어를 생성할 때 이미지의 어느 부분에 초점을 맞추는지 시각적으로 보여줌으로써, 블랙박스 모델의 해석 가능성 문제를 해결하는 데 중요한 기여를 합니다.
*   **유연성과 일반화 능력:** 이 모델은 명시적인 객체 탐지기를 사용하지 않고 처음부터 잠재적 정렬을 학습합니다. 이는 모델이 단순히 "객체"를 인식하는 것을 넘어 추상적인 개념이나 장면의 비객체 영역에도 주의를 기울일 수 있게 하여 유연성과 일반화 능력을 향상시킵니다.
*   **정보 보존의 중요성:** 합성곱 신경망의 하위 계층에서 특징을 추출하고 어텐션을 활용함으로써, 모델은 상위 계층에서 압축되어 손실될 수 있는 미세하고 풍부한 시각적 정보를 효과적으로 활용할 수 있게 됩니다. 이는 더 상세하고 정확한 캡션 생성의 핵심 요소입니다.
*   **정규화의 효과:** "Soft" 어텐션에 도입된 이중 확률적 정규화는 모델이 이미지의 모든 관련 부분에 고르게 주의를 기울이도록 장려하여, 캡션의 품질을 정량적, 정성적으로 모두 향상시키는 데 기여했습니다.
*   **확장 가능성:** 인코더-디코더 접근 방식과 어텐션의 모듈성은 이미지 캡션 생성 외의 다른 도메인에서도 유용한 응용을 가질 것으로 기대됩니다.

## 📌 TL;DR
기존 이미지 캡션 생성 모델이 전체 이미지를 정적 표현으로 압축하여 상세 정보를 놓칠 수 있는 문제를 해결하기 위해, 이 논문은 시각적 어텐션(visual attention) 메커니즘을 통합한 새로운 신경망 기반 이미지 캡션 생성 모델을 제안합니다. 이 모델은 "Soft" (결정론적) 및 "Hard" (확률론적) 두 가지 어텐션 방식을 사용하여, 각 단어를 생성할 때 이미지의 특정 영역에 동적으로 초점을 맞춥니다. 이는 모델이 더 풍부하고 상세한 캡션을 생성하도록 도울 뿐만 아니라, 어텐션 맵 시각화를 통해 모델의 결정 과정을 인간이 직관적으로 이해할 수 있도록 높은 해석 가능성을 제공합니다. Flickr8k, Flickr30k, MS COCO 벤치마크 데이터셋에서 최첨단 성능을 달성하며 어텐션의 유용성을 입증하고, 모델의 설명력과 유연성을 크게 향상시켰습니다.