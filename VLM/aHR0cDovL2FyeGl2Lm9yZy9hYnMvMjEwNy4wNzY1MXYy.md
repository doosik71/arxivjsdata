# Align before Fuse: Vision and Language Representation Learning with Momentum Distillation

Junnan Li, Ramprasaath R. Selvaraju, Akhilesh D. Gotmare, Shafiq Joty, Caiming Xiong, Steven C.H. Hoi

## 🧩 Problem to Solve

이 논문은 기존의 대규모 시각-언어 표현 학습(Vision and Language Representation Learning, VLP) 방법들이 가진 세 가지 주요 한계를 해결하고자 합니다.

1. **시각-단어 토큰 비정렬**: 기존 Transformer 기반 멀티모달 인코더는 시각 토큰(영역 기반 이미지 특징)과 단어 토큰을 공동으로 모델링하지만, 이들 토큰이 서로 정렬되어 있지 않아 멀티모달 인코더가 이미지-텍스트 상호작용을 효과적으로 학습하기 어렵습니다.
2. **객체 감지기 의존성**: 많은 기존 VLP 방법은 사전 학습된 객체 감지기(object detector)를 사용하여 영역 기반 이미지 특징을 추출합니다. 이는 바운딩 박스(bounding box) 주석이 필요하며, 추론 시 고해상도 이미지를 요구하여 계산 비용이 높습니다.
3. **노이즈 데이터 학습**: 대규모 웹 데이터셋은 본질적으로 노이즈가 많으며, 마스크 언어 모델링(Masked Language Modeling, MLM)과 같은 기존의 사전 학습 목표는 이러한 노이즈 텍스트에 과적합되어 모델의 일반화 성능을 저하시킬 수 있습니다.

## ✨ Key Contributions

- **ALBEF (ALign BEfore Fuse) 프레임워크 제안**: 이미지와 텍스트 특징을 멀티모달 인코더로 융합하기 전에 정렬하는 새로운 VLP 프레임워크를 도입했습니다.
- **이미지-텍스트 대조 학습(Image-Text Contrastive Learning, ITC) 손실 도입**: 유니모달(unimodal) 인코더의 이미지 및 텍스트 표현을 정렬하고, 더 나은 유니모달 표현을 학습하며, 공동의 저차원 공간을 학습하도록 하여 이미지-텍스트 매칭(Image-Text Matching, ITM) 목표에서 더 정보성 높은 샘플을 찾을 수 있도록 합니다.
- **모멘텀 증류(Momentum Distillation, MoD) 제안**: 노이즈가 많은 웹 데이터로부터의 학습을 개선하기 위한 자체 학습(self-training) 방법으로, 모멘텀 모델이 생성한 유사 타겟(pseudo-targets)을 추가적인 감독 신호로 활용합니다.
- **상호 정보 최대화 관점의 이론적 분석**: ITC, MLM, MoD가 이미지-텍스트 쌍의 다른 "뷰(view)" 간의 상호 정보(mutual information) 하한을 최대화하는 것으로 해석될 수 있음을 보여주며, 이는 모델이 의미 보존 변환에 불변하는 시각-언어 표현을 학습하도록 장려합니다.
- **최첨단 성능 달성**: 이미지-텍스트 검색, 시각 질의응답(Visual Question Answering, VQA), 시각 추론(Visual Reasoning, NLVR$_2$), 시각 함의(Visual Entailment, SNLI-VE), 약한 감독 시각 그라운딩(Weakly-supervised Visual Grounding) 등 다양한 다운스트림 V+L 작업에서 기존 방법들을 능가하는 최첨단 성능을 달성했습니다.
- **효율성 개선**: 객체 감지기나 고해상도 이미지가 필요 없어 기존 많은 방법보다 훨씬 빠른 추론 속도를 제공합니다.
- **암묵적인 시각 그라운딩 능력 입증**: Grad-CAM을 통해 객체, 속성 및 관계를 암묵적으로 정확하게 그라운딩하는 능력을 정성적/정량적으로 분석했습니다.

## 📎 Related Works

- **Vision-Language Representation Learning**:
  - **카테고리 1 (멀티모달 인코더 중심)**: Transformer 기반 멀티모달 인코더를 사용하여 이미지와 텍스트 특징 간의 상호작용을 모델링하는 방법들(예: LXMERT, UNITER, OSCAR). 복잡한 추론을 요구하는 V+L 작업에서 우수한 성능을 보이지만, 대부분 고해상도 이미지와 사전 학습된 객체 감지기가 필요합니다.
  - **카테고리 2 (유니모달 인코더 중심)**: 이미지와 텍스트에 대한 개별 유니모달 인코더를 학습하는 방법들(예: CLIP, ALIGN). 대규모 노이즈 웹 데이터에 대한 대조 손실(contrastive loss)을 사용하여 이미지-텍스트 검색 작업에서 뛰어난 성능을 보이지만, 다른 V+L 작업에 필요한 복잡한 이미지-텍스트 상호작용 모델링 능력은 부족합니다.
  - ALBEF는 이 두 가지 카테고리를 통합하여 검색 및 추론 작업 모두에서 강력한 유니모달 및 멀티모달 표현을 학습합니다.
- **Knowledge Distillation (지식 증류)**:
  - 학생 모델의 성능을 향상시키기 위해 교사 모델로부터 지식을 증류하는 일반적인 방법입니다.
  - 온라인 증류(online distillation)는 여러 모델을 동시에 훈련하고 그 앙상블을 교사로 사용합니다.
  - 이 논문에서 제안하는 모멘텀 증류는 학생 모델의 시간적 앙상블(temporal ensemble)을 교사로 사용하는 자체 증류(self-distillation)의 한 형태로, 반자기 지도 학습(semi-supervised learning) 및 레이블 노이즈 학습(label noise learning)에서도 유사한 아이디어가 탐구된 바 있습니다.

## 🛠️ Methodology

ALBEF는 이미지 인코더, 텍스트 인코더, 멀티모달 인코더로 구성되며, 세 가지 주요 사전 학습 목표와 모멘텀 증류를 통해 학습됩니다.

1. **모델 아키텍처**:

   - **이미지 인코더**: ImageNet-1k에서 사전 학습된 ViT-B/16 (12-레이어 Visual Transformer)을 사용하여 이미지를 시퀀스 임베딩 $(v_{\text{cls}}, v_1, ..., v_N)$으로 인코딩합니다.
   - **텍스트 인코더**: BERT$_{\text{base}}$의 첫 6 레이어로 초기화된 6-레이어 Transformer를 사용하여 텍스트를 임베딩 시퀀스 $(w_{\text{cls}}, w_1, ..., w_N)$로 변환합니다.
   - **멀티모달 인코더**: BERT$_{\text{base}}$의 마지막 6 레이어로 초기화된 6-레이어 Transformer로, 각 레이어에서 교차 어텐션(cross attention)을 통해 이미지 특징과 텍스트 특징을 융합합니다.

2. **사전 학습 목표**:

   - **이미지-텍스트 대조 학습 (ITC)**:
     - 융합 전에 유니모달 표현을 정렬하기 위해 사용됩니다.
     - $[CLS]$ 토큰 임베딩을 선형 변환 $g_v, g_w$를 통해 256차원 표현으로 매핑합니다.
     - 모멘텀 유니모달 인코더로부터 최신 $M$개의 이미지-텍스트 표현을 저장하는 두 개의 큐를 유지합니다.
     - 이미지-텍스트 유사도 $s(I,T) = g_v(v_{\text{cls}})^{\top} g'_w(w'_{\text{cls}})$ 및 $s(T,I) = g_w(w_{\text{cls}})^{\top} g'_v(v'_{\text{cls}})$를 계산합니다. (여기서 $g', v', w'$는 모멘텀 인코더의 출력을 나타냅니다.)
     - 소프트맥스 정규화된 이미지-텍스트 및 텍스트-이미지 유사도 확률 $p_{\text{i2t}}(I), p_{\text{t2i}}(T)$와 정답 원-핫 유사도 $y_{\text{i2t}}(I), y_{\text{t2i}}(T)$ 간의 교차 엔트로피 손실 $L_{\text{itc}}$를 최소화합니다.
       $$ L*{\text{itc}} = \frac{1}{2} E*{(I,T) \sim D} \left[ H(y_{\text{i2t}}(I), p_{\text{i2t}}(I)) + H(y_{\text{t2i}}(T), p_{\text{t2i}}(T)) \right] $$
   - **마스크 언어 모델링 (MLM)**:
     - 입력 텍스트 토큰 중 15%를 무작위로 마스킹하고, 이미지와 문맥적 텍스트를 기반으로 마스크된 단어를 예측합니다.
     - 모델의 예측 확률 $p_{\text{msk}}(I, \hat{T})$과 정답 원-핫 분포 $y_{\text{msk}}$ 간의 교차 엔트로피 손실 $L_{\text{mlm}}$를 최소화합니다.
   - **이미지-텍스트 매칭 (ITM)**:
     - 이미지-텍스트 쌍이 매칭되는지 여부를 예측합니다.
     - 멀티모달 인코더의 $[CLS]$ 토큰 출력에 FC 레이어와 소프트맥스를 적용하여 2클래스 확률 $p_{\text{itm}}$을 예측하고, 교차 엔트로피 손실 $L_{\text{itm}}$을 최소화합니다.
     - **대조 하드 네거티브 마이닝**: ITC 유사도를 활용하여 미니배치 내에서 의미론적으로 유사하지만 미세한 차이가 있는 "하드 네거티브(hard negative)" 샘플을 찾아 ITM 학습에 사용합니다.

3. **모멘텀 증류 (MoD)**:

   - 노이즈가 많은 웹 데이터로부터의 학습을 개선하기 위해 도입되었습니다.
   - 베이스 모델의 지수 이동 평균(Exponential Moving Average, EMA) 버전인 모멘텀 모델을 지속적으로 업데이트되는 교사 모델로 유지합니다.
   - 모멘텀 모델이 생성한 소프트 유사 타겟(pseudo-targets)을 추가적인 감독 신호로 사용합니다.
   - 예를 들어, ITC의 경우 손실은 다음과 같이 정의됩니다 ($q$는 모멘텀 모델의 유사 타겟 확률 분포).
     $$ L^{\text{MoD}}_{\text{itc}} = (1-\alpha)L_{\text{itc}} + \frac{\alpha}{2} E*{(I,T) \sim D} \left[ \text{KL}(q*{\text{i2t}}(I) \Vert p*{\text{i2t}}(I)) + \text{KL}(q*{\text{t2i}}(T) \Vert p\_{\text{t2i}}(T)) \right] $$
   - MLM 및 다운스트림 태스크에도 유사하게 적용되며, 이는 베이스 모델이 웹 주석과 다른 "합리적인" 출력을 생성하더라도 불이익을 받지 않도록 합니다.

4. **전체 사전 학습 목표**: $L = L_{\text{itc}} + L_{\text{mlm}} + L_{\text{itm}}$ (모멘텀 증류 적용 시 $L^{\text{MoD}}_{\text{itc}}$ 및 $L^{\text{MoD}}_{\text{mlm}}$ 사용).

5. **상호 정보 최대화 관점**: ITC, MLM, MoD는 이미지-텍스트 쌍의 다양한 "뷰" 간의 상호 정보 하한을 최대화하는 것으로 해석될 수 있습니다. MoD는 의미론적으로 유사한 샘플을 통해 새로운 뷰를 생성하며, 이는 모델이 뷰 불변(view-invariant) 의미 정보를 포착하는 표현을 학습하도록 유도합니다.

## 📊 Results

- **제안된 방법론의 효과**: ITC, 대조 하드 네거티브 마이닝, 모멘텀 증류가 각각 독립적으로 그리고 결합되었을 때 모든 다운스트림 V+L 작업의 성능을 크게 향상시킴을 입증했습니다. 특히, 모멘텀 증류는 노이즈가 많은 대규모 웹 데이터를 효율적으로 활용하여 모델 성능을 더욱 끌어올렸습니다.
- **이미지-텍스트 검색**: Flickr30K와 MSCOCO 데이터셋에서 ALBEF는 기존 최첨단 방법(CLIP, ALIGN)을 능가하는 성능을 달성했습니다. 특히, CLIP과 ALIGN이 훨씬 더 큰 데이터셋으로 사전 학습되었음에도 ALBEF는 뛰어난 결과를 보여주었습니다. 14M 이미지로 사전 학습된 ALBEF는 제로샷(zero-shot) 이미지-텍스트 검색에서도 최첨단 성능을 기록했습니다.
- **VQA, NLVR$_2$, SNLI-VE**:
  - ALBEF는 VQA, NLVR$_2$, SNLI-VE와 같은 V+L 이해 작업에서 기존 최첨단 방법들을 크게 능가했습니다.
  - VILLA(기존 SOTA) 대비 VQA test-std에서 2.37%, NLVR$_2$ test-P에서 3.84%, SNLI-VE test에서 1.88%의 절대적인 성능 향상을 보였습니다.
  - 객체 감지기 없이 저해상도 이미지를 사용하므로, 기존 대부분의 방법보다 훨씬 빠른 추론 속도를 제공합니다 (예: NLVR$_2$에서 VILLA보다 10배 이상 빠름).
- **약한 감독 시각 그라운딩**: RefCOCO+ 데이터셋에서 ALBEF는 기존 약한 감독 그라운딩 방법들을 크게 능가하는 최첨단 성능을 보여주었습니다.
  - Grad-CAM 시각화를 통해 ALBEF가 객체뿐만 아니라 "더 큰(larger)", "구부러진(curled)"과 같은 속성 및 관계도 정확하게 그라운딩 할 수 있음을 확인했습니다.
  - VQA 모델의 Grad-CAM 시각화는 인간의 시각적 주의(human attention)와 높은 상관관계를 보였으며, 이는 ALBEF가 의사 결정을 내릴 때 적절한 시각 영역에 집중함을 시사합니다.

## 🧠 Insights & Discussion

- **이미지-텍스트 표현 정렬의 중요성**: ALBEF는 융합 전에 유니모달 이미지 및 텍스트 표현을 명시적으로 정렬하는 것이 멀티모달 상호작용 학습의 효율성과 성능에 결정적인 영향을 미친다는 통찰을 제공합니다. ITC 손실은 이 정렬을 효과적으로 수행하여 전반적인 VLP 성능을 향상시킵니다.
- **노이즈 데이터의 효과적 활용**: 모멘텀 증류(MoD)는 노이즈가 많은 대규모 웹 데이터셋으로부터 강건하게 학습할 수 있는 실용적이고 효과적인 방법입니다. 이는 모델이 단일 정답 주석에 과도하게 의존하지 않고, 모멘텀 모델이 생성한 다양한 "합리적인" 유사 타겟으로부터 학습함으로써 일반화 성능을 높일 수 있음을 보여줍니다.
- **상호 정보 최대화의 이론적 기반**: ALBEF의 핵심 구성 요소인 ITC, MLM, MoD를 상호 정보 최대화라는 통일된 이론적 관점에서 해석할 수 있다는 점은 모델 학습 원리에 대한 깊이 있는 이해를 제공합니다. 이는 모델이 다양한 '뷰'에 불변하는 강력한 시각-언어 표현을 학습하도록 유도합니다.
- **높은 효율성과 확장성**: 객체 감지기 의존성을 제거하고 저해상도 이미지를 사용하는 ALBEF의 설계는 계산 효율성을 크게 높여 빠른 추론 속도를 가능하게 합니다. 또한, 14M 이미지로의 확장 시 성능 향상이 입증되어 더 큰 데이터셋에서도 잠재력이 있음을 시사합니다.
- **암묵적 그라운딩 능력**: 명시적인 영역 주석 없이도 모델이 Grad-CAM을 통해 정확한 시각 그라운딩을 수행할 수 있다는 점은 ALBEF가 이미지와 텍스트 간의 미세하고 복잡한 상호작용을 심층적으로 이해하고 있음을 보여줍니다.
- **윤리적 고려사항**: 웹 데이터셋은 의도치 않은 개인 정보, 부적절한 이미지, 유해한 텍스트를 포함할 수 있으므로, 실제 응용에 앞서 데이터 및 모델에 대한 추가적인 사회적, 윤리적 분석이 필요하며, 단순히 정확성만 최적화하는 것이 의도치 않은 사회적 영향을 초래할 수 있음을 강조합니다.

## 📌 TL;DR

ALBEF는 이미지-텍스트 표현을 융합하기 전에 정렬하는 새로운 Vision-and-Language Pre-training (VLP) 프레임워크입니다. 이 모델은 유니모달 인코더를 정렬하는 이미지-텍스트 대조 학습(ITC) 손실과, 노이즈가 많은 웹 데이터 학습을 개선하기 위한 모멘텀 증류(Momentum Distillation, MoD)를 제안합니다. ALBEF는 이미지-텍스트 검색, VQA, NLVR$_2$ 등 다양한 V+L 작업에서 최첨단 성능을 달성하며, 객체 감지기가 필요 없어 기존 방법보다 훨씬 빠른 추론 속도를 제공합니다. ITC, MLM, MoD는 상호 정보 최대화 관점에서 이미지-텍스트 쌍의 다른 '뷰' 간의 상호 정보를 최대화하는 것으로 이론적으로 분석됩니다.
