{
  "title": "Using Vision Language Models to Detect Students' Academic Emotion\n  through Facial Expressions",
  "authors": "Deliang Wang, Chao Yang, Gaowei Chen",
  "year": 2025,
  "url": "http://arxiv.org/abs/2506.10334v1",
  "abstract": "Students' academic emotions significantly influence their social behavior and\nlearning performance. Traditional approaches to automatically and accurately\nanalyze these emotions have predominantly relied on supervised machine learning\nalgorithms. However, these models often struggle to generalize across different\ncontexts, necessitating repeated cycles of data collection, annotation, and\ntraining. The emergence of Vision-Language Models (VLMs) offers a promising\nalternative, enabling generalization across visual recognition tasks through\nzero-shot prompting without requiring fine-tuning. This study investigates the\npotential of VLMs to analyze students' academic emotions via facial expressions\nin an online learning environment. We employed two VLMs,\nLlama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000\nimages depicting confused, distracted, happy, neutral, and tired expressions\nusing zero-shot prompting. Preliminary results indicate that both models\ndemonstrate moderate performance in academic facial expression recognition,\nwith Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.\nNotably, both models excel in identifying students' happy emotions but fail to\ndetect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits\nrelatively high performance in recognizing students' confused expressions,\nhighlighting its potential for practical applications in identifying content\nthat causes student confusion."
}