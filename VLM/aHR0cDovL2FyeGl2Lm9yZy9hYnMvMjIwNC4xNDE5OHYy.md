# Flamingo: a Visual Language Model for Few-Shot Learning
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan

## 🧩 Problem to Solve
멀티모달 머신러닝 연구에서 소수의 주석이 달린 예제(few-shot)만을 사용하여 새로운 작업에 빠르게 적응할 수 있는 모델을 구축하는 것은 여전히 어려운 과제입니다. 기존의 모델들은 다음과 같은 한계를 가집니다:
*   성공적인 파인튜닝을 위해서는 수천 개의 주석이 달린 데이터 포인트가 필요합니다.
*   새로운 작업에 대한 적응 능력이 부족하거나 (예: 대조 학습 모델은 유사성 점수만 제공하고 언어 생성 능력이 없음), 낮은 데이터 환경에서 좋은 성능을 보이지 못합니다.
*   강력한 사전 학습된 비전 전용 및 언어 전용 모델을 효과적으로 연결하는 방법이 부족합니다.
*   임의로 섞인 시각 및 텍스트 데이터를 시퀀스 형태로 처리하는 데 어려움이 있습니다.
*   이미지 또는 비디오를 입력으로 원활하게 처리해야 합니다.

## ✨ Key Contributions
*   **Flamingo VLM 제품군 소개:** 소수의 입력/출력 예제만을 사용하여 다양한 멀티모달 작업(캡션 생성, 시각적 질의응답, 시각적 대화)을 수행할 수 있는 Visual Language Model (VLM)인 Flamingo를 제안합니다.
*   **핵심 아키텍처 혁신:**
    *   강력한 사전 학습된 비전 전용 및 언어 전용 모델을 효과적으로 연결하는 방법을 제시합니다.
    *   임의로 섞인 시각 및 텍스트 데이터를 시퀀스로 처리할 수 있습니다.
    *   Perceiver Resampler 덕분에 이미지나 비디오를 입력으로 원활하게 받아들일 수 있습니다.
*   **퓨샷 학습 SOTA 달성:** 16가지 광범위한 멀티모달 언어 및 이미지/비디오 이해 작업에서 퓨샷 학습 성능에 대한 새로운 최첨단(SOTA)을 달성합니다.
*   **파인튜닝 SOTA 능가:** 고려된 16가지 작업 중 6가지에서, Flamingo는 수만 배 더 적은 작업별 학습 데이터를 사용했음에도 불구하고 기존의 파인튜닝된 SOTA 모델들을 능가합니다.
*   **효과적인 파인튜닝:** 더 많은 주석 예제가 주어질 경우, Flamingo는 추가적으로 5가지 벤치마크(VQAv2, VATEX, VizWiz, MSRVTTQA, HatefulMemes)에서 SOTA를 달성하도록 효과적으로 파인튜닝될 수 있음을 보여줍니다.
*   **대규모 웹 코퍼스 학습:** 임의로 섞인 텍스트와 이미지를 포함하는 대규모 멀티모달 웹 코퍼스(M3W, ALIGN, LTIP, VTP)로 학습하여 in-context few-shot learning 능력을 부여합니다.

## 📎 Related Works
*   **언어 모델링 및 퓨샷 적응:** GPT-3와 같은 대규모 언어 모델(LM)의 퓨샷 학습 능력에서 영감을 얻었으며, Chinchilla [42]를 기반 LM으로 사용합니다. 어댑터 모듈 [43], 파인튜닝 [141], 프롬프트에 in-context 예시 제공 [11] 등 LM 적응 기술과 관련됩니다.
*   **비전-언어 모델(VLM):** BERT 기반의 비전-언어 작업 [16, 28, 66] 연구와 달리 Flamingo는 새 작업에 대한 파인튜닝이 필요 없습니다. CLIP [50, 85]과 같은 대조 학습 기반 VLM과도 구별되지만, 비전 인코더로서 이를 활용합니다. VirTex [22], UniVL [67] 등과 같이 텍스트를 자동 회귀 방식으로 생성하는 VLM과 유사하며, 동시 연구 [17, 58, 119, 124, 154]와도 관련됩니다.
*   **사전 학습된 LM 동결:** 치명적인 망각을 방지하기 위해 사전 학습된 LM 가중치를 동결하고 학습 가능한 계층을 추가하는 최근 연구 [26, 68, 78, 114, 136, 144]의 아이디어를 따릅니다.
*   **웹 스케일 비전-언어 학습 데이터셋:** ALIGN [50], Conceptual Captions [98] 등 웹에서 자동으로 스크래핑된 쌍 데이터셋을 활용합니다. 또한 임의로 섞인 이미지와 텍스트를 포함하는 웹페이지 데이터셋(M3W)의 중요성을 강조합니다.

## 🛠️ Methodology
Flamingo는 시각 데이터가 텍스트와 섞인 것을 입력으로 받아 자유 형식의 텍스트를 출력하는 VLM입니다. 핵심 아키텍처는 사전 학습된 비전 및 언어 모델을 효과적으로 활용하고 연결하도록 설계되었습니다.

1.  **사전 학습 및 동결된 비전 인코더 (Vision Encoder):**
    *   미리 학습되어 동결된 Normalizer-Free ResNet (NFNet) F6 [10]을 사용합니다.
    *   이 인코더는 이미지 또는 비디오에서 시공간 특징(spatio-temporal features)을 추출합니다.
    *   비전 인코더는 대조 학습 목표로 이미지-텍스트 쌍 데이터셋에 사전 학습됩니다.
    *   비디오 입력의 경우, 초당 1프레임으로 샘플링되어 개별적으로 인코딩되며 학습된 시간 임베딩이 추가됩니다.

2.  **Perceiver Resampler:**
    *   비전 인코더에서 나온 다양한 크기의 시각적 특징 맵을 받아 고정된 수(64개)의 시각 토큰을 생성합니다.
    *   이 모듈은 Perceiver [48]와 유사하게 미리 정의된 수의 학습된 잠재 쿼리(latent queries)를 사용하여 트랜스포머에 입력하고, 시각적 특징에 교차 어텐션(cross-attend)합니다.
    *   이는 비전-텍스트 교차 어텐션의 계산 복잡성을 줄이는 역할을 합니다.

3.  **동결된 언어 모델 (Frozen Language Model):**
    *   텍스트 생성은 트랜스포머 디코더에 의해 수행되며, 이는 Perceiver Resampler에서 생성된 시각적 표현에 의해 조건화됩니다.
    *   Flamingo 모델은 1.4B, 7B, 70B 파라미터 Chinchilla 모델 [42]을 기반으로 하며 각각 Flamingo-3B, Flamingo-9B, Flamingo-80B로 명명됩니다.

4.  **GATED XATTN-DENSE 레이어:**
    *   사전 학습되고 동결된 LM 블록 사이에 새로 초기화된 'GATED XATTN-DENSE' 레이어가 삽입됩니다. 이 레이어는 시각 토큰에 교차 어텐션합니다.
    *   초기화 시 conditioned 모델이 원래 LM과 동일한 결과를 생성하도록 tanh-gating 메커니즘 [41]을 사용합니다. 이는 학습 안정성과 최종 성능을 향상시킵니다.

5.  **멀티 비주얼 입력 지원 (Per-image/video attention masking):**
    *   모델은 주어진 텍스트 토큰에서 시퀀스 내에서 해당 토큰 바로 앞에 나타난 이미지/비디오의 시각 토큰에만 어텐션합니다.
    *   이는 훈련 중 사용된 시각 입력의 수와 관계없이(훈련 시 최대 5개 이미지, 추론 시 최대 32개 이미지/비디오) 가변적인 수의 시각 입력에 원활하게 일반화할 수 있도록 합니다.

6.  **학습 데이터셋 구성:**
    *   **M3W (MultiModal MassiveWeb):** 약 4,300만 개의 웹페이지에서 추출한 텍스트와 이미지가 임의로 섞인 데이터셋. `<image>` 태그와 `<EOC>`(end of chunk) 토큰을 사용하여 텍스트와 이미지 위치를 나타냅니다.
    *   **ALIGN [50], LTIP (Long Text & Image Pairs), VTP (Video & Text Pairs):** 각각 18억, 3억 1200만, 2700만 개의 이미지-텍스트 또는 비디오-텍스트 쌍 데이터셋. LTIP와 VTP는 고품질의 긴 설명을 목표로 수집됩니다.

7.  **다중 목적 함수 학습:**
    *   각 데이터셋에 대한 텍스트의 음수 로그-우도(negative log-likelihood)의 가중치 합을 최소화하여 모델을 학습합니다:
        $$ \sum_{m=1}^{M} \lambda_m \cdot E_{(x,y) \sim \mathcal{D}_m} \left[ - \sum_{\ell=1}^{L} \log p(y_{\ell} | y_{<\ell}, x_{\le\ell}) \right] $$
    *   여기서 $y_{\ell}$은 $\ell$-번째 언어 토큰, $y_{<\ell}$은 이전 토큰들, $x_{\le\ell}$은 $y_{\ell}$ 앞에 오는 이미지/비디오 세트를 의미합니다.
    *   AdamW 옵티마이저를 사용하며, 선형 학습률 웜업과 고정된 학습률을 적용합니다.

8.  **퓨샷 in-context 학습을 통한 작업 적응:**
    *   훈련된 Flamingo는 multimodal interleaved 프롬프트를 사용하여 시각적 작업에 적용됩니다.
    *   프롬프트는 (이미지/비디오, 텍스트) 형태의 지원 예제 쌍과 질의 시각적 입력을 연결하여 구성됩니다.
    *   **개방형 평가:** 빔 서치(beam search)를 사용하여 텍스트를 생성합니다.
    *   **폐쇄형 평가:** 각 가능한 답변에 대한 모델의 로그-우도를 사용하여 점수를 매깁니다.
    *   **제로샷 일반화:** 이미지/비디오 없는 두 개의 텍스트 전용 예제로 모델에 프롬프트를 제공합니다.
    *   **확장된 지원 세트 처리:** Retrieval-based In-Context Example Selection (RICES) [136] 및 프롬프트 앙상블을 활용하여 더 많은 예제를 효율적으로 사용합니다.

## 📊 Results
*   **퓨샷 학습 성능:**
    *   Flamingo는 16가지 벤치마크에서 모든 이전의 제로샷 또는 퓨샷 방법을 크게 능가하며, 단 4가지 예제만으로도 새로운 작업에 실용적이고 효율적인 적응을 보여줍니다.
    *   특히 6가지 작업에서는 32가지 예제만 사용하고 모델 가중치를 조정하지 않았음에도 불구하고, 수십만 개의 주석 달린 예제로 파인튜닝된 SOTA 모델보다 뛰어난 성능을 보였습니다 (예: ImageNet 77.3% top-1, COCO 113.8 CIDEr).
*   **스케일링 효과:** 모델 크기(3B, 9B, 80B)가 커질수록 퓨샷 성능이 향상되며, 더 많은 샷(예제 수)을 사용할수록 성능이 개선됩니다. 특히 가장 큰 모델은 훈련 시 시퀀스 길이를 제한(5개 이미지)했음에도 불구하고 추론 시 최대 32개의 이미지나 비디오로부터 이점을 얻습니다.
*   **파인튜닝 성능:** 퓨샷 학습의 주요 초점에도 불구하고, Flamingo를 대규모 주석 데이터셋으로 파인튜닝할 경우 VQAv2, VATEX, VizWiz, MSRVTTQA, HatefulMemes와 같은 5가지 추가 벤치마크에서 새로운 SOTA를 설정할 수 있음을 입증했습니다.
*   **어블레이션 스터디 (Flamingo-3B):**
    *   **학습 데이터 조합:** M3W (교차된 이미지-텍스트 데이터) 또는 기존의 쌍으로 된 이미지/비디오-텍스트 데이터셋 중 하나라도 제거하면 성능이 크게 저하됩니다. LTIP가 ALIGN보다 품질이 더 중요함을 시사합니다.
    *   **최적화 전략:** 그래디언트 누적(accumulation) 방식이 라운드 로빈(round-robin) 방식보다 우수합니다.
    *   **Tanh 게이팅:** 0으로 초기화된 tanh 게이팅 메커니즘을 사용하면 안정성과 성능이 향상됩니다.
    *   **GATED XATTN-DENSE 아키텍처:** 제안된 GATED XATTN-DENSE 방식이 VANILLA XATTN 또는 GRAFTING [68]과 같은 대안보다 성능이 우수합니다.
    *   **Perceiver Resampler:** MLP나 일반 트랜스포머보다 뛰어난 성능을 보입니다.
    *   **비전 인코더:** 강력한 비전 백본(NFNet-F6)의 사용이 중요합니다.
    *   **LM 동결:** LM 계층을 동결하는 것이 "치명적인 망각(catastrophic forgetting)"을 방지하는 데 매우 중요하며, 이를 통해 사전 학습된 LM의 지식을 보존하고 안정적인 학습을 가능하게 합니다.

## 🧠 Insights & Discussion
*   **일반적인 시각 이해를 향한 진전:** Flamingo의 결과는 사전 학습된 대규모 언어 모델과 강력한 시각 모델을 연결하는 것이 범용 시각 이해를 향한 중요한 단계임을 시사합니다.
*   **퓨샷 학습의 강점:** in-context 학습은 극히 적은 데이터 환경(수십 개의 예제)에서 효과적이며, 최소한의 하이퍼파라미터 튜닝과 단순한 추론만으로 배포를 간소화할 수 있는 장점이 있습니다.
*   **한계점:**
    *   **언어 모델의 약점 상속:** Flamingo는 사전 학습된 LM의 약점(예: 환각, 훈련 시퀀스보다 훨씬 긴 시퀀스에 대한 낮은 일반화 능력, 낮은 샘플 효율성)을 직접 상속합니다.
    *   **분류 성능 격차:** 분류 작업에서는 SOTA 대조 학습 모델에 비해 성능이 뒤처집니다. 이는 언어 모델링 목표가 이미지-텍스트 검색에 최적화된 대조 학습과 다르기 때문일 수 있습니다.
    *   **in-context 학습의 민감성:** in-context 학습은 데모의 순서나 형식에 민감하며, 'task location' (모델이 훈련 중 학습한 작업을 인식하는 것)으로 인해 너무 많은 샷에서 성능이 정체될 수 있습니다.
    *   **제한적인 인터페이스:** 바운딩 박스나 광학 흐름과 같은 구조화되거나 밀도 높은 시공간 출력을 조건화하거나 예측하는 데는 현재의 언어 인터페이스가 다소 번거로울 수 있습니다.
*   **사회적 영향:**
    *   **긍정적 영향:** 비전문가 사용자도 적은 데이터로 좋은 성능을 얻을 수 있어 접근성이 향상됩니다. 사전 학습된 모델을 재활용하여 환경적, 실용적 효율성을 높입니다. 시각 장애인을 돕는 VizWiz 챌린지에서 강력한 성능을 보입니다.
    *   **위험:** 대규모 언어 모델의 위험(공격적인 언어, 사회적 편향, 개인 정보 유출)을 상속합니다. 이미지 콘텐츠와 관련된 성별 및 인종 편향과 같은 추가적인 시각 입력 관련 위험이 있습니다.
    *   **완화 전략:** 초기 연구에서는 COCO 캡션 생성에서 통계적으로 유의미한 성별 또는 인종 편향이 관찰되지 않았습니다. Flamingo는 유해 콘텐츠 필터링, '레드 팀(red team)' 테스트 케이스 생성, 그리고 모델 출력에 대한 설명 능력 등을 통해 이러한 위험을 완화하는 데 활용될 수 있습니다.

## 📌 TL;DR
Flamingo는 소수의 예제만으로 다양한 멀티모달 작업을 빠르게 학습하고 수행할 수 있는 Visual Language Model (VLM) 제품군입니다. 이 모델은 사전 학습된 강력한 비전 모델과 언어 모델을 Perceiver Resampler와 GATED XATTN-DENSE 레이어를 통해 효율적으로 연결하며, 임의로 시각 및 텍스트 데이터가 섞인 시퀀스를 처리할 수 있습니다. 방대한 웹 멀티모달 데이터(M3W, ALIGN, LTIP, VTP)로 훈련된 Flamingo는 16가지 이미지/비디오 이해 벤치마크에서 새로운 퓨샷 학습 SOTA를 달성했으며, 일부 작업에서는 수천 배 더 많은 데이터로 파인튜닝된 기존 SOTA 모델보다 우수한 성능을 보였습니다.