{
  "title": "Vision-Language Navigation: A Survey and Taxonomy",
  "authors": "Wansen Wu, Tao Chang, Xinmeng Li",
  "year": 2021,
  "url": "http://arxiv.org/abs/2108.11544v3",
  "abstract": "Vision-Language Navigation (VLN) tasks require an agent to follow human\nlanguage instructions to navigate in previously unseen environments. This\nchallenging field involving problems in natural language processing, computer\nvision, robotics, etc., has spawn many excellent works focusing on various VLN\ntasks. This paper provides a comprehensive survey and an insightful taxonomy of\nthese tasks based on the different characteristics of language instructions in\nthese tasks. Depending on whether the navigation instructions are given for\nonce or multiple times, this paper divides the tasks into two categories, i.e.,\nsingle-turn and multi-turn tasks. For single-turn tasks, we further subdivide\nthem into goal-oriented and route-oriented based on whether the instructions\ndesignate a single goal location or specify a sequence of multiple locations.\nFor multi-turn tasks, we subdivide them into passive and interactive tasks\nbased on whether the agent is allowed to question the instruction or not. These\ntasks require different capabilities of the agent and entail various model\ndesigns. We identify progress made on the tasks and look into the limitations\nof existing VLN models and task settings. Finally, we discuss several open\nissues of VLN and point out some opportunities in the future, i.e.,\nincorporating knowledge with VLN models and implementing them in the real\nphysical world."
}