{
  "title": "Exploring Vision-Language Models for Imbalanced Learning",
  "authors": "Yidong Wang, Zhuohao Yu, Jindong Wang, Qiang Heng, Hao Chen, Wei Ye, Rui Xie, Xing Xie, Shikun Zhang",
  "year": 2023,
  "url": "http://arxiv.org/abs/2304.01457v2",
  "abstract": "Vision-Language models (VLMs) that use contrastive language-image\npre-training have shown promising zero-shot classification performance.\nHowever, their performance on imbalanced dataset is relatively poor, where the\ndistribution of classes in the training dataset is skewed, leading to poor\nperformance in predicting minority classes. For instance, CLIP achieved only 5%\naccuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder\nto VLMs to avoid OOM (out of memory) problem caused by large number of classes\nand capture nuanced features for tail classes. Then, we explore improvements of\nVLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms\nsuch as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments\ndemonstrate that the performance of VLMs can be further boosted when used with\ndecoder and imbalanced methods. Specifically, our improved VLMs significantly\noutperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,\nand 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We\nfurther analyze the influence of pre-training data size, backbones, and\ntraining cost. Our study highlights the significance of imbalanced learning\nalgorithms in face of VLMs pre-trained by huge data. We release our code at\nhttps://github.com/Imbalance-VLM/Imbalance-VLM."
}