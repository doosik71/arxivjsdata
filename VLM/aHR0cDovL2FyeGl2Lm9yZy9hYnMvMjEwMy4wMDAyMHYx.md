# Learning Transferable Visual Models From Natural Language Supervision
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever

## 🧩 Problem to Solve
기존 컴퓨터 비전 시스템은 미리 정해진 고정된 객체 범주에 한정되어 학습되기 때문에 일반성과 활용성이 제한적입니다. 새로운 시각적 개념을 인식하려면 항상 추가적인 레이블링된 데이터가 필요합니다. 이미지를 설명하는 원시 텍스트에서 직접 학습하는 것은 훨씬 광범위한 감독 소스를 활용하는 유망한 대안이지만, 기존 접근 방식으로는 주요 벤치마크에서 성능이 현저히 낮아 실용적인 적용이 어려웠습니다.

## ✨ Key Contributions
- 인터넷에서 수집한 4억 개의 (이미지, 텍스트) 쌍 데이터셋에서 SOTA(State-of-the-Art) 이미지 표현을 처음부터 효율적이고 확장 가능한 방식으로 학습하는 CLIP(Contrastive Language-Image Pre-training) 방법을 제안했습니다.
- 사전 학습 후 자연어를 활용하여 학습된 시각적 개념을 참조하거나 새로운 개념을 설명함으로써 모델을 다운스트림 작업에 제로샷(zero-shot) 전이시키는 능력을 시연했습니다.
- OCR(광학 문자 인식), 동영상 내 액션 인식, 지리 위치 확인, 다양한 세분화된 객체 분류 등 30개 이상의 기존 컴퓨터 비전 데이터셋에서 CLIP의 제로샷 전이 성능을 광범위하게 평가했습니다.
- 작업별 지도 학습 베이스라인과 경쟁할 만한 성능을 보였으며, ImageNet ResNet-50의 정확도를 별도의 학습 데이터 없이 제로샷으로 달성했습니다.
- 자연스러운 분포 변화(natural distribution shift)에 대한 제로샷 CLIP 모델의 강력한 견고성(robustness)을 발견하고, 지도 학습 모델이 특정 분포에 과적합될 수 있음을 시사했습니다.
- 연구에 사용된 코드와 사전 학습된 모델 가중치를 공개했습니다.

## 📎 Related Works
- **NLP의 사전 학습 혁명:** Dai & Le (2015), Peters et al. (2018), Radford et al. (2018), Devlin et al. (2018), Raffel et al. (2019) 등 텍스트에서 직접 학습하는 사전 학습 방법이 NLP 분야에서 큰 발전을 이루었으며, GPT-3와 같은 모델의 제로샷 전이 능력을 입증했습니다.
- **초기 이미지-텍스트 학습:** Mori et al. (1999), Quattoni et al. (2007), Joulin et al. (2016), Li et al. (2017) 등 이미지를 텍스트와 연결하여 유용한 시각적 표현을 학습하는 아이디어를 탐구했습니다.
- **최근 자연어 감독 기반 이미지 학습:** VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), ConVIRT (Zhang et al., 2020) 등 트랜스포머 기반 언어 모델링, 마스크드 언어 모델링, 대조 학습 목표를 사용하여 텍스트에서 이미지 표현을 학습하는 개념 증명 연구들이 있었습니다.
- **약한 감독 기반 이미지 사전 학습:** Mahajan et al. (2018)은 Instagram 해시태그를 예측하여 ImageNet 성능을 개선했고, Kolesnikov et al. (2019) 및 Dosovitskiy et al. (2020)은 노이즈가 있는 대규모 데이터셋 클래스를 예측하여 광범위한 전이 벤치마크에서 성능 향상을 시연했습니다.
- **멀티모달 표현 학습:** Lu et al. (2019), Tan & Bansal (2019), Chen et al. (2019) 등 비전과 언어를 밀접하게 연결하여 시각 질의 응답(VQA)과 같은 복합적인 다운스트림 작업을 해결하는 연구들이 진행되고 있습니다.

## 🛠️ Methodology
CLIP은 자연어 감독을 활용하여 효율적인 대조 학습을 통해 시각적 모델을 사전 학습하고 제로샷 전이를 가능하게 합니다.

- **자연어 감독:**
    - 기존의 고정된 "골드 레이블" 대신, 인터넷에서 얻을 수 있는 방대한 양의 텍스트를 감독 신호로 활용합니다.
    - 이는 레이블링된 데이터의 확장성 한계를 극복하고 유연한 제로샷 전이를 가능하게 합니다.

- **대규모 데이터셋 구축 (WIT):**
    - 기존 이미지-텍스트 데이터셋(MS-COCO, Visual Genome, YFCC100M)의 규모 및 품질 한계를 해결하기 위해, 연구팀은 50만 개의 쿼리 목록을 바탕으로 인터넷에서 4억 개의 (이미지, 텍스트) 쌍을 수집하여 WIT(WebImageText) 데이터셋을 구축했습니다.
    - 이는 GPT-2 훈련에 사용된 WebText 데이터셋과 비슷한 총 단어 수를 가집니다.

- **효율적인 사전 학습 방법 선택:**
    - 초기에는 이미지 캡션 예측과 같이 정확한 단어를 예측하는 생성 모델 방식을 시도했으나, 효율성이 낮음을 발견했습니다.
    - 대신, **대조 학습(Contrastive Learning)** 방식을 채택했습니다. 주어진 배치에 $N$개의 (이미지, 텍스트) 쌍이 있을 때, $N \times N$개의 가능한 모든 쌍 중에서 실제로 일치하는 $N$개의 올바른 쌍을 예측하도록 학습합니다.
    - 이는 이미지 인코더와 텍스트 인코더를 공동으로 학습시켜 다중 모달 임베딩 공간을 구축하며, 실제 쌍의 코사인 유사도를 최대화하고 나머지 $N^2 - N$개의 잘못된 쌍의 유사도는 최소화합니다.
    - 손실 함수로는 InfoNCE 손실과 유사한 대칭 교차 엔트로피 손실(symmetric cross entropy loss)을 사용하며, 온도 파라미터 $\tau$는 학습 가능한 스칼라로 직접 최적화됩니다.

- **모델 아키텍처:**
    - **이미지 인코더:**
        - **ResNet 계열:** ResNet-50을 기반으로 ResNet-D 개선, anti-aliased rect-2 blur pooling, 어텐션 풀링을 적용한 수정된 ResNet 모델.
        - **Vision Transformer (ViT) 계열:** Dosovitskiy et al. (2020)의 Vision Transformer 구조를 따르며, 추가적인 레이어 정규화와 수정된 초기화 방식을 사용합니다.
    - **텍스트 인코더:** Vaswani et al. (2017)의 Transformer 구조를 기반으로 Radford et al. (2019)의 수정 사항을 적용했습니다. BPE(Byte Pair Encoding)를 사용하며, [SOS] 및 [EOS] 토큰을 활용하고 [EOS] 토큰의 활성화를 텍스트의 특징 표현으로 사용합니다.
    - 두 인코더는 각 모달리티의 표현을 선형 투영하여 하나의 공동 다중 모달 임베딩 공간으로 매핑합니다.

- **학습:**
    - 5개의 ResNet과 3개의 Vision Transformer를 포함한 총 8가지 모델을 32 에포크 동안 학습했습니다.
    - Adam 옵티마이저(Kingma & Ba, 2014), 분리된 가중치 감쇠(Loshchilov & Hutter, 2017), 코사인 학습률 스케줄링(Loshchilov & Hutter, 2016)을 사용합니다.
    - 대규모 미니배치 크기인 32,768을 사용했으며, 혼합 정밀도(mixed-precision) 학습과 그래디언트 체크포인팅(gradient checkpointing)으로 메모리를 절약했습니다.
    - 가장 큰 ResNet 모델(RN50x64)은 592개의 V100 GPU로 18일, 가장 큰 ViT 모델(ViT-L/14)은 256개의 V100 GPU로 12일이 소요되었습니다.

- **제로샷 전이:**
    - 테스트 시에는 각 다운스트림 데이터셋의 모든 클래스 이름을 텍스트 인코더로 임베딩하여 '제로샷 선형 분류기'를 생성합니다.
    - 예를 들어, `A photo of a {label}.`과 같은 프롬프트 템플릿을 사용하여 텍스트에 이미지를 설명하는 컨텍스트를 부여합니다.
    - 이미지 임베딩과 클래스 텍스트 임베딩 간의 코사인 유사도를 계산하고 소프트맥스 함수를 통해 확률 분포로 정규화하여 최종 예측을 수행합니다.
    - **프롬프트 엔지니어링(Prompt Engineering)** 및 **앙상블(Ensembling)**을 통해 제로샷 분류 성능을 추가적으로 개선할 수 있습니다.

## 📊 Results
- **제로샷 전이 성능:**
    - ImageNet에서 제로샷 정확도를 기존 11.5%에서 76.2%로 대폭 향상시켜, 128만 개의 학습 예제를 사용한 오리지널 ResNet-50과 동등한 성능을 달성했습니다.
    - 27개 데이터셋에 대한 평가 스위트에서 제로샷 CLIP은 ResNet-50 특징에 대해 학습된 완전 지도 학습 선형 분류기보다 16개 데이터셋에서 우수한 성능을 보였습니다.
    - 제로샷 CLIP은 자체 특징 공간에서 4-샷(few-shot) 선형 분류기와 유사한 성능을 보였고, 공개된 모델 중 최상위 16-샷 선형 분류기와도 경쟁할 만했습니다.
    - 제로샷 전이의 유효 데이터 효율성은 데이터셋별로 크게 달랐으며, 중앙값은 클래스당 5.4개의 레이블링된 예제에 해당했습니다.
- **표현 학습 성능 (선형 프로브):**
    - 가장 큰 CLIP 모델(ViT-L/14@336px)은 27개 데이터셋 평균에서 기존 SOTA 모델(Noisy Student EfficientNet-L2)보다 5% 더 높은 성능을 달성하여, 컴퓨트 효율성 측면에서도 우위를 보였습니다.
    - CLIP의 비전 트랜스포머는 CLIP의 ResNet보다 약 3배 더 컴퓨트 효율적임이 확인되었습니다.
    - OCR, 지리 위치, 액션 인식과 같은 다양한 작업에 대한 학습 능력을 보여주며, ImageNet 기반 모델보다 더 광범위한 시각적 개념을 학습했음을 시사했습니다.
- **자연스러운 분포 변화에 대한 견고성:**
    - 제로샷 CLIP 모델은 ImageNetV2, ObjectNet 등 7개 자연스러운 분포 변화 데이터셋에서 표준 ImageNet 모델보다 훨씬 더 견고했습니다. ImageNet 정확도와 분포 변화 정확도 간의 "견고성 격차(robustness gap)"를 최대 75%까지 줄였습니다.
    - ImageNet 데이터셋에 대한 지도 학습 적응은 ImageNet 정확도를 9.2% 증가시켰지만, 분포 변화 시 평균 정확도는 오히려 약간 감소했습니다. 이는 지도 학습이 특정 분포의 오염된(spurious) 상관관계를 악용할 수 있음을 나타냅니다.
- **인간 성능과의 비교:**
    - Oxford IIT Pets 데이터셋에서 CLIP의 제로샷 정확도(93.5%)는 인간의 제로샷 성능(53.7%)보다 훨씬 높았습니다. 그러나 인간은 단 한 개의 학습 예제를 보았을 때(1-샷) 75.7%까지 급격히 성능을 향상시키는 반면, CLIP의 소수샷 성능은 제로샷보다 떨어지는 현상을 보였습니다. 이는 기계와 인간의 소수샷 학습 방식에 근본적인 차이가 있음을 시사합니다.
- **데이터 중복 분석:** 사전 학습 데이터셋과 평가 데이터셋 간의 중복률은 중앙값 2.2%, 평균 3.2%로 낮았으며, 중복으로 인한 전체 성능 향상은 대부분 0.1% 미미했습니다.
- **특정 작업 결과:** OCR(Rendered SST2, Hateful Memes), 액션 인식(UCF101, Kinetics-700, RareAct), 지리 위치(Country211, IM2GPS) 등 다양한 작업에서 CLIP은 제로샷 또는 선형 프로브 설정에서 기존 모델과 경쟁하거나 능가하는 성능을 보였습니다.

## 🧠 Insights & Discussion
- **자연어 감독의 확장성:** NLP 분야의 성공을 이어받아, 대규모 자연어 감독을 통한 사전 학습은 컴퓨터 비전 분야에서도 매우 전이 가능하고 견고한 모델을 개발할 수 있음을 입증했습니다. 이는 미래의 AI 시스템이 복잡한 작업을 수행하는 데 중요한 전환점이 될 수 있습니다.
- **제로샷 전이의 잠재력:** CLIP은 사전 학습 과정에서 OCR, 지리 위치 확인, 액션 인식 등 광범위한 작업을 암묵적으로 학습하며, 자연어 프롬프트를 통해 새로운 작업에 효율적으로 제로샷 전이될 수 있는 강력한 능력을 보여줍니다. 이는 별도의 데이터셋 구축 및 학습 없이도 다양한 애플리케이션에 모델을 적용할 수 있는 길을 엽니다.
- **견고성 향상:** 제로샷 CLIP은 지도 학습 모델이 특정 데이터셋 분포에 과적합되는 경향을 보이는 것과 달리, 자연스러운 분포 변화에 대해 훨씬 더 견고한 성능을 보여줍니다. 이는 "효과적인 견고성(effective robustness)"을 달성하는 데 중요한 진전입니다.
- **한계점:**
    - **성능 격차:** 여전히 많은 작업에서 SOTA 모델에 미치지 못하며, 특정 세분화된 분류(예: 자동차 모델, 꽃 종)나 추상적인 작업(예: 객체 수 세기)에는 취약합니다.
    - **취약한 일반화:** CLIP은 대규모 데이터셋 학습을 통해 모든 데이터가 사실상 "분포 내(in-distribution)"에 있도록 하려 하지만, MNIST 손글씨와 같이 진정으로 분포를 벗어난 데이터에는 여전히 성능이 낮아 딥러닝 모델의 근본적인 취약한 일반화 문제를 완전히 해결하지 못합니다.
    - **데이터 효율성:** CLIP은 데이터 효율성 문제를 직접적으로 해결하기보다, 단순히 방대한 감독 소스를 활용하여 학습 데이터 규모를 확대하는 방식으로 접근합니다.
    - **유연성 제한:** CLIP은 텍스트로 지정된 개념 내에서만 분류할 수 있으며, 이미지 캡션 모델처럼 완전히 새로운 출력을 생성하는 유연성은 없습니다.
    - **평가 바이어스:** 모델 개발 과정에서 검증 데이터셋을 반복적으로 사용했기 때문에 "진정한" 제로샷 시나리오와는 거리가 있을 수 있으며, 평가 데이터셋의 선택이 CLIP의 능력에 맞춰져 있을 가능성이 있습니다.
- **사회적 영향 및 편향:**
    - CLIP은 인터넷에서 수집된 필터링되지 않은 이미지-텍스트 쌍으로 학습되므로, 인종, 성별, 나이에 따른 범죄 관련 연관성과 같은 **사회적 편향**을 학습하고 증폭시킬 수 있습니다.
    - 특정 인구 통계학적 그룹(예: 흑인, 0-20세)이 비인간 또는 범죄 관련 범주로 오분류될 가능성이 높음이 확인되었습니다.
    - **클래스 디자인(class design)**이 모델의 편향 및 성능에 큰 영향을 미칠 수 있으며, 잘못된 클래스 정의는 해로운 결과를 초래할 수 있습니다.
    - CLIP의 유연한 분류기 생성 능력은 감시(surveillance)와 같은 민감한 응용 분야에서 새로운 윤리적 문제를 야기할 수 있으므로, 모델 배포 전 광범위한 편향 테스트 및 책임감 있는 사용 지침 마련이 필수적입니다.

## 📌 TL;DR
CLIP은 기존 컴퓨터 비전 모델의 고정된 범주 한계를 극복하고자, 4억 개의 이미지-텍스트 쌍으로 구성된 대규모 웹 데이터셋(WIT)에서 대조 학습을 통해 시각적 표현을 사전 학습합니다. 이 모델은 이미지와 텍스트 임베딩 간의 유사도를 최대화하는 방식으로 학습되어, 자연어 프롬프트를 통해 30개 이상의 다양한 컴퓨터 비전 작업에 제로샷 전이가 가능합니다. 그 결과, ImageNet ResNet-50과 동등한 제로샷 정확도를 달성하고, 지도 학습 모델보다 뛰어난 자연스러운 분포 변화에 대한 견고성을 보여주며, OCR 및 액션 인식과 같은 새로운 능력을 습득합니다. 그러나 CLIP은 인터넷 데이터로부터 사회적 편향을 학습하고, 특정 세분화된 또는 추상적인 작업에는 여전히 취약하며, 감시와 같은 민감한 응용 분야에서 윤리적 고려와 광범위한 편향 분석이 필요합니다.