{
  "title": "Assessing and Learning Alignment of Unimodal Vision and Language Models",
  "authors": "Le Zhang, Qian Yang, Aishwarya Agrawal",
  "year": 2024,
  "url": "http://arxiv.org/abs/2412.04616v2",
  "abstract": "How well are unimodal vision and language models aligned? Although prior work\nhave approached answering this question, their assessment methods do not\ndirectly translate to how these models are used in practical vision-language\ntasks. In this paper, we propose a direct assessment method, inspired by linear\nprobing, to assess vision-language alignment. We identify that the degree of\nalignment of the SSL vision models depends on their SSL training objective, and\nwe find that the clustering quality of SSL representations has a stronger\nimpact on alignment performance than their linear separability. Next, we\nintroduce Swift Alignment of Image and Language (SAIL), a efficient transfer\nlearning framework that aligns pretrained unimodal vision and language models\nfor downstream vision-language tasks. Since SAIL leverages the strengths of\npretrained unimodal models, it requires significantly fewer (6%) paired\nimage-text data for the multimodal alignment compared to models like CLIP which\nare trained from scratch. SAIL training only requires a single A100 GPU, 5\nhours of training and can accommodate a batch size up to 32,768. SAIL achieves\n73.4% zero-shot accuracy on ImageNet (vs. CLIP's 72.7%) and excels in zero-shot\nretrieval, complex reasoning, and semantic segmentation. Additionally, SAIL\nimproves the language-compatibility of vision encoders that in turn enhance the\nperformance of multimodal large language models. The entire codebase and model\nweights are open-source: https://lezhang7.github.io/sail.github.io/"
}