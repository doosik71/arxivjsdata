{
  "title": "Can VLMs be used on videos for action recognition? LLMs are Visual\n  Reasoning Coordinators",
  "authors": "Harsh Lunia",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.14834v1",
  "abstract": "Recent advancements have introduced multiple vision-language models (VLMs)\ndemonstrating impressive commonsense reasoning across various domains. Despite\ntheir individual capabilities, the potential of synergizing these complementary\nVLMs remains underexplored. The Cola Framework addresses this by showcasing how\na large language model (LLM) can efficiently coordinate multiple VLMs through\nnatural language communication, leveraging their distinct strengths. We have\nverified this claim on the challenging A-OKVQA dataset, confirming the\neffectiveness of such coordination. Building on this, our study investigates\nwhether the same methodology can be applied to surveillance videos for action\nrecognition. Specifically, we explore if leveraging the combined knowledge base\nof VLMs and LLM can effectively deduce actions from a video when presented with\nonly a few selectively important frames and minimal temporal information. Our\nexperiments demonstrate that LLM, when coordinating different VLMs, can\nsuccessfully recognize patterns and deduce actions in various scenarios despite\nthe weak temporal signals. However, our findings suggest that to enhance this\napproach as a viable alternative solution, integrating a stronger temporal\nsignal and exposing the models to slightly more frames would be beneficial."
}