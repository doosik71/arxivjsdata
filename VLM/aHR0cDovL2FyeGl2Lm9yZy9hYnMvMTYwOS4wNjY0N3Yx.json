{
  "url": "http://arxiv.org/abs/1609.06647v1",
  "title": "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning\n  Challenge",
  "authors": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan",
  "year": 2016,
  "abstract": "Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. Finally, given the recent\nsurge of interest in this task, a competition was organized in 2015 using the\nnewly released COCO dataset. We describe and analyze the various improvements\nwe applied to our own baseline and show the resulting performance in the\ncompetition, which we won ex-aequo with a team from Microsoft Research, and\nprovide an open source implementation in TensorFlow.",
  "citation": 1201
}