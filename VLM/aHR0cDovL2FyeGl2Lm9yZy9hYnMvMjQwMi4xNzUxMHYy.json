{
  "title": "Demonstrating and Reducing Shortcuts in Vision-Language Representation\n  Learning",
  "authors": "Maurits Bleeker, Mariya Hendriksen, Andrew Yates, Maarten de Rijke",
  "year": 2024,
  "url": "http://arxiv.org/abs/2402.17510v2",
  "abstract": "Vision-language models (VLMs) mainly rely on contrastive training to learn\ngeneral-purpose representations of images and captions. We focus on the\nsituation when one image is associated with several captions, each caption\ncontaining both information shared among all captions and unique information\nper caption about the scene depicted in the image. In such cases, it is unclear\nwhether contrastive losses are sufficient for learning task-optimal\nrepresentations that contain all the information provided by the captions or\nwhether the contrastive learning setup encourages the learning of a simple\nshortcut that minimizes contrastive loss. We introduce synthetic shortcuts for\nvision-language: a training and evaluation framework where we inject synthetic\nshortcuts into image-text data. We show that contrastive VLMs trained from\nscratch or fine-tuned with data containing these synthetic shortcuts mainly\nlearn features that represent the shortcut. Hence, contrastive losses are not\nsufficient to learn task-optimal representations, i.e., representations that\ncontain all task-relevant information shared between the image and associated\ncaptions. We examine two methods to reduce shortcut learning in our training\nand evaluation framework: (i) latent target decoding and (ii) implicit feature\nmodification. We show empirically that both methods improve performance on the\nevaluation task, but only partly reduce shortcut learning when training and\nevaluating with our shortcut learning framework. Hence, we show the difficulty\nand challenge of our shortcut learning framework for contrastive\nvision-language representation learning."
}