{
  "title": "VideoChat-Flash: Hierarchical Compression for Long-Context Video\n  Modeling",
  "authors": "Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, Limin Wang",
  "year": 2025,
  "url": "http://arxiv.org/abs/2501.00574v4",
  "abstract": "Long-context video modeling is critical for multimodal large language models\n(MLLMs), enabling them to process movies, online video streams, and so on.\nDespite its advances, handling long videos remains challenging due to the\ndifficulty in efficiently understanding the extremely long video context. This\npaper aims to address this issue from aspects of model architecture, training\ndata, training strategy and evaluation benchmark. First, we propose a novel\nHierarchical video token Compression (HiCo) method, which leverages visual\nredundancy in long videos to compress long video context from Clip-level to\nVideo-level, reducing the computation significantly while preserving essential\ndetails, achieving an extreme compression ratio of approximately 1/50 with\nalmost no performance loss. Second, we introduce a multi-stage short-to-long\nlearning scheme, a large-scale dataset of real-world long videos named LongVid,\nand a challenging ``Multi-Hop Needle-In-A-Video-Haystack'' benchmark. Finally,\nwe build a powerful video MLLM named VideoChat-Flash, which shows a leading\nperformance on both mainstream long and short video benchmarks at the 2B and 7B\nmodel scale. It first gets 99.1% accuracy over 10,000 frames in NIAH among\nopen-source models."
}