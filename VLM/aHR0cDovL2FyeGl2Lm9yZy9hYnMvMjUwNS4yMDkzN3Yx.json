{
  "title": "On VLMs for Diverse Tasks in Multimodal Meme Classification",
  "authors": "Deepesh Gavit, Debajyoti Mazumder, Samiran Das, Jasabanta Patro",
  "year": 2025,
  "url": "http://arxiv.org/abs/2505.20937v1",
  "abstract": "In this paper, we present a comprehensive and systematic analysis of\nvision-language models (VLMs) for disparate meme classification tasks. We\nintroduced a novel approach that generates a VLM-based understanding of meme\nimages and fine-tunes the LLMs on textual understanding of the embedded meme\ntext for improving the performance. Our contributions are threefold: (1)\nBenchmarking VLMs with diverse prompting strategies purposely to each sub-task;\n(2) Evaluating LoRA fine-tuning across all VLM components to assess performance\ngains; and (3) Proposing a novel approach where detailed meme interpretations\ngenerated by VLMs are used to train smaller language models (LLMs),\nsignificantly improving classification. The strategy of combining VLMs with\nLLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,\noffensive and sentiment classification, respectively. Our results reveal the\nstrengths and limitations of VLMs and present a novel strategy for meme\nunderstanding."
}