# SIMVLM: SIMPLE VISUAL LANGUAGE MODEL PRE-TRAINING WITH WEAK SUPERVISION

Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao

## 🧩 Problem to Solve

기존 시각-언어 사전 학습(Vision-Language Pretraining, VLP) 방법론들은 다음과 같은 한계를 가지고 있습니다.

* **고비용의 주석 데이터 요구**: 깨끗한 이미지 캡션 및 영역별 레이블과 같은 값비싼 인간 주석 데이터가 필수적입니다.
* **복잡한 사전 학습 절차**: 여러 데이터셋별 목적 함수(예: 객체 탐지 사전 학습, 보조 손실)를 도입하여 학습 절차가 복잡해지고, 이는 확장성을 저해하며 품질 개선의 병목 현상을 초래합니다.
* **제한된 제로샷(Zero-shot) 능력**: 많은 기존 `사전 학습-미세 조정` 패러다임은 제로샷 일반화 능력이 부족하며, 일부 약한 감독 학습(weakly supervised learning)을 사용한 연구는 특정 작업에만 초점을 맞춥니다.

## ✨ Key Contributions

본 논문은 `Simple Visual Language Model (SimVLM)`이라는 최소주의적 사전 학습 프레임워크를 제안하며, 다음의 핵심 기여를 합니다.

* **단순화된 사전 학습 프레임워크**: 객체 탐지 모듈이나 보조 손실 함수 없이 단일 `Prefix Language Modeling (PrefixLM)` 목적 함수를 사용하여 대규모 약한 감독 하에 엔드투엔드로 모델을 학습합니다.
* **최고 수준(State-of-the-Art, SOTA) 성능 달성**: 추가 데이터나 작업별 맞춤화 없이도 VQA, NLVR2, SNLI-VE, 이미지 캡셔닝 등 다양한 판별 및 생성 시각-언어 벤치마크에서 이전의 모든 VLP 방법론을 크게 능가하는 새로운 SOTA 결과를 달성했습니다.
* **강력한 일반화 및 제로샷 능력**: 오픈엔드 시각 질의 응답(open-ended VQA), 제로샷 이미지 캡셔닝, 교차 모달리티 전이(cross-modality transfer) 등에서 강력한 일반화 및 전이 능력을 보여줍니다.
* **통합된 다중모달 표현 학습**: 단일 PrefixLM을 통해 BERT와 같은 양방향 컨텍스트 정보 처리와 GPT-3와 유사한 텍스트 생성이라는 두 가지 이점을 동시에 얻습니다.

## 📎 Related Works

* **객체 탐지 기반 VLP**: LXMERT, UNITER, OSCAR, VinVL과 같은 많은 VLP 모델들은 Fast(er) R-CNN과 같은 강력한 객체 탐지 모델을 사전 학습하여 이미지 영역 특징을 추출합니다. 이는 Visual Genome과 같은 수동 주석 데이터에 의존하여 학습 파이프라인 구축 비용을 증가시키고 확장성을 떨어뜨립니다.
* **객체 탐지 모듈 없는 VLP (제한된 규모)**: Xu et al. (2021), Kim et al. (2021), Huang et al. (2021) 등 일부 연구는 객체 탐지 모듈 없이 VLP를 탐색했지만, 주로 소규모의 깨끗한 사전 학습 데이터만 사용하여 제로샷 능력에 한계가 있었습니다.
* **다중 교차 모달리티 손실 함수**: 이미지-텍스트 매칭, 마스크된 영역 분류/특징 회귀, 객체 속성 예측, 대조 손실 등 다양한 보조 손실 함수들이 성능 향상을 위해 사용되었습니다. 이러한 손실 함수들의 혼합은 최적화 절차를 복잡하게 만듭니다.
* **약한 레이블/정렬 데이터를 활용한 사전 학습**: CLIP, DALL-E, ALIGN, Frozen 등은 웹에서 크롤링된 약한 레이블/정렬 데이터를 활용하여 특정 이미지-텍스트 작업에서 좋은 성능과 제로샷 학습 능력을 보여주었습니다. SimVLM은 이러한 연구에 영감을 받아 대규모 약한 감독 데이터를 사용합니다.

## 🛠️ Methodology

SimVLM은 Transformer 기반의 인코더-디코더 아키텍처를 사용하며, 단일 PrefixLM 목표로 엔드투엔드 학습됩니다.

* **Objective: Prefix Language Modeling (PrefixLM)**
  * **MLM (Masked Language Modeling)**: BERT에서 널리 사용되는 양방향 컨텍스트 학습 방식입니다. 손상된 토큰 $x_m$을 컨텍스트 $x_{\m}$로부터 재구성하는 것을 목표로 합니다:
    $$
    L_{MLM}(\theta) = -E_{x \sim D} \left\lbrack \log P_{\theta}(x_m | x_{\m}) \right\rbrack
    $$
  * **LM (Language Modeling)**: GPT 계열에서 사용되는 단방향(자기회귀적) 텍스트 생성 방식입니다. 시퀀스 $x$의 우도를 최대화하는 것을 목표로 합니다:
    $$
    L_{LM}(\theta) = -E_{x \sim D} \left\lbrack \log P_{\theta}(x) \right\rbrack = -E_{x \sim D} \left\lbrack \sum_{t=1}^{T} \log P_{\theta}(x_t | x_{\lt t}) \right\rbrack
    $$
  * **PrefixLM**: 표준 LM과 달리, 접두어 시퀀스($x_{\lt T_p}$)에 대해 양방향 어텐션을 허용하고, 나머지 토큰($x_{\ge T_p}$)에 대해서만 자기회귀적 인수분해를 수행합니다.
    * 이미지-텍스트 쌍의 경우, 이미지 특징 시퀀스를 텍스트 시퀀스 앞에 붙여 접두어로 간주하고 텍스트 데이터에 대해서만 LM 손실을 계산합니다.
    * 이를 통해 MLM과 같은 양방향 컨텍스트 표현 학습과 LM과 같은 텍스트 생성 능력을 동시에 확보합니다.
    $$
    L_{PrefixLM}(\theta) = -E_{x \sim D} \left\lbrack \log P_{\theta}(x_{\ge T_p} | x_{\lt T_p}) \right\rbrack = -E_{x \sim D} \left\lbrack \sum_{t=T_p}^{T} \log P_{\theta}(x_t | x_{\lbrack T_p, t\rbrack}, x_{\lt T_p}) \right\rbrack
    $$

* **Architecture**
  * **백본**: Transformer 기반의 인코더-디코더 모델을 채택했습니다. 인코더-디코더 모델이 인코딩과 생성을 분리하여 VL 표현 학습에 유리함이 확인되었습니다.
  * **시각 모달리티**: ViT 및 CoAtNet에서 영감을 받아 원본 이미지 $x \in R^{H \times W \times C}$를 평면화된 1D 패치 시퀀스 $x_p \in R^{T_i \times D}$로 변환하여 Transformer의 입력으로 사용합니다. 여기서 $T_i = HW/P^2$는 패치 크기 $P$에 대한 이미지 토큰의 길이입니다.
    * ResNet의 첫 세 블록으로 구성된 컨볼루션(Conv) 스테이지를 사용하여 컨텍스트화된 패치를 추출합니다. 이는 ViT의 단순 선형 투영보다 유리한 것으로 나타났습니다.
  * **텍스트 모달리티**: 표준 서브워드 토큰화(sub-word tokenization)를 따르고 고정된 어휘에 대한 임베딩을 학습합니다.
  * **위치 임베딩**: 이미지 및 텍스트 입력에 대해 별도의 학습 가능한 1D 위치 임베딩을 추가하며, 이미지 패치에는 2D 상대 어텐션을 적용합니다.

* **Data**
  * **대규모 잡음이 많은 이미지-텍스트 데이터**: ALIGN (Jia et al., 2021)의 약 1.8B 이미지-텍스트 쌍 훈련 세트를 사용하여 사전 학습합니다. 객체 탐지 모듈이 필요 없어 최소한의 전처리만으로 원본 이미지 패치 입력을 활용할 수 있습니다.
  * **텍스트 전용 코퍼스**: Colossal Clean Crawled Corpus (C4) 데이터셋(약 800GB의 웹 크롤링 문서)을 추가하여 잡음이 많은 이미지-텍스트 데이터의 텍스트 감독을 보완하고 더 나은 언어 이해 능력을 습득하도록 돕습니다.

## 📊 Results

SimVLM은 다양한 시각-언어 벤치마크에서 모든 기존 모델들을 능가하며 새로운 SOTA를 달성했습니다.

* **주요 벤치마크 성능**:
  * **VQA v2**: SimVLM$_{huge}$가 80.34%를 달성하여 이전 SOTA (VinVL) 대비 약 4%p 향상된 성능을 보여, 단일 모델로 VQA에서 80%를 돌파했습니다.
  * **NLVR2 및 SNLI-VE**: 더 복잡한 시각-언어 추론 작업에서도 이전 방법론들을 일관되게 능가했습니다.
  * **이미지 캡셔닝 (CoCo, NoCaps)**: CIDEr 최적화와 같은 작업별 트릭 없이 순수한 미세 조정(naive finetuning)만으로도 이전 모델들을 뛰어넘는 성능을 보여주었습니다.
  * **다중모달 번역 (Multi30k)**: 영어-독일어 이미지 번역에서도 효과적임을 입증했습니다.

* **제로샷 일반화 능력**:
  * **제로샷/Few-shot 이미지 캡셔닝**: 사전 학습된 SimVLM 모델을 이미지 캡셔닝 벤치마크에 직접 적용하거나 1% 훈련 데이터로 few-shot 미세 조정하여 경쟁력 있는 성능을 달성했습니다. 복잡한 장면과 세분화된 개념(예: 특정 자동차 브랜드)에 대한 설명을 제공할 수 있습니다.
  * **제로샷 교차 모달리티 전이**:
    * SNLI-VE에서 텍스트 전용 NLI 데이터셋으로 모델을 미세 조정한 후, 이미지-텍스트 결합 VL 작업에서 제로샷 전이 성능이 UNITER와 같은 완전 지도 학습 기준선과 경쟁력 있음을 보여주었습니다.
    * Multi30k (독일어 이미지 캡셔닝)에서는 영어-독일어 텍스트 전용 번역 데이터로 미세 조정한 후, 이미지 전용 인코더 입력으로 독일어 캡션을 생성하여 모달리티 및 언어 간 지식 전이 능력을 입증했습니다.
  * **오픈엔드 VQA**:
    * 사전 정의된 3,129개 답변 후보에 제약받지 않고 자유 형식 텍스트 답변을 생성합니다.
    * 희귀 답변(out-of-domain) 질문에서 판별 모델보다 17%p 이상 높은 점수를 기록하며 강력한 일반화 능력을 입증했습니다.
    * 모델이 학습 데이터셋에 없는 답변(예: "surgeon", "wood carving")도 생성할 수 있음을 보여주었습니다.
    * 더 깨끗한 WIT 데이터셋으로 추가 사전 학습 시 제로샷 오픈엔드 VQA 능력이 발현됨을 확인했습니다.

* **단일 모달리티 작업**:
  * **GLUE 벤치마크**: 기존 VLP 방법보다 우수하고 BERT와 경쟁력 있는 언어 이해 능력을 보여주었습니다.
  * **ImageNet 분류 (선형 평가)**: 고품질 이미지 표현 학습 능력도 겸비하고 있음을 확인했습니다.

## 🧠 Insights & Discussion

* **생성적 VLP의 유망성**: SimVLM의 결과는 생성적 사전 학습 방식이 기존 MLM 기반 모델에 필적할 뿐만 아니라, 강력한 제로샷 잠재력을 통해 VLP의 새로운 방향을 제시할 수 있음을 시사합니다.
* **간단한 프레임워크의 효율성**: 객체 탐지나 여러 보조 손실 없이 단일 PrefixLM 목표로 엔드투엔드 학습하는 `최소주의적 접근 방식`이 고품질 다중모달 표현 학습에 충분하며, 이는 VLP 프로토콜을 단순화하고 확장성을 높이는 데 기여합니다.
* **약한 감독 및 데이터 스케일링의 중요성**: 대규모 약한 레이블 데이터를 활용하는 것이 모델의 일반화 및 제로샷 전이 능력을 강화하는 데 결정적입니다. 이미지-텍스트 데이터뿐만 아니라 텍스트 전용 코퍼스를 함께 사용하는 것이 잡음이 많은 이미지-텍스트 데이터의 텍스트 신호를 보완하여 언어 이해 능력을 향상시키는 데 중요합니다.
* **아키텍처 구성 요소의 영향**:
  * **인코더-디코더 구조의 이점**: 인코더-디코더 모델이 디코더 전용 모델보다 VQA에서 현저히 우수한 성능을 보여, 양방향 인코딩과 단방향 디코딩의 분리가 VL 표현 학습에 유리한 귀납적 편향을 제공함을 시사합니다.
  * **PrefixLM 목표의 효율성**: `span corruption`이나 순수 `LM`보다 `PrefixLM`이 더 좋은 성능을 보여주며, 이미지-텍스트와 텍스트 전용 데이터를 위한 통합된 목표 공식화의 중요성을 강조합니다.
  * **컨볼루션 스테이지의 중요성**: 원본 이미지 입력 처리 시 컨볼루션 스테이지를 사용하여 컨텍스트화된 패치를 추출하는 것이 VL 성능에 중요하며, 이미지와 텍스트가 서로 다른 수준의 표현 세분성을 가짐을 나타냅니다.

## 📌 TL;DR

* **문제**: 기존 VLP는 고비용의 주석, 복잡한 사전 학습 절차, 제한된 제로샷 능력으로 인해 확장성과 일반화에 한계가 있었습니다.
* **방법**: SimVLM은 객체 탐지나 보조 손실 없이 대규모 약한 레이블 데이터(ALIGN 및 C4)를 활용하여 단일 `Prefix Language Modeling` 목표로 엔드투엔드 사전 학습됩니다. ViT/CoAtNet에서 영감을 받은 Transformer 기반 인코더-디코더 아키텍처는 ResNet 컨볼루션 스테이지를 통해 원본 이미지를 패치 시퀀스로 처리합니다.
* **결과**: SimVLM은 VQA, 이미지 캡셔닝 등 다양한 VL 벤치마크에서 SOTA를 달성했으며, 제로샷 이미지 캡셔닝, 교차 모달리티 전이, 오픈엔드 VQA 등에서 강력한 일반화 능력을 입증하며 생성적 VLP의 유망한 대안을 제시합니다.
