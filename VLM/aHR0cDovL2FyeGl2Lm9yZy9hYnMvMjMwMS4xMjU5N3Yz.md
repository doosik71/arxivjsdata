# BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi

## 🧩 Problem to Solve
기존의 비전-언어 사전 학습(VLP) 모델들은 대규모 모델과 데이터셋을 사용한 엔드투엔드(end-to-end) 학습으로 인해 엄청난 계산 비용이 발생했습니다. 또한, 이미 사전 학습된 단일 모달(unimodal) 모델(예: 이미지 인코더, 대규모 언어 모델(LLM))의 강력한 능력을 효과적으로 활용하기 어려워, 높은 계산 효율성과 유연성을 갖춘 VLP 방법이 필요했습니다. 특히, 이미지와 텍스트 모달리티 간의 간극을 효과적으로 연결하는 것이 핵심 과제였습니다.

## ✨ Key Contributions
*   **효율적인 사전 학습 전략:** 미리 학습되어 고정된(frozen) 이미지 인코더와 대규모 언어 모델을 활용하여 계산 효율적인 VLP 방법인 BLIP-2를 제안합니다. 이는 모델의 학습 가능한 파라미터 수를 크게 줄입니다.
*   **쿼리 트랜스포머(Q-Former) 도입:** 이미지 인코더와 LLM 사이의 모달리티 간극을 연결하는 경량의 쿼리 트랜스포머를 개발했습니다.
*   **2단계 사전 학습 전략:** Q-Former는 두 단계로 사전 학습됩니다.
    1.  **비전-언어 표현 학습(Vision-Language Representation Learning):** 고정된 이미지 인코더로부터 시각적 표현을 학습하여 텍스트와 관련된 정보를 추출합니다.
    2.  **비전-언어 생성 학습(Vision-to-Language Generative Learning):** 고정된 LLM으로부터 시각적 표현을 기반으로 텍스트를 생성하는 능력을 학습시킵니다.
*   **최첨단 성능 달성:** 적은 수의 학습 가능한 파라미터로 다양한 비전-언어 태스크(예: 시각 질의 응답, 이미지 캡셔닝, 이미지-텍스트 검색)에서 최첨단 성능을 달성했습니다. 예를 들어, VQAv2 제로샷(zero-shot) 태스크에서 Flamingo80B를 $8.7\%$ 능가하면서도 학습 가능한 파라미터 수는 $54$배 적습니다.
*   **제로샷 이미지-텍스트 생성 능력:** 자연어 명령을 따르는 제로샷 이미지-텍스트 생성(zero-shot instructed image-to-text generation)과 같은 새로운 능력을 보여주며, 시각적 지식 추론, 시각적 대화 등의 가능성을 열었습니다.

## 📎 Related Works
*   **엔드투엔드 비전-언어 사전 학습(End-to-end Vision-Language Pre-training):**
    *   **CLIP (Radford et al., 2021), ALBEF (Li et al., 2021), BLIP (Li et al., 2022), BEIT-3 (Wang et al., 2022b):** 다양한 아키텍처(듀얼 인코더, 퓨전 인코더, 인코더-디코더, 통합 트랜스포머)와 사전 학습 목표(이미지-텍스트 대조 학습, 이미지-텍스트 매칭, 마스크드 언어 모델링)를 사용합니다. 대규모 모델과 데이터셋을 엔드투엔드로 학습하여 계산 비용이 높습니다.
*   **모듈식 비전-언어 사전 학습(Modular Vision-Language Pre-training):**
    *   **Frozen (Tsimpoukelli et al., 2021):** 고정된 LLM을 활용하며, 이미지 인코더의 출력을 LLM의 소프트 프롬프트로 사용합니다.
    *   **Flamingo (Alayrac et al., 2022):** LLM에 새로운 크로스-어텐션 레이어를 삽입하여 시각적 특징을 주입하고, 대규모 이미지-텍스트 쌍으로 이 레이어를 사전 학습합니다.
    *   **LiT (Zhai et al., 2022):** 고정된 사전 학습 이미지 인코더를 CLIP 사전 학습에 사용합니다.
    *   이러한 방법들은 주로 이미지-텍스트 생성 손실을 사용하지만, BLIP-2는 고정된 이미지 인코더와 LLM을 모두 효과적으로 활용하고, Q-Former를 통한 2단계 학습으로 더 강력하고 효율적인 접근 방식을 제안합니다.

## 🛠️ Methodology
BLIP-2는 고정된 사전 학습 단일 모달 모델(이미지 인코더, LLM)을 활용하는 새로운 비전-언어 사전 학습 방법입니다. 모달리티 간극을 연결하기 위해 2단계로 사전 학습되는 **쿼리 트랜스포머(Q-Former)**를 제안합니다.

1.  **Q-Former 모델 아키텍처:**
    *   Q-Former는 고정된 이미지 인코더와 고정된 LLM 사이의 간극을 연결하는 학습 가능한 모듈입니다.
    *   고정된 이미지 인코더로부터 입력 이미지 해상도와 무관하게 고정된 수의 출력 특징을 추출합니다.
    *   **학습 가능한 쿼리 임베딩(learnable query embeddings)**을 입력으로 사용하며, 이 쿼리들은 셀프-어텐션 레이어를 통해 상호작용하고, 크로스-어텐션 레이어를 통해 고정된 이미지 특징과 상호작용합니다.
    *   쿼리들은 텍스트와도 상호작용할 수 있습니다.
    *   Q-Former는 정보 병목(information bottleneck) 역할을 하여 LLM에 가장 유용한 시각적 특징만 전달합니다. 총 188M 파라미터를 가지며, BERT$_{base}$의 가중치로 초기화됩니다.

2.  **1단계: 고정된 이미지 인코더로부터 비전-언어 표현 학습 (Representation Learning)**
    *   Q-Former를 고정된 이미지 인코더에 연결하고 이미지-텍스트 쌍을 사용하여 사전 학습합니다.
    *   쿼리가 텍스트와 가장 관련 있는 시각적 표현을 추출하도록 훈련합니다.
    *   세 가지 사전 학습 목표를 공동으로 최적화합니다:
        *   **이미지-텍스트 대조 학습(Image-Text Contrastive Learning, ITC):** 이미지 표현과 텍스트 표현을 정렬하여 상호 정보량을 최대화합니다. 쿼리와 텍스트 간의 직접적인 정보 누출을 방지하기 위해 단일 모달(unimodal) 셀프-어텐션 마스크를 사용합니다.
        *   **이미지 기반 텍스트 생성(Image-grounded Text Generation, ITG):** 입력 이미지를 조건으로 텍스트를 생성하도록 Q-Former를 훈련합니다. 쿼리들이 텍스트에 대한 모든 정보를 포착하도록 강제하며, 멀티모달 인과(causal) 셀프-어텐션 마스크를 사용합니다.
        *   **이미지-텍스트 매칭(Image-Text Matching, ITM):** 이미지-텍스트 쌍이 매칭되는지(긍정) 또는 매칭되지 않는지(부정) 예측하는 이진 분류 태스크입니다. 양방향(bi-directional) 셀프-어텐션 마스크를 사용하여 쿼리와 텍스트가 상호작용합니다. 하드 부정 샘플링(hard negative mining) 전략을 채택합니다.

3.  **2단계: 고정된 LLM으로부터 비전-언어 생성 학습 (Generative Learning)**
    *   Q-Former(고정된 이미지 인코더와 함께)를 고정된 LLM에 연결하여 LLM의 언어 생성 능력을 활용합니다.
    *   Q-Former의 출력 쿼리 임베딩 Z를 완전 연결(FC) 레이어를 통해 LLM의 텍스트 임베딩 차원과 동일하게 선형 투영합니다.
    *   투영된 쿼리 임베딩은 입력 텍스트 임베딩 앞에 추가되어 **소프트 시각 프롬프트(soft visual prompts)** 역할을 하여 LLM이 Q-Former가 추출한 시각적 표현을 조건으로 텍스트를 생성하도록 합니다.
    *   **디코더 기반 LLM (예: OPT):** 언어 모델링 손실(language modeling loss)로 사전 학습됩니다.
    *   **인코더-디코더 기반 LLM (예: FlanT5):** 프리픽스 언어 모델링 손실(prefix language modeling loss)로 사전 학습됩니다. 텍스트를 두 부분으로 나누어 프리픽스는 시각적 표현과 함께 인코더에, 서픽스는 디코더의 생성 목표로 사용됩니다.

4.  **모델 사전 학습 상세:**
    *   **사전 학습 데이터:** COCO, Visual Genome, CC3M/12M, SBU, LAION400M에서 총 129M 이미지를 사용합니다. CapFilt 방법을 사용하여 웹 이미지에 대한 합성 캡션을 생성하고, CLIP ViT-L/14 모델로 순위를 매겨 상위 2개의 캡션을 훈련 데이터로 사용합니다.
    *   **고정된 이미지 인코더:** CLIP ViT-L/14 및 EVA-CLIP ViT-g/14를 사용합니다.
    *   **고정된 LLM:** OPT (디코더 기반) 및 FlanT5 (인코더-디코더 기반) 모델 계열을 사용합니다.
    *   **훈련 설정:** 1단계에서 250k 스텝, 2단계에서 80k 스텝 동안 학습하며, 계산 비용이 효율적입니다.

## 📊 Results
*   **제로샷 비전-언어 태스크 성능:**
    *   VQAv2: Flamingo80B보다 $8.7\%$ 높은 성능(56.3% vs 65.0%)을 달성하면서 학습 가능한 파라미터는 $54$배 적습니다.
    *   NoCaps 이미지 캡셔닝: 최첨단 성능을 달성하며, 도메인 외 이미지에 대한 강력한 일반화 능력을 보여줍니다.
    *   Flickr30K 이미지-텍스트 검색: 제로샷 성능에서 최첨단을 기록했습니다.
*   **강력한 단일 모달 모델의 효과:** 더 강력한 이미지 인코더(ViT-g vs ViT-L)와 더 강력한 LLM(대형 모델, FlanT5 vs OPT) 모두 성능 향상으로 이어지는 것을 확인했습니다. 이는 BLIP-2가 최신 단일 모달 모델의 발전을 효율적으로 활용할 수 있는 일반적인 방법임을 증명합니다.
*   **1단계 표현 학습의 중요성:** 1단계 표현 학습 단계가 없으면, Q-Former는 모달리티 간극을 연결하는 데 실패하여 제로샷 VQA 성능이 크게 저하됩니다. 특히 OPT 모델은 catastrophic forgetting 현상을 겪습니다.
*   **ITG 손실의 기여:** 이미지-텍스트 검색에서 ITG(이미지 기반 텍스트 생성) 손실이 이미지-텍스트 정렬을 개선하여 성능 향상에 기여함을 입증했습니다.

## 🧠 Insights & Discussion
*   **제로샷 명령 기반 이미지-텍스트 생성의 가능성:** BLIP-2는 LLM이 이미지를 이해하고 텍스트 프롬프트에 따라 이미지-텍스트 생성을 제어할 수 있게 함으로써, 시각적 지식 추론, 시각적 상식 추론, 시각적 대화, 스토리텔링 등 다양한 새로운 제로샷 능력을 보여주었습니다.
*   **효율성과 확장성:** BLIP-2는 고정된 단일 모달 모델과 경량의 Q-Former를 사용하여 학습 가능한 파라미터를 최소화하면서 최첨단 성능을 달성하여, 계산 효율성과 확장성을 입증했습니다. 이는 빠르게 발전하는 비전 및 자연어 분야의 성과를 효율적으로 통합할 수 있는 기반을 마련합니다.
*   **제한 사항:**
    *   **In-context learning의 부재:** 현재 사전 학습 데이터셋(샘플당 단일 이미지-텍스트 쌍)의 한계로 인해, LLM에 few-shot VQA 예제를 제공하더라도 in-context learning 능력이 관찰되지 않았습니다. 이는 여러 이미지-텍스트 쌍 간의 상관관계를 학습하는 데 필요한 데이터 형식의 부족 때문으로 보입니다.
    *   **출력의 부정확성 및 모델의 한계 계승:** LLM의 부정확한 지식, 잘못된 추론 경로 활성화, 또는 최신 정보 부족 등으로 인해 이미지-텍스트 생성 결과가 만족스럽지 않을 수 있습니다. 또한, 고정된 LLM의 사용은 공격적인 언어 출력, 사회적 편견 전파, 사적 정보 유출과 같은 LLM의 내재된 위험을 BLIP-2도 계승하게 됩니다.

## 📌 TL;DR
BLIP-2는 고정된(frozen) 이미지 인코더와 대규모 언어 모델(LLM)을 활용하여 효율적인 비전-언어 사전 학습을 위한 새로운 프레임워크입니다. 경량의 쿼리 트랜스포머(Q-Former)를 2단계(표현 학습, 생성 학습)로 사전 학습하여 이미지와 텍스트 모달리티 간의 간극을 효과적으로 연결합니다. 이 방법은 학습 가능한 파라미터가 현저히 적음에도 불구하고 VQAv2, 이미지 캡셔닝 등 다양한 비전-언어 태스크에서 최첨단 성능을 달성했으며, 명령 기반 제로샷 이미지-텍스트 생성과 같은 새로운 능력을 보여주어 멀티모달 대화형 AI 에이전트 구축의 중요한 진전을 나타냅니다.