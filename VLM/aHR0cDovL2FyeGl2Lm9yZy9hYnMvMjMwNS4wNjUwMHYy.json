{
  "title": "InstructBLIP: Towards General-purpose Vision-Language Models with\n  Instruction Tuning",
  "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi",
  "year": 2023,
  "url": "http://arxiv.org/abs/2305.06500v2",
  "abstract": "Large-scale pre-training and instruction tuning have been successful at\ncreating general-purpose language models with broad competence. However,\nbuilding general-purpose vision-language models is challenging due to the rich\ninput distributions and task diversity resulting from the additional visual\ninput. Although vision-language pretraining has been widely studied,\nvision-language instruction tuning remains under-explored. In this paper, we\nconduct a systematic and comprehensive study on vision-language instruction\ntuning based on the pretrained BLIP-2 models. We gather 26 publicly available\ndatasets, covering a wide variety of tasks and capabilities, and transform them\ninto instruction tuning format. Additionally, we introduce an instruction-aware\nQuery Transformer, which extracts informative features tailored to the given\ninstruction. Trained on 13 held-in datasets, InstructBLIP attains\nstate-of-the-art zero-shot performance across all 13 held-out datasets,\nsubstantially outperforming BLIP-2 and larger Flamingo models. Our models also\nlead to state-of-the-art performance when finetuned on individual downstream\ntasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts).\nFurthermore, we qualitatively demonstrate the advantages of InstructBLIP over\nconcurrent multimodal models. All InstructBLIP models are open-sourced at\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip."
}