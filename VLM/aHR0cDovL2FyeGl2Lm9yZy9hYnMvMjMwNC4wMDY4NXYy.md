# Vision-Language Models for Vision Tasks: A Survey

Jingyi Zhang, Jiaxing Huang, Sheng Jin and Shijian Lu

## 🧩 Problem to Solve

기존 시각 인식 연구는 딥러닝 모델(DNNs) 훈련 시 대규모 크라우드 레이블링 데이터에 크게 의존하고, 각 시각 인식 작업을 위해 별도의 DNN을 훈련하여 노동 집약적이고 시간이 많이 소요되는 패러다임을 야기합니다. 또한, '사전 훈련, 미세 조정 및 예측(Pre-training, Fine-tuning and Prediction)' 패러다임도 여전히 각 다운스트림 작업에 대한 레이블링된 데이터와 미세 조정 단계를 필요로 합니다. 이러한 문제를 해결하기 위해, 본 논문은 VLM(Vision-Language Model)을 기반으로 한 시각 인식 연구의 포괄적인 개요가 부족하다는 점을 지적하며, 이 분야의 체계적인 정리와 향후 연구 방향 제시의 필요성을 강조합니다.

## ✨ Key Contributions

* **VLM 기반 시각 인식에 대한 최초의 체계적인 설문조사:** 이미지 분류, 객체 탐지, 의미론적 분할 등 다양한 시각 인식 작업을 위한 VLM의 포괄적인 개요, 기존 연구의 요약 및 분류를 제공합니다.
* **최신 VLM 진행 상황 분석 및 벤치마킹:** 여러 공개 데이터셋에 대한 기존 작업의 포괄적인 벤치마킹과 분석 및 논의를 포함하여 VLM의 최신 발전 상황을 연구합니다.
* **미래 연구 도전 과제 및 잠재적 방향 제시:** 시각 인식을 위한 VLM 연구에서 추구할 수 있는 몇 가지 연구 과제와 잠재적인 미래 연구 방향을 제시합니다.

## 📎 Related Works

본 설문조사는 시각 인식 작업을 위한 VLM에 중점을 둔 최초의 연구입니다. 기존 관련 설문조사는 주로 시각 질의 응답(Visual Question Answering), 시각적 추론을 위한 자연어(Natural Language for Visual Reasoning), 구문 접지(Phrase Grounding)와 같은 비전-언어(Vision-Language) 작업에 대한 VLM을 다루고 있습니다. 예를 들어, Li et al.은 VLM 사전 훈련을 포함한 비전-언어 작업의 발전을 공유했고, Du et al.과 Chen et al.은 비전-언어 작업에 대한 VLM 사전 훈련을 검토했습니다. Xu et al.과 Wang et al.은 멀티모달 작업을 위한 멀티모달 학습의 최신 진행 상황을 공유했습니다. 이와 달리 본 논문은 다음 세 가지 주요 측면에서 시각 인식 작업을 위한 VLM을 검토합니다:

1. 시각 인식 작업을 위한 VLM 사전 훈련의 최신 진행 상황
2. VLM을 시각 인식 작업으로 전이하는 두 가지 대표적인 전이 방법
3. 시각 인식 작업에 대한 VLM 사전 훈련 방법의 벤치마킹

## 🛠️ Methodology

본 논문은 VLM을 활용한 시각 인식 연구를 세 가지 주요 접근 방식(사전 훈련, 전이 학습, 지식 증류)으로 분류하고 각 방법론을 상세히 설명합니다.

### VLM 사전 훈련 (VLM Pre-training)

VLM 사전 훈련은 이미지와 텍스트의 상관관계를 학습하여 시각 인식 작업에서 효과적인 제로샷 예측(zero-shot prediction)을 목표로 합니다.

* **네트워크 아키텍처:**
  * **이미지 특징 학습:** CNN 기반(예: ResNet), 트랜스포머 기반(예: ViT) 아키텍처가 사용됩니다.
  * **언어 특징 학습:** 트랜스포머 및 그 변형(예: GPT-2 수정 버전)이 널리 채택됩니다.
* **사전 훈련 목표(Objectives):**
  * **대조 목표(Contrastive Objectives):**
    * **이미지 대조 학습:** 쿼리 이미지와 긍정적인 키(데이터 증강)를 가깝게, 부정적인 키를 멀리 학습합니다. (예: SLIP)
    * **이미지-텍스트 대조 학습:** 짝을 이루는 이미지와 텍스트 임베딩을 가깝게, 그렇지 않은 쌍은 멀리 학습합니다. (예: CLIP, ALIGN, FILIP)
    * **이미지-텍스트-레이블 대조 학습:** 이미지 분류 레이블을 이미지-텍스트 대조 학습에 통합하여 더 많은 감독 정보를 활용합니다. (예: UniCL)
    * **수학식 예시:** 이미지-텍스트 InfoNCE 손실 ($L_{\text{IT}}^{\text{InfoNCE}} = L_{\text{I}\to\text{T}} + L_{\text{T}\to\text{I}}$)은 다음과 같이 정의됩니다.
      $$L_{\text{I}\to\text{T}} = -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp (z_{\text{I}_i} \cdot z_{\text{T}_i} / \tau)}{\sum_{j=1}^{B} \exp(z_{\text{I}_i} \cdot z_{\text{T}_j} / \tau)}$$
      $$L_{\text{T}\to\text{I}} = -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp (z_{\text{T}_i} \cdot z_{\text{I}_i} / \tau)}{\sum_{j=1}^{B} \exp(z_{\text{T}_i} \cdot z_{\text{I}_j} / \tau)}$$
      여기서 $z_{\text{I}}$와 $z_{\text{T}}$는 각각 이미지 임베딩과 텍스트 임베딩을 나타냅니다.
  * **생성 목표(Generative Objectives):**
    * **마스크 이미지 모델링:** 이미지 패치를 마스킹하고 나머지 패치를 기반으로 재구성합니다. (예: FLAVA, KELIP)
    * **마스크 언어 모델링:** 텍스트 토큰의 일부를 마스킹하고 재구성합니다. (예: FLAVA, FIBER)
    * **마스크 교차-모달 모델링:** 이미지 패치와 텍스트 토큰을 모두 마스킹하고 재구성합니다. (예: FLAVA)
    * **이미지-텍스트 생성:** 주어진 이미지에 대한 설명 텍스트를 생성합니다. (예: COCA, NLIP, PaLI)
  * **정렬 목표(Alignment Objectives):**
    * **이미지-텍스트 매칭:** 이미지와 텍스트 쌍이 일치하는지 예측하여 전역적 상관관계를 모델링합니다. (예: FLAVA, FIBER)
    * **영역-단어 매칭:** 이미지 영역과 단어 토큰을 정렬하여 지역적, 세밀한 교차-모달 상관관계를 모델링합니다. (예: GLIP, FIBER, DetCLIP)
    * **수학식 예시:** 이미지-텍스트 매칭 손실 ($L_{\text{IT}}$)
      $$L_{\text{IT}} = p \log S(z_{\text{I}}, z_{\text{T}}) + (1-p) \log (1-S(z_{\text{I}}, z_{\text{T}}))$$
      여기서 $p$는 이미지와 텍스트가 짝을 이룰 경우 1, 그렇지 않을 경우 0입니다.
* **사전 훈련 프레임워크:**
  * **투-타워(Two-tower):** 이미지와 텍스트를 두 개의 개별 인코더로 인코딩합니다. (예: CLIP)
  * **투-레그(Two-leg):** 두 개의 개별 인코더에 추가적인 멀티모달 퓨전 레이어를 도입합니다. (예: FLAVA, COCA)
  * **원-타워(One-tower):** 단일 인코더로 비전 및 언어 학습을 통합합니다. (예: CLIPPO, OneR)
* **평가 설정 및 다운스트림 작업:**
  * **제로샷 예측:** 미세 조정 없이 VLM을 다운스트림 작업에 직접 적용합니다. (이미지 분류, 의미론적 분할, 객체 탐지 등)
  * **선형 프로빙(Linear Probing):** 사전 훈련된 VLM을 고정하고 선형 분류기를 훈련하여 VLM으로 인코딩된 임베딩을 분류합니다.

### VLM 전이 학습 (VLM Transfer Learning)

사전 훈련된 VLM을 특정 다운스트림 작업에 적응시키는 방법입니다.

* **전이 설정:**
  * **지도 전이(Supervised transfer):** 모든 레이블링된 다운스트림 데이터를 사용합니다.
  * **소수샷 지도 전이(Few-shot supervised transfer):** 적은 수의 레이블링된 샘플을 사용합니다.
  * **비지도 전이(Unsupervised transfer):** 레이블링되지 않은 다운스트림 데이터를 사용합니다.
* **주요 전이 방법:**
  * **프롬프트 튜닝(Prompt Tuning):** VLM 전체를 미세 조정하지 않고 최적의 프롬프트를 찾아 VLM을 적응시킵니다.
    * **텍스트 프롬프트 튜닝:** 학습 가능한 텍스트 프롬프트(예: CoOp, CoCoOp)를 사용합니다.
    * **시각 프롬프트 튜닝:** 이미지 인코더의 입력(예: VP, RePrompt)을 변조합니다.
    * **텍스트-시각 프롬프트 튜닝:** 텍스트와 이미지 입력을 동시에 변조합니다. (예: UPT, MaPLE)
  * **특징 어댑터(Feature Adapter):** 가벼운 특징 어댑터를 추가하여 이미지 또는 텍스트 특징을 적응시킵니다. (예: Clip-Adapter, Tip-Adapter)
  * **기타 전이 방법:** 직접 미세 조정(Wise-FT), 아키텍처 수정(MaskCLIP), 교차 어텐션(VT-CLIP, CALIP), 대규모 언어 모델(LLM) 활용(CuPL, VCD) 등이 있습니다.

### VLM 지식 증류 (VLM Knowledge Distillation)

VLM의 일반적이고 견고한 지식을 객체 탐지 및 의미론적 분할과 같은 복잡한 밀집 예측(dense prediction) 작업에 특화된 모델로 증류하는 방법입니다.

* **동기:** VLM 아키텍처의 제한 없이 VLM 지식을 작업별 모델로 전이하여 최신 탐지/분할 아키텍처의 이점을 활용합니다.
* **적용 분야:**
  * **객체 탐지:** 제로샷/오픈-어휘 객체 탐지(open-vocabulary object detection)를 위해 VLM 지식을 증류합니다. (예: ViLD, DetPro, HierKD)
  * **의미론적 분할:** 제로샷/오픈-어휘 의미론적 분할(open-vocabulary semantic segmentation) 및 약한 감독 의미론적 분할(weakly-supervised semantic segmentation)을 위해 VLM 지식을 활용합니다. (예: CLIPSeg, LSeg, ZegCLIP)

## 📊 Results

* **VLM 사전 훈련:**
  * **성능 향상:** VLM의 성능은 훈련 데이터의 크기와 모델의 크기에 비례하여 일관되게 향상됩니다. 대규모 이미지-텍스트 데이터셋(예: 수억에서 수십억 개)과 대규모 모델(예: ViT-G, 20억 개 파라미터)을 사용할 때 이미지 분류와 같은 다양한 다운스트림 작업에서 뛰어난 제로샷 성능을 달성합니다 (예: COCA, FILIP).
  * **강점:** 대규모 데이터, 대규모 모델, 작업 불가지론적(task-agnostic) 학습이 뛰어난 일반화 능력을 부여합니다.
  * **제한 사항:** 데이터/모델 크기 증가 시 성능 포화, 막대한 계산 자원(예: CLIP ViT-L 훈련에 V100 GPU 256개, 288시간) 및 메모리 오버헤드가 필요합니다.
* **VLM 전이 학습:**
  * **일관된 개선:** VLM 전이 학습은 다운스트림 작업의 성능을 일관되게 향상시킵니다. (예: ImageNet에서 지도 학습 Wise-FT는 10.9%, 소수샷 CoOp은 1.7%, 비지도 TPT는 0.8% 정확도 향상).
  * **소수샷 학습의 한계:** 소수샷 지도 전이는 모델이 소수샷 샘플에 과적합되어 일반화가 저하될 수 있어 지도 전이보다 성능이 떨어집니다.
  * **비지도 전이의 잠재력:** 비지도 전이는 대규모 레이블링되지 않은 데이터에 접근할 수 있어 과적합 위험이 낮고, 소수샷 지도 전이와 유사한 성능을 달성할 수 있습니다 (예: 비지도 UPL은 2샷 지도 CoOp보다 0.4% 더 나은 성능).
* **VLM 지식 증류:**
  * **명확한 성능 향상:** 객체 탐지 및 의미론적 분할 작업에서 VLM 지식 증류는 VLM의 일반적이고 견고한 지식을 활용하면서 작업별 모델 설계의 이점을 취함으로써 성능을 명확하게 향상시킵니다.
  * **유연성:** VLM 아키텍처에 관계없이 다양한 다운스트림 네트워크에 지식 증류를 적용할 수 있는 유연성이 있습니다.

## 🧠 Insights & Discussion

* **성능과 벤치마킹:**
  * VLM 사전 훈련은 이미지 분류에서 뛰어난 제로샷 예측을 보이지만, 밀집 시각 인식 작업(객체 탐지, 의미론적 분할)을 위한 발전은 아직 미흡합니다.
  * VLM 전이 학습은 여러 이미지 분류 데이터셋에서 상당한 진전을 이루었으나, 레이블링된 데이터를 요구하는 지도/소수샷 지도 전이가 주를 이루며, 더 유망하지만 도전적인 비지도 VLM 전이는 간과되고 있습니다.
  * VLM 전이 학습 연구는 공정한 벤치마킹을 위해 동일한 VLM과 다운스트림 작업을 사용하는 경향이 있지만, VLM 사전 훈련 및 지식 증류는 다양한 데이터셋, 네트워크, 비공개 데이터, 막대한 계산 자원, 다양한 작업별 백본을 사용하여 벤치마킹에 어려움이 있습니다. 표준화된 규범이 부족한 상황입니다.
* **미래 연구 방향:**
  * **VLM 사전 훈련:**
    1. **세밀한 비전-언어 상관관계 모델링:** 객체 탐지 및 의미론적 분할과 같은 밀집 예측 작업을 위해 패치 및 픽셀 수준의 로컬 비전-언어 지식을 학습합니다.
    2. **비전 및 언어 학습의 통합:** 단일 트랜스포머 내에서 이미지와 텍스트 학습을 통합하여 데이터 모달리티 간의 효율적인 통신을 가능하게 합니다.
    3. **다국어 VLM 사전 훈련:** 영어 외의 여러 언어로 VLM을 훈련하여 문화적 편향을 줄이고 다양한 언어 시나리오에서 효율성을 높입니다.
    4. **데이터 효율적인 VLM:** 제한된 이미지-텍스트 데이터로 효과적인 VLM을 훈련하여 지속 가능성을 높입니다.
    5. **LLM을 이용한 VLM 사전 훈련:** LLM(대규모 언어 모델)을 활용하여 이미지-텍스트 쌍의 텍스트를 증강하고 더 풍부한 언어 지식을 통합합니다.
  * **VLM 전이 학습:**
    1. **비지도 VLM 전이:** 레이블링되지 않은 대규모 데이터를 활용하여 과적합 위험을 줄이고 효율성을 높입니다.
    2. **시각 프롬프트/어댑터를 사용한 VLM 전이:** 텍스트 프롬프팅을 보완하고 다양한 밀집 예측 작업에서 픽셀 수준의 적응을 가능하게 합니다.
    3. **테스트-시간 VLM 전이:** 추론 중 즉석에서 프롬프트를 적응시켜 반복적인 훈련을 피합니다.
    4. **LLM을 이용한 VLM 전이:** LLM을 사용하여 다운스트림 작업을 더 잘 설명하는 맞춤형 프롬프트를 자동으로 생성합니다.
  * **VLM 지식 증류:**
    1. **다중 VLM으로부터의 지식 증류:** 여러 VLM의 시너지 효과를 활용하여 지식 증류를 조정합니다.
    2. **다른 시각 인식 작업을 위한 지식 증류:** 인스턴스 분할, 파노라마 분할, 사람 재식별(person re-identification) 등 다양한 작업에 지식 증류를 확장합니다.

## 📌 TL;DR

이 논문은 대규모 레이블링 데이터 의존성과 작업별 모델 훈련의 비효율성이라는 시각 인식의 고질적인 문제를 해결하기 위한 VLM(Vision-Language Model)의 역할과 발전을 종합적으로 분석합니다. VLM은 웹 스케일의 이미지-텍스트 쌍을 활용하여 제로샷 예측을 가능하게 함으로써 이러한 한계를 극복합니다. 본 설문조사는 VLM 사전 훈련의 다양한 목표(대조, 생성, 정렬)와 프레임워크(투-타워, 투-레그, 원-타워)를 설명하고, VLM을 다운스트림 작업에 적응시키는 전이 학습(프롬프트 튜닝, 특징 어댑터) 및 VLM 지식을 특정 작업 모델로 전달하는 지식 증류 방법론을 체계적으로 정리합니다. 결과적으로 VLM이 대규모 데이터와 모델을 통해 이미지 분류에서 뛰어난 제로샷 성능을 보이며, 전이 학습과 지식 증류가 밀집 예측 작업에서 효과적임을 보여줍니다. 마지막으로, 세밀한 모델링, 다국어 지원, 데이터 효율성, LLM 통합, 비지도 전이 등 VLM 연구의 미래 방향과 도전 과제를 제시하여 이 유망한 분야의 발전을 위한 로드맵을 제공합니다.
