{
  "url": "http://arxiv.org/abs/2204.07356v5",
  "title": "Vision-and-Language Pretrained Models: A Survey",
  "authors": "Siqu Long, Feiqi Cao, Soyeon Caren Han, Haiqin Yang",
  "year": 2022,
  "abstract": "Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.",
  "citation": 74
}