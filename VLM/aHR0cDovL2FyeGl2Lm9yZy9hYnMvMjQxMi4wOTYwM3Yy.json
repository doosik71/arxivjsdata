{
  "title": "Do Multimodal Large Language Models See Like Humans?",
  "authors": "Jiaying Lin, Shuquan Ye, Rynson W. H. Lau",
  "year": 2024,
  "url": "http://arxiv.org/abs/2412.09603v2",
  "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive results on\nvarious vision tasks, leveraging recent advancements in large language models.\nHowever, a critical question remains unaddressed: do MLLMs perceive visual\ninformation similarly to humans? Current benchmarks lack the ability to\nevaluate MLLMs from this perspective. To address this challenge, we introduce\nHVSBench, a large-scale benchmark designed to assess the alignment between\nMLLMs and the human visual system (HVS) on fundamental vision tasks that mirror\nhuman vision. HVSBench curated over 85K multimodal samples, spanning 13\ncategories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,\nFree-Viewing, and Searching. Extensive experiments demonstrate the\neffectiveness of our benchmark in providing a comprehensive evaluation of\nMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best models\nshow significant room for improvement, with most achieving only moderate\nresults. Our experiments reveal that HVSBench presents a new and significant\nchallenge for cutting-edge MLLMs. Diverse human participants attained strong\nperformance, significantly outperforming MLLMs, which further underscores the\nbenchmark's high quality. We believe that HVSBench will facilitate research on\nhuman-aligned and explainable MLLMs, marking a key step in understanding how\nMLLMs perceive and process visual information."
}