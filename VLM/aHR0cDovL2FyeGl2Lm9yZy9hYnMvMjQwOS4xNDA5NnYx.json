{
  "title": "VLM-Vac: Enhancing Smart Vacuums through VLM Knowledge Distillation and\n  Language-Guided Experience Replay",
  "authors": "Reihaneh Mirjalili, Michael Krawez, Florian Walter, Wolfram Burgard",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.14096v1",
  "abstract": "In this paper, we propose VLM-Vac, a novel framework designed to enhance the\nautonomy of smart robot vacuum cleaners. Our approach integrates the zero-shot\nobject detection capabilities of a Vision-Language Model (VLM) with a Knowledge\nDistillation (KD) strategy. By leveraging the VLM, the robot can categorize\nobjects into actionable classes -- either to avoid or to suck -- across diverse\nbackgrounds. However, frequently querying the VLM is computationally expensive\nand impractical for real-world deployment. To address this issue, we implement\na KD process that gradually transfers the essential knowledge of the VLM to a\nsmaller, more efficient model. Our real-world experiments demonstrate that this\nsmaller model progressively learns from the VLM and requires significantly\nfewer queries over time. Additionally, we tackle the challenge of continual\nlearning in dynamic home environments by exploiting a novel experience replay\nmethod based on language-guided sampling. Our results show that this approach\nis not only energy-efficient but also surpasses conventional vision-based\nclustering methods, particularly in detecting small objects across diverse\nbackgrounds."
}