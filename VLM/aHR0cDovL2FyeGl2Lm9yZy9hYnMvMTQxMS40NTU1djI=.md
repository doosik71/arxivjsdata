# Show and Tell: A Neural Image Caption Generator
Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan

## 🧩 Problem to Solve
이 논문은 이미지의 내용을 자동으로 이해하고 이를 자연어로 설명하는 문장을 생성하는 인공지능의 근본적인 문제를 다룹니다. 기존 접근 방식들은 시각 인식과 자연어 처리라는 두 가지 별개의 문제를 해결한 다음, 그 결과를 결합하는 경우가 많았습니다. 본 연구는 이러한 복잡한 시스템 대신, 이미지를 입력으로 받아 관련 설명을 직접 생성하는 단일하고 통합된(end-to-end) 모델을 제안하여 이미지 캡셔닝 태스크의 어려움을 극복하고자 합니다.

## ✨ Key Contributions
*   **최초의 End-to-End 신경망 시스템 제안:** 이미지를 입력으로 받아 자연어 문장을 직접 생성하는 완전 학습 가능한(fully trainable) 종단간(end-to-end) 신경망 시스템을 제시합니다. 이는 확률 $p(S|I)$를 최대화하도록 훈련됩니다.
*   **첨단 Vision 및 Language 모델 결합:** 이미지 인코딩을 위해 사전 훈련된 심층 CNN(Convolutional Neural Network)과 문장 생성을 위한 순환 신경망(RNN), 특히 LSTM(Long Short-Term Memory)을 결합하여 각 분야의 최신 기술 발전을 활용합니다.
*   **획기적인 성능 향상:** 다양한 데이터셋(Pascal, Flickr30k, SBU, COCO)에서 기존 최첨단 모델 대비 BLEU 스코어에서 상당한 개선을 달성합니다. 예를 들어, Pascal 데이터셋에서 BLEU-1 점수를 25에서 59로 대폭 향상시켰습니다 (인간 성능 약 69).
*   **다양하고 새로운 캡션 생성 능력:** 모델이 훈련 데이터셋에 없는 새로운 캡션을 생성하며, 빔 탐색(Beam Search)을 통해 다양하고 고품질의 설명을 제공할 수 있음을 입증합니다.

## 📎 Related Works
*   **초기 비디오 및 이미지 설명 시스템:** 비디오 설명에 중점을 두었으며, 시각적 원시 인식기와 구조화된 형식 언어를 결합한 복잡한 수작업 시스템이 주로 사용되었습니다.
*   **객체 감지 기반 텍스트 생성:** 객체, 속성, 위치 인식을 활용하여 텍스트를 생성하는 방식으로, Farhadi et al. [6]은 삼단 요소를 사용하여 템플릿 기반 텍스트를 생성하고, Kulkarni et al. [16]은 더 복잡한 감지 그래프를 사용했습니다. 이러한 접근 방식은 유연성이 부족합니다.
*   **이미지-텍스트 임베딩 및 랭킹:** 이미지와 텍스트를 동일한 벡터 공간에 공동 임베딩하여 주어진 이미지에 대한 설명을 랭킹하는 연구들이 있었습니다 [11, 8, 24]. neural network를 사용하여 이미지와 문장을 공동 임베딩하는 방식 [29]도 있었지만, 새로운 설명을 생성하지는 못했습니다.
*   **RNN 기반 이미지 설명 모델 (이전 연구):** Kiros et al. [15]는 피드포워드 신경망을 사용해 다음 단어를 예측했으며, Mao et al. [21]은 RNN을 사용했습니다. 본 연구는 이들과 유사하지만, 더 강력한 RNN 모델(LSTM)을 사용하고 시각적 입력을 RNN에 직접 제공하여 객체 추적 능력을 높여 더 우수한 성능을 달성합니다. Kiros et al. [14]는 별도의 경로를 통해 다중 모드 임베딩 공간을 구축했으나, 본 연구는 단일 네트워크로 설명을 생성합니다.
*   **기계 번역 분야의 Sequence-to-Sequence 모델:** 본 연구는 입력 문장을 고정 길이 벡터로 인코딩한 다음, 이 벡터를 사용하여 대상 문장을 디코딩하는 RNN 기반 기계 번역 모델 [3, 2, 30]에서 영감을 받았습니다. 본 연구에서는 "인코더" RNN을 CNN으로 대체합니다.

## 🛠️ Methodology
본 논문에서는 "Neural Image Caption (NIC)"이라는 종단간 신경망 모델을 제안합니다. 이 모델은 이미지를 입력으로 받아 자연어 설명을 생성합니다.

1.  **Objective Function:**
    *   모델은 주어진 훈련 이미지 $I$에 대한 올바른 설명 $S = \{S_0, S_1, \dots, S_N\}$의 확률 $p(S|I;\theta)$을 최대화하도록 훈련됩니다.
    *   이를 위해 다음의 목적 함수를 사용합니다:
        $$ \theta^* = \arg \max_{\theta} \sum_{(I,S)} \log p(S|I;\theta) $$
    *   문장의 길이가 가변적이므로, 조건부 확률의 연쇄 법칙을 사용하여 다음을 모델링합니다:
        $$ \log p(S|I) = \sum_{t=0}^{N} \log p(S_t|I, S_0, \dots, S_{t-1}) $$

2.  **Model Architecture (CNN + LSTM):**
    *   **Image Encoder (CNN):** 이미지를 고정 길이 벡터 표현으로 인코딩하는 역할을 합니다. 본 연구에서는 ILSVRC 2014 분류 경쟁에서 최신 성능을 달성한 CNN 모델 [12]을 사용하며, ImageNet과 같은 대규모 데이터셋에서 사전 훈련된 가중치를 활용합니다. CNN의 마지막 은닉층 출력이 RNN의 초기 입력으로 사용됩니다.
        $$ x_{-1} = \text{CNN}(I) $$
    *   **Sentence Decoder (LSTM):** CNN에서 인코딩된 이미지 벡터와 이전에 생성된 단어를 기반으로 다음 단어를 예측하는 순환 신경망입니다. vanishing 및 exploding gradient 문제를 잘 처리하는 LSTM [10]을 사용합니다.
        *   LSTM의 핵심은 메모리 셀 $c$와 세 개의 게이트(입력 게이트 $i_t$, 망각 게이트 $f_t$, 출력 게이트 $o_t$)입니다.
        *   게이트 및 셀 업데이트 방정식은 다음과 같습니다:
            $$ i_t = \sigma(W_{ix}x_t + W_{im}m_{t-1}) $$
            $$ f_t = \sigma(W_{fx}x_t + W_{fm}m_{t-1}) $$
            $$ o_t = \sigma(W_{ox}x_t + W_{om}m_{t-1}) $$
            $$ c_t = f_t \odot c_{t-1} + i_t \odot h(W_{cx}x_t + W_{cm}m_{t-1}) $$
            $$ m_t = o_t \odot c_t $$
            여기서 $x_t$는 현재 시점의 입력(단어 임베딩), $m_{t-1}$은 이전 시점의 LSTM 출력, $W$는 학습 가능한 가중치 행렬입니다.
        *   최종적으로 $m_t$를 Softmax 함수에 입력하여 다음 단어의 확률 분포 $p_{t+1}$을 생성합니다:
            $$ p_{t+1} = \text{Softmax}(m_t) $$
    *   **Word Embeddings ($W_e$):** 각 단어는 one-hot 벡터로 표현된 후, 학습 가능한 임베딩 행렬 $W_e$를 통해 저차원 밀집 벡터로 변환되어 LSTM의 입력으로 사용됩니다.
        $$ x_t = W_e S_t, \quad t \in \{0 \dots N-1\} $$
        여기서 $S_0$는 특별한 시작 단어, $S_N$은 종료 단어를 나타냅니다. 이미지는 시간 $t=-1$에 한 번만 입력되어 LSTM에 이미지 내용을 전달합니다.

3.  **Training:**
    *   손실 함수는 각 단계에서 올바른 단어에 대한 음의 로그 우도(negative log likelihood)의 합입니다:
        $$ L(I,S) = -\sum_{t=1}^{N} \log p_t(S_t) $$
    *   확률적 경사 하강법(Stochastic Gradient Descent)을 사용하여 LSTM의 모든 파라미터, CNN의 최상위 계층, 단어 임베딩 $W_e$를 최적화합니다.
    *   과적합(overfitting)을 피하기 위해 CNN 가중치는 사전 훈련된 모델로 초기화하고, 드롭아웃(dropout)과 모델 앙상블(ensembling) 기법을 사용합니다.

4.  **Inference:**
    *   주어진 이미지에 대한 문장 생성을 위해 **빔 탐색(Beam Search)** 방법을 사용합니다.
    *   이 방법은 매 단계에서 가장 가능성 있는 $k$개의 부분 문장을 유지하고, 다음 단어를 예측하여 $k$개의 최적의 문장 후보를 계속 업데이트합니다. 본 연구에서는 빔 크기 20을 사용했습니다.

## 📊 Results
*   **BLEU-1 스코어 대폭 개선:**
    *   **Pascal:** 기존 최첨단(SOTA) 25점에서 NIC는 **59점**을 달성 (인간 성능 69점).
    *   **Flickr30k:** 기존 SOTA 56점에서 NIC는 **66점**을 달성.
    *   **Flickr8k:** 기존 SOTA 58점에서 NIC는 **63점**을 달성.
    *   **SBU:** 기존 SOTA 19점에서 NIC는 **28점**을 달성.
*   **MSCOCO 데이터셋 최첨단 달성:**
    *   BLEU-4: **27.7점** (SOTA), METEOR: **23.7점**, CIDEr: **85.5점**. (인간 성능과 유사하거나 그 이상)
*   **Human Evaluation:** NIC의 설명은 다른 참조 시스템보다 우수하지만, 인간이 직접 작성한 설명보다는 낮았습니다. 이는 BLEU가 인간의 평가를 완벽하게 반영하지 못함을 시사합니다.
*   **다양한 캡션 생성:** 빔 탐색을 통해 상위 N-best 리스트를 분석한 결과, 모델이 동일한 이미지에 대해 다양하고 품질 좋은 설명을 생성할 수 있으며, 훈련 데이터셋에 없는 새로운(novel) 문장을 약 50%의 경우에 생성함을 확인했습니다.
*   **전이 학습(Transfer Learning) 효과:** 더 큰 데이터셋(예: Flickr30k)으로 훈련하면 성능이 향상되며, MSCOCO와 같이 규모가 크고 품질 좋은 데이터셋으로 훈련된 모델을 Pascal에 전이했을 때도 좋은 성능을 보였습니다. SBU와 같이 레이블 품질이 낮은 데이터셋에서는 성능 저하가 있었습니다.
*   **랭킹 태스크 성능:** 캡션 생성과 함께 이미지 랭킹 태스크(Image Annotation, Image Search)에서도 이전 연구 대비 높은 Recall@k 및 낮은 Median rank를 달성하며 준수한 성능을 보였습니다.
*   **단어 임베딩 학습:** 학습된 단어 임베딩 공간에서 "horse", "pony", "donkey"와 같이 의미론적으로 유사한 단어들이 가깝게 위치하는 것을 보여주며, 모델이 언어의 통계적 특성에서 의미론을 포착했음을 시사합니다.

## 🧠 Insights & Discussion
*   **데이터 규모의 중요성:** 과적합 문제를 해결하기 위해 사전 훈련된 CNN 가중치를 사용했으나, 이미지 설명 작업의 특성상 아직 데이터셋 규모가 작아 과적합에 취약합니다. 향후 훈련 데이터셋 규모가 커지면 NIC와 같은 데이터 기반 방식의 성능 이점이 더욱 커질 것으로 예상됩니다.
*   **평가 지표의 한계:** BLEU 스코어는 기계 번역에서 널리 사용되지만, 이미지 설명과 같이 여러 가지 정답이 존재할 수 있는 태스크에서는 인간의 평가와 완벽하게 일치하지 않을 수 있습니다. METEOR, CIDEr와 같은 다른 지표들을 함께 보고하며, 더 적절한 평가 지표에 대한 추가 연구의 필요성을 강조합니다.
*   **새로운 캡션 생성의 의의:** 모델이 단순히 훈련 데이터를 암기하는 것이 아니라, 새로운 이미지 구성에 대해서도 개별 객체에 대한 학습을 바탕으로 새로운 설명을 생성할 수 있다는 점은 모델의 일반화 및 창의적 능력의 가능성을 보여줍니다.
*   **Word Embedding의 역할:** 학습된 단어 임베딩은 단어 간의 의미론적 관계를 포착하여 시각적 구성 요소가 "말과 같은 동물"과 관련된 특징을 추출하도록 돕는 등, 시각 및 언어 이해에 시너지를 창출합니다. 이는 소수의 예시만 있는 클래스에 대한 설명을 개선하는 데 도움이 될 수 있습니다.
*   **향후 연구 방향:** 이미지 단독 또는 텍스트 단독의 비지도 학습 데이터를 활용하여 이미지 설명 접근 방식을 개선하는 연구가 흥미로울 것이라고 제안합니다.

## 📌 TL;DR
이 논문은 이미지를 자연어로 설명하는 문제에 대해, 사전 훈련된 CNN(Convolutional Neural Network) 인코더와 LSTM(Long Short-Term Memory) RNN(Recurrent Neural Network) 디코더를 결합한 종단간(end-to-end) 신경망 모델인 NIC를 제안합니다. 이 모델은 이미지를 입력받아 해당 설명 문장의 우도(likelihood)를 최대화하도록 훈련되며, Pascal, Flickr, SBU, MSCOCO 등 다양한 데이터셋에서 기존 최첨단 모델 대비 BLEU 스코어에서 획기적인 성능 향상을 달성했습니다. NIC는 이미지 이해와 자연어 생성을 통합하여, 새로운 이미지에 대한 다양하고 고품질의 설명을 효과적으로 생성할 수 있음을 입증합니다.