# Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao

## 🧩 Problem to Solve
기존의 대규모 비전-언어 사전 학습(VLP) 방법들은 이미지 영역 특징과 텍스트 특징을 단순히 연결하고 자기 주의(self-attention) 메커니즘을 통해 이미지-텍스트 의미론적 정렬을 "무차별적"으로 학습합니다. 하지만 이는 다음과 같은 문제점을 야기합니다:
- **명시적인 정렬 정보 부족**: 이미지 영역과 텍스트 간의 명확한 연결이 없어 약한 감독 학습(weakly-supervised learning) 문제가 됩니다.
- **모호한 시각적 영역**: 객체 탐지기(Faster R-CNN)로 추출된 시각적 영역 특징들은 종종 과도하게 샘플링(over-sampled)되고, 노이즈가 많으며, 서로 중첩되어 모호성을 가집니다(예: "개"와 "소파" 영역이 겹쳐 구분이 어려운 경우).
이러한 문제들로 인해 교차 모달(cross-modal) 표현 학습이 비효율적이고 도전적입니다.

## ✨ Key Contributions
이 연구의 주요 기여는 다음과 같습니다:
- **Oscar 제안**: 이미지 내에서 탐지된 객체 태그(object tags)를 "앵커 포인트(anchor points)"로 활용하여 이미지-텍스트 정렬 학습을 획기적으로 개선하는 강력한 VLP 방법인 Oscar를 제안합니다.
- **새로운 SoTA 달성**: Oscar 모델이 다양한 비전-언어 벤치마크에서 기존 접근 방식들을 상당한 차이로 능가하며, 6개 주요 비전-언어 이해 및 생성 태스크에서 새로운 SoTA(State-of-the-Art)를 달성합니다.
- **심층적인 분석**: 객체 태그를 앵커 포인트로 사용하는 것의 효과에 대한 광범위한 실험과 분석을 제공하여 교차 모달 표현 학습 및 다운스트림 태스크에서의 이점을 입증합니다.

## 📎 Related Works
- **비전-언어 사전 학습(VLP)**: VilBERT, VL-BERT, VisualBERT, LXMERT, UNITER 등과 같은 기존 VLP 모델들은 BERT와 유사한 Transformer 기반 아키텍처를 사용하여 이미지-텍스트 쌍에서 교차 모달 표현을 학습합니다. 이들은 주로 자기 주의를 통해 정렬을 학습하며, 명시적인 정렬 정보 없이 시각적 영역 특징과 언어 토큰 임베딩을 결합합니다.
- **객체 태그 활용**: 이전 연구들(예: [46], [42], [43])은 객체 탐지 확률이나 이미지 수준 레이블/속성을 시각적 특징 표현을 강화하는 데 사용했지만, 이미지-텍스트 정렬을 명시적으로 학습하기 위한 앵커 포인트로는 활용하지 않았습니다. Anderson et al. [2]의 bottom-up 어텐션은 객체 수준에서 어텐션을 계산하는 표준이 되었습니다.
- **멀티모달 임베딩**: DeViSE [8]와 같은 초기 연구들([33], [14], [29], [34], [15], [25])은 공유 임베딩 공간에서 이미지와 텍스트 간의 교차 모달 대응 관계를 정렬하는 것의 이점을 보여주었습니다. 이는 언어적 정보를 활용하여 제로샷(zero-shot) 예측 및 교차 모달 전이 학습의 효율성을 향상시킵니다. Oscar는 현대 언어 모델의 풍부한 의미론을 활용하여 이 아이디어를 재해석합니다.

## 🛠️ Methodology
Oscar는 이미지에서 탐지된 객체 태그가 종종 짝을 이루는 텍스트에 언급된다는 관찰에 기반합니다. 이를 통해 객체 태그를 "앵커 포인트"로 사용하여 이미지 영역과 텍스트 단어 사이의 의미론적 정렬 학습을 용이하게 합니다.

1.  **입력 표현**: 각 이미지-텍스트 쌍을 **Word-Tag-Image 세 개의 요소($w, q, v$)**로 표현합니다.
    *   $w$: 텍스트의 단어 임베딩 시퀀스.
    *   $q$: 이미지에서 탐지된 객체 태그의 단어 임베딩 시퀀스. (언어적 의미 공간)
    *   $v$: 이미지의 영역 벡터 세트. (시각적 의미 공간)

2.  **객체 태그 및 영역 특징 생성**:
    *   Faster R-CNN [28]을 사용하여 이미지에서 $K$개의 객체 영역을 탐지합니다.
    *   각 영역에서 시각적 특징 $v' \in \mathbb{R}^{P}$ (예: $P=2048$)와 영역 위치 $z \in \mathbb{R}^{R}$ (예: $R=4$ 또는 $6$)를 추출합니다. $v'$와 $z$를 연결한 후 선형 투영을 통해 $v$를 생성하여 단어 임베딩과 동일한 차원을 갖도록 합니다.
    *   동일한 Faster R-CNN을 사용하여 고정밀 객체 태그 세트를 탐지하고, 이 태그들의 단어 임베딩으로 $q$를 구성합니다.

3.  **모델 아키텍처**: BERT base 또는 large 모델의 파라미터로 초기화된 다층 Transformer를 사용합니다.

4.  **사전 학습 목표**:
    *   **마스크드 토큰 손실(Masked Token Loss, MTL) - "사전적 관점(Dictionary View)"**:
        *   단어 토큰과 객체 태그를 결합한 시퀀스 $h = [w, q]$의 15%를 무작위로 마스킹합니다.
        *   마스킹된 토큰 $h_i$를 주변 토큰 $h_{\backslash i}$와 모든 이미지 특징 $v$를 기반으로 예측합니다.
        *   손실 함수: $L_{MTL} = -E_{(v,h) \sim D} \log p(h_i | h_{\backslash i}, v)$
        *   BERT의 마스크드 언어 모델과 유사하지만, 시각적 정보가 추가로 활용됩니다.
    *   **대조 손실(Contrastive Loss, C) - "모달리티 관점(Modality View)"**:
        *   $h' = [q, v]$를 이미지 모달리티로, $w$를 언어 모달리티로 간주합니다.
        *   $q$를 데이터셋 $D$에서 무작위로 샘플링된 다른 태그 시퀀스로 교체하여(50% 확률) "오염된" 이미지 표현을 생성합니다.
        *   [CLS] 토큰의 출력에 이진 분류기 $f(.)$를 적용하여 주어진 쌍이 원래의 정렬된 쌍인지(y=1) 오염된 쌍인지(y=0) 예측합니다.
        *   손실 함수: $L_C = -E_{(h',w) \sim D} \log p(y | f(h',w))$
        *   원래 이미지-텍스트 쌍은 유사하게, 오염된 쌍은 비유사하게 만들도록 학습합니다.
    *   **최종 사전 학습 목표**: $L_{Pre-training} = L_{MTL} + L_C$

5.  **사전 학습 데이터셋**: COCO, Conceptual Captions, SBU Captions, Flicker30k, GQA 등을 포함한 650만 개의 텍스트-이미지 쌍으로 구성된 대규모 코퍼스에 대해 사전 학습을 진행합니다.

6.  **파인튜닝**: 사전 학습된 Oscar 모델은 이미지-텍스트 검색, 이미지 캡셔닝, VQA(Visual Question Answering), GQA(Visual Reasoning), NLVR2(Natural Language Visual Reasoning) 등 7개의 다운스트림 비전-언어 태스크에 대해 파인튜닝됩니다.

## 📊 Results
Oscar는 6개 주요 비전-언어 이해 및 생성 태스크에서 새로운 SoTA를 달성하며 뛰어난 성능을 보였습니다.
- **전반적인 성능 향상**: Oscar$_{B}$ (BERT-base 크기) 모델은 대부분의 태스크에서 기존의 더 큰 SoTA$_{L}$ 모델들을 능가하여 뛰어난 파라미터 효율성을 입증했습니다. Oscar$_{L}$ (BERT-large 크기) 모델은 모든 태스크에서 일관되게 새로운 SoTA를 수립했습니다.
- **주요 태스크별 성과**:
    - **이미지-텍스트 검색 (COCO 1K Test Set)**: Oscar$_{L}$은 이미지 검색 R@1에서 57.5%를 달성하여 기존 SoTA$_{L}$인 51.7%를 크게 상회했습니다.
    - **이미지 캡셔닝 (COCO Karpathy Test Split)**: Oscar$_{L}$은 CIDEr 점수 140.0을 기록하며, 기존 VLP 방법인 AoANet (CIDEr 129.8)을 크게 능가했습니다.
    - **VQA (VQA v2.0 Test-std)**: Oscar$_{L}$은 73.82%의 정확도를 달성하여 기존 SoTA$_{L}$인 UNITER (73.40%)보다 우수했습니다.
    - **NoCaps (Novel Object Captioning)**: Oscar는 COCO 훈련 데이터만 사용하여 사전 학습 없이도 Out-of-Domain 및 Near-Domain 시나리오에서 기존 SoTA 모델인 UpDown을 크게 앞섰으며, 강력한 일반화 능력을 보였습니다.
- **효율성**: 객체 태그를 사용하는 Oscar는 태그가 없는 VLP 방식보다 훨씬 빠르고 안정적으로 수렴하는 학습 곡선을 보였습니다.

## 🧠 Insights & Discussion
- **객체 태그의 핵심 역할**: 객체 태그는 교차 모달 특징 학습에서 "앵커 포인트" 역할을 하여 이미지 영역과 텍스트 사이의 모호한 정렬 문제를 해소하고 의미론적 정렬 학습을 크게 용이하게 합니다. 이는 모델이 각 모달리티에서 모호하게 표현될 수 있는 객체들을 언어 공간의 명확한 엔티티로 "접지(grounding)"하는 데 도움을 줍니다.
- **의미론적 공간의 개선**: t-SNE 시각화 결과는 객체 태그의 도입이 동일한 객체 클래스 내의 모달리티 간 거리를 줄이고, 관련 의미론을 가진 객체 클래스들을 더 가깝지만 여전히 구별 가능하도록 만듦으로써 의미론적 특징 공간을 개선함을 보여줍니다.
- **정성적 개선**: Oscar는 더 정확하고 다양한 객체 태그를 활용하여 기준 모델보다 이미지에 대한 더 상세한 설명을 생성합니다.
- **빠른 수렴 및 효율성**: 객체 태그를 사용한 파인튜닝은 태그가 없는 VLP 방식보다 훨씬 빠르고 더 나은 성능으로 수렴하여, Oscar가 더욱 실용적이고 효율적인 VLP 방안임을 입증합니다.
- **일반화 능력**: Novel Object Captioning(NoCaps) 태스크에서 보여지는 강력한 일반화 능력은 Oscar가 사전 학습된 언어 정보를 효과적으로 활용하여 새로운 객체에 대한 설명을 생성할 수 있음을 시사합니다.
- **한계점 및 향후 연구**: NLVR2 태스크의 파인튜닝 아키텍처는 아직 최적이 아니며, UNITER의 Pair-biattn과 같은 개선된 접근 방식을 통해 성능 향상의 여지가 있습니다. GQA와 같은 추론 태스크에는 향후 더 복잡한 추론 메커니즘을 통합할 수 있습니다.

## 📌 TL;DR
**문제**: 기존 비전-언어 사전 학습(VLP) 모델은 모호한 시각적 영역과 명시적인 정렬 정보 부족으로 인해 이미지와 텍스트 간의 교차 모달 의미론적 정렬을 학습하는 데 어려움을 겪습니다.

**제안 방법**: Oscar는 이미지에서 탐지된 객체 태그를 "앵커 포인트"로 활용하여 이 문제를 해결합니다. 모델은 텍스트의 단어, 이미지의 객체 태그, 이미지 영역 특징으로 구성된 세 개의 요소($w, q, v$)를 입력으로 받습니다. 사전 학습은 단어와 태그의 언어적 의미론을 학습하는 마스크드 토큰 손실($L_{MTL}$)과 이미지와 텍스트 모달리티 간의 정렬을 학습하는 대조 손실($L_C$)을 결합하여 수행됩니다.

**주요 결과**: Oscar는 6개 주요 비전-언어 이해 및 생성 태스크(이미지-텍스트 검색, 이미지 캡셔닝, NoCaps, VQA, NLVR2, GQA)에서 새로운 SoTA 성능을 달성했습니다. 객체 태그는 교차 모달 표현의 "접지(grounding)"를 통해 더 빠르고 안정적인 학습 수렴과 향상된 일반화 능력을 가져왔습니다.