{
  "url": "http://arxiv.org/abs/2202.03052v2",
  "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework",
  "authors": "Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang",
  "year": 2022,
  "abstract": "In this work, we pursue a unified paradigm for multimodal pretraining to\nbreak the scaffolds of complex task/modality-specific customization. We propose\nOFA, a Task-Agnostic and Modality-Agnostic framework that supports Task\nComprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,\nincluding image generation, visual grounding, image captioning, image\nclassification, language modeling, etc., in a simple sequence-to-sequence\nlearning framework. OFA follows the instruction-based learning in both\npretraining and finetuning stages, requiring no extra task-specific layers for\ndownstream tasks. In comparison with the recent state-of-the-art vision &\nlanguage models that rely on extremely large cross-modal datasets, OFA is\npretrained on only 20M publicly available image-text pairs. Despite its\nsimplicity and relatively small-scale training data, OFA achieves new SOTAs in\na series of cross-modal tasks while attaining highly competitive performances\non uni-modal tasks. Our further analysis indicates that OFA can also\neffectively transfer to unseen tasks and unseen domains. Our code and models\nare publicly available at https://github.com/OFA-Sys/OFA."
}