# ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee

## 🧩 Problem to Solve

시각-언어(Vision-and-Language, V&L) 태스크에서 시각적 접지(visual grounding)를 학습하기 위한 통합된 기반이 부족하다는 것이 주요 문제입니다. 기존 접근 방식은 시각 및 언어 모델을 개별적으로 사전 학습하고, 태스크 학습의 일부로 접지를 학습하여 데이터가 제한적이거나 편향될 경우 일반화 성능이 떨어지는 제한적인 접지를 생성했습니다. 이 논문은 광범위한 V&L 태스크에 활용할 수 있는, 태스크에 구애받지 않는 시각적 접지(visual grounding)를 위한 공통 모델을 개발하고자 합니다.

## ✨ Key Contributions

* **ViLBERT 모델 제안:** 이미지 콘텐츠와 자연어의 태스크에 구애받지 않는(task-agnostic) 공동 표현을 학습하기 위한 다중 모달(multi-modal) 투 스트림(two-stream) 모델인 ViLBERT를 제시합니다.
* **새로운 공동 주의(Co-Attentional) 트랜스포머 레이어 도입:** 시각 및 텍스트 입력을 별도의 스트림으로 처리하면서 공동 주의 트랜스포머 레이어를 통해 상호 작용하도록 하여 각 모달리티의 고유한 처리 요구 사항을 수용합니다.
* **프록시 태스크를 통한 사전 학습:** 대규모 Conceptual Captions 데이터셋에서 마스킹된 다중 모달 모델링(masked multi-modal modeling) 및 다중 모달 정렬 예측(multi-modal alignment prediction)이라는 두 가지 프록시(proxy) 태스크를 통해 모델을 사전 학습합니다.
* **다양한 V&L 태스크에서 SOTA 달성:** 사전 학습된 모델을 시각 질의 응답(VQA), 시각 상식 추론(VCR), 참조 표현(Referring Expressions), 캡션 기반 이미지 검색(Caption-Based Image Retrieval) 등 4가지 주요 V&L 태스크에 적용하여 기존의 태스크별 SOTA 모델보다 뛰어난 성능을 달성합니다.
* **전이 학습 능력 입증:** 시각적 접지를 태스크 훈련의 일부로만 배우는 것이 아니라, 사전 학습 가능하고 전이 가능한(transferable) 능력으로 다룸으로써 V&L 연구의 패러다임 전환을 보여줍니다.

## 📎 Related Works

* **자기 지도 학습(Self-Supervised Learning):** ELMo [13], BERT [12], GPT [14]와 같은 언어 모델과 이미지 색상화 [20], 마스킹된 텍스트 재구성 [12] 등 비전 분야의 발전 [21–23]을 언급합니다.
* **BERT (Bidirectional Encoder Representations from Transformers):** 논문은 BERT 아키텍처를 다중 모달로 확장합니다 [12].
* **시각-언어(Vision-and-Language) 태스크:** VQA [3], VCR [25], Referring Expressions [2], 이미지 캡셔닝 [5] 등 다양한 V&L 태스크 및 관련 연구를 참조합니다.
* **공동 주의 메커니즘(Co-attention mechanisms):** [31]에서 처음 제안되었고, 동시 연구 [32, 33]에서 VQA 태스크에 대한 유사한 공동 주의 트랜스포머 구조의 효과를 보여주었습니다.
* **이미지 표현 추출:** Visual Genome [16]으로 사전 학습된 Faster R-CNN [31] (ResNet-101 [11] 백본)을 사용합니다.
* **동시 연구:** 언어와 비디오 시퀀스를 모델링하는 단일 스트림 BERT 아키텍처인 VideoBERT [29]를 언급합니다.

## 🛠️ Methodology

1. **ViLBERT 아키텍처:**
    * **투 스트림 모델:** 이미지 영역($v_1, ..., v_{T_v}$)과 텍스트 입력($w_0, ..., w_{T_w}$)을 처리하는 두 개의 병렬 BERT 스타일 스트림으로 구성됩니다.
    * **공동 주의 트랜스포머 레이어 (Co-TRM):** 표준 트랜스포머 블록과 유사하게 쿼리(Q), 키(K), 값(V) 행렬을 계산하지만, 각 모달리티의 키와 값이 다른 모달리티의 멀티 헤드 어텐션 블록의 입력으로 전달됩니다. 이를 통해 한 모달리티가 다른 모달리티에 조건화된 어텐션 풀링 특징을 생성하여 모달 간 정보 교환을 가능하게 합니다.
    * **시각 스트림:**
        * 사전 학습된 Faster R-CNN (Visual Genome에 학습된 ResNet-101 백본)에서 바운딩 박스와 시각적 특징을 추출합니다.
        * 이미지 영역($v_i$)은 평균 풀링된 컨볼루션 특징과 5차원 공간 위치 인코딩 (정규화된 좌상단/우하단 좌표 및 이미지 면적 비율)의 합으로 구성됩니다.
        * 전체 이미지를 나타내는 특수 `[IMG]` 토큰을 사용합니다.
    * **언어 스트림:**
        * BERT `BASE` 모델 [12]을 기반으로 BookCorpus [17] 및 English Wikipedia [18]로 사전 학습된 언어 모델로 초기화됩니다.
        * 입력 표현은 토큰별 학습된 임베딩, 위치 인코딩, 세그먼트 인코딩의 합입니다.
2. **사전 학습 태스크 (Conceptual Captions 데이터셋, 약 310만 쌍):**
    * **마스킹된 다중 모달 모델링 ($L_{MM}$):**
        * 단어와 이미지 영역 입력의 약 15%를 마스킹합니다.
        * **단어 마스킹:** BERT와 동일하게 처리 (80% `[MASK]`, 10% 랜덤 단어, 10% 원본 유지).
        * **이미지 영역 마스킹:** 이미지 특징을 90% 확률로 0으로 만들고 10% 확률로 원본 유지합니다. 모델은 마스킹된 영역의 특징 값을 직접 회귀하는 대신, 해당 이미지 영역의 의미 클래스 분포를 예측합니다 (KL 발산 손실 사용, 특징 추출에 사용된 탐지 모델의 출력 분포를 타겟으로 함).
    * **다중 모달 정렬 예측 ($L_{MA}$):**
        * 이미지-텍스트 쌍이 정렬되었는지 (즉, 텍스트가 이미지를 설명하는지) 예측합니다.
        * `[IMG]` 토큰의 최종 표현($h_{IMG}$)과 `[CLS]` 토큰의 최종 표현($h_{CLS}$)의 요소별 곱(element-wise product)을 사용하여 전체 표현을 얻고, 이 위에 선형 레이어를 학습하여 이진 예측을 수행합니다.
        * 부정 샘플은 이미지 또는 캡션을 무작위로 다른 샘플로 교체하여 생성합니다.
3. **전이 학습 및 미세 조정:**
    * 사전 학습된 ViLBERT는 최소한의 변경 (일반적으로 분류 레이어 추가)으로 V&L 태스크에 미세 조정됩니다.
    * **평가 태스크:** VQA 2.0, VCR (Q→A, QA→R), RefCOCO+, Flickr30k 캡션 기반 이미지 검색.
    * **'제로 샷' 캡션 기반 이미지 검색:** 미세 조정 없이 사전 학습된 다중 모달 정렬 예측 메커니즘을 직접 적용하여 모델의 일반화 능력을 평가합니다.

## 📊 Results

* **아키텍처 우수성:** ViLBERT의 투 스트림 아키텍처는 단일 스트림 베이스라인보다 모든 태스크에서 성능이 향상되었으며, 특히 VQA와 RefCOCO+에서 상당한 개선을 보였습니다.
* **사전 학습의 효과:** 제안된 프록시 태스크를 통한 사전 학습은 모든 태스크에서 2%에서 13%의 성능 향상을 가져왔으며, 이는 학습된 시각-언어 표현의 효과를 입증합니다.
* **새로운 SOTA 달성:** ViLBERT는 실험에 사용된 VQA, VCR, Referring Expressions, 캡션 기반 이미지 검색의 4가지 V&L 태스크 모두에서 기존 SOTA 모델을 능가하는 성능을 달성했습니다. 특히 VCR, RefCOCO+, 이미지 검색에서는 7~10% 포인트의 상당한 개선을 보였습니다.
* **시각 스트림 깊이의 영향:** VQA와 이미지 검색은 더 깊은 시각 스트림(최대 6개 Co-TRM 레이어)에서 이점을 얻는 반면, VCR과 RefCOCO+는 얕은 모델에서 더 좋은 성능을 보였습니다. 제로 샷 이미지 검색은 깊이가 증가함에 따라 꾸준히 성능이 향상되었습니다.
* **대규모 훈련 데이터의 이점:** 사전 학습 데이터셋(Conceptual Captions)의 크기가 증가함에 따라 성능이 단조적으로 증가하여, 더 많은 사전 학습 데이터가 ViLBERT의 성능을 더욱 향상시킬 수 있음을 시사합니다.
* **제로 샷 성능:** 미세 조정 없이도 Flickr30k 데이터셋에서 상당한 제로 샷 이미지 검색 성능(R1 31.86)을 보여, 사전 학습이 시각과 언어 사이의 의미론적으로 유의미한 정렬 능력을 성공적으로 학습했음을 입증합니다.

## 🧠 Insights & Discussion

* **전이 가능한 시각적 접지:** 이 연구는 시각적 접지가 태스크별 학습 대신 사전 학습을 통해 효과적으로 학습되고 다양한 V&L 태스크로 전이될 수 있는 능력임을 성공적으로 입증했습니다. 이는 V&L 분야의 중요한 진전을 나타냅니다.
* **투 스트림 아키텍처의 효율성:** 공동 주의 트랜스포머 블록을 포함한 투 스트림 아키텍처는 각 모달리티의 고유한 처리 요구 사항을 수용하고 유연한 모달 간 상호 작용을 가능하게 하여, 단일 스트림 통합 모델보다 우수한 성능을 보였습니다.
* **광범위한 적용 가능성:** ViLBERT는 다양한 V&L 태스크에서 최소한의 아키텍처 변경(대부분 분류기 추가)만으로 SOTA 성능을 달성하여, 강력하고 일반화 가능한 시각-언어 표현을 제공하는 기반 모델로서의 잠재력을 보여줍니다.
* **향후 연구 방향:** 대화, 신체화된 태스크, 비디오 처리와 같이 긴 시퀀스의 이미지와 텍스트를 포함하는 태스크로의 확장, 그리고 멀티태스크 학습이 흥미로운 미래 연구로 제시됩니다. 또한, 현재의 양방향 모델에서 빔 탐색(beam-search)과 같은 기존 디코더가 적용되기 어려운 텍스트 생성 태스크에 대한 효과적인 디코딩 방법도 탐색할 필요가 있습니다.

## 📌 TL;DR

* **문제:** 기존 시각-언어(V&L) 모델은 시각적 접지를 태스크별로 학습하여 일반화 능력이 부족했습니다.
* **방법:** BERT를 확장한 ViLBERT는 이미지와 텍스트를 위한 별도의 두 스트림과 모달리티 간 상호 작용을 위한 새로운 공동 주의 트랜스포머 레이어를 사용합니다. 이는 Conceptual Captions 데이터셋에서 마스킹된 다중 모달 모델링과 다중 모달 정렬 예측이라는 두 가지 프록시 태스크를 통해 사전 학습됩니다.
* **결과:** ViLBERT는 VQA, VCR, Referring Expressions, 이미지 검색 등 네 가지 주요 V&L 태스크에서 간단한 미세 조정만으로 SOTA 성능을 달성했습니다. 이는 시각적 접지가 사전 학습을 통해 효과적으로 학습되고 태스크에 구애받지 않는 전이 가능한 능력임을 증명합니다.
