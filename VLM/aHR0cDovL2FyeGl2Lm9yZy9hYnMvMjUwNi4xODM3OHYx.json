{
  "title": "Taming Vision-Language Models for Medical Image Analysis: A\n  Comprehensive Review",
  "authors": "Haoneng Lin, Cheng Xu, Jing Qin",
  "year": 2025,
  "url": "http://arxiv.org/abs/2506.18378v1",
  "abstract": "Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in\ncross-modal semantic understanding between visual and textual modalities. Given\nthe intrinsic need for multi-modal integration in clinical applications, VLMs\nhave emerged as a promising solution for a wide range of medical image analysis\ntasks. However, adapting general-purpose VLMs to medical domain poses numerous\nchallenges, such as large domain gaps, complicated pathological variations, and\ndiversity and uniqueness of different tasks. The central purpose of this review\nis to systematically summarize recent advances in adapting VLMs for medical\nimage analysis, analyzing current challenges, and recommending promising yet\nurgent directions for further investigations. We begin by introducing core\nlearning strategies for medical VLMs, including pretraining, fine-tuning, and\nprompt learning. We then categorize five major VLM adaptation strategies for\nmedical image analysis. These strategies are further analyzed across eleven\nmedical imaging tasks to illustrate their current practical implementations.\nFurthermore, we analyze key challenges that impede the effective adaptation of\nVLMs to clinical applications and discuss potential directions for future\nresearch. We also provide an open-access repository of related literature to\nfacilitate further research, available at\nhttps://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this\narticle can help researchers who are interested in harnessing VLMs in medical\nimage analysis tasks have a better understanding on their capabilities and\nlimitations, as well as current technical barriers, to promote their\ninnovative, robust, and safe application in clinical practice."
}