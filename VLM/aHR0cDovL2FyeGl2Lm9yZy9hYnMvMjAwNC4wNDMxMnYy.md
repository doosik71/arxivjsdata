# Learning to Scale Multilingual Representations for Vision-Language Tasks
Andrea Burns, Donghyun Kim, Derry Wijaya, Kate Saenko, and Bryan A. Plummer

## 🧩 Problem to Solve
기존의 다국어 시각-언어(Vision-Language) 모델들은 지원하는 언어의 수에 따라 언어별로 많은 추가 파라미터를 요구하거나, 파라미터 수가 적을 경우 언어를 추가함에 따라 성능 저하를 겪는 문제가 있습니다. 이는 모델의 확장성(scalability)과 하위 작업(downstream task) 성능 사이의 트레이드오프를 야기하며, 특히 많은 수의 언어를 지원해야 할 때 심화됩니다.

## ✨ Key Contributions
*   **확장 가능한 다국어 정렬 언어 표현(Scalable Multilingual Aligned Language Representation, SMALR) 제안:** 적은 모델 파라미터로 많은 언어를 지원하면서도 하위 작업 성능을 희생하지 않는 모델입니다.
*   **하이브리드 임베딩 모델(Hybrid Embedding Model, HEM) 도입:** 대부분의 단어에 대해 고정 크기의 언어 불가지론적(language-agnostic) 표현을 학습하고, 소수의 단어에 대해서만 언어별(language-specific) 특성을 유지하여 효율적인 언어 임베딩을 가능하게 합니다.
*   **마스크 교차 언어 모델링(Masked Cross-Language Modeling, MCLM) 손실 함수 개발:** 다른 언어의 문맥을 활용하여 마스크된 단어의 특징을 예측함으로써 언어 간 특징 정렬을 강화합니다. 이는 데이터 증강 기법으로도 활용됩니다.
*   **교차 언어 일관성(Cross-Lingual Consistency, CLC) 모듈 제안:** 쿼리와 해당 기계 번역(machine translation)에 대한 예측이 서로 비교 가능하도록 하여 다국어 지원 시 예측을 집계하고 성능을 향상시킵니다.
*   **10개 언어에 대한 최고 성능 달성:** 다국어 이미지-문장 검색(image-sentence retrieval) 작업에서 기존 모델들을 3-4% 능가하며, 훈련 파라미터는 기존 단어 임베딩 방식 대비 1/5 미만으로 크게 줄였습니다.

## 📎 Related Works
*   **BERT 및 M-BERT [13, 21]:** Transformer 기반의 강력한 언어 표현 모델이지만, 대규모 파라미터 수(예: M-BERT Base 1.1억 개)와 언어 간 전이(transfer)의 불균형 문제가 존재합니다. 다국어 설정에서 성능 저하를 상쇄하기 위해 더 많은 파라미터가 필요합니다.
*   **LIWE (Language-Independent Word Embeddings) [41]:** 경량 문자 임베딩 모델로 적은 파라미터로 많은 언어를 표현할 수 있지만, 여러 언어로 공동 학습 시 성능 저하가 발생합니다.
*   **MULE (Multimodal Universal Language Embedding) [23]:** 범용 언어 임베딩을 학습하여 멀티모달 모델에서 단일 언어 브랜치를 사용합니다. 교차 언어 제약 조건과 적대적 언어 분류기를 활용하여 언어 정렬을 강화하지만, 단어 수준 임베딩으로 인해 언어 수가 늘어날수록 상당한 파라미터가 필요합니다.
*   **Gella et al. [16]:** 이미지와 언어별 표현을 연결하고, 의미적으로 유사한 문장들을 언어에 관계없이 가깝게 임베딩하는 모델을 학습합니다.

## 🛠️ Methodology
SMALR은 확장성과 성능 간의 트레이드오프를 해결하기 위해 다음과 같은 모듈들을 결합합니다:

1.  **하이브리드 임베딩 모델 (Hybrid Embedding Model, HEM)**
    *   **목표:** 저빈도 단어는 언어 불가지론적 공유 잠재 어휘(shared latent vocabulary)로, 고빈도 단어(각 언어에서 상위 $K$개)는 언어별 고유 표현을 갖도록 합니다. 본 연구에서는 $K=5000$을 사용합니다.
    *   **초기 표현:** 300차원의 단일 언어 FastText 임베딩 [11]을 PCA [30]로 50차원으로 축소한 후, 완전 연결 계층(Fully Connected Layer)을 통해 512차원의 범용 임베딩 공간으로 투영합니다.
    *   **잠재 어휘 학습 (사전 훈련):**
        *   FastText 임베딩을 완전 연결 계층에 입력하여 문장 표현을 얻습니다.
        *   의미적으로 유사한 문장들을 가깝게 임베딩하도록 삼중항 손실(triplet loss) $L_{triplet}$ [Eq. (1)]을 사용하여 잠재 어휘의 초기 표현과 단어를 잠재 어휘에 할당하는 방법을 학습합니다.
        *   $$L_{triplet}(x, y^+, y^-) = \max(0, m + d(x, y^+) - d(x, y^-))$$
        *   여기서 $d(x, y)$는 거리 함수(코사인 거리 사용), $m$은 스칼라 파라미터($0.05$ 설정)입니다.
        *   **하드 어텐션 메커니즘:** 각 어휘 단어를 단일 언어 불가지론적 토큰에 매핑합니다.
        *   **탐색 파라미터:** 사전 훈련 시, 가장 좋은 점수를 얻는 토큰에 결정론적으로 매핑하는 대신, 상위 $M$개 토큰 중 확률 $p$로 무작위 선택하여 견고성을 높입니다 (본 연구에서는 $p=0.2, M=20$).

2.  **마스크 교차 언어 모델링 (Masked Cross-Language Modeling, MCLM)**
    *   **목표:** 다국어 시나리오에서 언어 간 정렬을 강화합니다.
    *   **과정:**
        *   동일한 이미지를 설명하는 의미적으로 유사한 두 언어($i, j$)의 문장 ($S_i, S_j$)을 쌍으로 얻습니다.
        *   두 문장의 일부 단어를 무작위로 MASK 토큰으로 대체하여 마스크된 표현 ($S^m_i, S^m_j$)을 만듭니다.
        *   이들을 연결하여 언어 쌍에 공유되는 완전 연결 계층에 입력하고, 두 문장의 마스크된 정보를 예측하여 재구성된 문장 ($S'_i, S'_j$)을 얻습니다.
        *   **MCLM 손실:** 마스크된 재구성 문장과 원본 마스크 해제 문장을 비교합니다.
        *   $$L_{mask} = ||\ell_2(S^m_i + S'_i) - \ell_2(S_i)|| + ||\ell_2(S^m_j + S'_j) - \ell_2(S_j)||$$
        *   $\ell_2$는 단위 노름(unit norm)을 갖도록 강제된 벡터를 나타냅니다.

3.  **다국어 시각-의미 정렬 (Multilingual Visual-Semantic Alignment)**
    *   MULE [23]에서 사용된 정렬 제약 조건을 통합합니다.
    *   **이웃 제약 조건 ($L_{nc}$):** HEM 출력과 최종 언어 표현 모두에 삼중항 손실을 적용하여 유사한 문장이 가깝게 임베딩되도록 합니다.
    *   **적대적 언어 분류기 ($L_{adv}$):** 범용 임베딩에서 다른 언어들의 특징 분포가 유사하도록 보장합니다.
    *   **멀티모달 손실 ($L_{mm}$):** 이미지와 문장을 가깝게 임베딩하는 양방향 삼중항 손실 [Eq. (3)]을 사용합니다. MCLM 모듈의 마스크된 문장도 $L_{mm}$ 계산에 사용됩니다.
    *   **총 손실 함수:** $L_{SMALR} = L_{mm} + \lambda_2 L_{mask} + \lambda_3 L_{adv} + \lambda_4 L_{nc}$

4.  **교차 언어 일관성 (Cross-Lingual Consistency, CLC) 모듈**
    *   **목표:** 추론 시(test time) 다국어 정보를 활용하여 성능을 향상시킵니다.
    *   **과정:** 질의(query)가 주어지면, 이를 모델이 지원하는 다른 모든 언어로 기계 번역합니다.
    *   **CLC-A (평균 앙상블):** 모든 언어의 매칭 점수를 단순히 평균합니다. 추가 파라미터가 필요 없습니다.
    *   **CLC-C (가중 앙상블):** 소규모 다층 퍼셉트론(MLP)을 사용하여 각 언어의 점수를 집계합니다. 이는 각 언어 예측의 상대적 정보를 고려하며, 352개의 학습 가능한 파라미터만을 사용합니다.

## 📊 Results
*   **성능 우위:** 다국어 이미지-문장 검색 작업에서 10개 언어(영어, 독일어, 프랑스어, 체코어, 중국어, 일본어, 아랍어, 아프리칸스어, 한국어, 러시아어)에 대해 SMALR이 기존 최첨단 모델(S-LIWE, MULE)보다 평균 3-4% 높은 성능을 달성했습니다.
    *   MSCOCO 데이터셋에서 S-LIWE 대비 11점, Multi30K에서 5.8점 성능 향상.
    *   MULE 대비 1/5 미만의 훈련 파라미터로 더 나은 성능을 보였습니다.
*   **HEM의 효과:** 언어 불가지론적 어휘만을 사용하는 LA(Language-Agnostic) 베이스라인보다 HEM이 MSCOCO에서 평균 3.4 mR, Multi30K에서 2.4 mR 성능을 지속적으로 향상시켰습니다.
*   **MCLM의 기여:** MCLM 손실이 추가된 SMALR은 거의 모든 언어에서 mR 성능을 향상시켜, LIWE와 같이 압축된 모델의 언어 추가 시 성능 저하 문제를 완화했습니다.
*   **CLC의 효과:** CLC-A는 추가 파라미터 없이도 평균 성능을 1-3점 향상시켰습니다. CLC-C는 CLC-A보다 MSCOCO에서 0.9점, Multi30K에서 0.5점 더 향상시키며 미미한 파라미터 추가로 효과를 입증했습니다.
*   **파라미터 효율성:** SMALR은 학습 시 2천만 개 미만의 파라미터를 사용하여, 대규모 파라미터를 사용하는 MULE 모델의 1/5 수준으로 모델 크기를 대폭 줄였습니다. 추론 시에는 이미지-문장 매칭 모델과 CLC 모듈만을 사용하여 총 7.1M 파라미터로 S-LIWE보다도 적은 파라미터를 사용합니다.

## 🧠 Insights & Discussion
*   **확장성 및 성능 균형:** SMALR은 다국어 모델 크기와 시각-언어 하위 작업 성능 간의 고질적인 트레이드오프를 성공적으로 해결했습니다. 이는 특히 적은 자원(GPU 메모리 등)으로 다수의 언어를 처리해야 하는 실제 적용 시나리오에서 큰 이점을 제공합니다.
*   **모듈식 설계:** SMALR의 구성 요소(HEM, MCLM, CLC)는 모듈식으로 설계되어 다른 시각-언어 방법론이나 작업에도 쉽게 통합될 수 있습니다.
*   **효율적인 언어 표현의 중요성:** HEM을 통해 고빈도 단어와 저빈도 단어를 분리하여 처리하는 방식은 전체 어휘 공간을 크게 줄이면서도 성능 손실을 최소화하는 효과적인 전략임을 보여주었습니다.
*   **교차 언어 문맥 활용:** MCLM은 단순히 마스킹을 통한 단일 언어 문맥 학습을 넘어, 다른 언어의 문맥 정보를 활용하여 언어 표현의 정렬을 강화한다는 새로운 접근 방식을 제시합니다. 이는 시각-언어 데이터셋의 특성상 직접적인 번역 쌍이 아닌 의미적으로 관련된 문장 쌍으로도 효과를 볼 수 있음을 시사합니다.
*   **추론 시 다국어 정보 활용:** CLC 모듈은 학습 단계뿐만 아니라 추론 단계에서도 다국어 정보를 적극적으로 활용하여 예측의 정확성을 높이는 방법을 제시합니다. 이는 다양한 언어의 문장이 상호보완적인 정보를 제공할 수 있다는 통찰력을 기반으로 합니다.
*   **제한 사항:** 본 연구에서는 주로 ResNet-152 특징을 사용했으며, LIWE와 같은 일부 비교 대상 모델은 다른 시각 특징(Faster R-CNN)을 사용한 점이 직접적인 비교에 미묘한 영향을 미칠 수 있습니다. 또한, 기계 번역된 데이터의 품질이 모델 성능에 영향을 미칠 수 있습니다.

## 📌 TL;DR
SMALR은 다국어 시각-언어 모델의 확장성과 성능 트레이드오프를 해결하기 위해 제안된 모델입니다. HEM(하이브리드 임베딩)으로 효율적인 언어 표현을 학습하고, MCLM(마스크 교차 언어 모델링)으로 언어 간 정렬을 강화하며, CLC(교차 언어 일관성) 모듈로 추론 시 다국어 정보를 활용합니다. 그 결과, SMALR은 10개 언어에 대한 다국어 이미지-문장 검색 작업에서 기존 모델들보다 3-4% 높은 성능을 달성했으며, 파라미터 수는 1/5 미만으로 크게 줄였습니다.