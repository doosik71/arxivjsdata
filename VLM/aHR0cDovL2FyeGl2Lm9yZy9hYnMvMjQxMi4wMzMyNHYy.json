{
  "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for\n  Accelerating Large VLMs",
  "authors": "Wangbo Zhao, Yizeng Han, Jiasheng Tang, Zhikai Li, Yibing Song, Kai Wang, Zhangyang Wang, Yang You",
  "year": 2024,
  "url": "http://arxiv.org/abs/2412.03324v2",
  "abstract": "Vision-language models (VLMs) have shown remarkable success across various\nmulti-modal tasks, yet large VLMs encounter significant efficiency challenges\ndue to processing numerous visual tokens. A promising approach to accelerating\nlarge VLM inference is using partial information, such as attention maps from\nspecific layers, to assess token importance and prune less essential tokens.\nHowever, our study reveals three key insights: (i) Partial attention\ninformation is insufficient for accurately identifying critical visual tokens,\nresulting in suboptimal performance, especially at low token retention ratios;\n(ii) Global attention information, such as the attention map aggregated across\nall layers, more effectively preserves essential tokens and maintains\ncomparable performance under aggressive pruning. However, the attention maps\nfrom all layers requires a full inference pass, which increases computational\nload and is therefore impractical in existing methods; and (iii) The global\nattention map aggregated from a small VLM closely resembles that of a large\nVLM, suggesting an efficient alternative. Based on these findings, we introduce\na \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM\n\\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs\n(\\textbf{SGL}). Specifically, we employ the attention map aggregated from a\nsmall VLM to guide visual token pruning in a large VLM. Additionally, an early\nexiting mechanism is developed to fully use the small VLM's predictions,\ndynamically invoking the larger VLM only when necessary, yielding a superior\ntrade-off between accuracy and computation. Extensive evaluations across 11\nbenchmarks demonstrate the effectiveness and generalizability of SGL, achieving\nup to 91\\% pruning ratio for visual tokens while retaining competitive\nperformance."
}