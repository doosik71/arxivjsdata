# A Survey on Multimodal Large Language Models

Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen

## 🧩 Problem to Solve

최근 대규모 언어 모델(LLM)은 놀라운 추론 능력을 보여주지만, 본질적으로 시각 정보를 이해하지 못하는 "맹인"입니다. 반면, 대규모 비전 모델(LVM)은 시각 정보를 잘 처리하지만 추론 능력이 부족합니다. 이러한 상호 보완성을 해결하고 LLM의 강력한 추론 능력을 다양한 양식(모달리티)으로 확장하여, 이미지 기반 스토리 생성이나 OCR 없는 수학 추론과 같은 기존 멀티모달 방법에서는 드물었던 새로운 능력을 가진 모델을 개발하는 것이 핵심 문제입니다. 본 논문은 급속도로 발전하는 멀티모달 대규모 언어 모델(MLLM) 분야의 최신 진행 상황을 체계적으로 추적하고 요약하는 것을 목표로 합니다.

## ✨ Key Contributions

- **MLLM의 기본 개념 및 구성 요소 정의:** MLLM의 기본 아키텍처, 훈련 전략 및 데이터, 평가 방법을 체계적으로 제시합니다.
- **MLLM 확장 연구 주제 소개:** 세분성, 모달리티, 언어, 시나리오 측면에서 MLLM의 확장 가능성에 대한 연구 동향을 소개합니다.
- **멀티모달 환각(Hallucination) 문제 분석 및 완화 방법 제시:** MLLM의 주요 문제점인 환각 현상을 유형별로 분류하고 이를 평가하며 완화하는 다양한 접근 방식을 설명합니다.
- **주요 확장 기술 논의:** 멀티모달 문맥 내 학습(M-ICL), 멀티모달 사고의 사슬(M-CoT), LLM 기반 시각 추론(LAVR)과 같은 핵심 기술들을 심층적으로 다룹니다.
- **기존 과제 및 유망한 연구 방향 제시:** MLLM 분야의 현재 한계점을 파악하고 향후 연구에 대한 통찰력을 제공합니다.
- **최초의 MLLM 종합 설문조사:** MLLM 분야의 방대한 문헌을 정리하고 최신 동향을 제공하는 최초의 포괄적인 설문조사 논문입니다.

## 📎 Related Works

- **LLM의 발전:** GPT-3 [7], ChatGPT [2], GPT-4 [3], LLaMA [5], Vicuna [4], Flan-T5 [56], Qwen [58] 등 데이터 및 모델 크기 확장을 통해 뛰어난 능력을 보여준 LLM들의 발전에 기반을 둡니다.
- **LVM의 발전:** Segment Anything [9], DINO [11] 등 시각 인지에서 뛰어난 성능을 보이는 대규모 비전 모델들이 언급됩니다.
- **기존 멀티모달 연구:**
  - **판별(Discriminative) 패러다임:** CLIP [13], UNITER [15] 등 시각 및 텍스트 정보를 통합된 표현 공간에 투사하여 다운스트림 작업의 기반을 마련한 모델들이 있습니다.
  - **생성(Generative) 패러다임:** OFA [16], SimVLM [18] 등 멀티모달 작업을 sequence-to-sequence 방식으로 통합한 모델들이 있습니다.
- **주요 MLLM 모델:** BLIP-2 [59], LLaVA [20], MiniGPT-4 [21], InstructBLIP [60], Qwen-VL [34], CogVLM [75], NExT-GPT [32] 등 다양한 MLLM 모델들이 아키텍처, 훈련, 응용 측면에서 참조됩니다.
- **강화 학습 기반 정렬:** InstructGPT [95]의 RLHF(Reinforcement Learning with Human Feedback) 기술은 MLLM의 인간 선호도 정렬에 활용됩니다.

## 🛠️ Methodology

본 논문은 MLLM의 기본 구성부터 고급 기술 및 미래 방향까지 포괄적으로 분석하는 설문조사 방법론을 따릅니다.

### MLLM 아키텍처

MLLM은 크게 세 가지 모듈로 추상화됩니다.

1. **사전 학습된 모달리티 인코더 (Modality Encoder):** 이미지 (예: CLIP, EVA-CLIP, ConvNext-L), 오디오 (예: CLAP), 비디오 등 원시 정보를 압축하여 LLM이 이해할 수 있는 특징으로 변환합니다. 고해상도 이미지 처리 기법 (직접 스케일링, 패치 분할)도 논의됩니다.
2. **사전 학습된 LLM (Pre-trained LLM):** Flan-T5, LLaMA, Vicuna, Qwen과 같은 대규모 언어 모델이 "두뇌" 역할을 하며, 풍부한 세계 지식과 강력한 추론 능력을 제공합니다. 매개변수 스케일링 (7B에서 70B 이상)이나 MoE (Mixture of Experts) 아키텍처가 성능 향상에 기여합니다.
3. **모달리티 인터페이스 (Modality Interface):** LLM이 텍스트만 인식할 수 있으므로, 다른 모달리티와 언어 간의 격차를 연결하는 역할을 합니다.
   - **학습 가능한 커넥터 (Learnable Connector):** 인코더의 출력을 LLM이 이해할 수 있는 토큰으로 변환합니다.
     - **토큰 수준 융합 (Token-level fusion):** Q-Former [59]와 같이 학습 가능한 쿼리 토큰을 사용하거나, MLP 기반 인터페이스 [20]를 사용하여 시각 토큰을 텍스트 토큰과 연결합니다. 시각 토큰 수와 입력 해상도가 중요하게 작용합니다.
     - **특징 수준 융합 (Feature-level fusion):** Flamingo [74], CogVLM [75]처럼 LLM의 Transformer 레이어 내에 추가 교차-어텐션 레이어를 삽입하여 텍스트 및 시각 특징 간의 깊은 상호작용을 가능하게 합니다.
   - **전문가 모델 (Expert Model):** 이미지 캡션 모델과 같은 외부 전문가 모델을 사용하여 멀티모달 입력을 텍스트로 변환하는 방식 [77]. 유연성이 떨어지고 정보 손실이 발생할 수 있습니다.

### 훈련 전략 및 데이터

MLLM은 일반적으로 세 단계의 훈련을 거칩니다.

1. **사전 학습 (Pre-training):**
   - **목표:** 다른 모달리티를 정렬하고 멀티모달 세계 지식을 학습합니다.
   - **데이터:** 대규모 텍스트-페어드 데이터 (캡션 데이터)가 사용됩니다.
     - **거친 입자 (Coarse-grained):** CC-3M [84], LAION-5B [87] 등 웹에서 수집된 대규모의 짧고 노이즈가 있는 캡션 데이터. CLIP [13] 등으로 필터링됩니다.
     - **미세 입자 (Fine-grained):** ShareGPT4V-PT [83], LVIS-Instruct4V [91] 등 강력한 MLLM (예: GPT-4V)을 통해 생성된 고품질의 길고 정확한 캡션 데이터.
   - **훈련:** 시각 인코더와 LLM을 고정하고 학습 가능한 인터페이스만 훈련하거나, 일부 모듈을 동결 해제하여 더 많은 매개변수를 학습합니다. 고해상도와 고품질 데이터 사용이 중요합니다.
2. **명령어 튜닝 (Instruction-tuning):**
   - **목표:** 사용자의 명령을 더 잘 이해하고 요구되는 작업을 수행하도록 모델을 가르칩니다. 제로샷 성능을 향상시킵니다.
   - **데이터 형식:** 선택적 명령과 입력-출력 쌍으로 구성된 멀티모달 명령어 샘플 (예: 이미지, 질문, 답변). 다중 턴 대화도 포함될 수 있습니다.
   - **데이터 수집:**
     - **데이터 적응 (Data Adaptation):** 기존 VQA, 캡션 데이터셋을 명령어 형식으로 변환하고, GPT의 도움을 받아 명령어를 생성하거나 확장합니다.
     - **자기 명령어 (Self-Instruction):** LLaVA [20]처럼 LLM (GPT-4 등)을 활용하여 소수의 수동 주석 샘플로부터 더 많은 명령어-따라하기 데이터를 생성합니다.
     - **데이터 혼합 (Data Mixture):** 언어 전용 대화 데이터와 멀티모달 데이터를 혼합하여 대화 능력 및 명령어 따르기 능력을 향상시킵니다.
   - **데이터 품질:** 명령어 다양성과 작업 범위(예: 시각 추론 작업)가 모델 성능에 중요합니다.
3. **정렬 튜닝 (Alignment tuning):**
   - **목표:** 모델이 특정 인간 선호도(예: 환각 현상 감소)에 맞게 정렬되도록 합니다.
   - **기술:**
     - **RLHF (Reinforcement Learning with Human Feedback) [110]:** 지도 학습 파인튜닝, 보상 모델 학습, 강화 학습의 세 단계를 통해 인간 피드백을 활용하여 모델을 최적화합니다. 예: LLaVA-RLHF [112].
     - **DPO (Direct Preference Optimization) [113]:** 명시적인 보상 모델 학습 없이 인간 선호도 라벨을 사용하여 이진 분류 손실로 학습 과정을 간소화합니다. 예: RLHF-V [114], Silkie [115].
   - **데이터:** LLaVA-RLHF [112], RLHF-V [114], VLFeedback [115]와 같이 모델 응답에 대한 인간 또는 AI(GPT-4V)의 피드백 데이터가 사용됩니다.

### 평가

MLLM 평가는 포괄성과 새로운 능력 평가에 중점을 둡니다. 질문 유형에 따라 두 가지로 분류됩니다.

1. **폐쇄형 (Closed-set):**
   - 미리 정의된 제한된 답변 옵션을 가진 질문입니다.
   - **방법:** ScienceQA [116]의 정확도, NoCaps [118] 및 Flickr30K [119]의 CIDEr 점수와 같은 벤치마크 지표를 사용합니다.
   - **설정:** 제로샷 또는 파인튜닝 설정에서 다양한 작업 및 도메인별 작업 (예: 의료 VQA)에 대해 평가합니다.
   - **벤치마크:** MME [123], MMBench [124], Video-ChatGPT [130] 등 MLLM을 위해 특별히 설계된 포괄적인 벤치마크가 있습니다. 환각 평가를 위한 POPE [132]도 사용됩니다.
2. **개방형 (Open-set):**
   - 유연한 응답이 가능한 질문입니다 (챗봇 역할).
   - **방법:**
     - **수동 채점:** mPLUG-Owl [81]처럼 인간이 특정 차원(예: 자연 이미지 이해)을 평가합니다.
     - **GPT 채점:** LLaVA [20]는 텍스트 전용 GPT-4를 사용하여 응답을 평가하고, 최신 연구에서는 GPT-4V [77]를 사용하여 시각 정보에 직접 접근하여 더 정확하게 평가합니다.
     - **사례 연구:** GPT-4V [135]와 Gemini-Pro [137]와 같은 상용 모델의 다양한 기능(예: 유머 이해, 자율 주행 시나리오)을 심층적으로 질적 분석합니다.

## 📊 Results

설문조사된 연구들에 따르면 MLLM은 다음과 같은 주요 결과를 보였습니다.

- **놀라운 새로운 능력:** GPT-4V로 대표되는 MLLM은 이미지 기반 스토리 작성, 밈의 깊은 의미 이해, OCR 없는 수학 추론과 같은 기존 멀티모달 방식에서는 보기 힘든 놀라운 능력을 보여줍니다.
- **아키텍처 스케일링의 이점:**
  - LLM 매개변수 크기 증가 (예: 7B에서 13B 또는 34B로)는 다양한 벤치마크에서 포괄적인 성능 향상을 가져오며, 영어 데이터로만 훈련해도 중국어와 같은 새로운 언어에 대한 제로샷 능력을 유도할 수 있습니다 [50].
  - 고해상도 입력 사용은 성능 향상에 크게 기여합니다 [34], [50].
  - MoE (Mixture of Experts) 아키텍처는 밀집 모델보다 더 나은 성능을 달성합니다 [52], [68].
  - 모달리티 어댑터의 유형보다는 시각 토큰 수와 입력 해상도가 토큰 수준 융합에 더 중요합니다 [52].
- **훈련 데이터의 중요성:**
  - 사전 학습 단계에서 고품질 캡션 데이터는 더 나은 정렬을 촉진하며, 시각 인코더의 동결 해제가 성능을 향상시킵니다 [83].
  - 명령어 튜닝에서 데이터의 양뿐만 아니라 **품질**이 중요하며 [108], **명령어의 다양성** [73] 및 **시각 추론 작업 포함** [109]이 모델 성능 및 일반화 능력 향상에 결정적입니다.
  - RLHF 및 DPO를 통한 정렬 튜닝은 모델이 인간 선호도에 더 잘 부합하고 환각 현상을 줄이는 데 효과적입니다 [112], [114], [115].
- **환각 현상 완화의 진전:** 사전 교정 (예: 부정적 명령어 포함 데이터 튜닝 [164]), 과정 내 교정 (예: 건축 설계 개선 [159], [165]), 사후 교정 (예: Woodpecker [77]의 외부 전문가 모델 활용) 등 다양한 방법론이 환각을 줄이는 데 효과를 보입니다.
- **확장 기술의 활용:**
  - **M-ICL (Multimodal In-Context Learning):** 몇 가지 예제를 통해 복잡하거나 새로운 시각 추론 작업을 제로샷/퓨샷 방식으로 해결하는 능력을 향상시킵니다 [22], [74]. 외부 도구 사용을 가르치는 데도 활용됩니다.
  - **M-CoT (Multimodal Chain of Thought):** LLM이 중간 추론 단계를 명시적으로 생성하도록 유도하여 복잡한 추론 작업을 효과적으로 해결합니다 [185], [116]. 단일 사슬 및 트리 형태의 추론 구조가 탐색됩니다.
  - **LLM-Aided Visual Reasoning (LAVR):** LLM을 컨트롤러, 의사 결정자, 의미 정제자 역할로 활용하여 외부 도구나 시각 기반 모델을 통합한 시각 추론 시스템을 구축하며, 강력한 일반화 능력과 상호작용성을 보여줍니다 [22], [169].

## 🧠 Insights & Discussion

MLLM의 급속한 발전에도 불구하고, 여전히 해결해야 할 많은 도전 과제와 유망한 연구 방향이 존재합니다.

### 통찰

- **LLM과 LVM의 시너지:** MLLM은 LLM의 강력한 추론 능력과 LVM의 시각 인지 능력을 결합하여, 기존 모델들이 달성하기 어려웠던 새로운 '창발적 능력(emergent capabilities)'을 보여주고 있습니다. 이는 인공 일반 지능(AGI)으로 가는 잠재적인 경로를 제시합니다.
- **데이터 품질의 결정적 중요성:** 스케일업과 더불어 고품질의 미세 입자 데이터, 특히 명령어 튜닝 데이터의 다양성과 복잡성은 MLLM 성능 향상에 핵심적인 역할을 합니다. GPT-4V와 같은 강력한 모델을 활용한 데이터 생성이 중요한 트렌드입니다.
- **모델 아키텍처 및 훈련 전략의 진화:** 모달리티 인터페이스 설계 (토큰/특징 수준 융합), MoE 아키텍처 도입, 그리고 사전 학습, 명령어 튜닝, 정렬 튜닝의 다단계 훈련 접근 방식은 MLLM의 다양한 능력을 발현하는 데 필수적입니다.
- **환각 현상의 근본적 문제:** MLLM이 생성하는 응답이 이미지 내용과 불일치하는 환각 현상은 여전히 중요한 도전 과제이며, 이를 완화하기 위한 다양한 사전/과정 내/사후 교정 기법들이 개발되고 있습니다. 이는 MLLM의 신뢰성을 높이는 데 중요합니다.
- **추론 능력 강화의 필요성:** M-ICL과 M-CoT는 LLM의 추론 능력을 멀티모달 영역으로 확장하는 유망한 기술이지만, 아직 초기 단계에 있으며 더 심층적인 메커니즘 탐구와 개선이 필요합니다.

### 한계 및 미래 연구 방향

- **장문 컨텍스트 처리 능력 제한:** 현재 MLLM은 긴 컨텍스트의 멀티모달 정보 (예: 긴 비디오, 이미지와 텍스트가 섞인 장문 문서)를 처리하는 데 한계가 있습니다. 이는 고급 모델 개발에 제약을 줍니다.
- **복잡한 명령어 추종 능력 향상:** 여전히 GPT-4V와 같은 폐쇄형 모델이 가장 발전된 명령어 추종 능력을 보여주며, 다른 모델들은 복잡한 명령어에 대해 어려움을 겪습니다. 모델이 더 미묘하고 복잡한 사용자 요구사항을 이해하고 수행할 수 있도록 개선해야 합니다.
- **M-ICL 및 M-CoT 기술 발전:** 멀티모달 문맥 내 학습 및 사고의 사슬 기술은 아직 초기 단계이며, 그 기저 메커니즘을 탐구하고 성능을 향상시키는 연구가 유망합니다.
- **MLLM 기반 현실 세계 에이전트 개발:** MLLM을 기반으로 현실 세계와 상호작용하는 구체화된(embodied) 에이전트를 개발하는 것은 매우 중요합니다. 이를 위해서는 인지, 추론, 계획, 실행과 같은 핵심 역량이 통합된 모델이 필요합니다. (예: GUI 에이전트, 로봇)
- **안전성 문제 해결:** LLM과 마찬가지로 MLLM도 조작된 공격(adversarial attacks)에 취약하여 편향되거나 바람직하지 않은 응답을 생성할 수 있습니다. 모델의 안전성과 견고성을 개선하는 것이 중요한 연구 주제입니다.

## 📌 TL;DR

이 설문조사 논문은 LLM의 언어 추론 능력과 LVM의 시각 인지 능력을 결합하여 새로운 멀티모달 능력을 보여주는 **멀티모달 대규모 언어 모델(MLLM)**의 최신 연구 동향을 종합적으로 정리합니다. 논문은 MLLM의 **아키텍처(인코더, LLM, 인터페이스)**, **훈련 전략(사전 학습, 명령어 튜닝, 정렬 튜닝)** 및 **평가 방법(폐쇄형/개방형)**을 체계적으로 분석합니다. 주요 내용으로는 이미지/비디오/오디오 등 다양한 모달리티 지원 확장, 세분성(영역/픽셀) 제어 개선, 다국어 지원, 의학/문서 이해 등 특정 시나리오 적용 연구가 있습니다. 특히, **멀티모달 환각(hallucination)** 문제를 존재, 속성, 관계 환각으로 분류하고 이를 완화하기 위한 다양한 사전/과정 내/사후 교정 방법을 제시합니다. 또한 **멀티모달 문맥 내 학습(M-ICL)**, **멀티모달 사고의 사슬(M-CoT)**, **LLM 기반 시각 추론(LAVR)**과 같은 핵심 기술들을 심층적으로 다루며, LLM이 컨트롤러, 의사 결정자, 의미 정제자로서의 역할을 수행할 수 있음을 보여줍니다. 마지막으로, 긴 컨텍스트 처리, 복잡한 명령어 추종, M-ICL/M-CoT 능력 강화, 구체화된 에이전트 개발, 안전성 문제 해결 등 MLLM 분야의 **현재 한계점과 미래 연구 방향**을 제시하여 이 신생 분야의 발전을 위한 청사진을 제공합니다.
