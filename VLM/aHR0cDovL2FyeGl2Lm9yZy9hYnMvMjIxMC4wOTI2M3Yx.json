{
  "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends",
  "authors": "Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao",
  "year": 2022,
  "url": "http://arxiv.org/abs/2210.09263v1",
  "abstract": "This paper surveys vision-language pre-training (VLP) methods for multimodal\nintelligence that have been developed in the last few years. We group these\napproaches into three categories: ($i$) VLP for image-text tasks, such as image\ncaptioning, image-text retrieval, visual question answering, and visual\ngrounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image\nclassification, object detection, and segmentation; and ($iii$) VLP for\nvideo-text tasks, such as video captioning, video-text retrieval, and video\nquestion answering. For each category, we present a comprehensive review of\nstate-of-the-art methods, and discuss the progress that has been made and\nchallenges still being faced, using specific systems and models as case\nstudies. In addition, for each category, we discuss advanced topics being\nactively explored in the research community, such as big foundation models,\nunified modeling, in-context few-shot learning, knowledge, robustness, and\ncomputer vision in the wild, to name a few."
}