{
  "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
  "authors": "Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, Li Yuan",
  "year": 2024,
  "url": "http://arxiv.org/abs/2411.10440v6",
  "abstract": "Large language models have demonstrated substantial advancements in reasoning\ncapabilities. However, current Vision-Language Models (VLMs) often struggle to\nperform systematic and structured reasoning, especially when handling complex\nvisual question-answering tasks. In this work, we introduce LLaVA-CoT, a large\nVLM designed to conduct autonomous multistage reasoning. Unlike\nchain-of-thought prompting, LLaVA-CoT independently engages in sequential\nstages of summarization, visual interpretation, logical reasoning, and\nconclusion generation. This structured approach enables LLaVA-CoT to achieve\nmarked improvements on reasoning-intensive tasks. To accomplish this, we\nconstruct the LLaVA-CoT-100k dataset, integrating samples from various visual\nquestion answering sources and providing structured reasoning annotations.\nBesides, we propose a test-time stage-wise retracing search method (SWIRES),\nwhich enables effective and efficient test-time scaling. Remarkably, with only\n100k training samples and test-time scaling, LLaVA-CoT not only outperforms its\nbase model by 9.4% on a wide range of multimodal reasoning benchmarks, but also\nsurpasses the performance of larger and even closed-source models, such as\nGemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct. The code,\ndataset, and pre-trained weights are publicly available at\nhttps://github.com/PKU-YuanGroup/LLaVA-CoT.",
  "citation": 221
}