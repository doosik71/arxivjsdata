# VQA: Visual Question Answering

Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh

## 🧩 Problem to Solve

이 논문은 이미지에 대한 자유 형식의 개방형 자연어 질문에 정확한 자연어 답변을 제공하는 **시각적 질문 답변(Visual Question Answering, VQA)**이라는 새로운 과제를 제시합니다. 기존의 이미지 캡셔닝 시스템은 일반적인 장면 설명을 생성하지만, VQA는 특정 정보에 대한 상세한 이미지 이해와 컴퓨터 비전(CV), 자연어 처리(NLP), 지식 표현 및 추론(KR), 상식 추론(commonsense reasoning)을 포함하는 복잡한 다중 모달 추론 능력을 요구합니다. 또한, 진행 상황을 정량적으로 측정할 수 있는 명확한 평가 지표가 필요합니다.

## ✨ Key Contributions

- **VQA 태스크 제안:** 자유 형식 및 개방형 시각적 질문 답변(VQA)이라는 새로운 연구 과제를 정의하고 제안했습니다.
- **대규모 VQA 데이터셋 구축:** 약 25만 개의 이미지(MS COCO 및 추상 장면), 약 76만 개의 질문, 약 1천만 개의 답변으로 구성된 대규모 데이터셋을 공개했습니다(<http://www.visualqa.org>).
- **다양한 질문 및 답변 분석:** 데이터셋 내 질문과 답변의 다양성을 분석하고, 이미지 캡션이나 상식만으로는 질문에 답하기 어렵다는 점을 시연했습니다.
- **베이스라인 및 방법론 제시:** VQA를 위한 다양한 베이스라인과 이미지 및 질문 특징을 결합한 새로운 모델을 제안하고 평가했습니다.
- **자동 평가 방식 도입:** 개방형 답변 및 객관식 답변을 모두 지원하는 정량적 자동 평가 방법을 제시했습니다.
- **연례 챌린지 및 워크숍 조직:** VQA 연구의 발전을 촉진하기 위해 연례 챌린지와 워크숍을 개최할 것을 발표했습니다.

## 📎 Related Works

- **기존 VQA 연구:** 이 논문 이전에도 [19], [36], [50], [3] 등의 시각적 질문 답변 연구가 있었으나, 데이터셋 규모가 작거나(예: 2,591개 또는 1,449개 이미지) 제한적인 환경(예: 16가지 색상 또는 고정된 객체 어휘)에 국한되었습니다.
- **텍스트 기반 Q&A:** [15], [14], [54], [45]와 같은 텍스트 기반 Q&A 연구로부터 영감을 얻었지만, VQA는 질문이 이미지에 자연스럽게 접목되어 시각 정보 이해가 필수적임을 강조했습니다.
- **시각 콘텐츠 설명:** [11], [29](이미지 태깅), [30], [17], [40], [9], [16], [53], [12], [24], [38], [26](이미지 캡셔닝) 및 [46], [21](비디오 캡셔닝)과 같은 연구는 시각 콘텐츠를 설명하지만, VQA는 일반적인 캡션으로는 얻을 수 없는 세부적이고 특정 정보를 요구합니다.
- **기타 시각+언어 태스크:** [28], [43](공동 참조 해결) 또는 [25], [42](참조 표현 생성) 등은 특정 객체 식별에 중점을 두지만, VQA는 더 풍부하고 다양한 시각적 개념을 다룹니다.

## 🛠️ Methodology

1. **데이터셋 수집:**
   - **이미지:** MS COCO 데이터셋의 실사 이미지(123,287개)와 'paperdoll' 인간 모델 및 다양한 객체/동물을 사용하여 생성한 추상 장면(5만 개)을 활용했습니다. 추상 장면은 저수준 비전 태스크의 필요성을 줄여 고수준 추론에 집중할 수 있게 합니다.
   - **질문:** Amazon Mechanical Turk(AMT)를 통해 각 이미지/장면에 대해 3개의 개방형 자연어 질문을 수집했습니다. 작업자들은 "스마트 로봇이 답하기 어려워할 질문"을 만들도록 지시받아 다양하고 복잡한 질문을 유도했습니다.
   - **답변:** 각 질문에 대해 10명의 작업자로부터 답변을 수집했습니다. 답변은 짧은 구절이어야 하며, "예" 또는 "아니오" 같은 단일 단어 답변이 많습니다. 질문에 대한 답변은 이미지 없이도 수집되어 상식 추론의 역할을 분석하는 데 사용되었습니다.
   - **평가 메트릭:**
     - **개방형(Open-ended):** `accuracy = min(# humans that provided that answer / 3, 1)`로 계산되며, 모든 답변은 소문자화, 숫자 변환, 구두점 및 관사 제거 등의 전처리를 거칩니다.
     - **객관식(Multiple-choice):** 18개의 후보 답변(정답, 그럴듯한 오답, 인기 있는 오답, 무작위 정답) 중 하나를 선택하며, 정확도 계산은 개방형과 유사합니다.
2. **베이스라인 모델:**
   - `random`: 상위 1,000개 답변 중 무작위 선택.
   - `prior ("yes")`: 항상 "yes"로 답변.
   - `per Q-type prior`: 질문 유형별로 가장 인기 있는 답변 선택.
   - `nearest neighbor`: Skip-Thought 임베딩으로 유사 질문을 찾고, fc7 특징으로 유사 이미지를 찾아 가장 흔한 정답을 반환.
3. **제안된 방법론 (2-채널 비전 + 언어 모델):**
   - **출력:** 상위 $K=1000$개의 가장 빈번한 답변에 대한 소프트맥스 분포.
   - **이미지 채널:** VGGNet [48]의 마지막 은닉층에서 추출한 4096차원 특징을 사용하며, `L2` 정규화를 적용합니다.
   - **질문 채널:**
     - **Bag-of-Words Question (BoW Q):** 상위 1,000개 단어 및 질문의 첫 세 단어를 기반으로 1,030차원 임베딩 생성.
     - **LSTM Q:** 1개 은닉층을 가진 LSTM을 사용하여 1,024차원 임베딩 생성.
     - **deeper LSTM Q:** 2개 은닉층을 가진 LSTM을 사용하여 2,048차원 임베딩을 생성한 후 1,024차원으로 변환.
   - **융합 및 분류:** 이미지 및 질문 임베딩을 동일한 공간(1,024차원)으로 변환 후 원소별 곱셈(`element-wise multiplication`)으로 융합(어블레이션 스터디에서는 `concatenation`도 사용). 이후 2개의 은닉층을 가진 MLP(각 1,000개 유닛, tanh 활성화, 0.5 드롭아웃)를 통과시켜 소프트맥스 분류를 수행합니다.
   - **학습:** 전체 모델은 교차 엔트로피 손실로 종단 간 학습되며, VGGNet의 파라미터는 ImageNet 분류를 위해 학습된 값으로 고정하고 미세 조정하지 않습니다.
   - **캡션 활용:** 인간이 생성한 캡션을 입력으로 사용하는 `BoW Q + C` 모델도 실험했습니다.

## 📊 Results

- **인간 성능:** 실사 이미지에 대한 개방형 태스크의 인간 간 일치율은 83.30%였으며, 이미지 없이 질문에 답할 경우 정확도는 40.81%로 크게 떨어져 이미지 정보의 중요성을 시사합니다.
- **모델 성능:**
  - 이미지만 사용하는 모델(I)은 28.13%, "yes"로만 답하는 베이스라인보다도 낮아 매우 저조한 성능을 보였습니다.
  - 질문만 사용하는 모델(예: `BoW Q` 48.09%, `LSTM Q` 48.76%)은 이미지 정보를 무시했음에도 놀랍도록 높은 성능을 보였는데, 이는 질문 유형에 대한 언어 모델의 통계적 편향을 활용한 결과로 분석됩니다.
  - 가장 성능이 좋은 모델(`deeper LSTM Q + norm I`)은 실사 이미지 test-dev 세트에서 개방형 57.75%, 객관식 62.70%의 정확도를 달성하여 베이스라인을 크게 능가했습니다 (test-standard에서는 개방형 58.16%, 객관식 63.09%).
- **어블레이션 연구:** `L2` 정규화된 이미지 특징, `element-wise multiplication` 방식의 특징 융합, 더 큰 답변 후보 수($K=2000$)가 성능 향상에 기여했습니다.
- **질문 유형별 분석:** "What is"나 "How many"와 같이 추론이 많이 필요한 질문 유형에서는 장면 수준의 이미지 특징이 추가 정보를 많이 제공하지 못했습니다.
- **나이/상식 추정:** 모델은 평균 4.74세 어린이 수준의 답변 능력을 보였고, 상식 추론 요구도는 17.35%로 평가되었습니다. 질문의 난이도(요구되는 인간의 연령)가 높아질수록 모델의 정확도는 감소했습니다.

## 🧠 Insights & Discussion

- VQA는 이미지 캡셔닝보다 훨씬 깊은 시각적 이해와 복잡한 추론을 요구하는 "AI-완전(AI-complete)" 문제에 한 걸음 더 다가선 중요한 과제입니다.
- 제안된 대규모 VQA 데이터셋은 시각적, 언어적, 상식적 추론 능력을 종합적으로 평가할 수 있는 풍부한 자원입니다.
- 답변의 간결성 덕분에 자동 평가가 가능하여 연구 진행 상황을 효과적으로 추적할 수 있습니다.
- 현재 모델의 성능은 인간에 비해 아직 현저히 낮으므로, 컴퓨터 비전 및 자연어 처리 분야의 상당한 발전을 위한 여지가 많습니다.
- 모델은 질문 유형에 따른 통계적 편향(언어적 편향)을 활용하는 경향이 있어, 시각적 정보와 질문 정보를 더욱 효과적으로 융합하는 방법론 연구가 필요합니다.
- 시각 장애인 지원과 같은 특정 응용 분야를 위한 태스크별 질문 수집을 통해 실제 VQA 애플리케이션의 개발이 가속화될 수 있습니다.

## 📌 TL;DR

**문제:** 기존 이미지 캡셔닝의 한계를 넘어, 이미지에 대한 자유 형식의 개방형 자연어 질문에 정확히 답변하여 깊이 있는 시각-언어-상식 추론 능력을 요구하는 새로운 과제인 VQA를 정의하고 해결하는 것.

**방법:** 약 25만 개의 이미지, 76만 개의 질문, 1천만 개의 답변으로 구성된 대규모 VQA 데이터셋을 구축하고 공개했습니다. 이 데이터셋은 MS COCO 실사 이미지와 추상 장면으로 구성되며, AMT를 통해 다양한 질문과 간결한 답변을 수집했습니다. 제안된 모델은 VGGNet 이미지 특징과 LSTM 질문 임베딩을 원소별 곱셈으로 융합하고 MLP를 통해 최종 답변을 분류하는 2-채널 구조를 가집니다.

**발견:** `deeper LSTM Q + norm I` 모델은 개방형 태스크에서 58.16%의 정확도를 달성하며 비전 전용 또는 언어 전용 베이스라인을 크게 능가했습니다. 데이터셋 분석 결과, 이미지 정보가 질문 답변에 필수적이며, 모델은 상식 추론이 필요한 질문이나 계수 질문에서 여전히 어려움을 겪는 것으로 나타났습니다. 인간 성능(83.30%)과 비교했을 때 현재 모델은 아직 큰 격차를 보여 VQA 분야에 대한 활발한 연구 개발의 필요성을 시사합니다.
