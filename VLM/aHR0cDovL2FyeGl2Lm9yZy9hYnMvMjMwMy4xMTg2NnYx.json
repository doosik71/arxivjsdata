{
  "title": "Contrastive Alignment of Vision to Language Through Parameter-Efficient\n  Transfer Learning",
  "authors": "Zaid Khan, Yun Fu",
  "year": 2023,
  "url": "http://arxiv.org/abs/2303.11866v1",
  "abstract": "Contrastive vision-language models (e.g. CLIP) are typically created by\nupdating all the parameters of a vision model and language model through\ncontrastive training. Can such models be created by a small number of parameter\nupdates to an already-trained language model and vision model? The literature\ndescribes techniques that can create vision-language models by updating a small\nnumber of parameters in a language model, but these require already aligned\nvisual representations and are non-contrastive, hence unusable for\nlatency-sensitive applications such as neural search. We explore the\nfeasibility and benefits of parameter-efficient contrastive vision-language\nalignment through transfer learning: creating a model such as CLIP by minimally\nupdating an already-trained vision and language model. We find that a minimal\nset of parameter updates ($<$7%) can achieve the same performance as full-model\ntraining, and updating specific components ($<$1% of parameters) can match 75%\nof full-model training. We describe a series of experiments: we show that\nexisting knowledge is conserved more strongly in parameter-efficient training\nand that parameter-efficient scaling scales with model and dataset size. Where\npaired-image text data is scarce but strong multilingual language models exist\n(e.g. low resource languages), parameter-efficient training is even preferable\nto full-model training. Given a fixed compute budget, parameter-efficient\ntraining allows training larger models on the same hardware, achieving\nequivalent performance in less time. Parameter-efficient training hence\nconstitutes an energy-efficient and effective training strategy for contrastive\nvision-language models that may be preferable to the full-model training\nparadigm for common use cases. Code and weights at\nhttps://github.com/codezakh/LilT."
}