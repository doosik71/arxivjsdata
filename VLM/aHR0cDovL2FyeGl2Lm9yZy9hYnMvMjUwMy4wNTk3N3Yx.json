{
  "title": "Is Your Video Language Model a Reliable Judge?",
  "authors": "Ming Liu, Wensheng Zhang",
  "year": 2025,
  "url": "http://arxiv.org/abs/2503.05977v1",
  "abstract": "As video language models (VLMs) gain more applications in various scenarios,\nthe need for robust and scalable evaluation of their performance becomes\nincreasingly critical. The traditional human expert-based evaluation of VLMs\nhas limitations in consistency and scalability, which sparked interest in\nautomatic methods such as employing VLMs to evaluate VLMs. However, the\nreliability of VLMs as judges remains underexplored. Existing methods often\nrely on a single VLM as the evaluator. However, this approach can be unreliable\nor biased because such a model may lack the ability to fully understand the\ncontent and may have inherent biases, ultimately compromising evaluation\nreliability. A remedy is to apply the principle of collective thoughts,\naggregating evaluations from multiple VLMs to enhance reliability. This study\ninvestigates the efficacy of such approaches, particularly when the pool of\njudges includes both reliable and unreliable models. Our findings reveal that\nincorporating collective judgments from such a mixed pool does not necessarily\nimprove the accuracy of the final evaluation. The inclusion of less reliable\njudges can introduce noise, undermining the overall reliability of the\noutcomes. To explore the factors that impact evaluation reliability, we\nfine-tune an underperforming VLM judge, Video-LLaVA, and observe that improved\nunderstanding ability alone is insufficient to make VLM judges more reliable.\nThese findings stress the limitations of collective thought approaches and\nhighlight the need for more advanced methods that can account for the\nreliability of individual models. Our study promotes the development of more\nreliable evaluation methods for VLMs"
}