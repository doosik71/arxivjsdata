{
  "title": "EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for\n  Visual Emotion Analysis",
  "authors": "SangEun Lee, Yubeen Lee, Eunil Park",
  "year": 2025,
  "url": "http://arxiv.org/abs/2505.07164v1",
  "abstract": "Visual emotion analysis, which has gained considerable attention in the field\nof affective computing, aims to predict the dominant emotions conveyed by an\nimage. Despite advancements in visual emotion analysis with the emergence of\nvision-language models, we observed that instruction-tuned vision-language\nmodels and conventional vision models exhibit complementary strengths in visual\nemotion analysis, as vision-language models excel in certain cases, whereas\nvision models perform better in others. This finding highlights the need to\nintegrate these capabilities to enhance the performance of visual emotion\nanalysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned\nvision-language model augmented with a lightweight module distilled from\nconventional vision models. Instead of deploying both models simultaneously,\nwhich incurs high computational costs, we transfer the predictive patterns of a\nconventional vision model into the vision-language model using a knowledge\ndistillation framework. Our approach first fine-tunes a vision-language model\non emotion-specific instruction data and then attaches a distilled module to\nits visual encoder while keeping the vision-language model frozen. Predictions\nfrom the vision language model and the distillation module are effectively\nbalanced by a gate module, which subsequently generates the final outcome.\nExtensive experiments show that EmoVLM-KD achieves state-of-the-art performance\non multiple visual emotion analysis benchmark datasets, outperforming the\nexisting methods while maintaining computational efficiency. The code is\navailable in https://github.com/sange1104/EmoVLM-KD."
}