{
  "title": "Parrot: Multilingual Visual Instruction Tuning",
  "authors": "Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye",
  "year": 2024,
  "url": "http://arxiv.org/abs/2406.02539v3",
  "abstract": "The rapid development of Multimodal Large Language Models (MLLMs), such as\nGPT-4o, marks a significant step toward artificial general intelligence.\nExisting methods typically align vision encoders with LLMs via supervised\nfine-tuning (SFT), but this often deteriorates their ability to handle multiple\nlanguages as training progresses. We empirically observe that imbalanced SFT\ndatasets, largely English-centric, degrade performance on non-English languages\ndue to the failure in multilingual token alignment. To address this, we propose\nPARROT, a novel approach that leverages textual guidance for visual token\nalignment at the language level. PARROT conditions visual tokens on diverse\nlanguage inputs and uses Mixture-of-Experts (MoE) to align multilingual tokens.\nBy computing cross-attention between initial visual features and textual\nembeddings, we select the most relevant experts, converting visual tokens into\nlanguage-specific representations. Additionally, we introduce the Massive\nMultilingual Multimodal Benchmark (MMMB), a new benchmark comprising 6\nlanguages, 15 categories, and 12,000 questions, to assess multilingual\ncapabilities. PARROT achieves state-of-the-art performance on both the\nmultilingual benchmarks and a wide range of multimodal tasks. Code and dataset\nare available at: https://github.com/AIDC-AI/Parrot"
}