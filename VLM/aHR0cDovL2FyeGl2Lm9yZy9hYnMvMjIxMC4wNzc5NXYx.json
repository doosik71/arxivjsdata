{
  "title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge\n  Distillation and Modal-adaptive Pruning",
  "authors": "Tiannan Wang, Wangchunshu Zhou, Yan Zeng, Xinsong Zhang",
  "year": 2022,
  "url": "http://arxiv.org/abs/2210.07795v1",
  "abstract": "Pre-trained vision-language models (VLMs) have achieved impressive results in\na range of vision-language tasks. However, popular VLMs usually consist of\nhundreds of millions of parameters which brings challenges for fine-tuning and\ndeployment in real-world applications due to space, memory, and latency\nconstraints. In this work, we introduce a distilling then pruning framework to\ncompress large vision-language models into smaller, faster, and more accurate\nones. We first shrink the size of a pre-trained large VLM and apply knowledge\ndistillation in the vision-language pre-training stage to obtain a\ntask-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm\nto automatically infer the importance of vision and language modalities for\ndifferent downstream tasks and adaptively remove redundant structures and\nneurons in different encoders with controllable target sparsity. We apply our\nframework to train EfficientVLM, a fast and accurate vision-language model\nconsisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers,\naccounting for only 93 million parameters in total, which is 44.3% of the\nteacher model. EfficientVLM retains 98.4% performance of the teacher model and\naccelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute\nimprovement over previous SoTA efficient VLMs of similar sizes by a large\nmargin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2\n(+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation\n(CIDEr +6.5), demonstrating a large potential on training lightweight VLMs."
}