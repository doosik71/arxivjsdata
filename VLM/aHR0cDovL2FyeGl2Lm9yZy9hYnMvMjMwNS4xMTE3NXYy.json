{
  "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for\n  Vision-Centric Tasks",
  "authors": "Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai",
  "year": 2023,
  "url": "http://arxiv.org/abs/2305.11175v2",
  "abstract": "Large language models (LLMs) have notably accelerated progress towards\nartificial general intelligence (AGI), with their impressive zero-shot capacity\nfor user-tailored tasks, endowing them with immense potential across a range of\napplications. However, in the field of computer vision, despite the\navailability of numerous powerful vision foundation models (VFMs), they are\nstill restricted to tasks in a pre-defined form, struggling to match the\nopen-ended task capabilities of LLMs. In this work, we present an LLM-based\nframework for vision-centric tasks, termed VisionLLM. This framework provides a\nunified perspective for vision and language tasks by treating images as a\nforeign language and aligning vision-centric tasks with language tasks that can\nbe flexibly defined and managed using language instructions. An LLM-based\ndecoder can then make appropriate predictions based on these instructions for\nopen-ended tasks. Extensive experiments show that the proposed VisionLLM can\nachieve different levels of task customization through language instructions,\nfrom fine-grained object-level to coarse-grained task-level customization, all\nwith good results. It's noteworthy that, with a generalist LLM-based framework,\nour model can achieve over 60\\% mAP on COCO, on par with detection-specific\nmodels. We hope this model can set a new baseline for generalist vision and\nlanguage models. The demo shall be released based on\nhttps://github.com/OpenGVLab/InternGPT. The code shall be released at\nhttps://github.com/OpenGVLab/VisionLLM."
}