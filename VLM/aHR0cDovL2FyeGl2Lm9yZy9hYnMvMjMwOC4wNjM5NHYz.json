{
  "title": "Detecting and Preventing Hallucinations in Large Vision Language Models",
  "authors": "Anisha Gunjal, Jihan Yin, Erhan Bas",
  "year": 2023,
  "url": "http://arxiv.org/abs/2308.06394v3",
  "abstract": "Instruction tuned Large Vision Language Models (LVLMs) have significantly\nadvanced in generalizing across a diverse set of multi-modal tasks, especially\nfor Visual Question Answering (VQA). However, generating detailed responses\nthat are visually grounded is still a challenging task for these models. We\nfind that even the current state-of-the-art LVLMs (InstructBLIP) still contain\na staggering 30 percent of the hallucinatory text in the form of non-existent\nobjects, unfaithful descriptions, and inaccurate relationships. To address\nthis, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion\nDataset that can be used to train and benchmark models for hallucination\ndetection and prevention. M-HalDetect consists of 16k fine-grained annotations\non VQA examples, making it the first comprehensive multi-modal hallucination\ndetection dataset for detailed image descriptions. Unlike previous work that\nonly consider object hallucination, we additionally annotate both entity\ndescriptions and relationships that are unfaithful. To demonstrate the\npotential of this dataset for hallucination prevention, we optimize\nInstructBLIP through our novel Fine-grained Direct Preference Optimization\n(FDPO). We also train fine-grained multi-modal reward models from InstructBLIP\nand evaluate their effectiveness with best-of-n rejection sampling. We perform\nhuman evaluation on both FDPO and rejection sampling, and find that they reduce\nhallucination rates in InstructBLIP by 41% and 55% respectively. We also find\nthat our reward model generalizes to other multi-modal models, reducing\nhallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has\nstrong correlation with human evaluated accuracy scores."
}