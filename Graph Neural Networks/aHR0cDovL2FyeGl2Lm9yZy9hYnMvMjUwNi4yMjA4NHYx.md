# Transformers are Graph Neural Networks

Chaitanya K. Joshi

## Problem to Solve

이 논문은 자연어 처리(NLP)를 위해 개발된 신경망 모델인 **트랜스포머(Transformer)**와 그래프 데이터 학습을 위한 **그래프 신경망(Graph Neural Networks, GNN)** 사이의 근본적인 관계를 정립하고, 두 아키텍처의 수학적 유사성에도 불구하고 트랜스포머가 실제 하드웨어에서 더 효율적인 이유를 '하드웨어 로터리(hardware lottery)' 관점에서 분석합니다.

## Key Contributions

- 트랜스포머가 토큰들의 완전 연결 그래프(fully connected graphs) 상에서 작동하는 GNN, 특히 **그래프 어텐션 네트워크(Graph Attention Network, GAT)** 의 한 형태로 볼 수 있음을 수학적으로 증명했습니다.
- 트랜스포머의 셀프 어텐션(self-attention) 메커니즘이 GNN의 메시지 전달(message passing) 및 집계(aggregation) 과정과 동일함을 보였습니다.
- 트랜스포머의 위치 인코딩(positional encodings)이 GNN의 그래프 구조 정보를 "힌트" 형태로 주입하는 방식과 유사하게 작동함을 제시했습니다.
- 트랜스포머가 GNN보다 실제 하드웨어에서 훨씬 효율적으로 작동하는 이유를 **밀집 행렬 연산**의 최적화 덕분이라고 설명하며, 이를 '하드웨어 로터리' 개념과 연결 지었습니다.
- 이러한 연결을 통해 그래프 학습을 위한 새로운 아키텍처인 **그래프 트랜스포머(Graph Transformers)** 의 등장을 고무했으며, 이는 지역적 메시지 전달과 전역적 어텐션 메커니즘을 결합합니다.

## Methodology

1. **트랜스포머 아키텍처 분석:** 논문은 먼저 트랜스포머의 핵심 구성 요소인 셀프 어텐션 메커니즘을 상세히 설명합니다.
   - 각 토큰 $i$의 표현 $h^{\ell}_{i}$로부터 Query ($Q = W^{\ell}_{Q} h^{\ell}_{i}$), Key ($K = \{W^{\ell}_{K} h^{\ell}_{j}, \forall j \in S\}$), Value ($V = \{W^{\ell}_{V} h^{\ell}_{j}, \forall j \in S\}$)를 생성합니다.
   - 토큰 $i$와 $j$ 사이의 상대적 중요도인 어텐션 가중치 $w_{ij}$를 다음과 같이 계산합니다:
     $$w_{ij} = \text{softmax}_{j \in S} (W^{\ell}_{Q} h^{\ell}_{i} \cdot W^{\ell}_{K} h^{\ell}_{j})$$
   - 이 가중치를 사용하여 토큰 $i$의 업데이트된 표현 $h^{\ell+1}_{i}$를 다음과 같이 얻습니다:
     $$h^{\ell+1}_{i} = \sum_{j \in S} w_{ij} \cdot W^{\ell}_{V} h^{\ell}_{j}$$
   - 이후 다중 헤드 어텐션(multi-head attention), 잔차 연결(residual connections), 계층 정규화(layer normalization), 토큰별 MLP(Multi-Layer Perceptron) 등 트랜스포머 레이어의 전체 구성 요소를 설명합니다.
2. **GNN 메시지 전달 프레임워크 분석:** 이어서 그래프 데이터 처리를 위한 GNN의 메시지 전달 프레임워크를 설명합니다.
   - **메시지 생성:** 각 노드 $i$와 그 이웃 $j \in N_i$에 대해 메시지 $m^{\ell}_{ij}$를 생성합니다:
     $$m^{\ell}_{ij} = \psi(h^{\ell}_{i}, h^{\ell}_{j})$$
   - **집계:** 노드 $i$의 모든 이웃으로부터 받은 메시지를 집계합니다:
     $$m^{\ell}_{i} = \sum_{j \in N_i} m^{\ell}_{ij}$$
   - **업데이트:** 집계된 메시지와 이전 표현을 사용하여 노드 $i$의 표현을 업데이트합니다:
     $$h^{\ell+1}_{i} = \phi(h^{\ell}_{i}, m^{\ell}_{i})$$
   - 특히 GNN의 한 종류인 GAT가 어텐션 메커니즘을 사용하여 이웃 노드의 중요도를 가중하는 방식을 제시합니다.
3. **형식적 동등성 수립:** 논문은 트랜스포머의 셀프 어텐션 메커니즘이 모든 토큰을 이웃으로 간주하는 **완전 연결 그래프**에서 GAT의 메시지 전달이 이루어지는 것과 수학적으로 동일함을 보여줍니다. 즉, GAT의 이웃 집합 $N_i$를 모든 토큰 집합 $S$로 확장하고 트랜스포머의 잔차 연결과 MLP를 포함하면 두 모델의 업데이트 식이 정확히 일치함을 증명합니다.
4. **하드웨어 효율성 분석:** 마지막으로, 트랜스포머가 최신 GPU/TPU와 같은 하드웨어에 최적화된 **밀집 행렬 곱셈**을 활용하는 반면, GNN은 **희소 메시지 전달**에 의존하여 효율성에서 상당한 차이가 발생함을 분석합니다.

## Results

- 논문은 트랜스포머와 GNN, 특히 GAT 간의 **수학적 동등성**을 성공적으로 입증했습니다. 이는 트랜스포머가 사전 정의된 그래프 구조가 없는 데이터에서도 관계 학습이 가능한 강력한 집합 처리 네트워크임을 시사합니다.
- 하지만, 이러한 수학적 연결에도 불구하고 트랜스포머는 최신 하드웨어에 최적화된 밀집 행렬 연산을 활용하여 **실질적인 성능 및 확장성 우위**를 가진다는 점을 강조합니다. 이는 '하드웨어 로터리' 개념, 즉 아키텍처가 특정 하드웨어에 얼마나 잘 맞는지가 그 성공에 중요한 역할을 한다는 관점으로 설명됩니다.
- 이러한 통찰은 그래프 구조에 대한 귀납적 편향(inductive bias)을 유지하면서 트랜스포머의 표현력과 확장성을 결합한 **그래프 트랜스포머**와 같은 새로운 연구 방향을 제시했습니다. 결론적으로, 트랜스포머는 현재 '하드웨어 로터리'에서 승리하고 있는 GNN이라고 할 수 있습니다.
