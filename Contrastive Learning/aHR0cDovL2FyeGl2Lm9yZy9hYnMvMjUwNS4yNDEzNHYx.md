# A Mathematical Perspective On Contrastive Learning
Ricardo Baptista, Andrew Stuart, and Son Tran

## Problem to Solve

다중 모달 대조 학습(multimodal contrastive learning)은 특히 이미지-텍스트와 같은 이중 모달(bimodal) 설정에서 서로 다른 데이터 양식(modality) 간의 연결을 학습하여 공통 잠재 공간(latent space)에 표현을 정렬하는 방법론입니다. 기존 대조 학습 방법론은 확률적 관점에서 명확하게 해석되지 않아 알고리즘의 동작을 이해하고 새로운 변형을 제안하는 데 한계가 있었습니다. 이 논문은 이러한 방법론을 확률적 언어로 정형화하고, 이를 통해 교차 모달 검색, 분류 및 생성 모델링과 같은 응용 분야를 위한 새로운 접근 방식을 탐구하고자 합니다.

## Key Contributions

*   **확률론적 정식화:** 대조 학습을 각 양식의 주변 분포(marginal distribution) 곱에서 "틸팅(tilting)"이라는 측도 변화(change of measure)를 통해 기본 공동 분포(joint distribution)를 결정하는 문제로 정형화했습니다.
*   **새로운 확률 손실 함수 도입:** 공동 분포, 한쪽 조건부 분포, 또는 두 조건부 분포의 합을 일치시키는 것을 포함하여 새로운 확률 기반 손실 함수를 제안했습니다. 표준 대조 학습은 두 조건부 분포의 합을 일치시키는 경우에 해당합니다.
*   **새로운 틸팅 방법 도입:** 코사인 유사도(cosine similarity) 기반의 표준 대조 학습을 특수 사례로 포함하는 새로운 틸팅(예: 정규화되지 않은 인코더, L2 거리 기반 틸팅)을 제안했습니다.
*   **가우시안 설정 분석:** 가우시안(Gaussian) 분포 환경에서 새로운 대조 학습 방법론을 분석하고, 잠재 공간 식별을 저랭크 행렬 근사(low-rank matrix approximation) 문제로 정형화했습니다. 이를 통해 점 추정량(point estimator) 및 생성 모델로서의 다양한 접근 방식의 역량을 특성화했습니다.
*   **응용 가능성 입증:** 제안된 방법론이 라그랑주 데이터 동화(Lagrangian data assimilation)와 같은 과학 및 공학 문제, 그리고 검색 및 MNIST 숫자 분류와 같은 데이터 과학 응용 분야에 적용 가능함을 입증했습니다.

## Methodology

이 논문은 대조 학습을 조건부 확률 분포를 정의하는 매개변수화된 인코더($g_u$, $g_v$)의 최적화 문제로 해석합니다.

*   **확률론적 관점:**
    *   두 양식 $u$와 $v$에 대한 학습된 공동 분포 $\nu(du,dv;\theta)$를 주변 분포의 곱 $\mu_u(du)\mu_v(dv)$에 틸팅 밀도 $\rho(u,v;\theta)$를 곱하여 정의합니다.
    *   **표준 틸팅:** 인코더의 출력을 $L_2$ 노름으로 정규화한 후 코사인 유사도를 사용합니다. 즉, $\rho(u,v;\theta) \propto \exp(\langle \bar{g}_u(u;\theta_u), \bar{g}_v(v;\theta_v) \rangle / \tau)$ 형태를 가집니다.
    *   **일반화된 손실 함수:**
        *   **조건부 손실 ($J_{\text{cond}}$):** 실제 조건부 분포($\mu_{u|v}$, $\mu_{v|u}$)와 학습된 조건부 분포($\nu_{u|v}$, $\nu_{v|u}$) 간의 Kullback-Leibler (KL) 발산의 합을 최소화합니다. 이 손실은 단방향 조건부 분포를 일치시키는 데 특화될 수 있습니다.
        *   **공동 손실 ($J_{\text{joint}}$):** 실제 공동 분포($\mu$)와 학습된 공동 분포($\nu$) 간의 KL 발산을 최소화합니다. Maximum Mean Discrepancy (MMD)와 같은 다른 발산 측도도 고려됩니다.
    *   **일반화된 틸팅:**
        *   **비정규화 인코더:** $\bar{g}_u, \bar{g}_v$ 대신 정규화되지 않은 $g_u, g_v$를 사용하여 내적 $\langle g_u(u;\theta_u), g_v(v;\theta_v) \rangle$을 틸팅에 사용합니다.
        *   **$L_2$ 거리 틸팅:** $\exp(-\frac{1}{2\tau} |g_u(u;\theta_u) - g_v(v;\theta_v)|^2)$와 같이 $L_2$ 거리를 기반으로 한 틸팅을 사용합니다.
*   **가우시안 설정 분석:**
    *   데이터 분포를 다변량 가우시안 $N(0, C)$로 가정하고, 선형 인코더 $g_u(u) = Gu$와 $g_v(v) = Hv$를 사용합니다.
    *   다양한 손실 함수와 틸팅에 대해 최적의 인코더 파라미터 (행렬 $G, H$)에 대한 폐쇄형 해(closed-form solution)를 도출하고, 이를 저랭크 행렬 근사 문제와 연관 지어 분석합니다.

## Results

*   **가우시안 모델 분석 (이론 및 수치 실험):**
    *   **코사인 거리 & 양방향 조건부 손실 (표준 방식):** 조건부 평균은 일치시킬 수 있지만, 조건부 공분산은 일치시키지 못하며, 학습된 분산이 실제보다 커집니다 (Corollary 5.2, Figure 2).
    *   **양의 이차 형식 & 단방향 조건부 손실 ($L_2$ 거리 틸팅):** 한쪽 조건부 분포(예: $u|v$)의 평균과 공분산을 모두 정확히 일치시킬 수 있습니다 (Corollary 5.4, Figure 3).
    *   **코사인 거리 & 공동 손실:** 조건부 손실에 비해 주변 공분산을 더 잘 근사하며, 전체 공동 분포에 대한 더 가까운 근사를 제공합니다 (Corollary 5.7, Figure 4, 5).
    *   **정규화된 인코더:** 가우시안 데이터에서도 비선형성을 통해 비가우시안 조건부 분포를 학습할 수 있음을 보여줍니다 (Figure 6).
*   **고차원 가우시안 실험:**
    *   임베딩 차원을 $\min(n_u, n_v)$까지 증가시키면 조건부 기대치 근사가 개선됩니다 (Figure 8).
    *   배치 크기와 학습 샘플 크기를 늘리면 근사 오차가 줄어들고 참값으로 수렴합니다 (Figure 9, 10).
*   **MNIST 데이터셋 분류:**
    *   기존 MNIST 분류 방법론이 비정규화 임베딩을 사용하는 단방향 조건부 손실과 등가임을 보였습니다 (Proposition 6.1).
    *   레이블이 주어진 이미지에 대한 단방향 손실은 분류 정확도가 높지만, 이미지 샘플의 다양성이 떨어집니다 (Figure 11, 12).
    *   양방향 조건부 손실 또는 이미지가 주어진 레이블에 대한 단방향 손실은 더 다양한 이미지 샘플을 생성하여 분포를 더 잘 포착함을 시사합니다 (Figure 12).
*   **라그랑주 데이터 동화:**
    *   대조 학습을 사용하여 라그랑주 유동 데이터로부터 오일러(Eulerian) 속도장을 성공적으로 복구했습니다.
    *   학습된 임베딩은 잠재장(potential field)에서 궤적(trajectory)으로, 궤적에서 잠재장으로의 검색에서 높은 정확도(R@5에서 거의 100%)를 보여주었습니다 (Figure 14, 15).