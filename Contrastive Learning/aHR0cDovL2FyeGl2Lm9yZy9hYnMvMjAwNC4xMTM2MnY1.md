# Supervised Contrastive Learning
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan

## Problem to Solve
기존의 딥 러닝 분류 모델에서 널리 사용되는 **교차 엔트로피**(Cross-Entropy) 손실 함수는 **노이즈 레이블**(noisy labels)에 대한 **강건성**(robustness) 부족, 낮은 **마진**(margins)으로 인한 일반화 성능 저하 등 여러 단점을 가지고 있습니다. 최근 **자기 지도 학습**(self-supervised learning) 분야에서 **대조 학습**(contrastive learning)이 **최고 성능**(**SOTA**)을 달성하며 큰 발전을 이루었지만, 이는 주로 레이블 정보가 없는 **비지도 학습**(unsupervised learning) 환경에 초점을 맞추고 있습니다. 이 논문은 **완전 지도 학습**(fully-supervised setting) 환경에서 대조 학습의 이점을 효과적으로 활용하여 레이블 정보를 적극적으로 사용하는 새로운 손실 함수를 제안함으로써, 기존 **교차 엔트로피**의 한계를 극복하고 이미지 분류 성능을 향상시키는 것을 목표로 합니다.

## Key Contributions
*   **새로운 지도 대조 학습 손실 함수 제안**: 단일 **긍정 샘플**(positive sample)만 사용하는 **자기 지도 대조 학습**과 달리, 각 **앵커**(anchor)에 대해 **여러 개의 긍정 샘플**을 허용하는 새로운 손실 함수 `SupCon`을 제안했습니다.
*   **최고 성능 달성**: **ImageNet** 데이터셋의 **ResNet-200** 아키텍처에서 **81.4%**의 **Top-1 정확도**를 달성하여, 해당 아키텍처의 기존 최고 기록보다 **0.8%** 높은 **최고 성능**을 기록했습니다.
*   **강건성**(Robustness) 향상**: **ImageNet-C** 데이터셋에서 측정했을 때, `SupCon` 손실 함수로 학습된 모델이 자연적인 손상에 대해 **교차 엔트로피** 모델보다 더 높은 **강건성**을 보임을 입증했습니다.
*   **하이퍼파라미터 안정성**: `SupCon` 손실 함수가 **최적화 도구**(optimizers) 및 **데이터 증강**(data augmentations)과 같은 **하이퍼파라미터** 설정에 대해 **교차 엔트로피**보다 더 안정적인 성능을 보임을 확인했습니다.
*   **내재적 어려운 샘플 마이닝**: 제안된 손실 함수의 **경사**(gradient)가 **어려운 긍정 샘플**(hard positives) 및 **어려운 부정 샘플**(hard negatives)로부터 학습을 촉진하여, 수동적인 **어려운 샘플 마이닝**(hard mining)의 필요성을 제거함을 분석적으로 증명했습니다.
*   **기존 손실 함수의 일반화**: `SupCon` 손실 함수는 **트리플릿 손실**(triplet loss)과 **N-쌍 손실**(N-pairs loss)을 포함하는 **상위 개념**의 손실 함수로 설명될 수 있음을 보였습니다.

## Methodology
저자들은 **자기 지도 대조 학습**의 구조를 바탕으로 **지도 분류**(supervised classification)에 맞게 수정된 프레임워크를 제안합니다.

*   **표현 학습 프레임워크 구성 요소**:
    *   **데이터 증강 모듈**(`Aug(·)`)**: 각 입력 샘플 `x`에 대해 두 개의 무작위 증강(`$\tilde{x}=Aug(x)$`)을 생성하여 데이터의 다른 "뷰"를 만듭니다.
    *   **인코더 네트워크**(`Enc(·)`)**: `x`를 **표현 벡터**(`$r=Enc(x) \in R^{D_E}$`)로 매핑합니다. 두 증강된 샘플 모두 동일한 **인코더**에 입력되며, `$r$`은 단위 **하이퍼스피어**(hypersphere)로 **정규화**(normalized)됩니다.
    *   **투영 네트워크**(`Proj(·)`)**: `$r$`을 벡터 `$z=Proj(r) \in R^{D_P}$`로 매핑합니다. 이 네트워크는 일반적으로 단일 은닉층을 가진 **다층 퍼셉트론**(multi-layer perceptron) 또는 단일 선형층으로 구현되며, 출력 `z` 또한 단위 **하이퍼스피어**로 **정규화**됩니다. 이 네트워크는 추론 시 폐기됩니다.

*   **2단계 학습 과정**:
    1.  **1단계 (대조 학습)**: **투영 네트워크**의 출력에 대해 **지도 대조 손실**(`SupCon`)을 계산하여 좋은 **표현**(representations)을 학습합니다.
    2.  **2단계 (분류)**: 학습된 **인코더**(encoder)의 **고정된 표현**(frozen representations) 위에 **선형 분류기**(linear classifier)를 학습하고 **교차 엔트로피** 손실을 사용하여 최종 분류를 수행합니다.

*   **지도 대조 손실 함수 (SupCon)**:
    *   **자기 지도 대조 손실**(Self-Supervised Contrastive Loss, `L_self`)을 확장하여 레이블 정보를 활용합니다. `L_self`는 각 **앵커**에 대해 단 하나의 **긍정 샘플**(**같은 이미지의 증강된 버전**)만을 대조합니다.
    *   **SupCon**은 **다중 긍정 샘플**을 고려합니다. 즉, **앵커**와 같은 클래스에 속하는 모든 샘플을 **긍정 샘플**로 간주합니다.
    *   논문에서는 두 가지 공식화를 분석합니다:
        *   **`L_sup_out` (선택된 손실)**: 긍정 샘플에 대한 합산이 `log` 함수 **바깥에** 위치합니다.
            $$L_{sup}^{out} = \sum_{i \in I} -\frac{1}{|P(i)|} \sum_{p \in P(i)} \log \frac{\exp(z_i \cdot z_p / \tau)}{\sum_{a \in A(i)} \exp(z_i \cdot z_a / \tau)}$$
            여기서 `$P(i)$`는 `i`와 같은 클래스에 속하는 모든 **긍정 샘플**의 인덱스 집합이고, `$A(i)$`는 배치 내 모든 샘플의 인덱스 집합입니다 (자기 자신 제외). `$\tau$`는 **온도 파라미터**(temperature parameter)입니다.
        *   **`L_sup_in`**: 긍정 샘플에 대한 합산이 `log` 함수 **안에** 위치합니다.
    *   실험적으로 **`L_sup_out`이 `L_sup_in`보다 훨씬 우수한 성능**을 보임을 확인했습니다. 이는 `L_sup_out`의 **경사**(gradient)가 `L_sup_in`보다 학습에 더 최적화되어 있기 때문입니다. 특히 `L_sup_out`은 `1/|P(i)|`라는 **정규화 계수**가 `log` 바깥에 있어 **긍정 샘플**의 편향을 제거하는 효과가 있습니다.
    *   **`L_sup_out`**의 **경사**는 **어려운 긍정/부정 샘플**에 대해 더 큰 가중치를 부여하여, **내재적인 어려운 샘플 마이닝** 효과를 제공합니다.

*   **주요 학습 기법**:
    *   **임베딩 정규화**: **임베딩 공간**의 거리를 **내적**(inner product)으로 측정하며, 이는 코사인 유사도와 동일합니다. 표현을 정규화하면 성능이 향상되고 **내재적인 어려운 샘플 마이닝**이 가능해집니다.
    *   **온도 파라미터 ($\tau$)**: 성능에 중요한 영향을 미치며, 최적 값은 **0.1**로 설정되었습니다.
    *   **대규모 배치 크기**: 더 많은 **부정 샘플**과 **어려운 긍정/부정 샘플**을 포함할 수 있어 성능 향상에 기여합니다.

## Results
*   **분류 정확도**:
    *   **ImageNet** 데이터셋에서 **ResNet-200**으로 **81.4%**의 **Top-1 정확도**를 달성하여 기존 **교차 엔트로피** 기반 모델을 **0.8%** 능가했습니다.
    *   **ResNet-50**에서는 **AutoAugment**를 사용했을 때 **78.7%**의 **Top-1 정확도**를 기록하며 **CutMix**[60] 등 **최신 데이터 증강**(data augmentation) 전략과 비교해서도 우월함을 보였습니다.
    *   **CIFAR-10**과 **CIFAR-100**에서도 **교차 엔트로피** 및 비지도 대조 학습 **(SimCLR)** 대비 일관되게 높은 **Top-1 정확도**를 보여주었습니다 (**CIFAR-10: 96.0%**, **CIFAR-100: 76.5%**).

*   **강건성**(**Robustness**):
    *   **ImageNet-C** 데이터셋에 대한 평가에서 **교차 엔트로피** 모델보다 낮은 **평균 손상 오류**(**mCE**) 및 **상대 평균 손상 오류**(**rel. mCE**) 값을 기록하여, 이미지 손상에 대한 **강건성**이 향상되었음을 입증했습니다.
    *   손상 심도가 증가함에 따라 **정확도 감소폭**이 더 작고 **예상 보정 오류**(Expected Calibration Error, **ECE**) 증가도 더 안정적이었습니다.

*   **하이퍼파라미터 안정성**:
    *   다양한 **데이터 증강**(RandAugment[6], AutoAugment[5] 등), **최적화 도구**(LARS[59], SGD with Momentum, RMSProp[20]), **학습률** 변화에 따른 **Top-1 정확도**의 **분산**(variance)이 **교차 엔트로피**에 비해 현저히 낮아, `SupCon` 손실 함수가 더 안정적으로 학습됨을 보였습니다.
    *   특히, `SupCon`은 더 작은 배치 크기에서도 **교차 엔트로피**보다 일관되게 더 높은 **Top-1 정확도**를 유지했습니다.

*   **전이 학습**(Transfer Learning):
    *   12개의 자연 이미지 데이터셋에 대한 **전이 학습** 성능 평가에서 **교차 엔트로피** 및 **자기 지도 대조 학습**과 동등한 수준의 성능을 보였습니다. 이는 `SupCon`으로 학습된 **표현**(representations)이 **다운스트림 태스크**(downstream tasks)에도 효과적으로 전이될 수 있음을 시사합니다.

*   **학습 세부 사항**:
    *   `SupCon` 사전 학습은 **ResNet-200**의 경우 **700 에포크**(epochs), 소형 모델의 경우 **350 에포크**로 진행되었습니다.
    *   최종 **선형 분류기** 학습은 최소 **10 에포크**면 충분했습니다.
    *   배치 크기는 최대 **6144**를 사용했지만, 대부분의 경우 **2048**로도 충분한 성능을 보였습니다.
    *   **온도 파라미터** `$\tau=0.1$`이 최적의 **Top-1 정확도**를 달성하는 데 기여했습니다.
    *   **ImageNet** 학습 시, **인코더** 사전 학습에는 **LARS** **최적화 도구**를, **선형층** 학습에는 **RMSProp** **최적화 도구**를 사용했을 때 가장 좋은 성능을 얻었습니다.