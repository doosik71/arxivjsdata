# Continual Multimodal Contrastive Learning
Xiaohao Liu, Xiaobo Xia, See-Kiong Ng, Tat-Seng Chua

## Problem to Solve
멀티모달 대조 학습(**Multimodal Contrastive Learning, MCL**)은 다양한 모달리티를 정렬하고 공동 공간에서 멀티모달 표현을 생성하는 데 큰 발전을 이루었습니다. 그러나 멀티모달 데이터는 한 번에 수집되는 경우가 거의 없고, 처음부터 학습하는 것은 계산 비용이 매우 높다는 문제가 있습니다. 대신 새로운 멀티모달 데이터가 점진적으로 기존 모델을 최적화하는 데 사용될 수 있습니다. 즉, 모델이 일련의 모달리티 쌍 데이터를 통해 학습됩니다. 본 논문은 이 문제를 **연속 멀티모달 대조 학습**(**Continual Multimodal Contrastive Learning, CMCL**)으로 정의합니다. `CMCL`은 멀티모달 및 연속 학습의 교차점에서 아직 충분히 탐구되지 않은 중요 연구 방향입니다. 핵심 과제는 **안정성**(**stability** - 이전에 학습된 지식 유지)과 **유연성**(**plasticity** - 새로운 모달리티 쌍에서 효과적으로 학습)이라는 두 가지 원칙을 동시에 만족시키면서, **재앙적 망각**(**catastrophic forgetting**) 없이 모델을 지속적으로 업데이트하는 것입니다.

## Key Contributions
*   **개념적으로**, **연속 멀티모달 대조 학습**(**CMCL**)을 명확하게 공식화하고 방법론 개발을 안내하는 **안정성**과 **유연성**에 대한 특화된 정의를 제공합니다.
*   **기술적으로**, 업데이트된 그래디언트가 이전 지식에 영향을 미치지 않도록 **특수 부분 공간**(**specialized subspaces**)에 투영하는 새로운 **이중 측면 널 공간**(**Dual-sided Null Space, DNS**) 방법을 제시합니다. 이 방법은 기존 모달리티 결합 모델에 쉽게 확장될 수 있습니다.
*   **이론적으로**, **안정성**과 **유연성**의 근본적인 요구 사항을 기반으로 `DNS` 방법을 엄격하게 도출하고, 두 가지 이론적 상한을 제공하여 방법의 효과를 보장합니다.
*   **경험적으로**, `CMCL` 설정에서 7개 데이터셋에 대한 광범위한 실험을 통해 최첨단 연속 학습 기준선 대비 `DNS`의 우수한 성능을 입증합니다.

## Methodology
본 논문은 **안정성**과 **유연성**을 모두 만족시키기 위해 **이중 측면 널 공간**(**Dual-sided Null Space, DNS**) 그래디언트 투영 방법을 제안합니다.
*   **문제 설정**: 모델 $W_t^{m_1}$는 이전 데이터셋 $X_{t-1}^{m_1, m_2}$로 학습된 $W_{t-1}^{m_1}$에 기반하여 새로운 데이터셋 $X_t^{m_1, m_3}$에 대해 연속적으로 최적화됩니다.
*   **두 가지 목표**:
    *   **안정성**: 이전 모달리티 쌍 ($X_{t-1}^{m_1, m_2}$)에 대한 모델의 정렬 점수 $A_{t-1;t}^{m_1, m_2}$가 현재 단계 학습 후에도 이전과 동일하게 유지되어야 합니다. 즉, $A_{t-1;t}^{m_1, m_2} = A_{t-1;t-1}^{m_1, m_2}$여야 합니다.
    *   **유연성**: 모델이 새로운 데이터셋에서 효과적으로 학습하고, 멀티모달 대조 학습 목표($Z_{t;t}^{m \top} Z_{t;t}^{m'} \sim p_{t}^{m,m'}$)를 준수해야 합니다.
*   **`DNS` 방법**:
    1.  **전역 매개변수 업데이트 식 도출**: $W_t^{m_1} = W_{t-1}^{m_1} - \eta \nabla W_t^{m_1}$ 규칙에 따라 업데이트된 $Z_{t-1;t}^{m_1}$ 및 $Z_{t-1;t}^{m_2}$의 내적 표현을 확장합니다.
    2.  **전역 그래디언트 투영**: 전역 매개변수 업데이트로 인한 총 변화 $\tilde{W}$를 계산합니다: $\tilde{W} = (**W_{t-1}^{m_1}**)^{ \top} \nabla W_t^{m_2} + (**\nabla W_t^{m_1}**)^{ \top} W_{t-1}^{m_2} - \eta (**\nabla W_t^{m_1}**)^{ \top} \nabla W_t^{m_2}$. 안정성을 유지하기 위해, 이 $\tilde{W}$를 이전 지식에 영향을 미치지 않는 $\bar{W} := \tilde{W} - P' \tilde{W} P$로 투영합니다 (**Theorem 3**). 여기서 $P'$와 $P$는 각각 이전 모달리티 $Z_{t-1;*}^{m_1}$와 $Z_{t-1;*}^{m_2}$의 공간 투영 행렬입니다.
    3.  **지역 그래디언트 투영**: 실제 모델 업데이트를 위해 각 모달리티의 국소 그래디언트 $\nabla W_t^m$를 다음과 같이 변환합니다 (**Theorem 4**):
        *   $\Delta W_t^{m_1} := \nabla W_t^{m_1} - \tilde{P}\nabla W_t^{m_1}P'$
        *   $\Delta W_t^{m_2} := \nabla W_t^{m_2} - \tilde{P}'\nabla W_t^{m_2}P$
        여기서 $\tilde{P}'$와 $\tilde{P}$는 각각 $Z_{t-1;t-1}^{m_1}$와 $Z_{t-1;t-1}^{m_2}$의 공간 투영 행렬입니다. 이 투영은 그래디언트가 이전 지식 공간에 영향을 미치지 않도록 합니다.
*   **이론적 보장**:
    *   **안정성 손실의 상한**: $||A_{t-1;t}^{m_1,m_2} - A_{t-1;t-1}^{m_1,m_2}||_2 \le \eta^2 \cdot ||Z_{t-1;*}^{m_1}||_2 ||Z_{t-1;*}^{m_2}||_2 \cdot F(\nabla W)$ (**Theorem 5**). 이 상한은 안정성 손실이 학습률 $\eta$의 제곱에 비례하며 작게 유지될 수 있음을 나타냅니다.
    *   **유연성 상한**: $L_t - L_{t-1} \le 0$ (**Theorem 6**). 이 상한은 손실이 감소하여 새로운 데이터셋으로부터 학습이 이루어짐을 보장합니다.
*   **확장성**: `DNS`는 임의의 학습 단계와 다양한 모달리티 쌍에 대해서도 확장 가능하며, 효율적으로 이전 지식을 유지할 수 있습니다.

## Results
`DNS`는 7개의 멀티모달 데이터셋(**UCF101**, **ESC50**, **NYUDv2**, **VGGSound-S**, **Clotho**, **TVL**, **LLVIP**)에서 분류 및 검색 작업을 통해 평가되었습니다. `ImageBind`, `LanguageBind`, `UniBind`를 백본으로 사용하여 **Vanilla**, **GEM**, **DER & DER++**, **EWC**, **Co2L**, **C-FLAT**, **CILA**와 같은 최첨단 연속 학습 기준선과 비교했습니다.
*   **전반적인 성능**: `DNS`는 분류 작업에서 평균 정확도(**Acc**)와 검색 작업에서 **Recall@k** (**k**=1, 5, 10) 모두에서 모든 기준선을 능가하는 **우수한 성능**을 일관되게 달성했습니다.
*   **안정성 분석**: `DNS`는 분류 작업에서 **BWT**(**Backward Transfer**)가 **최소**이거나 심지어 **양수**를 보이는 등, **재앙적 망각 완화**에 매우 효과적이었습니다. 이는 다른 방법들이 일반적으로 더 높은 음의 전이(성능 하락)를 겪는 것과 대조적입니다. 정렬 점수 편차 분석(Figure 4 (a))에서도 `DNS`의 편차가 작거나 거의 0에 가까워 **안정성 유지**를 뒷받침합니다.
*   **유연성 분석**: `DNS`는 새로운 데이터셋으로부터 학습하는 능력(**유연성**)도 뛰어났습니다. 고차항 분석(Figure 4 (b))에서 $\frac{o(\eta)}{\eta} \le 0$가 유지됨을 확인하여 **이론적 통찰을 실증적으로 뒷받침**했습니다. 배치별 학습 손실 곡선(Figure 5)은 `DNS`가 안정적인 학습 손실을 달성하며 성공적으로 수렴함을 보여, **안정성과 유연성 간의 균형**을 입증합니다.
*   **효율성**: `DNS`는 **리플레이 기반 방법**보다 더 효율적이며, 그래디언트 투영 및 `SVD` 근사로 인한 시간 증가가 미미하여 **전반적인 학습 효율성**을 유지합니다 (Figure 6).