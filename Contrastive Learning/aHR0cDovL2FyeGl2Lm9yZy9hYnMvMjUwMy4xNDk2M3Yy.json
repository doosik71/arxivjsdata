{
  "url": "http://arxiv.org/abs/2503.14963v2",
  "title": "Continual Multimodal Contrastive Learning",
  "authors": "Xiaohao Liu, Xiaobo Xia, See-Kiong Ng, Tat-Seng Chua",
  "year": 2025,
  "abstract": "Multimodal contrastive learning (MCL) advances in aligning different\nmodalities and generating multimodal representations in a joint space. By\nleveraging contrastive learning across diverse modalities, large-scale\nmultimodal data enhances representational quality. However, a critical yet\noften overlooked challenge remains: multimodal data is rarely collected in a\nsingle process, and training from scratch is computationally expensive.\nInstead, emergent multimodal data can be used to optimize existing models\ngradually, \\textit{i.e.}, models are trained on a sequence of modality pair\ndata. We define this problem as Continual Multimodal Contrastive Learning\n(CMCL), an underexplored yet crucial research direction at the intersection of\nmultimodal and continual learning. In this paper, we formulate CMCL through two\nspecialized principles of stability and plasticity. We theoretically derive a\nnovel optimization-based method, which projects updated gradients from dual\nsides onto subspaces where any gradient is prevented from interfering with the\npreviously learned knowledge. Two upper bounds provide theoretical insights on\nboth stability and plasticity in our solution. Beyond our theoretical\ncontributions, we conduct experiments on multiple datasets by comparing our\nmethod against advanced continual learning baselines. The empirical results\nfurther support our claims and demonstrate the efficacy of our method. The code\nwill be publicly available."
}