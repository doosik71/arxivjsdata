{
  "title": "Foundation models in brief: A historical, socio-technical focus",
  "authors": "Johannes Schneider",
  "year": 2022,
  "url": "http://arxiv.org/abs/2212.08967v1",
  "abstract": "Foundation models can be disruptive for future AI development by scaling up\ndeep learning in terms of model size and training data's breadth and size.\nThese models achieve state-of-the-art performance (often through further\nadaptation) on a variety of tasks in domains such as natural language\nprocessing and computer vision. Foundational models exhibit a novel {emergent\nbehavior}: {In-context learning} enables users to provide a query and a few\nexamples from which a model derives an answer without being trained on such\nqueries. Additionally, {homogenization} of models might replace a myriad of\ntask-specific models with fewer very large models controlled by few\ncorporations leading to a shift in power and control over AI. This paper\nprovides a short introduction to foundation models. It contributes by crafting\na crisp distinction between foundation models and prior deep learning models,\nproviding a history of machine learning leading to foundation models,\nelaborating more on socio-technical aspects, i.e., organizational issues and\nend-user interaction, and a discussion of future research."
}