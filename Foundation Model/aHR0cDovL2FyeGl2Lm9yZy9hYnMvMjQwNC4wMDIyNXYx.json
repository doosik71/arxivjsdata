{
  "title": "Heterogeneous Contrastive Learning for Foundation Models and Beyond",
  "authors": "Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He",
  "year": 2024,
  "url": "http://arxiv.org/abs/2404.00225v1",
  "abstract": "In the era of big data and Artificial Intelligence, an emerging paradigm is\nto utilize contrastive self-supervised learning to model large-scale\nheterogeneous data. Many existing foundation models benefit from the\ngeneralization capability of contrastive self-supervised learning by learning\ncompact and high-quality representations without relying on any label\ninformation. Amidst the explosive advancements in foundation models across\nmultiple domains, including natural language processing and computer vision, a\nthorough survey on heterogeneous contrastive learning for the foundation model\nis urgently needed. In response, this survey critically evaluates the current\nlandscape of heterogeneous contrastive learning for foundation models,\nhighlighting the open challenges and future trends of contrastive learning. In\nparticular, we first present how the recent advanced contrastive learning-based\nmethods deal with view heterogeneity and how contrastive learning is applied to\ntrain and fine-tune the multi-view foundation models. Then, we move to\ncontrastive learning methods for task heterogeneity, including pretraining\ntasks and downstream tasks, and show how different tasks are combined with\ncontrastive learning loss for different purposes. Finally, we conclude this\nsurvey by discussing the open challenges and shedding light on the future\ndirections of contrastive learning."
}