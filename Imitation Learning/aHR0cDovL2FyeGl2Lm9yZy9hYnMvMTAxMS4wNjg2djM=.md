# A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning

Stéphane Ross, Geoffrey J. Gordon, J. Andrew Bagnell

## 🧩 Problem to Solve

모방 학습(imitation learning)과 같은 순차적 예측 문제에서는 미래의 관측치가 이전 예측(행동)에 의존하므로, 통계 학습에서 흔히 가정하는 독립 동일 분포(i.i.d.) 가정이 위반됩니다. 이로 인해 이론적으로나 실제적으로 성능 저하가 발생하며, 특히 $T$ 단계에 걸쳐 오류가 $O(T^2\epsilon)$로 누적될 수 있습니다. 기존 접근 방식(비정상 또는 확률적 정책 훈련)은 대규모 문제에 비실용적이거나 학습된 컨트롤러가 불안정하다는 단점이 있었습니다. 따라서 목표는 유도된 상태 분포(induced state distribution)에서 좋은 성능(선형 오류 성장 $O(T\epsilon)$)을 보장하는 **정상(stationary)의 결정론적(deterministic) 정책**을 학습하는 효율적인 알고리즘을 개발하는 것입니다.

## ✨ Key Contributions

- **DAGGER(Dataset Aggregation) 알고리즘 제안:** 순차적 예측 및 모방 학습을 위한 새로운 반복적 메타 알고리즘인 DAGGER를 제시합니다.
- **정상 결정론적 정책 학습:** DAGGER는 기존 방법들과 달리, 유도된 상태 분포에서 우수한 성능을 보장하는 정상의 결정론적 정책을 훈련합니다.
- **강력한 이론적 보장:** DAGGER가 $T$ 단계에 걸쳐 $O(T\epsilon)$의 선형적인 오류 성장만 발생하도록 보장함을 입증하며, 이는 기존 $O(T^2\epsilon)$에 비해 크게 향상된 결과입니다.
- **무회귀 온라인 학습으로의 환원:** 모방 학습과 구조적 예측 문제를 무회귀 온라인 학습(no-regret online learning) 문제로 환원(reduction)할 수 있음을 보여주며, 이를 통해 DAGGER의 이론적 기반을 마련합니다.
- **실험적 우월성 입증:** 두 가지 도전적인 모방 학습 문제(Super Tux Kart, Super Mario Bros.)와 벤치마크 순서 라벨링 문제(필기 인식)에서 기존 접근 방식(SMILe, SEARN 및 지도 학습 기준선)보다 우수한 성능을 시연합니다.
- **간단한 구현 및 효율성:** DAGGER는 구현이 간단하고, 지도 학습 서브루틴 외에 별도의 자유 매개변수가 없으며, 문제의 유효 작업 범위($T$)에 거의 선형적으로 비례하는 반복 횟수만 필요합니다.

## 📎 Related Works

- **전통적인 지도 학습 (Traditional Supervised Learning):** 분포 변화를 무시하여 $O(T^2\epsilon)$의 이차 오류 성장을 보입니다 (Ross and Bagnell, 2010).
- **Forward Training (Ross and Bagnell, 2010):** 비정상 정책을 순차적으로 훈련하여 선형 오류 성장을 달성하지만, $T$가 클 때 비실용적입니다.
- **SMILe (Ross and Bagnell, 2010), SEARN (Daumé III et al., 2009), CPI (Kakade and Langford, 2002):** 확률적(stochastic) 정상 정책을 훈련하여 선형 오류 성장을 목표로 하지만, 정책이 불안정하거나 특정 하이퍼파라미터 선택에 민감한 단점이 있습니다.
- **환원 기반 접근법 (Reduction-based approaches, Beygelzimer et al., 2005):** 기존의 지도 학습 알고리즘을 재사용하는 방법론입니다.
- **무회귀 온라인 학습 알고리즘 (No-regret online learning algorithms, Cesa-Bianchi et al., 2004; Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008):** DAGGER는 특히 Follow-The-Leader 알고리즘과 밀접하게 관련되어 있습니다.

## 🛠️ Methodology

DAGGER는 다음 단계를 반복적으로 수행하여 정상 결정론적 정책 $\hat{\pi}$를 학습합니다:

1. **초기화:** 빈 데이터셋 $D \leftarrow \emptyset$와 임의의 초기 정책 $\hat{\pi}_1$을 설정합니다.
2. **데이터 수집 및 집계 (반복 $i=1$ to $N$):**
   - **정책 실행:** 현재 학습된 정책 $\hat{\pi}_i$와 전문가 정책 $\pi^*$를 혼합한 정책 $\pi_i = \beta_i \pi^* + (1-\beta_i) \hat{\pi}_i$를 사용하여 $T$ 단계 궤적(trajectory)을 샘플링합니다. ($\beta_i$는 전문가 정책을 따를 확률을 제어하며, 보통 첫 번째 반복에서 $\beta_1=1$로 설정하고 이후 점차 감소시킵니다.)
   - **데이터셋 생성:** $\pi_i$에 의해 방문된 상태 $s$와 그 상태에서 전문가가 선택한 행동 $\pi^*(s)$로 구성된 데이터셋 $D_i = \{(s, \pi^*(s))\}$를 수집합니다.
   - **데이터셋 집계:** $D \leftarrow D \cup D_i$를 통해 $D_i$를 기존 데이터셋 $D$에 추가합니다.
   - **정책 훈련:** 집계된 데이터셋 $D$를 사용하여 전문가를 가장 잘 모방하는 새로운 분류기 $\hat{\pi}_{i+1}$을 훈련합니다.
3. **최종 정책 반환:** 검증 데이터셋에서 가장 좋은 성능을 보인 $\hat{\pi}_i$를 최종 정책으로 반환합니다.

DAGGER의 이론적 분석은 기본 Follow-The-Leader 알고리즘의 무회귀(no-regret) 속성에 기반하며, 단일 정책 아래의 미니배치 궤적을 단일 온라인 학습 예제로 취급하여 모방 학습을 무회귀 온라인 학습으로 환원합니다.

## 📊 Results

- **Super Tux Kart (3D 레이싱 게임):**
  - 성능 지표: 랩당 평균 이탈 횟수(falls per lap).
  - 결과: DAGGER($\beta_i = I(i=1)$)는 15회 반복 후 랩당 이탈 횟수를 0으로 줄여, SMILe(약 2회/랩) 및 지도 학습 기준선(약 4회/랩)보다 훨씬 뛰어난 성능을 보였습니다. DAGGER로 학습된 정책은 질적으로도 더 부드러웠습니다.
- **Super Mario Bros. (플랫폼 게임):**
  - 성능 지표: 죽거나 시간 초과되기 전까지 스테이지당 평균 이동 거리.
  - 결과: 지도 학습 접근법은 마리오가 장애물에 갇히는 등의 문제로 낮은 점수를 기록했습니다. 반면, DAGGER(D0, D0.5)는 SMILe 및 SEARN의 모든 $\alpha$ 선택보다 유의미하게 우수한 성능을 달성했으며, 특히 D0.5($p=0.5$)가 D0(indicator function)보다 약간 더 나은 결과를 보였습니다.
- **Handwriting Recognition (필기 인식, 구조적 예측):**
  - 성능 지표: 테스트 폴드에서의 문자 정확도.
  - 결과: 구조가 없는 기준선은 82%, 이전 문자를 정답으로 사용한 지도 학습은 83.6%였습니다. DAGGER는 정확도를 85.5%까지 향상시켰습니다. SEARN($\alpha=1$, 순수 정책 반복)도 이 실험에서는 DAGGER와 유사하게 매우 잘 수행되었는데, 이는 정책이 입력에 미치는 영향이 작기 때문으로 분석되었습니다.

## 🧠 Insights & Discussion

- **오류 누적 문제 해결:** DAGGER는 학습된 정책이 유도하는 상태 분포에서 데이터를 반복적으로 수집하고 이를 집계함으로써, 미래 관측치에 이전 행동이 영향을 미쳐 오류가 누적되는 근본적인 모방 학습 문제를 효과적으로 해결합니다. 이는 기존 지도 학습의 한계를 뛰어넘는 핵심적인 통찰입니다.
- **안정적이고 예측 가능한 정책:** DAGGER는 단일의 정상 결정론적 정책을 학습하므로, 여러 정책의 혼합(SMILe)이나 시점별 정책(Forward Training)에 비해 실제 적용에서 더 안정적이고 예측 가능한 제어를 제공합니다.
- **무회귀 학습과의 연결:** 모방 학습을 무회귀 온라인 학습 문제로 환원하는 이론적 틀은 DAGGER의 견고한 성능 보장을 설명하며, 더 나아가 다른 무회귀 온라인 학습 알고리즘의 적용 가능성을 제시합니다.
- **폭넓은 적용 가능성:** 모방 학습뿐만 아니라 구조적 예측 문제에도 성공적으로 적용됨으로써, 다양한 순차적 의사결정 문제에 대한 일반적인 접근 방식이 될 수 있음을 시사합니다.
- **한계 및 향후 연구:** 구조적 예측을 위한 더 정교한 디코딩 전략이나 Inverse Optimal Control 기술의 활용, 강화 학습에서의 유사한 데이터 집계 방법 연구가 향후 과제로 언급되었습니다. 현재 이론적 분석은 강하게 볼록한 손실 함수(strongly convex loss function) 또는 무회귀 방법에 대한 가정을 필요로 합니다.

## 📌 TL;DR

- **문제:** 모방 학습 및 순차적 예측은 i.i.d. 가정이 위반되어 오류가 누적되며, 기존 지도 학습 방법은 이러한 문제를 해결하지 못해 $O(T^2\epsilon)$의 성능 한계를 보입니다.
- **방법:** DAGGER(Dataset Aggregation)는 반복적으로 현재 학습된 정책과 전문가 정책을 혼합하여 데이터를 수집하고 이를 누적된 데이터셋에 추가하여 새로운 결정론적 정책을 훈련하는 알고리즘입니다. 이는 무회귀 온라인 학습으로 환원될 수 있습니다.
- **결과:** DAGGER는 유도된 상태 분포에서 $O(T\epsilon)$의 선형 오류 성장을 보장하는 강력한 이론적 기반을 가지며, Super Tux Kart, Super Mario Bros.와 같은 도전적인 모방 학습 문제 및 필기 인식 문제에서 SMILe, SEARN 등 기존 최첨단 방법들을 일관되게 능가하는 우수하고 안정적인 성능을 입증했습니다.
