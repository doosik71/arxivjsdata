# Active Imitation Learning via Reduction to I.I.D. Active Learning

Kshitij Judah, Alan P. Fern, Thomas G. Dietterich

## 🧩 Problem to Solve

표준 수동 모방 학습(passive imitation learning)은 전문가의 완전한 행동 궤적(trajectory)을 관찰하여 목표 정책을 학습합니다. 그러나 이러한 궤적을 생성하는 것은 상당한 전문가의 노력을 필요로 하며 어떤 경우에는 비현실적일 수 있습니다. 본 연구는 이러한 노력을 줄이기 위해 학습자가 환경 시뮬레이터와의 상호작용 및 과거 쿼리 답변을 기반으로 개별 상태에서 전문가에게 원하는 행동을 질의하는 **능동 모방 학습(active imitation learning)** 문제를 다룹니다.

주요 도전 과제는 i.i.d. 능동 학습(active learning) 기법을 직접 적용하기 어렵다는 것입니다. i.i.d. 능동 학습은 비표지(unlabeled) 데이터에 대한 목표 분포에 접근할 수 있다고 가정하지만, 능동 모방 학습에서는 학습자가 알 수 없는 전문가 정책에 의해 유도되는 목표 상태 분포에 직접 접근할 수 없습니다. 따라서 전문가가 거의 방문하지 않거나 전혀 방문하지 않는 비유용한 상태에 대한 질의로 이어져 성능 저하 및 전문가의 낭비된 노력을 초래할 수 있습니다.

## ✨ Key Contributions

- **i.i.d. 능동 학습으로의 환원 프레임워크 제안:** 능동 모방 학습을 i.i.d. 능동 학습 문제로 환원(reduction)하는 새로운 접근 방식을 제안하여, i.i.d. 설정에서의 발전을 모방 학습에 활용할 수 있도록 합니다.
- **이론적 레이블 복잡도 분석:** 비정상(non-stationary) 및 정상(stationary) 정책 모두에 대한 환원 알고리즘의 레이블 복잡도(expert 쿼리 수)를 분석했습니다. 이를 통해 능동 모방 학습의 레이블 복잡도가 수동 학습보다 **상당히 적을 수 있음**을 이론적으로 증명했습니다 (예: 실현 가능(realizable) 학습 시 $\frac{1}{\epsilon}$에 대한 의존성에서 지수적 개선).
- **실용적인 알고리즘 RAIL-DW 제안:** 이론적 환원으로부터 영감을 받은 **RAIL-DW (Reduction-based Active Imitation Learning - Density Weighted)** 라는 실용적인 알고리즘을 제안했습니다. 이 알고리즘은 기존 대안들과 비교하여 4가지 테스트 도메인에서 높은 효과를 보였습니다.

## 📎 Related Works

- **i.i.d. 능동 학습:** Settles (2009) 등 폭넓게 연구되었지만, 순차적 의사 결정(sequential decision making)에 대한 적용은 미미합니다.
- **강화 학습(RL)을 위한 능동 학습:** Clouse (1996) 등의 연구가 있지만, 본 연구는 보상 신호가 없는 모방 학습에 중점을 둡니다.
- **역강화 학습(IRL) 기반 모방 학습:** Ng & Russell (2000) 등이 제안했으며, 보상 함수를 학습합니다. 능동 IRL 연구도 있었으나 (Lopes et al., 2009; Cohn et al., 2010), 도메인 역학(dynamics)과 효율적인 플래너(planner)가 필요하여 확장성에 한계가 있습니다. 본 연구는 보상 함수 대신 정책을 직접 학습하는 **직접 모방(direct imitation)** 프레임워크를 따릅니다.
- **직접 모방 프레임워크의 능동 학습:**
  - **신뢰도 기반 자율성(Confidence-based autonomy, CBA):** Chernova & Veloso (2009), Grollman & Jenkins (2007)는 정책 실행 중 불확실할 때 전문가에게 질의합니다. 임계값 설정의 어려움이 있습니다.
  - **Ross & Bagnell (2010); Ross et al. (2011)의 접근:** 정책 실행 중 전문가에게 능동적으로 질의하며, 수동 모방 학습보다 이론적으로 나은 성능 보장을 보입니다. 그러나 공격적인 질의 방식으로 인해 인간 전문가에게는 비실용적일 수 있습니다. 본 연구는 전문가의 **레이블링 노력 최소화**에 초점을 맞춥니다.

## 🛠️ Methodology

본 연구는 능동 모방 학습을 i.i.d. 능동 학습으로 환원하는 이론적 접근 방식과 이를 기반으로 한 실용적인 알고리즘을 제시합니다. 학습자는 전문가 쿼리에 대한 접근 외에 MDP 시뮬레이터에 접근할 수 있다고 가정합니다.

**1. 이론적 환원:**

- **비정상 정책 (Non-Stationary Policies) 학습:**

  - Ross & Bagnell (2010)의 "능동 전방 훈련(Active Forward Training)" 알고리즘을 활용합니다.
  - 각 반복 $t$에서, 학습자는 이전에 학습된 정책 $\hat{\pi}_{t-1}$에 의해 유도된 상태 분포 $d_t^{\hat{\pi}_{t-1}}$에서 비표지 데이터를 샘플링하고, 이를 사용하여 i.i.d. 능동 학습자 $L_a$를 호출하여 $\hat{\pi}_t$를 학습합니다.
  - $d_t^{\hat{\pi}_{t-1}}$에서 샘플링하는 데는 전문가 쿼리가 필요하지 않으므로, i.i.d. 능동 학습의 쿼리 절약을 모방 학습으로 이전할 수 있습니다.
  - 레이블 복잡도: $T \cdot N_a(\frac{\epsilon}{T^2}, \frac{\delta}{T})$.

- **정상 정책 (Stationary Policies) 학습 (RAIL - Reduction-based Active Imitation Learning):**
  - RAIL은 $T$번 반복하며, 각 반복에서 새로운 정상 정책 $\hat{\pi}_t$를 학습합니다.
  - 반복 $t+1$에서, $\hat{\pi}_{t+1}$는 이전 정책 $\hat{\pi}_t$에 의해 유도된 상태 분포 $d^{\hat{\pi}_t}$에 대해 전문가 행동을 예측하는 낮은 오류율을 달성하도록 학습됩니다.
  - 수식적으로: $\hat{\pi}_t = L_a(\frac{\epsilon}{T}, \frac{\delta}{T}, d^{\hat{\pi}_{t-1}})$.
  - $d^{\hat{\pi}_{t-1}}$ 분포로부터 비표지 데이터를 샘플링하는 데 시뮬레이터가 사용되며, 전문가 쿼리는 필요하지 않습니다.
  - 이론적 후회(regret) 상한: $V(\pi^*) - V(\hat{\pi}_T) \le \epsilon T^3$.
  - 레이블 복잡도: $T \cdot N_a(\frac{\epsilon}{T^3}, \frac{\delta}{T})$. i.i.d. 능동 학습의 레이블 복잡도 개선이 능동 모방 학습으로 이어짐을 보여줍니다.

**2. 실용적인 RAIL-DW (Density-Weighted) 알고리즘:**

- **점진적 접근(Incremental Approach):** 각 반복마다 하나의 쿼리만 요청하고 데이터를 누적하여, 초기 부정확한 분포에 쿼리 예산을 낭비하지 않고 이후 더 정확한 분포에 집중할 수 있도록 합니다.
- **베이시안 분류기 기반 $d_D^t$:** 정책 $\hat{\pi}_t$에 대한 점 추정(point estimate) 대신, 수집된 데이터 $D_t$를 기반으로 정책 클래스 $H$에 대한 후방 분포 $P(\hat{\pi}|D_t)$를 정의합니다. 이를 통해 $d_D^t = E_{\hat{\pi} \sim P(\hat{\pi}|D_t)}[d_{\hat{\pi}}(s)]$라는 기대 비표지 상태 분포를 사용하여 $d_{\pi^*}$를 더 효과적으로 추정합니다.
- **밀도 가중치 질의-위원회(Density-Weighted Query-by-Committee, QBC):**
  - i.i.d. 능동 학습 알고리즘으로 밀도 가중치 QBC를 사용합니다.
  - 이는 상태 밀도(state density)와 위원회 불일치(committee disagreement)의 곱을 최대화하는 상태를 질의로 선택합니다.
  - 비표지 상태의 밀도 추정에는 간단한 거리 기반 비닝(binning)을 사용합니다.
- **$d_D^t$로부터의 샘플링:** 부트스트랩 샘플링(bagging)을 통해 근사합니다. $K$개의 부트스트랩 샘플을 생성하고, 각 샘플로부터 정책을 학습한 후 $K$개의 궤적을 생성하여 이 궤적상의 상태들을 비표지 데이터셋으로 활용합니다 ($K=5$).

## 📊 Results

RAIL-DW는 네 가지 도메인(Cart-pole, Bicycle balancing, Wargus, NETtalk)에서 다음 기준선들과 비교되었습니다: Passive, unif-QBC (균일 분포 가정), unif-RAND (균일 무작위 쿼리), CBA (Confidence-based autonomy).

- **RAIL-DW의 우수한 성능:** 모든 도메인에서 RAIL-DW가 가장 강력하고 효과적인 능동 모방 학습 접근 방식임을 입증했습니다.
  - 예: Cart-pole에서 RAIL-DW는 30-35개의 쿼리로 최적 성능에 도달한 반면, Passive는 100개의 쿼리가 필요했습니다.
- **상태 분포 활용의 중요성:** unif-QBC 및 unif-RAND는 실제 데이터 분포를 무시하여 Cart-pole 및 Bicycle과 같은 도메인에서 저조한 성능을 보였습니다. 이는 전문가 정책과 관련 없는 비유용한 상태(예: 회복 불가능한 상태)에 많은 쿼리를 낭비하기 때문입니다.
- **CBA의 한계:** CBA는 빨리 학습하지만, 때때로 임계값 조정 메커니즘의 민감성으로 인해 조기에 쿼리를 중단하여 (Cart-pole, NETtalk) 최적이 아닌 성능에 머무르거나 (Wargus) 개선에 실패했습니다.
- **RAIL 내 밀도 가중치의 중요성:** RAIL-DW가 표준 QBC를 사용하는 RAIL-QBC 및 무작위 선택을 사용하는 RAIL-RAND보다 훨씬 뛰어난 성능을 보였습니다. 이는 i.i.d. 능동 학습자가 각 반복에서 RAIL에 의해 추정된 상태 밀도 정보를 활용하는 것이 중요함을 보여줍니다.

## 🧠 Insights & Discussion

- **능동 모방 학습의 효율성:** 본 연구는 능동 모방 학습이 수동 모방 학습에 비해 전문가의 레이블링 노력을 크게 줄일 수 있음을 이론적, 실증적으로 증명했습니다. 특히 상태 밀도 정보를 효과적으로 활용하는 능동 학습자가 이점을 극대화할 수 있습니다.
- **i.i.d. 능동 학습 발전의 활용:** 능동 모방 학습을 i.i.d. 능동 학습으로 환원하는 프레임워크는 i.i.d. 설정에서 개발된 진보된 능동 학습 기법들을 모방 학습 문제에 적용할 수 있는 길을 엽니다.
- **전문가 상태 분포 문제 해결:** RAIL 및 RAIL-DW의 핵심 기여는 전문가의 알 수 없는 상태 분포 문제에 대처하기 위해 학습자 자체의 유도된 상태 분포를 비표지 풀로 반복적으로 사용하고, RAIL-DW에서는 베이시안 접근 방식을 통해 이를 더욱 정교하게 추정한다는 것입니다.
- **단순한 i.i.d. 능동 학습 적용의 비효율성:** 순차적 특성과 유도된 상태 분포를 고려하지 않고 i.i.d. 능동 학습을 모방 학습에 직접 적용하는 것은 효과적이지 않습니다.
- **실용적 고려사항 반영:** RAIL-DW는 이상적인 RAIL 알고리즘의 초기 반복에서의 비효율적인 쿼리 할당 및 데이터 공유 부족과 같은 실용적인 문제점을 개선하여, 쿼리를 더 관련성 높은 상태에 집중시킬 수 있습니다.

## 📌 TL;DR

**문제:** 기존 수동 모방 학습은 전문가의 완전한 행동 궤적에 대한 비싼 수집 비용이 필요합니다. 능동 모방 학습은 학습자가 선택한 상태에서만 전문가에게 질의하여 비용을 절감하고자 하지만, 전문가의 행동이 유도하는 상태 분포를 알 수 없어 어떤 상태를 질의할지 결정하기 어렵습니다.

**제안 방법:** 본 논문은 능동 모방 학습을 i.i.d. 능동 학습 문제로 환원하는 프레임워크를 제안합니다.

1. **이론적 환원:** 비정상 및 정상 정책에 대해 능동 학습을 통해 수동 학습보다 레이블 복잡도를 크게 줄일 수 있음을 이론적으로 분석했습니다. 특히, 시뮬레이터를 활용하여 학습자의 현재 정책이 유도하는 비표지 상태를 생성하고, 이를 i.i.d. 능동 학습 알고리즘의 입력으로 사용합니다.
2. **실용 알고리즘 (RAIL-DW):** **밀도 가중치 질의-위원회(Density-Weighted Query-by-Committee)** 기반의 능동 학습 알고리즘인 RAIL-DW를 제안합니다. 이 알고리즘은 점진적으로 데이터를 수집하고, 베이시안 후방 분포를 사용하여 비표지 상태 분포를 더 정확하게 추정하며, 상태 밀도와 불확실성을 동시에 고려하여 질의할 상태를 선택합니다.

**핵심 결과:** Cart-pole, Bicycle, Wargus, NETtalk의 네 가지 도메인에서 실험한 결과, RAIL-DW는 다른 수동 및 능동 학습 기준선들보다 **훨씬 적은 전문가 질의만으로 뛰어난 성능**을 달성했습니다. 특히, i.i.d. 능동 학습 알고리즘이 상태 밀도 정보를 효과적으로 활용하는 것이 능동 모방 학습의 성능에 결정적임을 보여주었습니다.
