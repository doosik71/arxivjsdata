# Generative Adversarial Imitation Learning

Jonathan Ho, Stefano Ermon

## 🧩 Problem to Solve

본 논문은 전문가 행동 예시(expert demonstrations)로부터 정책을 학습하는 모방 학습(imitation learning)의 특정 설정을 다룹니다. 특히, 학습자는 전문가의 궤적 샘플만 제공받으며, 훈련 중 전문가에게 추가 데이터를 질의하거나 어떠한 종류의 강화 신호도 제공받지 못하는 상황입니다.

기존 접근 방식은 두 가지입니다:

1. **행동 복제(Behavioral Cloning, BC):** 전문가 궤적의 상태-행동 쌍에 대한 지도 학습 문제로 정책을 학습합니다. 단순하지만, 공변량 이동(covariate shift)으로 인한 복합 오류(compounding error) 때문에 많은 데이터가 필요합니다.
2. **역 강화 학습(Inverse Reinforcement Learning, IRL):** 전문가가 유일하게 최적인 비용 함수를 찾은 다음, 이 비용 함수를 사용하여 강화 학습을 통해 정책을 추출합니다. 복합 오류 문제를 피하지만, 내부 루프에서 강화 학습을 반복해야 하므로 계산 비용이 매우 비싸고 간접적입니다.

본 논문은 IRL의 간접적이고 느린 방식을 우회하여, 강화 신호 없이도 전문가 데이터로부터 직접 정책을 추출하는 빠르고 효율적인 일반 프레임워크와 알고리즘을 제안하고자 합니다.

## ✨ Key Contributions

- **새로운 모방 학습 프레임워크 제안:** IRL의 중간 단계를 우회하여 데이터로부터 정책을 직접 학습하는 일반 프레임워크를 제시합니다. 이는 강화 학습이 역 강화 학습을 따른 결과와 동일하게 작동하는 정책을 직접 학습합니다.
- **IRL과 GAN의 연결 고리 발견:** 제안된 프레임워크의 특정 구현이 모방 학습과 생성적 적대 신경망(Generative Adversarial Networks, GAN) 사이에 강력한 유추 관계를 형성함을 보입니다.
- **GAIL (Generative Adversarial Imitation Learning) 알고리즘 개발:** 모델-프리(model-free) 모방 학습 알고리즘인 GAIL을 제안합니다. 이 알고리즘은 전문가와 학습된 정책 간의 점유 측정(occupancy measure)의 젠슨-섀넌(Jensen-Shannon) 발산($D_{\text{JS}}$)을 최소화하도록 설계되었습니다.
- **우수한 성능 입증:** 복잡하고 고차원적인 물리 기반 제어 환경에서 기존 모델-프리 방법(행동 복제, 특징 기대치 매칭, 게임 이론적 도제 학습) 대비 현저한 성능 향상을 달성했습니다.
- **수학적 특성화:** 최대 인과 엔트로피 IRL(maximum causal entropy IRL)이 학습하는 비용 함수에서 강화 학습을 실행하여 얻는 정책이 점유 측정 일치 문제를 푼다는 것을 증명했습니다 ($\text{Proposition 3.2}$).

## 📎 Related Works

- **행동 복제 (Behavioral Cloning, BC) [20]:** 전문가의 상태-행동 쌍을 지도 학습하여 정책을 모방하는 방식. 간단하지만 공변량 이동(covariate shift)에 취약합니다.
- **역 강화 학습 (Inverse Reinforcement Learning, IRL) [25, 18, 31, 32]:** 전문가의 행동을 설명하는 비용 함수를 찾아냅니다. 일반적으로 내부 루프에서 강화 학습을 수행해야 하므로 계산 비용이 높습니다.
- **도제 학습 (Apprenticeship Learning) [1, 29, 28]:** 학습된 정책이 특정 비용 함수 클래스에서 전문가보다 나은 성능을 보이도록 합니다. 특징 기대치 매칭(feature expectation matching) [1] 및 게임 이론적 도제 학습(game-theoretic apprenticeship learning) [28] 등이 있습니다.
- **생성적 적대 신경망 (Generative Adversarial Networks, GANs) [9]:** 생성자(generator)와 판별자(discriminator)가 적대적으로 학습하며 데이터 분포를 모방하는 딥러닝 기법입니다. 본 논문의 GAIL 알고리즘에 영감을 주었습니다.
- **신뢰 영역 정책 최적화 (Trust Region Policy Optimization, TRPO) [26]:** 정책 업데이트의 안정성을 보장하기 위해 사용되는 강화 학습 알고리즘으로, GAIL의 정책 업데이트 단계에 활용됩니다.
- **유도 비용 학습 (Guided Cost Learning, GCL) [7]:** 모델 기반 역 최적 제어(inverse optimal control) 기법으로, 정책 최적화와 비용 학습을 교대로 수행합니다.

## 🛠️ Methodology

본 논문의 핵심 방법론은 다음과 같습니다.

1. **유도된 최적 정책 특성화:**

   - 최대 인과 엔트로피 IRL(maximum causal entropy IRL)과 일반적인 볼록 비용 함수 정규화자($\psi$)를 통해 학습된 비용 함수에 대한 강화 학습으로 얻어지는 정책을 수학적으로 특성화합니다.
   - **Proposition 3.2**는 이러한 정책이 다음 최적화 문제를 푼다는 것을 보여줍니다:
     $$ \min*{\pi \in \Pi} -H(\pi) + \psi^\*(\rho*{\pi} - \rho*{\pi_E}) $$
        여기서 $H(\pi)$는 정책 $\pi$의 인과 엔트로피, $\rho*{\pi}$는 정책 $\pi$의 점유 측정, $\rho_{\pi_E}$는 전문가 정책 $\pi_E$의 점유 측정이며, $\psi^*$는 $\psi$의 볼록 켤레(convex conjugate)입니다. 이 식은 학습된 정책의 점유 측정이 전문가의 점유 측정에 가깝도록 만드는 것을 목표로 합니다.
   - $\psi$가 상수 함수인 경우, 결과 정책은 전문가의 점유 측정과 정확히 일치한다는 것을 **Corollary 3.2.1**을 통해 보입니다.

2. **새로운 비용 정규화자 $\psi_{\text{GA}}$ 도입:**

   - 점유 측정 일치를 부드럽게 페널티화하면서도, 기존 도제 학습 알고리즘의 제한적인 비용 클래스($C_{\text{linear}}$, $C_{\text{convex}}$)보다 더 표현력이 풍부한 새로운 비용 정규화자 $\psi_{\text{GA}}$를 제안합니다 (Eq. 13).
   - $\psi_{\text{GA}}$의 볼록 켤레 $\psi^*_{\text{GA}}$는 전문가와 학습된 정책의 상태-행동 쌍을 구별하는 이진 분류 문제의 최적 음의 로그 손실과 동일함을 보입니다 (Eq. 14). 이는 젠슨-섀넌 발산($D_{\text{JS}}(\rho_{\pi}, \rho_{\pi_E})$)과 상수 차이만 있습니다.
   - 결과적으로, GAIL의 최적화 목표는 다음과 같습니다:
     $$ \min*{\pi} D*{\text{JS}}(\rho*{\pi}, \rho*{\pi_E}) - \lambda H(\pi) $$
     이는 정책의 인과 엔트로피를 정규화하면서 점유 측정 간의 젠슨-섀넌 발산을 최소화합니다.

3. **GAIL (Generative Adversarial Imitation Learning) 알고리즘 (Algorithm 1):**
   - 이 최적화 문제는 생성적 적대 신경망(GAN)의 프레임워크와 유사하게, 정책($\pi_{\theta}$, 생성자 역할)과 판별자($D_w$, 비용 함수 역할) 간의 새들 포인트(saddle point) 문제를 푸는 방식으로 해결됩니다.
   - **알고리즘 단계:**
     1. 정책 매개변수 $\theta$와 판별자 매개변수 $w$를 초기화합니다.
     2. **판별자 업데이트:** 현재 정책 $\pi_{\theta_i}$에서 궤적 $\tau_i$를 샘플링하고, 전문가 궤적 $\tau_E$와 함께 판별자 $D_w$를 업데이트합니다. 판별자는 생성된 데이터와 실제 전문가 데이터를 구분하도록 학습됩니다. Adam 최적화와 같은 경사 상승(gradient ascent) 방법을 사용합니다 (Eq. 17).
     3. **정책 업데이트:** 판별자 $D_{w_{i+1}}$가 제공하는 비용 함수 $\log(D_{w_{i+1}}(s,a))$를 기반으로 정책 $\pi_{\theta}$를 업데이트합니다. 정책은 판별자를 속이도록 (즉, $D(s,a)$가 0.5에 가까워지도록) 학습되며, 동시에 인과 엔트로피를 최대화합니다. 정책 업데이트에는 안정성을 위해 TRPO(Trust Region Policy Optimization)를 사용합니다 (Eq. 18).
   - 정책과 판별자 모두 신경망으로 근사하며, 모델-프리 방식으로 환경과 상호작용하여 데이터를 수집합니다.

## 📊 Results

GAIL 알고리즘은 9가지 물리 기반 제어 태스크(Cartpole, Acrobot, Mountain Car, Reacher, HalfCheetah, Hopper, Walker, Ant, Humanoid)에서 평가되었으며, 다음과 같은 결과를 보였습니다:

- **전반적인 우수성:** GAIL은 대부분의 태스크, 특히 복잡한 고차원 MuJoCo 환경에서 행동 복제(BC), 특징 기대치 매칭(FEM), 게임 이론적 도제 학습(GTAL) 등 모든 기준선(baseline) 방법보다 훨씬 뛰어난 성능을 보였습니다.
- **전문가 데이터 효율성:** GAIL은 테스트된 거의 모든 데이터셋 크기에서 최소 70% 이상의 전문가 성능을 달성했으며, Humanoid와 같은 어려운 태스크에서는 모든 데이터셋 크기에서 정확히 전문가 성능을 모방했습니다.
- **기준선의 한계:**
  - BC는 고차원 태스크에서 만족스러운 성능을 얻기 위해 훨씬 더 많은 데이터가 필요했으며, Humanoid에서는 60% 이상의 성능을 달성하지 못했습니다.
  - FEM과 GTAL은 Ant 태스크에서 무작위 정책보다도 못한 성능을 보였습니다.
- **인과 엔트로피 정규화 ($\lambda$):** Reacher 태스크에서 $\lambda=0$에서 $\lambda=10^{-3}$으로 변경했을 때 성능 향상이 통계적으로 유의미함을 보였습니다. 이는 정책 다양성을 유지하는 데 도움이 될 수 있음을 시사합니다.

## 🧠 Insights & Discussion

- **직접적인 정책 학습의 중요성:** GAIL은 IRL의 복잡한 비용 함수 역추정 없이도 전문가의 행동을 직접 모방하는 정책을 효과적으로 학습할 수 있음을 보여줍니다. 이는 모방 학습의 효율성을 크게 개선하는 중요한 통찰입니다.
- **GAN 프레임워크의 힘:** GAN의 적대적 훈련 메커니즘을 모방 학습에 적용함으로써, 복잡한 전문가 행동 분포를 학습하는 데 매우 강력한 도구가 될 수 있음을 입증했습니다. 판별자는 정책이 따라야 할 암묵적인 비용 함수 역할을 합니다.
- **전문가 데이터 효율성:** GAIL은 전문가 데이터 샘플 측면에서 일반적으로 효율적입니다. 이는 실제 적용에서 전문가 데이터를 얻기 어려운 경우 큰 장점이 됩니다.
- **환경 상호작용 샘플 효율성:** 현재 GAIL은 훈련 중 환경 상호작용 샘플 측면에서는 TRPO와 유사한 수준으로, 아직 최적화의 여지가 있습니다. 행동 복제(BC)로 정책 매개변수를 초기화하는 방식으로 학습 속도를 크게 개선할 수 있을 것으로 예상됩니다.
- **모델-프리 방식:** GAIL은 환경 역학 모델을 필요로 하지 않는 모델-프리 방식입니다. 이는 모델 기반 방식(예: Guided Cost Learning)이 가지는 모델 근사화의 제약에서 자유롭다는 장점이 있습니다.
- **향후 연구:** 잘 선택된 환경 모델과 전문가와의 상호작용을 결합한 방법이 전문가 데이터 및 환경 상호작용 모두에서 궁극적인 샘플 효율성을 달성할 수 있을 것이라는 점을 시사하며, 이는 GAIL의 확장 가능성을 열어줍니다.

## 📌 TL;DR

**문제:** 강화 신호나 전문가 상호작용 없이 전문가 행동을 모방하는 것은 기존 행동 복제(compounding error) 또는 역 강화 학습(계산 비용) 방식의 한계로 인해 어렵습니다.

**방법:** 본 논문은 **GAIL(Generative Adversarial Imitation Learning)**을 제안합니다. 이는 모방 학습을 생성적 적대 신경망(GAN) 프레임워크에 접목한 모델-프리 방법입니다. GAIL은 정책(생성자)을 직접 학습하여 전문가의 상태-행동 점유 측정과 일치시키고, 이 과정은 학습된 행동과 전문가 행동을 구별하려는 판별자(비용 함수)의 지도를 받습니다. 이는 새로운 비용 정규화자를 통해 젠슨-섀넌 발산을 최소화하는 IRL의 간접 단계를 우회하는 방식으로 이론적으로 유도됩니다.

**발견:** GAIL은 다양한 복잡하고 고차원적인 제어 태스크에서 기존 모델-프리 모방 학습 방법들보다 훨씬 뛰어난 성능을 보였으며, 높은 전문가 데이터 효율성을 입증했습니다.
