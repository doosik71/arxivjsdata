# Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations

Daniel S. Brown, Wonjoon Goo, Scott Niekum

## 🧩 해결할 문제

기존 모방 학습(Imitation Learning, IL) 방법론은 일반적으로 시연자(demonstrator)의 성능을 뛰어넘기 어렵다는 근본적인 한계가 있습니다. 시연자보다 우수한 성능(Better-than-Demonstrator, BtD)을 달성할 수 있음을 보여주는 최근 연구 결과들이 있지만, 이러한 성능 향상이 언제 가능한지에 대한 이론적 조건이 부족했습니다. 또한, BtD 성능을 위해 필수적인 '순위가 매겨진 시연(ranked demonstrations)'을 얻는 것은 수고롭고 오류가 발생하기 쉬워, 사람의 선호도나 추가적인 보상 정보 없이 활용하기 어렵다는 문제가 있습니다.

## ✨ 주요 기여

- **이론적 조건 제시**: 역 강화 학습(Inverse Reinforcement Learning, IRL) 설정에서 시연자보다 우수한 성능을 달성하기 위한 충분 조건(sufficient condition)을 이론적으로 규명했습니다.
- **보상 함수 모호성 감소**: 순위가 매겨진 시연이 보상 함수(reward function)의 모호성을 효과적으로 줄여 더 정확한 보상 학습으로 이어진다는 이론적 결과를 제시했습니다.
- **D-REX(Disturbance-based Reward Extrapolation) 제안**: 행동 복제(Behavioral Cloning, BC)로 학습된 정책에 노이즈를 주입하여 다양한 성능 수준의 시연을 자동으로 생성하고 순위를 매기는 새로운 모방 학습 방법론을 소개했습니다.
- **자동 순위 기반 IRL 적용**: D-REX는 인간의 선호도 라벨이나 보상 정보와 같은 추가적인 감독 없이도 순위 기반 IRL을 기존 모방 학습 환경에 적용할 수 있게 합니다.
- **시연자 성능 초과**: 시뮬레이션 로봇 및 Atari 게임 벤치마크에서 D-REX가 기존 모방 학습 방식보다 훨씬 우수하며, 시연자의 성능을 **유의미하게 능가**함을 실증적으로 입증했습니다. 이는 추가적인 정보 없이 BtD 성능을 달성한 최초의 모방 학습 방법입니다.

## 📎 관련 연구

- **기존 모방 학습**: 시연자의 성능에 상한선이 있는 경우가 많으며, BtD 성능을 위해서는 수동으로 제작된 보상 함수 $[1,2,3,4]$나 인간 감독자의 개입 $[5,6,7]$이 필요했습니다.
- **역 강화 학습 (IRL)**: 시연자의 의도를 추정하여 보상 함수를 추론합니다 $[8]$. 부분적으로 최적화되지 않은 시연으로부터 학습하는 연구도 있었으나, 수동 클러스터링 $[21]$ 또는 라벨링 $[22]$이 필요했습니다.
- **선호도 학습 (Preference Learning)**: 쌍별 선호도 라벨을 수집하거나 $[6,9,10,11]$ 사전에 순위가 매겨진 시연 $[7,27]$을 활용하여 BtD 정책을 학습하는 연구가 있었습니다. 특히, T-REX $[7]$는 순위가 매겨진 시연을 사용하여 시연자보다 뛰어난 성능을 보였으나, 이론적 근거가 부족하고 인간의 순위 부여가 필수적이었습니다.
- **노이즈/랜덤 궤적 활용**: Boularias et al. $[28]$ 및 Kalakrishnan et al. $[29]$은 노이즈 궤적을 사용하여 Maximum Entropy IRL의 파티션 함수를 추정했습니다. DART $[31]$는 행동 복제 시 노이즈를 추가하여 풍부한 상태-액션 쌍을 수집합니다. 본 논문은 이와 달리 다양한 노이즈 수준을 주입하여 **자동으로 순위가 매겨진 시연**을 생성합니다.

## 🛠️ 방법론

D-REX는 순위가 매겨지지 않은 시연으로부터 자동으로 순위를 생성하여 시연자보다 뛰어난 성능을 달성하는 것을 목표로 합니다.

1. **행동 복제(Behavioral Cloning, BC)**: 주어진 시연 $\mathcal{D}$에 대해 행동 복제를 수행하여 정책 $\pi_{\text{BC}}$를 학습합니다. $\pi_{\text{BC}}$는 시연자의 평균 성능과 유사하거나 약간 나은 수준의 정책입니다.
2. **노이즈 주입을 통한 궤적 생성**: 다양한 노이즈 수준 $\epsilon \in \mathcal{E}$에 따라 $\epsilon$-greedy 정책인 $\pi_{\text{BC}}(\cdot | \epsilon)$를 사용하여 K개의 궤적을 생성합니다. 여기서 $\epsilon$은 정책 $\pi_{\text{BC}}$에 따라 행동을 선택할 확률을 $(1-\epsilon)$로, 무작위 행동을 선택할 확률을 $\epsilon$으로 정의합니다. 노이즈 수준이 높을수록 성능이 저하된다는 가정에 기반합니다.
3. **자동 순위 부여**: 생성된 궤적들은 노이즈 수준에 따라 자동으로 순위가 매겨집니다. 예를 들어, $\epsilon_i > \epsilon_j$라면, $\pi_{\text{BC}}(\cdot | \epsilon_i)$에서 생성된 궤적 $\tau_i$는 $\pi_{\text{BC}}(\cdot | \epsilon_j)$에서 생성된 궤적 $\tau_j$보다 선호도가 낮다고 간주합니다 (즉, $\tau_i \prec \tau_j$).
4. **보상 함수 학습 (T-REX)**: 자동으로 순위가 매겨진 시연 $\mathcal{D}_{\text{ranked}}$을 사용하여 T-REX $[7]$ 알고리즘으로 보상 함수 $\hat{R}_{\theta}(s)$를 학습합니다. T-REX는 쌍별 순위 손실(pairwise ranking loss)을 사용하여 보상 함수를 추론합니다.
5. **정책 최적화 (RL)**: 학습된 보상 함수 $\hat{R}_{\theta}(s)$를 기반으로 강화 학습(예: PPO $[44]$)을 사용하여 최적화된 정책 $\hat{\pi}$를 훈련합니다.

## 📊 결과

- **노이즈 기반 자동 순위 유효성 검증**: 시뮬레이션된 로봇(MuJoCo) 및 Atari 게임 벤치마크에서 노이즈 주입 시 $\epsilon$ 값이 증가함에 따라 정책 성능이 단조적으로 감소하는 경향을 보여, 자동 순위 생성의 전제가 유효함을 입증했습니다.
- **보상 외삽 정확도**: D-REX가 학습한 보상 함수 $\hat{R}_{\theta}$는 실제 보상(ground-truth returns)과 강력한 상관관계를 가지며, 훈련에 사용되지 않은 궤적에 대해서도 잘 외삽(extrapolate)됨을 확인했습니다. 이는 D-REX가 의미 있는 보상 특징을 학습한다는 것을 시사합니다.
- **시연자 성능 초과**: D-REX로 최적화된 정책은 Pong을 제외한 9개 과제 중 8개에서 시연자의 최고 성능을 뛰어넘었습니다.
  - MuJoCo 로봇 과제 (Hopper, HalfCheetah): 시연자 대비 77%(Hopper), 418%(HalfCheetah)의 성능 향상을 보였습니다.
  - Atari 과제: Q\*Bert를 제외하고 평균 39%의 성능 향상을 달성했습니다.
- **SOTA 모방 학습 방식 능가**: 대부분의 과제에서 행동 복제(BC) 및 GAIL $[47]$과 같은 최신 모방 학습 알고리즘보다 뛰어난 성능을 보였습니다.
- **향상된 최악의 경우 성능**: D-REX는 시연자나 다른 모방 학습 알고리즘보다 더 안전한(worst-case performance가 더 좋은) 정책을 학습할 수 있음을 보여주었습니다.
- **단순 보상 이상 학습**: 단순히 오래 살아남는 것에 대한 보상(live-long baseline)보다 D-REX가 훨씬 더 복잡하고 의미 있는 게임 플레이 전략을 학습한다는 것을 확인했습니다.

## 🧠 통찰 및 논의

- **이론적 근거의 중요성**: 이 연구는 시연자의 성능을 뛰어넘는 모방 학습이 가능한 시점에 대한 명확한 이론적 조건을 제공하며, 순위가 매겨진 시연이 보상 함수 모호성을 줄이는 데 핵심적인 역할을 한다는 점을 강조합니다.
- **추가 감독 없는 외삽 능력**: D-REX는 인간의 선호도, 보상, 또는 추가적인 사이드 정보 없이도 시연자의 성능을 유의미하게 능가하는 최초의 모방 학습 접근 방식이라는 점에서 중요합니다. 이는 실용적인 모방 학습 적용 가능성을 크게 확장합니다.
- **자동화의 파급력**: 노이즈 주입을 통한 자동 순위 생성은 순위 기반 학습 기술 $[41,51]$을, 순위가 없는 시연만 존재하는 표준 모방 학습 도메인에 적용할 수 있는 길을 열어줍니다.
- **보상 함수의 의미론적 학습**: D-REX가 학습하는 보상 함수가 실제 보상과 높은 상관관계를 가지며, 특정 게임에서는 "위험한 상황 회피"와 같은 의미 있는 특징을 학습하여 시연자보다 더 나은 전략을 구사하게 돕는다는 점은 흥미로운 통찰입니다.
- **한계**: Pong 게임에서는 D-REX가 다른 방법들보다 성능이 낮았으며, "아무것도 하지 않는 것이 최적인" 매우 위험한 시나리오에 대한 연구는 향후 과제로 남습니다.

## 📌 TL;DR

기존 모방 학습의 시연자 성능 한계를 극복하기 위해, 본 연구는 **노이즈 주입을 통해 시연 궤적을 자동으로 생성하고 순위를 매기는** 새로운 방법인 **D-REX**를 제안합니다. D-REX는 이론적으로 순위가 매겨진 시연이 보상 함수 모호성을 줄여 시연자보다 우수한 성능을 가능하게 함을 보였습니다. 실험 결과, D-REX는 인간의 선호도나 보상 없이도 시연자의 성능을 크게 뛰어넘으며(대부분의 벤치마크에서 8/9), 기존 최신 모방 학습 방법보다 우수한 성능을 달성했습니다.
