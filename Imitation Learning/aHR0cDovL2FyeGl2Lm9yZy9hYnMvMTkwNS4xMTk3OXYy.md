# Causal Confusion in Imitation Learning

Pim de Haan, Dinesh Jayaraman, Sergey Levine

## 🧩 Problem to Solve

모방 학습(Imitation Learning, IL)은 전문가의 시연을 모방하여 정책(policy)을 학습합니다. 이 과정에서 학습 시 전문가 정책이 마주친 상태 분포와 실제 배포 시 학습된 정책이 마주칠 상태 분포가 달라지는 **분포 변화(distributional shift)** 문제가 발생합니다. 본 논문은 이 분포 변화가 "인과 관계 오인(causal misidentification)"이라는 예상치 못한 심각한 문제를 야기함을 지적합니다. 인과 관계 오인이란 정책이 실제 행동의 원인(true causes)이 아닌, 단순히 행동과 상관관계가 있는 교란 변수(nuisance correlates)에 의존하여 학습되는 현상을 말합니다. 이는 직관에 반하게도, 더 많은 정보를 입력으로 제공할수록 성능이 저하되는 결과를 초래할 수 있습니다 (예: 운전 시 브레이크 등(effect)을 브레이크 밟는 행동(cause)의 원인으로 오인). 본 논문은 이러한 인과 관계 오인 문제를 해결하여 모방 학습의 견고성을 높이는 것을 목표로 합니다.

## ✨ Key Contributions

* 모방 학습에서 분포 변화로 인해 발생하는 "인과 관계 오인" 문제를 최초로 규명하고 심층적으로 탐구했습니다.
* 표적 개입(targeted interventions)—환경 상호작용 또는 전문가 질의—을 통해 올바른 인과 모델을 학습함으로써 인과 관계 오인을 해결하는 새로운 방법론을 제안했습니다.
* 제안된 방법이 DAgger와 같은 기존 베이스라인보다 훨씬 적은 질의/에피소드만으로 인과 관계 오인 문제를 효율적으로 해결함을 입증했습니다.
* MountainCar, Hopper, Atari 게임과 같은 벤치마크 제어 도메인뿐만 아니라 실제 운전 환경에서도 인과 관계 오인 현상이 발생함을 실험적으로 확인하고, 제안 솔루션의 효과를 검증했습니다.
* 분리된(disentangled) 상태 표현이 인과 관계 오인 해결에 중요함을 밝혔습니다.

## 📎 Related Works

* **모방 학습(Imitation Learning)**: 행동 복제(Behavioral Cloning)는 Widrow와 Smith로부터 시작되어 널리 사용되어 왔습니다. 분포 변화 문제는 오래전부터 인식되어 왔으며, DAgger (Ross et al.)는 중간 정책이 마주하는 상태에 대해 전문가에게 반복적으로 질의하여 이 문제를 해결하고자 했습니다. Bansal et al.은 과거 행동의 단서를 제거하기 위해 드롭아웃을 사용했으며, GAIL (Ho & Ermon)은 궤적을 매칭하는 방식으로 분포 변화에 대응합니다. 본 논문은 이러한 문제의 근본 원인이 인과 관계 오인에 있음을 지적합니다.
* **인과 추론(Causal Inference)**: 변수 간의 원인-결과 관계를 추론하는 일반적인 문제입니다 (Pearl, Spirtes). 인과 발견(Causal Discovery)은 기록된 관측치로부터 인과 관계를 추론하지만, 관측 데이터만으로는 인과 관계를 완전히 식별하기 어려운 경우가 많습니다. 본 연구는 인과 구조를 발견하기 위해 개입(interventions)을 사용하는 접근 방식(interventional regime)을 따릅니다. 모방 학습 환경에서는 결정론적 전환 함수(deterministic transition functions)로 인해 인과 그래프의 faithfulness 가정이 위반되는 경우가 많아, 기존의 수동적 인과 발견(passive causal discovery) 방법은 부적합합니다.

## 🛠️ Methodology

본 논문은 인과 관계 오인을 해결하기 위해 다음과 같은 파이프라인을 제안합니다.

1. **인과 그래프-매개변수화된 정책 학습 (Causal Graph-Parameterized Policy Learning)**
    * 전문가의 행동 $A$에 영향을 미치는 상태 변수 $X = [X_1, ..., X_n]$의 부분집합을 나타내는 인과 그래프 $G$를 정의합니다. 각 $X_k$는 행동의 원인이거나 아닐 수 있으므로 $2^n$개의 가능한 그래프가 있습니다.
    * 정책 $\pi_G(X) = f_\phi([X \odot G, G])$를 학습합니다. 여기서 $\odot$는 요소별 곱셈이며, $G$는 $X$의 어떤 차원이 활성화될지(즉, 원인으로 간주될지)를 제어하는 이진 벡터입니다.
    * $f_\phi$는 신경망으로, $G$가 모든 가능한 그래프에서 균일하게 샘플링될 때 (UNIF-INTERVENTION) 또는 변분 베이즈 인과 발견(variational Bayesian causal discovery)을 통해 선호되는 그래프에서 샘플링될 때 (DISC-INTERVENTION) 손실을 최소화하도록 훈련됩니다.

2. **표적 개입 (Targeted Intervention)**
    * 학습된 $f_\phi$를 사용하여 각 인과 그래프 가설 $G$의 가능도 $L(G)$를 평가합니다.
    * **전문가 질의 모드 (Expert Query Mode, Algorithm 1)**:
        * 균일하게 샘플링된 $G$에 대한 정책 $\pi_{mix}$의 혼합을 실행하여 상태 $S$를 수집합니다.
        * $D(X) = E_G[D_{KL}(\pi_G(X), \pi_{mix}(X))]$와 같이 정책들 간의 불일치가 가장 큰 상태 $S'$를 선택합니다.
        * $S'$에 대해 전문가에게 질의하여 상태-행동 쌍 $T$를 얻습니다.
        * $p(G) \propto \exp\langle w, G \rangle$에 따라 $G$를 샘플링하고 손실 $L$을 계산한 후, 선형 회귀로 $w$를 업데이트하여 가장 가능성 있는 $G$를 찾습니다.
    * **정책 실행 모드 (Policy Execution Mode, Algorithm 2)**:
        * $p(G) \propto \exp\langle w, G \rangle$에 따라 $G$를 샘플링합니다.
        * 정책 $\pi_G$를 환경에서 실행하고 에피소드 보상 $R_G$를 수집합니다.
        * $(G, R_G)$ 쌍에 대한 선형 회귀로 $w$를 업데이트하여 가장 높은 보상을 주는 $G$를 찾습니다.
    * 두 모드 모두 에너지 기반 모델 $E(G) = \langle w, G \rangle + b$를 가정하며, $p(G)$는 독립적인 베르누이 인자로 분해됩니다.

3. **관측치 분리 (Disentangling Observations)**
    * 이미지와 같은 고차원 관측치의 경우, 각 차원이 단일 요인을 나타내지 않을 수 있으므로, $\beta$-VAE를 훈련하여 관측치로부터 분리된(disentangled) 잠재 표현 $X_t$를 생성합니다. VAE는 전문가 및 무작위 궤적 상태를 혼합하여 훈련됩니다.

## 📊 Results

* **인과 관계 오인 현상**: MountainCar, Hopper, Atari 게임 모두에서 교란 변수를 포함하는 "CONFOUNDED" 정책은 교란 변수가 없는 "ORIGINAL" 정책보다 현저히 낮은 보상을 얻었습니다. 심지어 CONFOUNDED 정책이 전문가 데이터에 대한 검증 손실은 더 낮은 경우도 있었습니다. 이는 인과 관계 오인이 널리 퍼져 있음을 시사합니다. 실제 운전 시나리오에서도 과거 이력 정보가 주어졌을 때 모방 학습 성능이 저하되는 유사한 증상이 관찰되었습니다.
* **개입의 효과**:
  * **정책 실행 개입**: UNIF-INTERVENTION 및 DISC-INTERVENTION은 수십에서 수백 에피소드 내에 CONFOUNDED와 ORIGINAL 사이의 성능 격차를 대부분 해소하며 ORIGINAL에 가까운 보상을 달성했습니다. GAIL은 Hopper에서 유사한 성능을 보였지만, 약 1,500 에피소드로 훨씬 더 많은 상호작용을 필요로 했습니다. DROPOUT은 미미한 개선을 보였습니다.
  * **전문가 질의 개입**: 본 연구의 방법은 MountainCar 및 Hopper에서 소수의 질의만으로 CONFOUNDED보다 성능을 크게 향상시켰습니다. DAgger는 수백에서 수만 번의 질의가 필요하여 본 방법보다 효율성이 떨어졌습니다.
  * DISC-INTERVENTION은 초기 단계에서 약간의 이점을 제공할 뿐, 대부분의 경우 더 간단한 UNIF-INTERVENTION과 유사하거나 더 나은 성능을 보였습니다.
* **인과 그래프 복구**: 본 방법은 Pong 환경에서 공과 패들(원인 변수)을 유지하고 숫자(교란 변수)를 변경하는 등, 실제 원인 변수를 성공적으로 식별했음을 시각적으로 입증했습니다.
* **분리된 표현의 필요성**: MountainCar에서 3차원 상태 벡터를 임의로 회전시켜 얽힌(entangled) 표현을 만들었을 때, 본 방법의 성능이 크게 저하되었습니다. 이는 본 접근 방식의 효과를 위해 분리된 상태 표현이 필수적임을 보여줍니다.

## 🧠 Insights & Discussion

본 연구는 모방 학습에서 "인과 관계 오인"이 분포 변화로 인해 발생하는 근본적인 문제임을 강력하게 시사합니다. 이 문제는 학습된 정책이 전문가 행동의 진정한 원인이 아닌, 단순히 상관관계가 있는 교란 변수에 의존하게 하여 배포 환경에서 낮은 일반화 성능을 초래합니다. 심지어 더 많은 정보를 제공하는 것이 오히려 해로울 수 있습니다.

제안된 표적 개입 기반의 솔루션은 전문가 질의 또는 환경 상호작용을 통해 올바른 인과 모델을 효율적으로 학습함으로써 이 문제를 성공적으로 해결했습니다. 특히, 기존 베이스라인(DAgger, Dropout)보다 훨씬 적은 수의 상호작용으로 뛰어난 성능을 달성했다는 점이 중요합니다. 또한, 이미지와 같은 복잡한 관측치로부터 분리된 상태 표현을 얻는 것이 이 방법의 효과를 위해 필수적임을 입증했습니다.

본 연구는 현재까지는 주로 비교적 간단한 합성 환경에서 솔루션을 검증했지만, 인과 관계 오인이 실제 모방 학습 설정에서도 널리 퍼져 있음을 시사합니다. 향후 연구에서는 이러한 현실적인 시나리오로 솔루션을 확장하는 것이 중요합니다. 나아가, 모방 학습 외에 실제 환경에 배포되는 일반적인 머신러닝 시스템에서도 "피드백"으로 인해 인과 관계 오인 문제가 발생할 수 있으며, 이 분야에도 본 연구의 통찰이 적용될 수 있습니다.

## 📌 TL;DR

**문제**: 모방 학습은 분포 변화로 인해 정책이 전문가 행동의 **진정한 원인**이 아닌 **교란 변수**에 의존하여 학습되는 "인과 관계 오인" 문제에 직면하며, 이는 더 많은 정보가 주어져도 성능 저하를 야기합니다.
**방법**: 본 논문은 인과 그래프-매개변수화된 정책을 학습한 후, 정책 간의 불일치를 기반으로 전문가에게 질의하거나, 환경에서 정책을 실행하여 얻은 보상을 통해 **표적 개입**을 수행함으로써 올바른 인과 그래프를 효율적으로 식별하고 인과 관계 오인을 해결하는 방법을 제안합니다. 이미지 관측치에는 $\beta$-VAE를 사용하여 분리된 상태 표현을 얻습니다.
**결과**: 제안된 방법은 기존 DAgger, Dropout 등 베이스라인보다 훨씬 적은 개입(질의/에피소드)으로 인과 관계 오인 문제를 성공적으로 해결하고, 올바른 인과 변수를 식별하며, 분리된 상태 표현이 이 방법의 효과에 필수적임을 입증했습니다.
