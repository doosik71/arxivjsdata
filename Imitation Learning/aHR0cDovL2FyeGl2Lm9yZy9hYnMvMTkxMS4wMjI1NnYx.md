# 모방 학습 방법에 대한 발산 최소화 관점

Seyed Kamyar Seyed Ghasemipour, Richard Zemel, Shixiang Gu

## 🧩 Problem to Solve

최근 적대적 역강화 학습(IRL) 방법들(예: GAIL, AIRL)은 행동 복제(BC)에 비해 특히 제한된 전문가 시연 데이터(low-data regime) 환경에서 훨씬 뛰어난 성능을 보인다. 하지만 최적 상태에서 두 방법 모두 전문가 정책을 정확히 복구한다는 점에서, 이러한 성능 차이의 근본적인 이유가 명확하게 밝혀지지 않았다. 본 논문은 이러한 의문을 해소하고 모방 학습(IL) 알고리즘들을 발산 최소화(divergence minimization) 관점에서 통합적으로 이해하는 것을 목표로 한다.

## ✨ Key Contributions

- **`f`-MAX 알고리즘 제안:** 최신 IRL 방법인 AIRL [1]을 `f`-발산(f-divergence)으로 일반화한 `f`-MAX를 제안하여 GAIL [2] 및 AIRL과 같은 알고리즘의 관계와 특성을 이해할 수 있는 통찰을 제공합니다.
- **모방 학습 방법의 통합적 관점:** `f`-발산 최소화 관점을 통해 BC와 성공적인 IRL 접근 방식 간의 차이점을 명확히 분석합니다.
- **IRL 성능 우위의 핵심 요인 규명:** 시뮬레이션된 고차원 연속 제어 도메인에서 경험적 평가를 통해 IRL의 우수한 성능이 **상태 주변 분포(state-marginal) 일치 목표**에 가장 크게 기여함을 밝혀냈습니다 (가설 1 지지).
- **FAIRL (Forward KL AIRL) 소개:** `f`-MAX가 전방 KL(forward KL)에 적용될 때 발생하는 문제점을 해결하기 위해, AIRL의 보상 함수를 단순 변경하여 전방 KL을 최적화하는 FAIRL을 제시합니다.
- **상태 주변 분포 일치 문제에 적용:** IL 방법에 대한 새로운 이해를 바탕으로, 전문가 시연이나 보상 함수 없이도 단순히 수작업으로 지정된 상태 분포를 사용하여 에이전트에게 다양한 행동을 가르칠 수 있음을 시뮬레이션된 로봇 팔 밀기(arm pushing) 환경에서 시연합니다.

## 📎 Related Works

- **확률론적 추론으로서의 RL:** 제어와 발산 최소화의 관계는 확률론적 추론 [17, 18, 19, 20, 21, 22, 23] 문헌에서 오랫동안 연구되었으며, 엔트로피 정규화된 최적 제어가 역 KL(reverse KL) 발산 최소화와 연관됨이 밝혀졌습니다 [20, 24].
- **행동 복제(Behavioural Cloning, BC):** 가장 널리 사용되는 IL 알고리즘으로, 단순하지만 훈련-테스트 시점 간의 공변량 이동(covariate shift) 문제로 어려움을 겪습니다. DAgger [27]와 Dart [28]는 이를 완화하려 했으나, 전문가 정책에 대한 상호작용적 접근을 가정합니다.
- **역강화 학습(Inverse Reinforcement Learning, IRL):** BC를 능가하는 유망한 결과를 보여왔습니다 [29, 30, 2, 1, 7]. 초기 IRL은 특징 기대치(feature expectation)를 일치시키는 방식으로 작동했으며 [9], 최대 엔트로피(Max-Ent) IRL [21, 22]은 원래 IRL의 퇴화(degeneracy) 문제를 해결했습니다.
- **적대적 Max-Ent IRL:** GAIL [2], AIRL [1], [12]과 같은 최근의 스케일러블한 Max-Ent IRL 접근 방식은 GAN [33]에 기반하여 분포 일치(distribution matching)와 추가적인 연관성을 보여줍니다.
- **`f`-GAN:** Nowozin et al. [15]은 `f`-발산의 변분 하한(variational lower bound) [37]과 GAN에 착안하여 임의의 `f`-발산을 사용하여 분포를 일치시키는 반복 최적화 방안을 제시했습니다.
- **동시 연구:** Ke et al. [34] 또한 IL에 대한 통합적인 확률론적 관점을 제시했지만, 이 연구는 그리드 월드(grid world) 도메인에만 초점을 맞추는 반면, 본 연구는 고차원 연속 제어 환경에서 비교 결과를 제공하고 상태 주변 분포 일치 [16]의 효과를 평가합니다.

## 🛠️ Methodology

1. **`f`-MAX: `f`-발산 Max-Ent IRL 정식화:**
   - 주어진 `f`-발산 $D_f(ρ_{exp}(s,a)||ρ_π(s,a))$를 최소화하는 것을 목표로 합니다.
   - `f`-GAN [15] 프레임워크를 사용하여 다음의 minimax 최적화 문제를 정의합니다:
     $$ \min*π \max*{T*ω} E*{(s,a) \sim ρ*{exp}(s,a)}[T*ω(s,a)] - E*{(s,a) \sim ρ*π(s,a)}[f^*(T_ω(s,a))] $$
        여기서 $T_ω$는 차별자(discriminator) 역할을 하는 함수이고, $f^*$는 $f$의 볼록 켤레(convex conjugate)입니다.
   - 이를 다음의 반복 최적화 절차로 풀이합니다:
     - **차별자 업데이트:** $ \max*{T*ω} E*{(s,a) \sim ρ*{exp}(s,a)}[T_ω(s,a)] - E*{(s,a) \sim ρ*π(s,a)}[f^*(T_ω(s,a))] $
     - **정책 업데이트:** $ \max*π E*{τ \sim π} [ \sum_t f^*(T_ω(s_t,a_t))] $
   - AIRL [1]은 $f(u) := - \log u$를 선택하여 역 KL 발산 $KL(ρ_π(s,a)||ρ_{exp}(s,a))$을 최소화하는 `f`-MAX의 특수 사례임을 증명합니다.
   - `f`-MAX는 Ho & Ermon [2]의 비용-정규화된 Max-Ent IRL 프레임워크에 속함을 보여줍니다.
2. **모방 학습 방법 간의 관계 이해:**
   - 다양한 IL 알고리즘의 목적 함수를 통계적 발산 최소화 형태로 정리합니다 (표 1).
     - **BC:** $E_{ρ_{exp}(s)}[KL(π_{exp}(a|s)||π(a|s))]$ (조건부 행동 분포 일치)
     - **AIRL:** $KL(ρ_π(s,a)||ρ_{exp}(s,a))$ (상태-행동 주변 분포 일치, 역 KL)
     - **GAIL:** $D_{JS}(ρ_π(s,a)||ρ_{exp}(s,a)) - \lambda H_{causal}(π)$ (상태-행동 주변 분포 일치, JS 발산)
   - 이로부터 두 가지 가설을 도출합니다:
     - **가설 1:** IRL의 주요 성능 향상은 명시적인 전문가 **상태 주변 분포** 일치 목표 때문이다.
     - **가설 2:** 역 KL의 모드-추구(mode-seeking) 행동이 전방 KL의 모드-커버링(mode-covering) 행동보다 RL에서 더 유리하다.
3. **FAIRL (Forward KL AIRL) 개발:**
   - 전방 KL $KL(ρ_{exp}(s,a)||ρ_π(s,a))$의 특수 사례에서는 `f`-MAX가 정책 훈련에 신호를 제공하지 않는 문제(degeneracy)가 있음을 발견합니다.
   - AIRL의 보상 함수를 수정하여 전방 KL을 최적화하는 FAIRL을 제안합니다:
     - $h(s,a) := \log D(s,a) - \log(1-D(s,a))$
     - $r(s,a) := \exp(h(s,a)) \cdot (-h(s,a))$
4. **`f`-MAX를 이용한 상태 주변 분포 일치:**
   - IL이 아닌 상태 주변 분포 일치(state-marginal matching) 문제에 `f`-MAX 프레임워크를 적용하여 $D_f(ρ_{target}(s)||ρ_π(s))$를 최소화합니다. 이는 전문가 시연 없이 수작업으로 설계한 목표 상태 분포를 에이전트가 따르도록 합니다.

## 📊 Results

- **가설 평가:**
  - HalfCheetah, Ant, Walker, Hopper 환경에서 4, 16, 32개의 시연 궤적을 사용하여 BC, AIRL, FAIRL의 성능을 비교했습니다.
  - **결과:** AIRL과 FAIRL은 모든 시나리오에서 BC를 큰 폭으로 능가했습니다. 특히 제한된 데이터 환경에서 이러한 차이가 두드러졌습니다.
  - **가설 1 지지:** 전방 KL을 사용하는 FAIRL이 BC보다 우수한 성능을 보였다는 점은 IRL 방법의 주요 성능 향상이 사용된 KL 발산의 방향(전방/역방향)보다는 **정책이 전문가의 상태 주변 분포를 명시적으로 일치시키도록 장려하는 목표**에서 비롯된다는 가설 1을 강력하게 지지합니다.
  - **가설 2 미확인:** FAIRL과 AIRL이 유사한 성능을 보였으므로, 본 실험 설정에서는 KL 발산의 모드-추구/모드-커버링 특성이 성능에 미치는 영향이 지배적이지 않았습니다.
  - F/AIRL은 DAgger와 유사한 결과를 얻었는데, 이는 DAgger가 BC의 상태 분포 불일치 문제를 완화하기 위해 설계된 점과 일맥상통합니다.
- **`f`-MAX를 이용한 상태 주변 분포 일치:**
  - **Point-Mass:** 복잡하고 다중 모드(multi-modal)의 목표 분포(무한대 기호, 나선형 궤적)를 따르는 정책을 성공적으로 학습했습니다.
  - **Pusher Draw:** Pusher 로봇 팔이 3D 공간의 가상 원통 표면에 사인파 함수를 그리는 동작을 학습했습니다.
  - **Pusher Push:** 로봇 팔이 물체를 목표 영역으로 미는 작업을 수행하도록 학습했습니다. 이는 RL로 얻은 결과에 미치지는 못하지만, 3D 공간에 간단한 선과 점을 그려 지시한 것만으로도 의미 있는 행동을 유도할 수 있음을 보여줍니다.
  - **Fetch Push:** Fetch 로봇이 테이블 위 특정 영역을 균일하게 탐색하는 정책을 학습하여, 효과적인 탐색 전략을 설계하는 데 활용될 수 있음을 보여주었습니다.

## 🧠 Insights & Discussion

- **IRL 성능의 핵심 동인:** 본 연구는 IRL이 BC보다 우수한 성능을 보이는 주된 이유가 행동 조건부 분포 일치뿐만 아니라 **전문가의 상태 주변 분포를 명시적으로 일치시키려는 목표** 덕분임을 명확히 밝혔습니다. 이는 FAIRL이 BC보다 우수하다는 결과로 뒷받침됩니다.
- **발산 방향의 영향:** 테스트된 벤치마크 환경에서는 역 KL과 전방 KL의 모드-추구/모드-커버링 특성이 성능 차이에 결정적인 요인이 아니었습니다.
- **전문가 시연의 대안:** `f`-MAX를 상태 주변 분포 일치 문제에 적용함으로써, 고비용의 전문가 시연 대신 사람이 설계한 간단한 목표 상태 분포만으로도 에이전트의 다양한 행동을 유도할 수 있는 가능성을 제시합니다. 이는 모방 학습의 새로운 방향을 열 수 있습니다.
- **한계 및 향후 연구:**
  - **FAIRL의 하이퍼파라미터 튜닝:** FAIRL은 AIRL보다 하이퍼파라미터 튜닝이 더 어려울 수 있습니다.
  - **샘플 기반 KL 추정:** 샘플 기반 KL 발산 추정의 정확도 문제는 여전히 존재하며, 본 연구는 밀도비(density ratio) 추정 방식을 사용했습니다.
  - **온-정책 훈련의 비효율성:** IRL 및 상태 주변 분포 일치(SMM)는 온-정책 훈련으로 인해 샘플 비효율적일 수 있으므로, 오프-정책(off-policy) 방법을 개발하는 것이 중요합니다.
  - **단절된 모드:** SMM에서 목표 분포가 단절된 모드를 가질 경우, 정책이 모든 모드를 탐색하고 일치시키기 어려울 수 있습니다.

## 📌 TL;DR

본 논문은 `f`-MAX라는 `f`-발산 기반 AIRL 일반화를 통해 모방 학습 방법들을 발산 최소화 관점에서 통합적으로 분석했습니다. 핵심적인 발견은 **적대적 IRL 방법이 행동 복제(BC)보다 우수한 성능을 보이는 주된 이유가 전문가의 상태 주변 분포를 명시적으로 일치시키는 목표**에 있다는 것입니다. 이를 증명하기 위해 전방 KL을 최적화하는 FAIRL을 제시하고 BC와 비교했으며, `f`-MAX를 전문가 시연 없이 수작업으로 설계된 상태 분포를 통해 에이전트의 다양한 행동을 유도하는 데 성공적으로 적용하여 모방 학습의 새로운 방향을 제시했습니다.
