# Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation

YuXuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine

## 🧩 Problem to Solve

기존의 모방 학습(Imitation Learning)은 일반적으로 에이전트가 동일한 환경 설정에서 관찰-액션 쌍(observation-action tuples) 형태의 시연 데이터를 받는 것을 전제로 합니다. 하지만 인간과 동물은 다른 사람의 행동을 관찰하고, 시점, 주변 환경, 객체 위치 및 종류 등의 맥락(context) 변화에도 불구하고 어떤 액션이 해당 행동을 재현할지 스스로 파악하여 모방합니다.

본 연구는 이러한 "관찰 기반 모방 학습(imitation-from-observation)" 문제를 해결하고자 합니다. 이는 즉, (카메라 이미지와 같은) 고차원 원시 관찰 시퀀스만을 통해 원하는 행동을 학습하며, 각 시퀀스가 환경, 객체, 시점 등 다양한 맥락 변화 하에서 얻어진 경우에도 성공적으로 모방하는 것을 목표로 합니다. 주요 과제는 다음과 같습니다:

1. 학습자의 맥락(시연과 다를 수 있음)에서 어떤 정보를 추적해야 할지 결정하는 것.
2. 시연된 행동을 추적하기 위해 어떤 액션을 취해야 할지 결정하는 것.

## ✨ Key Contributions

- **"Imitation-from-Observation" 문제 공식화**: 액션 정보 없이 다양한 맥락의 원시 비디오 관찰만으로 행동을 모방하는 새로운 모방 학습 패러다임을 제안하고 공식화했습니다.
- **맥락 변환 모델(Context Translation Model) 개발**: 전문가 시연 비디오(소스 맥락)를 학습 에이전트(타겟 맥락)의 관찰 공간으로 변환하는 딥러닝 모델을 제안했습니다. 이 모델은 맥락 불일치(context mismatch) 문제를 효과적으로 해결하고, 행동 추적에 적합한 특징 표현을 학습합니다.
- **강화 학습 기반 정책 학습 통합**: 변환된 시연 관찰을 추적하는 것을 목표로 하는 보상 함수를 정의하고, 이를 심층 강화 학습(Deep Reinforcement Learning)과 결합하여 로봇이 행동을 재현하는 제어 정책을 학습하도록 했습니다.
- **실세계 로봇 조작 작업에서의 효과 입증**: 시뮬레이션뿐만 아니라, 로봇이 인간의 도구 사용(예: 청소, 국자질, 밀기)을 모방하는 것을 포함한 다양한 실제 로봇 조작 작업에서 제안 방법의 뛰어난 성능과 견고함을 실험적으로 검증했습니다.

## 📎 Related Works

- **전통적인 모방 학습**:
  - **행동 복제(Behavioral Cloning)**: 전문가의 상태-액션 쌍을 사용한 지도 학습([4]). 본 연구는 액션이 주어지지 않으므로 직접 적용 불가.
  - **역강화 학습(Inverse Reinforcement Learning, IRL)**: 전문가 시연으로부터 보상 함수를 학습([6], [15], [16], [17]). 고차원 관찰에 직접 적용하기 어렵다는 문제.
- **맥락 및 도메인 이동 처리**:
  - **Third-Person Imitation Learning (TPIL) [7]**: 적대적 손실(adversarial loss)을 사용하여 도메인 이동을 보상하지만, 여러 맥락에 대한 일반화 및 복잡한 조작 작업에서 한계가 있음.
  - **Unsupervised Perceptual Rewards [9]**: 사전 훈련된 시각 특징(pretrained visual features)을 사용하여 맥락 차이를 다루지만, 명시적인 맥락 변환 메커니즘은 없음.
  - **Generative Adversarial Imitation Learning (GAIL) [8]**: 적대적 IRL과 유사한 알고리즘.
- **비전 기반 로봇 학습**:
  - **End-to-end training of deep visuomotor policies [3]**: 원시 센서 관찰로부터 복잡한 기술 학습.
  - **Time-contrastive networks [23]**: 다중 시점 훈련을 통한 시각 특징의 불변성 증가.
  - **Learning robot activities from first-person human videos [24]**: 명시적인 손 감지 및 시각 파이프라인 사용.
- **시각 도메인 적응 및 이미지 변환**:
  - GAN(Generative Adversarial Networks)을 활용한 픽셀 수준 도메인 적응 및 시각 스타일 변환([26], [27], [28], [29]). 본 연구는 로봇 제어에 초점을 맞추어 데모 변환에 활용.

## 🛠️ Methodology

본 논문의 방법론은 크게 두 단계로 나뉩니다: 맥락 변환 모델 학습과 이를 활용한 강화 학습 기반 정책 학습.

### 1. 맥락 변환 모델 학습 (Learning to Translate Between Contexts)

- **목표**: 주어진 소스 맥락($\omega_i$)의 관찰 $o_i^t$를, 타겟 맥락($\omega_j$)의 초기 관찰 $o_j^0$에 조건화하여 타겟 맥락의 관찰 $o_j^t$로 변환하는 모델 $M(o_i^t, o_j^0) = (\hat{o}_j^t)_{\text{trans}}$을 학습합니다. $o_j^0$는 타겟 맥락의 정보를 암시적으로 인코딩하는 역할을 합니다.
- **모델 아키텍처 (Fig 2)**:
  - **Encoders**:
    - `Enc_1(o_i^t)`: 소스 관찰 $o_i^t$를 특징 $z_1$으로 인코딩합니다.
    - `Enc_2(o_j^0)`: 타겟 맥락의 초기 관찰 $o_j^0$를 특징 $z_2$로 인코딩합니다.
    - `Enc_1`과 `Enc_2`는 시연 장면의 다양성에 따라 가중치를 공유할 수 있습니다.
  - **Translator**: `T(z_1, z_2)`: 소스 특징 $z_1$과 타겟 특징 $z_2$를 결합하여 타겟 맥락에 맞는 변환된 특징 $z_3$를 생성합니다. ($F(o_i^t, o_j^0) = z_3$로 특징 추출기로 표현됨)
  - **Decoder**: `Dec(z_3)`: 변환된 특징 $z_3$를 최종 타겟 관찰 $(\hat{o}_j^t)_{\text{trans}}$로 디코딩합니다. `Enc_2`로부터 `Dec`로 스킵 연결(skip connections)이 있어 픽셀 수준 재구성의 복잡성을 처리합니다.
- **손실 함수**: 모델은 종단 간(end-to-end)으로 훈련되며, 다음 손실 함수의 조합으로 최적화됩니다:
  - **변환 손실 (Translation Loss)**: $L_{\text{trans}} = \| (\hat{o}_j^t)_{\text{trans}} - o_j^t \|_2^2$
    - 모델이 예측한 변환된 관찰이 실제 타겟 관찰과 일치하도록 합니다.
  - **재구성 손실 (Reconstruction Loss)**: $L_{\text{rec}} = \| \text{Dec}(\text{Enc}_1(o_j^t)) - o_j^t \|_2^2$
    - `Enc_1`과 `Dec`를 오토인코더처럼 훈련하여 유용한 특징 표현을 학습하도록 합니다.
  - **정렬 손실 (Alignment Loss)**: $L_{\text{align}} = \| z_3 - \text{Enc}_1(o_j^t) \|_2^2$
    - 변환된 특징 $z_3$와 타겟 관찰을 인코딩한 특징 `Enc_1(o_j^t)`이 동일하고 일관된 특징 공간에 있도록 강제합니다.
  - **최종 학습 목표**: $L = \sum_{(i,j)} (L_{\text{trans}} + \lambda_1 L_{\text{rec}} + \lambda_2 L_{\text{align}})$

### 2. 맥락 변환을 통한 정책 학습 (Learning Policies via Context Translation)

- **보상 함수**: 학습된 맥락 변환 모델의 특징을 활용하여 강화 학습을 위한 보상 함수를 구성합니다.
  - **특징 추적 보상 (Feature Tracking Reward)**:
    $\hat{R}_{\text{feat}}(o_l^t) = -\| \text{Enc}_1(o_l^t) - \frac{1}{n} \sum_{i=1}^n F(o_i^t, o_l^0) \|_2^2$
    - 학습자의 현재 관찰 $o_l^t$의 인코딩된 특징 `Enc_1(o_l^t)`과 모든 전문가 시연의 평균 변환 특징 $\frac{1}{n} \sum_{i=1}^n F(o_i^t, o_l^0)$ 사이의 제곱 유클리드 거리를 최소화하여 행동을 추적하도록 합니다.
  - **약한 이미지 추적 보상 (Weak Image Tracking Reward)**:
    $\hat{R}_{\text{img}}(o_l^t) = -\| o_l^t - \frac{1}{n} \sum_{i=1}^n M(o_i^t, o_l^0) \|_2^2$
    - 정책 학습 중 발생할 수 있는 분포 외(out-of-distribution) 샘플 문제를 완화하기 위해, 학습자의 현재 관찰 $o_l^t$와 모든 전문가 시연의 평균 변환 이미지 $M(o_i^t, o_l^0)$ 사이의 픽셀 단위 거리를 최소화하는 보상을 추가합니다.
  - **최종 보상**: $\hat{R}(o_l^t) = \hat{R}_{\text{feat}}(o_l^t) + w_{\text{rec}} \hat{R}_{\text{img}}(o_l^t)$
- **강화 학습 알고리즘**:
  - 시뮬레이션 환경에서는 TRPO(Trust Region Policy Optimization) [36]를 사용합니다.
  - 실세계 로봇 실험에서는 샘플 효율성을 위해 Guided Policy Search (GPS) [3]의 궤적 중심(trajectory-centric) RL 방법을 사용합니다. 이 경우, 이미지 특징 $z_3$를 상태의 일부로 포함하고 LQR 기반 업데이트를 수행하며, 이미지 추적 비용은 생략합니다.

## 📊 Results

본 연구는 제안된 맥락 변환 모델이 "관찰 기반 모방 학습"을 가능하게 하는지, 그리고 기존 방법들이 이러한 유형의 모방 학습 작업에서 얼마나 잘 수행되는지를 평가했습니다.

- **시뮬레이션 환경에서의 비교 평가 (Fig 4)**:
  - **4가지 조작 작업**: 도달(reaching), 밀기(pushing), 쓸기(sweeping), 치기(striking).
  - **비교 대상**: Oracle (지상 진실 보상), 제안 방법 (Ours), Pretrained Visual Features [9], Third Person Imitation Learning (TPIL) [7], Generative Adversarial Imitation Learning (GAIL) [8].
  - **결과**: 제안 방법은 모든 시뮬레이션 작업에서 성공적으로 학습되었으며, 가장 높은 성공률을 보였습니다. 특히, 사전 훈련된 시각 특징 기반 방법만이 쓸기 작업에서 약간의 개선을 보였을 뿐, 다른 기존 방법들은 도달, 밀기, 치기 작업에서 거의 성공하지 못했습니다. 이는 맥락 차이가 있는 모방 학습 문제가 매우 어렵다는 것을 시사합니다.
- **어블레이션 연구 (Ablation Study) (Fig 6)**:
  - 모델 학습의 다양한 손실 함수($L_{\text{trans}}$, $L_{\text{rec}}$, $L_{\text{align}}$)와 보상 함수의 구성 요소($\hat{R}_{\text{feat}}$, $\hat{R}_{\text{img}}$)의 중요성을 평가했습니다.
  - 각 구성 요소를 제거하면 모방 성능이 크게 저하되는 것을 확인했습니다. 이는 제안 방법의 각 요소가 모두 성공적인 학습에 필수적임을 보여줍니다.
- **실세계 로봇 조작 (Sawyer 로봇) (Fig 7, 8)**:
  - **3가지 도구 사용 조작 작업**: 밀기(Pushing), 종이/아몬드 쓸기(Sweeping), 아몬드 국자질(Ladling Almonds).
  - **인간 시연 활용**: 모든 시연은 인간이 제공한 비디오로 이루어졌으며, 로봇은 이를 모방하여 기술을 수행했습니다. 인간과 로봇은 동일한 도구를 사용하여 체화(embodiment) 차이로 인한 도메인 이동을 최소화했습니다.
  - **결과**: 제안 방법은 모든 실제 로봇 작업에서 consistently 높은 성공률을 보이며, 사전 훈련된 시각 특징 기반 및 고정된 조인트 각도 기반 베이스라인 방법들을 뛰어넘었습니다. 특히 아몬드 쓸기 및 국자질과 같이 분석적으로 표현하기 어려운 과립형 매체 조작 작업에서 제안 방법의 우수성이 더욱 두드러졌습니다.

## 🧠 Insights & Discussion

- **시사점**: 본 연구는 맥락 변환(context translation)이라는 새로운 접근 방식을 통해 "관찰 기반 모방 학습"이라는 어려운 문제를 성공적으로 해결했음을 보여줍니다. 다양한 시점, 객체 위치, 환경 변화에도 불구하고 원시 비디오 관찰만으로 로봇이 인간의 행동을 효과적으로 모방할 수 있음을 입증했습니다. 이는 특히 가정 환경 청소와 같은 다양한 실제 로봇 조작 기술 학습에 큰 잠재력을 가집니다.
- **한계점**:
  1. **데이터 요구량**: 변환 모델 학습을 위해 상당한 양의 시연 데이터가 필요합니다. 각 작업마다 모델을 처음부터 훈련하는 것은 비효율적일 수 있으며, 고수준의 표현과 결합하면 훈련 효율을 높일 수 있을 것입니다.
  2. **다중 맥락 시연의 필요성**: 모델이 맥락 간의 변환을 학습하기 위해 여러 다른 맥락에서 얻어진 시연 관찰이 요구됩니다. 실제 환경에서는 이러한 맥락의 수가 제한적일 수 있습니다.
  3. **제한적인 도메인 이동 처리**: 본 연구에서는 시연과 학습자의 맥락이 동일한 분포에서 샘플링된다고 가정하여 체화(embodiment)의 큰 차이(예: 인간 팔과 로봇 팔의 외형적 차이)로 인한 도메인 이동은 명시적으로 다루지 않았습니다.
- **향후 연구 방향**:
  - 여러 작업을 단일 모델로 결합하여 다양한 맥락에서 오는 여러 작업을 효율적으로 처리하는 방법을 탐구할 수 있습니다.
  - 체화의 큰 차이와 같은 명시적인 도메인 이동 문제를 해결하여, 인터넷에서 얻은 인간 시연 비디오와 같이 더욱 일반적인 데이터로부터 로봇이 직접 기술을 학습할 수 있도록 하는 연구가 필요합니다.

## 📌 TL;DR

이 논문은 로봇이 단순히 **관찰된 원시 비디오**를 통해 행동을 모방하는 새로운 패러다임인 "관찰 기반 모방 학습"을 제시합니다. 핵심 아이디어는 **맥락 변환 모델**을 학습시켜 인간 시연(소스 맥락) 비디오를 로봇(타겟 맥락)의 관찰 공간으로 변환하는 것입니다. 변환된 시연을 추적하도록 설계된 보상 함수를 정의하고, 이를 심층 강화 학습과 결합하여 로봇이 실제 행동 정책을 학습하게 합니다. 시뮬레이션 및 실제 로봇(Sawyer) 실험을 통해, 이 방법이 시점, 객체 위치 등 다양한 맥락 변화에도 불구하고 밀기, 쓸기, 국자질과 같은 복잡한 조작 작업을 성공적으로 수행하며 기존 모방 학습 방법들을 능가함을 입증했습니다. 주요 한계점으로는 변환 모델 학습을 위한 많은 양의 시연 데이터와 체화의 큰 차이에 대한 도메인 이동 처리 능력의 부족이 있습니다.
