# Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios

Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bronstein, Rebecca Roelofs, Benjamin Sapp, Brandyn White, Aleksandra Faust, Shimon Whiteson, Dragomir Anguelov, Sergey Levine

## 🧩 Problem to Solve

자율 주행 시스템은 방대한 양의 실제 인간 주행 데이터를 통해 인간과 유사한 행동을 모방 학습(Imitation Learning, IL)할 수 있지만, 이러한 정책은 데이터에 잘 나타나지 않는 드물고 도전적인 시나리오, 즉 '엣지 케이스'에서 안전성과 신뢰성 문제를 겪기 쉽습니다. IL 단독으로는 위험하거나 부적절한 반응에 대한 명시적인 지침이 부족하여 예측 불가능한 행동을 유발할 수 있으며, 공변량 이동(covariate shift) 문제에도 취약합니다. 반대로 강화 학습(Reinforcement Learning, RL)은 명시적인 보상 함수를 통해 안전성을 학습할 수 있지만, 현실적인 보상 설계가 매우 어렵고, IL 없이 RL만 사용할 경우 자연스럽지 않거나 비효율적인 주행 행동을 보일 수 있습니다. 본 연구는 이러한 IL과 RL의 한계를 극복하고, 도전적인 주행 시나리오에서 안전성과 신뢰성을 크게 향상시키면서도 인간과 유사한 주행 행동을 유지하는 강력한 자율 주행 정책을 개발하는 것을 목표로 합니다.

## ✨ Key Contributions

* 자율 주행 분야에서 대규모(10만 마일 이상의 실제 도시 주행 데이터)의 실세계 데이터를 활용하고 간단한 보상 함수와 결합된 IL + RL 접근 방식의 첫 번째 대규모 적용 사례를 제시했습니다.
* 데이터셋을 난이도별로 체계적으로 분류하여 성능을 평가함으로써, IL과 RL의 조합이 IL 단독 방식보다 정책의 안전성과 신뢰성을 크게 향상시킴을 입증했습니다(가장 어려운 시나리오에서 안전 이벤트 38% 이상 감소).

## 📎 Related Works

* **학습 기반 자율 주행 접근 방식:** 초기에는 행동 복제(Behavioral Cloning, BC)와 같은 IL 방식이 많이 사용되었으나, 공변량 이동 문제와 명시적인 안전 지식 부족이 단점으로 지적됩니다. 이를 해결하기 위해 역강화 학습(Inverse Reinforcement Learning, IRL)이나 적대적 IL(Adversarial IL, GAIL)과 같은 폐쇄 루프(closed-loop) IL 방식이 제안되었습니다. RL은 차선 유지, 교차로 통과, 차선 변경 등 특정 시나리오에서 효과적임을 보여주었지만, 보상 함수 설계의 어려움이 있습니다.
* **IL과 RL의 결합:** DQfD, DDPGfD, DAPG와 같은 방법들은 희소한 보상 환경에서 IL이 RL의 탐색 문제를 해결하는 데 도움을 줄 수 있음을 보여주었습니다. TD3+BC, CQL과 같은 오프라인 RL 접근 방식은 IL 목표를 RL 목표와 결합하여 Q-학습 업데이트를 정규화합니다. 본 연구는 새로운 알고리즘 조합보다는 이러한 일반적인 접근 방식을 대규모 자율 주행 문제에 적용하는 데 초점을 맞춥니다.
* **자율 주행의 도전적이고 안전에 중요한 시나리오 해결:** 앙상블 IL 플래너와 모델 예측 제어(MPC)를 결합하거나, 규칙 기반 폴백(fallback) 계층으로 안전성을 보장하는 방법 등이 연구되었습니다. 본 연구는 보상 함수를 통해 안전 인식을 모델 학습 과정에 직접 통합한다는 점에서 차이가 있습니다. 또한, 난이도가 높은 사례에 대한 훈련 노출을 늘리는 커리큘럼 학습 [11]과 유사하지만, RL이 가장 어려운 시나리오에서 훨씬 큰 성능 향상을 가져옴을 보여줍니다.

## 🛠️ Methodology

1. **IL과 RL의 결합 목표 함수:** 데이터가 풍부한 곳에서는 시연 데이터로부터 학습 신호를, 데이터가 희소한 곳에서는 보상 신호를 활용하는 가중 혼합 목표 함수를 사용합니다.
    $$ \max_{\pi} E_{T,\pi,\rho_0} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t,a_t) \right] + \lambda E_{s,a \sim D} [\log\pi(a|s)] $$
2. **BC-SAC (Behavior Cloned Soft Actor-Critic):** 널리 사용되는 SAC(Soft Actor-Critic) 프레임워크를 기반으로 Actor-Critic 알고리즘에 IL 목표를 추가합니다. Actor 목표 함수는 Q-함수의 예상 값과 엔트로피 정규화 항에 IL 목표를 더한 형태로 정의됩니다. Critic 업데이트는 SAC와 동일하게 Bellman 오차를 최소화합니다. 가중치 $\lambda$를 적절히 설정하여, 데이터 분포 내에서는 전문가를 모방하고, 분포 밖에서는 보상 학습에 주로 의존하도록 유도합니다.
    * Actor Objective: $E_{s,a \sim \pi}[Q(s,a) + H(\pi(\cdot|s))] + \lambda E_{s,a \sim D}[\log\pi(a|s)]$
3. **간단한 보상 함수:** 복잡한 주행 행동을 위한 보상 설계 부담을 줄이고, 안전 제약 조건을 인코딩하는 데 집중합니다.
    * **충돌 보상 ($R_{\text{collision}}$):** 자율 주행 차량(ego vehicle)과 가장 가까운 다른 차량의 바운딩 박스 간의 유클리드 거리($d_{\text{collision}}$)를 기반으로 합니다. $d_{\text{coffset}}$ (기본 1.0m) 오프셋을 두어 인접 객체로부터 거리를 유지하도록 장려합니다.
        $$ R_{\text{collision}} = \min(d_{\text{collision}} - d_{\text{coffset}}, 0) $$
    * **도로 이탈 보상 ($R_{\text{off-road}}$):** 차량과 가장 가까운 도로 가장자리($d_{\text{to-edge}}$) 간의 거리를 기반으로 합니다. $d_{\text{ooffset}}$ (기본 1.0m) 오프셋을 두어 도로 가장자리로부터 거리를 유지하도록 장려합니다.
        $$ R_{\text{off-road}} = \text{clip}(-d_{\text{ooffset}} - d_{\text{to-edge}}, -2.0, 0.0) $$
    * 총 보상은 두 보상 항을 합산한 $R = R_{\text{collision}} + R_{\text{off-road}}$ 입니다.
4. **차량 동역학 모델 및 역동역학 모델:** 차량의 상태는 운동학적 자전거 동역학 모델(kinematic bicycle dynamics model)을 사용하여 업데이트됩니다. IL을 위한 전문가 행동은 로그 데이터의 다음 상태와 동일한 상태를 달성하는 행동을 역동역학 모델(inverse dynamics model)로 추정하여 얻습니다.
5. **모델 아키텍처:** TD3 및 SAC와 유사한 듀얼 Actor-Critic 아키텍처를 사용합니다. 각 네트워크는 차량 상태, 도로 그래프 포인트, 신호등, 경로 목표 등을 인코딩하는 Transformer 기반 관측 인코더를 포함합니다.
6. **난이도 높은 예제 훈련:** [11]의 연구를 따라 난이도 모델을 통해 충돌 또는 아차 사고 가능성이 높은 어려운 주행 시나리오 (예: Top10)에 집중하여 훈련합니다.

## 📊 Results

* **데이터셋 및 시뮬레이션:** 샌프란시스코에서 수집된 10만 마일 이상의 실제 인간 주행 데이터셋(Waymo)을 사용했습니다. 이 데이터셋을 [11]에서 제안된 난이도 모델을 이용해 난이도별(Top1, Top10, Top50)로 분류하여 훈련 및 평가했습니다. 시뮬레이션 환경에서는 2D 자전거 동역학 모델을 사용하며, 다른 차량 및 보행자 행동은 로그를 기반으로 재생됩니다.
* **평가 지표:**
    1. **실패율 (Failure Rate):** 주행 중 최소 한 번의 충돌(Collision) 또는 도로 이탈(Off-road) 이벤트가 발생한 세그먼트의 백분율 (낮을수록 좋음).
    2. **경로 진행률 (Route Progress Ratio):** 정책이 주행한 경로 길이와 전문가 시연 경로 길이의 비율 (높을수록 좋음).
* **주요 결과:**
  * **BC-SAC의 우수성:** BC-SAC는 모든 훈련 및 평가 구성에서 IL 단독 방식(BC, MGAIL) 및 RL 단독 방식(SAC)을 일관되게 능가했습니다. 특히 가장 어려운 시나리오(Top1 및 Top10)에서 BC-SAC는 안전 이벤트 발생률을 현저히 낮추어 BC 대비 38% 이상, MGAIL 대비 40% 이상의 실패율 감소를 보였습니다.
  * **난이도별 강건성:** BC-SAC는 다양한 난이도 시나리오에서 가장 낮은 실패율과 성능 저하를 보였으며, 성능 편차도 가장 작았습니다 ($\sigma=0.37$). 이는 BC ($\sigma=1.29$)나 MGAIL ($\sigma=0.78$)보다 훨씬 강건합니다.
  * **RL 단독 방식과의 비교:** SAC(RL 단독)는 안전 지표에서 BC-SAC보다 낮은 성능을 보였고, 로그와 크게 벗어나는 비자연스러운 행동(잦은 급회전, 급가속)을 생성했습니다. 반면 BC-SAC는 IL 손실 덕분에 로그와 유사한 행동 분포를 유지하며 자연스러운 주행을 보여주었습니다.
  * **보상 함수 및 가중치 분석:** 제안된 형태의 조밀한(dense) 보상 함수가 간단한 이진(binary) 보상보다 성능이 우수함을 확인했습니다. 또한, 보상 파라미터(오프셋)의 적절한 선택과 IL 및 RL 목표 항 간의 균형이 최적의 성능을 이끌어내는 데 중요함을 입증했습니다.
  * **진행-안전 균형:** 진행 보상(progress reward)을 소량 도입하면 안전 지표를 크게 손상시키지 않으면서 진행률을 유의미하게 향상시킬 수 있음을 보였습니다.
  * **심층 실패 분석:** MGAIL은 주로 클리핑(CLIP) 충돌과 도로 이탈(OFF)이 많았던 반면, BC-SAC는 다른 차량에 의한 충돌(DIV)이 상대적으로 많았습니다. 이는 MGAIL이 충돌에 대한 명시적 패널티가 부족하여 작은 충돌에 둔감한 반면, BC-SAC는 로그에서 이탈했을 때 다른 차량의 예측에 영향을 미치는 경향이 있음을 시사합니다.

## 🧠 Insights & Discussion

* **IL과 RL의 상호 보완성:** 본 연구는 모방 학습(IL)이 인간과 유사하고 자연스러운 주행 행동을 효율적으로 학습하는 데 기여하고, 강화 학습(RL)이 시연 데이터에 부족한 분포 외(out-of-distribution) 시나리오에서의 안전 문제를 처리하는 '안전장치' 역할을 성공적으로 수행함을 보여주었습니다. 이는 두 패러다임의 강점을 결합하는 것이 자율 주행의 강건성을 높이는 효과적인 방법임을 시사합니다.
* **난이도별 훈련의 중요성:** IL 단독 방식과 유사하게, 전체 데이터셋이 아닌 가장 도전적인 상위 10% 시나리오에 집중하여 훈련하는 것이 IL과 RL 결합 설정에서도 가장 강건한 성능을 이끌어냈습니다. 이는 희귀하고 어려운 엣지 케이스에 대한 정책의 노출을 늘리는 것이 자율 주행 시스템의 실제 배포를 위한 핵심 요소임을 강조합니다.
* **현재 한계 및 향후 연구 방향:**
  * 현재 보상 함수는 주로 안전 관련 보상에 중점을 두었으므로, 진행률, 교통 법규 준수, 승객 편의와 같은 다른 중요한 요소를 목표 함수에 통합하는 것이 필요합니다.
  * 다른 차량들의 행동이 로그 재생 기반이므로, 자율 주행 차량(ego vehicle)의 분포 외 행동에 대한 다른 에이전트들의 예상치 못한 반응을 모델링하지 못합니다. 향후에는 반응형 시뮬레이션 에이전트를 도입하여 이러한 상호작용을 고려해야 합니다.
  * IL과 RL 목표 간의 가중치($\lambda$)를 경험적으로(heuristically) 선택해야 하는 문제가 있으며, 이를 자동으로 조절하는 방법론 연구가 필요합니다.
  * 안전성을 명시적인 제약 조건으로 강제하고 분포 이동(distributional shift)을 완화하는 방법론과 결합하는 방향으로 연구를 확장할 수 있습니다.

## 📌 TL;DR

**문제:** 자율 주행에서 모방 학습(IL)은 자연스러운 주행을 제공하지만 위험한 '엣지 케이스'에 취약하고, 강화 학습(RL)은 안전성을 높일 수 있지만 보상 설계가 어렵고 부자연스러운 행동을 초래할 수 있다.

**방법:** 본 연구는 10만 마일 이상의 실제 도시 주행 데이터를 활용하여 IL과 RL을 결합한 BC-SAC(Behavior Cloned Soft Actor-Critic)를 제안한다. BC-SAC는 간단한 충돌 및 도로 이탈 보상 함수를 RL에 사용하고, IL 목표와 RL 목표를 동시에 최적화하며, 특히 난이도가 높은 시나리오 데이터에 집중하여 훈련함으로써 두 방법의 장점을 결합한다.

**결과:** BC-SAC는 가장 도전적인 주행 시나리오에서 IL 단독 또는 RL 단독 방식보다 정책의 안전성과 신뢰성을 크게 향상시켰으며, 안전 이벤트 발생률을 38% 이상 감소시켰다. 이는 IL의 인간과 유사한 행동 학습 능력과 RL의 안전 보장 능력을 효과적으로 융합한 결과이다.
