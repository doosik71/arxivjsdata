# A Survey of Imitation Learning Methods, Environments and Metrics

NATHAN GAVENSKI, FELIPE MENEGUZZI, MICHAEL LUCK, ODINALDO RODRIGUES

## 🧩 Problem to Solve

모방 학습(Imitation Learning, $IL$) 분야의 급격한 발전은 다양한 기술과 응용 사례를 낳았지만, 연구 결과를 통일된 방식으로 비교하고 평가하는 데 어려움을 초래했습니다. 기존의 학습 방법 분류 방식은 제한적이며, $IL$ 환경 및 평가 지표에 대한 표준화된 분류 체계가 부족하여 서로 다른 접근 방식들 간의 의미 있는 비교를 어렵게 만듭니다.

## ✨ Key Contributions

본 설문 조사는 $IL$ 분야의 주요 과제를 해결하기 위해 다음을 수행합니다:

* **새로운 분류 체계 제시:** $IL$ 방법론, 환경 및 평가 지표의 핵심 측면에 초점을 맞춘 새로운 분류 체계를 도입합니다.
* **주요 문제점 논의:** 다양한 연구에서 관찰되는 중요한 문제점들을 반영하고 논의합니다.
* **미래 연구 방향 제시:** 해결되지 않은 과제들을 강조하고 향후 연구 방향을 제안합니다.

## 📎 Related Works

본 논문은 기존의 여러 $IL$ 설문 조사들을 참고하고 보완합니다:

* **Hussein et al. [47] (2017):** 모방 학습의 다양한 접근 방식에 중점을 둔 초기 설문.
* **Zheng et al. [121] (2021):** 비교적 최신 $IL$ 방법론들을 다룬 설문.
* **Torabi et al. [105]:** 전통적인 모델 기반/모델 프리 관점에서 $IL$을 분류.
또한, $IL$은 강화 학습(Reinforcement Learning, $RL$) [2], 다중 에이전트 학습(Multi-agent Learning) [13, 26], 전이 학습(Transfer Learning) [75, 123] 등 관련 분야에서 영감을 받았습니다.

## 🛠️ Methodology

본 설문 조사는 $IL$ 문헌 검토를 위해 다음과 같은 체계적인 방법론을 사용했습니다:

* **문헌 수집:** Hussein et al. [47]과 Zheng et al. [121]의 설문 조사를 시작점으로 하는 스노우볼링(snowballing) [115] 접근 방식을 사용하여 관련 논문을 광범위하게 수집했습니다.
* **포함 기준:**
  * 역강화 학습(Inverse Reinforcement Learning) 또는 모방 학습 문헌에 속하는 연구만 포함했습니다.
  * 피어 리뷰를 거친 논문만 포함하되, 2023년 이후의 프리프린트(pre-print)는 예외적으로 포함했습니다.
  * 에이전트 관련 연구 맥락 밖의 모방 학습(예: 메타 학습)은 제외했습니다.
* **새로운 분류 체계 제안:**
  * **학습 방법:** 전통적인 모델 기반/모델 프리 분류 대신, 행동 복제(Behavioural Cloning), 동역학 모델(Dynamics Model), 적대적 학습(Adversarial Learning), 하이브리드(Hybrid) 및 온라인 학습(Online Learning)의 다섯 가지 주요 범주를 제시합니다.
  * **환경:** 환경의 학습 과정에서의 역할에 따라 검증(Validation), 정밀(Precision), 순차(Sequential) 환경으로 분류하는 첫 번째 분류 체계를 제안합니다.
  * **지표:** 지표가 측정하는 내용에 따라 행동(Behaviour), 도메인(Domain), 모델(Model) 지표로 분류하는 첫 번째 분류 체계를 제안합니다. 각 범주는 다시 정량적(Quantitative) 및 질적(Qualitative) 지표로 세분화됩니다.

## 📊 Results

* **학습 방법 동향 분석:**
  * 총 $50$편의 새로운 방법론 논문이 검토되었으며, 적대적 학습($15$편), 동역학 모델($14$편), 행동 복제($11$편), 하이브리드($8$편), 온라인 학습($2$편) 순으로 많았습니다.
  * 특히 하이브리드 접근 방식과 동역학 모델의 인기가 증가하고 있으며, 이는 효율성과 효과성 간의 균형 때문으로 분석됩니다.
* **환경 사용 패턴 분석:**
  * 총 $66$개의 환경이 검토되었으며, $6$개 환경(Walker-$2D$, MuJoCo 로봇 팔 시뮬레이션, Hopper, CartPole, MountainCar, HalfCheetah)이 전체 사용의 $80$% 이상을 차지하는 Zipf의 법칙과 유사한 분포를 보였습니다.
  * $42$개의 환경은 문헌에서 단 한 번만 언급되어, $IL$ 연구에서 실험 프로토콜의 표준화 부족을 시사합니다.
  * 주로 검증 환경이 과도하게 사용되는 경향이 있으며, 이는 에이전트가 기본 과제를 깊이 이해하지 못한 채 교사의 행동을 과도하게 모방($overimitate$)할 위험을 내포합니다.
* **평가 지표 사용 분석:**
  * $50$가지의 다양한 지표가 발견되었으며, 대부분은 정량적 분석(보상 기반 지표가 가장 흔함)에 초점을 맞추고 있었습니다.
  * 에이전트의 인간 유사성(human-like behavior)을 측정하는 질적 지표는 비용 문제로 인해 드물게 사용됩니다 (정량적 지표 $41$종 $109$회 사용 대비, 질적 지표 $9$종 $10$회 사용).
  * 대부분의 평가 프로토콜은 방법론을 비교하는 데 동일한 가이드라인을 따르지만, 효율성이나 효과성과 같은 다른 특성을 측정하는 데는 부족했습니다.

## 🧠 Insights & Discussion

* **$IL$ 평가의 특수성과 과제:**
  * $IL$은 기계 학습($ML$) 및 강화 학습($RL$)과 달리, 훈련 데이터에 없는 미지의 상태에 대한 정보가 부족하여 테스트 과정에서 불일치($compounding$ $errors$)가 발생할 수 있습니다.
  * 표준화된 벤치마킹 데이터셋과 평가 프로토콜의 부재는 연구 간의 객관적인 비교를 어렵게 하며, 데이터 누수($data$ $leakage$)와 같은 문제가 발생할 수 있습니다.
  * $IL$의 궁극적인 목표는 단순히 교사의 행동을 복사하는 것을 넘어, 교사의 데이터를 사용하여 과제를 수행하는 방법을 학습하고 미지의 상태에서도 교사처럼 행동할 수 있는 일반화 능력을 갖추는 것입니다.
* **'전문가' 최적성 가정의 비현실성:**
  * 훈련 샘플이 항상 이론적으로 "최적($optimal$)"인 "전문가($expert$)"로부터 온다는 암묵적인 가정은 문제가 될 수 있습니다. 실제로는 "교사($teacher$)"는 능숙하지만 반드시 최적이지는 않을 수 있습니다.
  * 복잡한 환경에서 샘플의 최적성을 확인하기 어려우며, 샘플이 안전성이나 공정성 같은 부가적인 목표를 완전히 포착하지 못할 수 있습니다.
  * 인간 피드백 기반 강화 학습($RLHF$)에서 얻은 통찰(미묘함, 본질적인 솔루션 품질, 훈련 시간 단축 등)은 $IL$ 에이전트가 질적 측면을 고려하는 데 도움이 될 수 있습니다.
* **전문가 검증의 필수 요구사항:**
  * 데이터를 신뢰할 수 있는 전문가로 간주하기 위한 필수 조건은 (i) 유한/이산 상태 및 행동 공간, (ii) 완전 관측 가능한 상태, (iii) $MDP$ 상태 공간의 완전한 커버리지입니다. 하지만 실제 적용에서는 이러한 조건들이 종종 완화됩니다.
* **도전 과제 및 미래 방향:**
  * **안전성 ($Safety$):** 에이전트가 안전하지 않은 행동을 하지 않도록 안전한 샘플과 교사를 구별하고, 안전하게 학습하는 방법 연구.
  * **학습 효율성 ($Learning$ $efficiency$):** 단순히 샘플 수가 아닌 안전성 등 다른 요소를 고려한 포괄적인 효율성 측정.
  * **학습 효과성 ($Learning$ $effectiveness$):** 최적이지 않은 교사로부터도 효과적으로 학습할 수 있는 에이전트 개발에 집중하는 것이 효율성보다 더 중요할 수 있습니다.
  * **적응성 ($Adaptability$):** 일반화(미지의 시나리오 적용)와 전이 학습(다른 작업/기술 적용) 능력을 향상시키기 위한 연구. 상징 학습($symbolic$ $learning$)과 같은 다른 학습 패러다임과의 결합이 필요할 수 있습니다.
  * **다중 에이전트 시스템 ($Multi$-$agent$ $systems$):** $IL$을 조정 및 통신과 같은 고수준 작업에 적용하고, 계획 및 탐색과 같은 다른 기술과 결합하는 연구.

## 📌 TL;DR

모방 학습($IL$) 분야는 빠르게 발전하고 있지만, 방법론, 환경 및 평가 지표의 표준화 부족으로 인해 연구 간의 체계적인 비교가 어렵습니다. 이 설문 조사는 $IL$ 방법론($Behavioural$ $Cloning$, $Dynamics$ $Model$, $Adversarial$ $Learning$, $Hybrid$, $Online$ $Learning$), 환경($Validation$, $Precision$, $Sequential$), 그리고 평가 지표($Behaviour$, $Domain$, $Model$)에 대한 새로운 분류 체계를 제시하여 이 문제를 해결하고자 합니다. 주요 통찰로는 '전문가' 최적성 가정의 비현실성, 벤치마킹 표준화의 필요성, 그리고 $IL$의 궁극적인 목표가 단순히 행동 복사를 넘어 과제를 완수하는 것임을 강조합니다. 미래 연구 방향으로는 안전성, 효율성, 효과성, 적응성, 다중 에이전트 시스템에서의 $IL$ 적용 확대를 제안합니다.
