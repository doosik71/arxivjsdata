# One-Shot Visual Imitation Learning via Meta-Learning

Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, Sergey Levine

## 🧩 Problem to Solve

로봇이 다양한 작업을 신속하고 효율적으로 수행하기 위해서는 복잡한 비정형 환경에서 다양한 기술을 빠르게 습득할 수 있어야 합니다. 딥러닝과 같은 고용량 모델은 복잡한 기술을 표현할 수 있게 하지만, 각 기술을 처음부터 학습하는 것은 방대한 데이터 요구사항 때문에 비현실적입니다. 기존의 학습 방법은 각 작업마다 많은 감독 또는 경험을 필요로 하며, 이전 작업의 경험을 재사용하여 새로운 작업을 더 빨리 해결하는 메커니즘이 부족합니다. 따라서 이 연구는 **이전 기술의 정보를 활용하여 새로운 행동을 단 한 번의 시연(one-shot demonstration)만으로 빠르게 학습하는 방법**, 특히 원시 픽셀 입력으로부터 복잡한 시각-운동 기술을 효율적으로 습득하는 방법을 탐구합니다.

## ✨ Key Contributions

- **메타 모방 학습(Meta-Imitation Learning) 방법 제안**: 로봇이 단 한 번의 시연만으로 새로운 기술을 효율적으로 학습할 수 있도록 하는 메타 모방 학습 방법을 제시합니다.
- **원시 픽셀 입력으로 확장 가능**: 기존의 원샷 모방 학습 방법과 달리, 이 방법은 원시 픽셀 입력으로 확장할 수 있으며, 새로운 기술의 효과적인 학습을 위해 훨씬 적은 수의 이전 작업 데이터만 필요합니다.
- **시각 기반 정책의 엔드-투-엔드 미세 조정**: 메타 학습을 사전 훈련 절차로 사용하여 다양한 환경에서의 시연 데이터를 활용하여 시각 기반 정책을 단 한 번의 시연만으로 엔드-투-엔드(end-to-end)로 미세 조정할 수 있음을 최초로 입증합니다.
- **두 가지 아키텍처 개선**:
  - **Two-Head Architecture**: 네트워크의 최종 레이어 파라미터를 공유하지 않아 정책이 적응하는 방식에 더 많은 유연성을 제공하며, 전문가의 행동 데이터 없이도 학습할 수 있도록 손실 함수를 메타 학습하는 데 사용됩니다.
  - **Bias Transformation**: 네트워크의 히든 레이어에 파라미터 벡터를 연결하는 방식으로, 기울기 기반 메타 학습의 안정성과 효율성을 크게 향상시킵니다.
- 시뮬레이션 및 실제 로봇 플랫폼 모두에서 단일 시각 시연을 통해 새로운 작업을 엔드-투-엔드로 학습하는 능력을 입증했습니다.

## 📎 Related Works

- **모방 학습(Imitation Learning) [6]**: 환경 상태(예: 객체 포즈)가 알려진 경우 적은 수의 시연으로 효율적인 모방 학습이 성공적이었지만 [8, 9, 10, 11], 본 연구는 원시 센서 입력으로부터 환경 상태가 알려지지 않은 설정에 중점을 둡니다.
- **원시 픽셀 기반 모방 학습 [12, 13, 14, 15]**: 모바일 로봇 분야에서 광범위하게 연구되었으나, 각 작업에 많은 시연이 필요하다는 한계가 있습니다.
- **역강화 학습(Inverse Reinforcement Learning) [18]**: 필요한 시연 수를 줄일 수 있지만, 보상 최적화를 위해 추가적인 로봇 경험을 요구합니다 [19, 20, 21].
- **다중 작업 로봇 학습**: 작업 간 정보 공유는 새로운 개념이 아니며, 태스크-투-태스크 매핑 [22], 게이팅 [23], 공유 특징 [24] 등이 있습니다. 컨텍스트 정책 [1, 2, 3, 4, 5]은 태스크를 정책의 입력으로 제공하며, 다른 접근 방식은 태스크 표현을 컨트롤러 파라미터로 매핑합니다 [10, 25, 26].
- **모델 불가지론적 메타 학습(Model-Agnostic Meta-Learning, MAML) [27]**: 소수샷 이미지 인식 및 강화 학습에 적용되었던 MAML을 시각적 메타 모방 학습에 확장하여 사용합니다. MAML은 소수의 예제에 과적합되지 않으면서 새로운 작업에 대해 빠르게 학습할 수 있도록 모델의 가중치를 학습합니다.
- **원샷 모방 학습 [5]**: 순환 신경망(RNN)을 기반으로 한 이전 원샷 모방 학습 방법과 비교하며, 본 연구는 기울기 기반 업데이트를 통해 더욱 효율적인 적응과 원시 픽셀 처리 능력을 보여줍니다.

## 🛠️ Methodology

1. **메타 모방 학습 문제 정의**:
   - 목표는 새로운 작업에 대해 단 한 번의 시연만으로 빠르게 적응할 수 있는 정책 $\pi$를 학습하는 것입니다.
   - 이를 위해 여러 다른 작업의 시연 데이터를 재사용하여 새로운 작업의 효율적인 학습을 가능하게 합니다.
   - 각 모방 작업 $T_i$는 전문가 정책 $\pi^*_i$에 의해 생성된 시연 데이터 $\tau = \{o_1, a_1, \dots, o_T, a_T\}$와 모방에 사용되는 손실 함수 $L$로 구성됩니다.
2. **MAML (Model-Agnostic Meta-Learning) 확장**:
   - 정책의 파라미터 $\theta$를 학습하여 새로운 작업 $T_i$에 대해 표준 기울기 하강법으로 빠르게 적응할 수 있도록 합니다.
   - **내부 업데이트(Inner Update)**: 특정 작업 $T_i$에서 가져온 단일 시연 $\tau$를 사용하여 정책 파라미터 $\theta$를 업데이트하여 $\theta'_i = \theta - \alpha \nabla_\theta L_{T_i}(f_\theta)$를 계산합니다. 여기서 $L_{T_i}(f_\theta)$는 평균 제곱 오차(Mean Squared Error) 손실 함수입니다: $$L_{T_i}(f_\phi) = \sum_{\tau^{(j)} \sim T_i} \sum_t \|f_\phi(o^{(j)}_t) - a^{(j)}_t\|^2_2$$
   - **외부 업데이트(Meta-Update)**: $\theta$는 $\theta'_i$의 성능을 최적화하는 방식으로 업데이트됩니다: $\min_\theta \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta'_i})$. 메타 훈련 시 각 작업당 두 개의 시연(하나는 내부 업데이트용, 다른 하나는 메타 업데이트용)이 사용됩니다.
3. **Two-Head Architecture (섹션 4.1)**:
   - MAML의 표준 설정과 달리, 네트워크의 마지막 레이어 파라미터를 공유하지 않아 pre-update 및 post-update 단계에서 별도의 "헤드"를 형성합니다.
   - 이는 효과적으로 메타 학습된 손실 함수를 사용하는 것과 동일하며, 적응 과정에서 정책 파라미터를 조정하는 데 더 많은 유연성을 제공합니다. 이 아키텍처를 통해 전문가 행동 없이 학습이 가능해집니다.
4. **전문가 행동 없는 모방 학습 (섹션 4.2)**:
   - 시연 시 전문가의 행동 $a_t$에 접근할 수 없는 시나리오(예: 비디오만 제공)를 위해, 메타 학습된 손실 함수를 전문가 행동 없이 수정합니다: $$L^*_{T_i}(f_\phi) = \sum_{\tau^{(j)} \sim T_i} \sum_t \|W y^{(j)}_t + b\|^2_2$$
   - 이 손실 함수는 메타-최적화 목표 (수식 4)와 함께 사용하여 원시 관찰만으로 학습할 수 있도록 합니다.
5. **모델 아키텍처 및 Bias Transformation (섹션 5)**:
   - 정책은 카메라 이미지와 로봇 구성을 입력으로 받는 컨볼루션 신경망(CNN)을 사용합니다.
   - 컨볼루션 레이어, ReLU 활성화, 공간 소프트-최대값(spatial soft-argmax)을 사용하여 공간 특징 포인트로 변환한 후 로봇 구성과 연결합니다.
   - 이후 완전 연결 레이어를 거쳐 로봇 행동을 출력합니다.
   - **Bias Transformation**: 히든 레이어의 후-시냅스 활성화에 파라미터 벡터 $z$를 연결하는 방식으로, $y = W_1 x + W_2 z + b$와 같이 재파라미터화된 바이어스를 사용합니다. 이는 바이어스 업데이트에 더 직접적인 제어를 제공하여 기울기 기반 메타 학습의 안정성과 효율성을 높입니다.

## 📊 Results

- **시뮬레이션 도달(Simulated Reaching) 작업**:
  - 원시 픽셀 입력을 사용하여 새로운 작업을 효과적으로 학습할 수 있음을 입증했습니다.
  - MAML 기반 메타 모방 학습(MIL)은 겸손한 크기의 메타 학습 데이터셋으로도 LSTM 및 컨텍스트 정책보다 훨씬 우수한 성능을 보였습니다 (그림 4 참조).
  - Bias transformation(bt)을 사용한 MIL은 데이터셋 크기에 관계없이 더 일관된 성능을 보였습니다.
- **시뮬레이션 밀기(Simulated Pushing) 작업**:
  - 7-자유도 토크 제어, 3D 환경, 다양성이 높은 객체를 포함하는 도전적인 도메인에서 평가되었습니다.
  - MIL은 가장 큰 데이터셋 크기에서 85.8%의 원샷 성공률을 달성했으며, LSTM 기반 접근 방식보다 평균 6.5% 더 높은 성공률을 보였습니다 (그림 4 및 표 1 참조).
  - 전문가 행동이 없는 시연(비디오+상태만)의 경우, MIL은 성능 저하가 덜 심각했습니다 (72.52% 성공률) 반면 LSTM은 크게 어려움을 겪었습니다 (37.61% 성공률).
- **실제 로봇 배치(Real-World Placing) 작업**:
  - PR2 로봇과 RGB 카메라를 사용하여 이전에 보지 못한 객체에 대한 학습 능력을 평가했습니다.
  - MIL 정책은 단일 시각 시연만으로 이전에 보지 못한 대상 객체를 성공적으로 찾아내고 아이템을 목표 컨테이너에 배치하는 데 90%의 성공률을 보였습니다 (표 2 참조).
  - LSTM 및 컨텍스트 정책은 올바른 대상 객체를 찾아내지 못하여 25%의 성공률을 기록했습니다.
  - 비디오만 제공된 경우에도 MIL은 68.33%의 성공률을 달성하여, 로봇의 최종 효과기 궤적이나 제어 없이도 학습할 수 있음을 보여주었습니다.

## 🧠 Insights & Discussion

- **효율성 및 일반화**: 제안된 메타 모방 학습 방법은 이전의 순환 신경망(RNN) 기반 원샷 모방 학습 방법을 능가하며, 메타 훈련에 필요한 시연 수가 현저히 적어 원시 픽셀 입력 및 실제 로봇 시스템에 적용 가능합니다. 이는 특히 이미지와 같은 복잡한 센서 입력을 사용하는 기술 학습에서 엔드-투-엔드 훈련의 유연성과 일반성을 유지하면서 로봇 학습 방법의 효율성을 크게 향상시킵니다.
- **확장 가능성**: 메타 학습 알고리즘은 사용 가능한 모든 작업의 시연 데이터를 통합할 수 있으므로, 로봇 학습 컨텍스트에서 대규모 데이터셋을 활용하는 자연스러운 방법을 제공합니다. 이를 통해 로봇은 더 많은 시연을 획득함에 따라 더 많은 기술을 학습할 뿐만 아니라, 새로운 기술을 학습하는 과정에서 실제로 **더 빠르고 효과적**이 될 수 있습니다.
- **한계 및 향후 연구**:
  - 현재 실험은 객체 다양성 외에는 제한된 다양성을 가진 작업에 사용되었지만, 더 다양한 시연이 메타 훈련에 제공될수록 방법의 능력은 크게 증가할 것으로 예상됩니다.
  - 시연 학습의 고질적인 문제인 누적 오류(compounding errors)는 본 논문에서 다루지 않았습니다.
  - 인간의 비디오와 로봇의 시야 간의 도메인 시프트(domain shift) 문제는 향후 연구 과제로 남겨져 있습니다.
  - 전문가 행동 없이 비디오만으로 학습하는 경우 성능 저하가 있었으며, 이는 추가적인 데이터 또는 연구의 필요성을 시사합니다.

## 📌 TL;DR

**문제**: 로봇이 딥러닝의 데이터 요구사항과 작업 간 지식 공유 부족으로 인해 제한된 시연(원샷)과 원시 시각 입력만으로 다양한 복합 기술을 빠르게 학습하기 어렵습니다.

**방법**: 본 논문은 MAML(Model-Agnostic Meta-Learning)을 시각 기반 모방 학습에 확장한 메타 모방 학습 방법을 제안합니다. 이 방법은 초기 정책 파라미터를 메타 학습하여 단일 시각 시연의 단일 기울기 업데이트만으로 새로운 작업에 빠르게 적응할 수 있도록 합니다. 또한, 'two-head' 아키텍처(손실 함수 메타 학습) 및 'bias transformation'(안정성 향상)과 같은 아키텍처 개선을 도입하여, 전문가 행동 없이도 시연 학습이 가능하도록 합니다.

**결과**: 시뮬레이션된 도달 및 밀기 작업에서 기존 RNN 기반 원샷 모방 학습 방법보다 우수한 성능을 보였으며, 실제 PR2 로봇으로 이전에 보지 못한 객체를 배치하는 복잡한 작업에서 단일 시각 시연으로 90%의 성공률을 달성했습니다. 이 방법은 원시 픽셀 입력에서 효과적으로 작동하며 메타 훈련 데이터 요구량이 현저히 적음을 입증했습니다.
