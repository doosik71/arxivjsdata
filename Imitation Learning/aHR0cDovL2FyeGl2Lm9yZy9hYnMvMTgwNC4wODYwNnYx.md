# ZERO-SHOT VISUAL IMITATION

Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell

## 🧩 Problem to Solve

기존의 모방 학습(Imitation Learning) 패러다임은 전문가의 행동에 대한 강한 감독(관찰-행동 쌍)에 의존하며, 이는 인간 전문가에게 번거롭고 새로운 작업마다 새로운 시연이 필요하다는 한계가 있습니다. 이러한 방식은 '무엇을(what)' 모방할지 뿐만 아니라 '어떻게(how)' 모방할지에 대한 정보까지 필요로 합니다. 대신, 전문가는 목표(예: 시퀀스 이미지)만 제공하고 에이전트가 행동을 스스로 추론하는 관찰 학습(Observational Learning) 접근 방식이 더 유망하지만, 초기 지식이 없는 에이전트에게는 매우 어려운 문제입니다. 특히, 목표 조건부 기술 정책(Goal-Conditioned Skill Policy, GSP) 학습 시, 주어진 시작 상태에서 목표 상태에 도달하는 다양한 방법(다중 모드 행동 분포)이 존재하여 학습을 어렵게 만드는 문제가 있습니다.

## ✨ Key Contributions

- **제로샷 시각 모방 패러다임 제안**: 훈련 중이거나 추론 시에도 전문가의 행동 데이터에 전혀 접근하지 않고 시각적 목표만으로 작업을 모방하는 새로운 방법을 제시합니다.
- **자율 탐색 기반 GSP 학습**: 에이전트가 전문가 감독 없이 스스로 환경을 탐색하고, 이 경험을 '목표 지향 기술 정책(Goal-Conditioned Skill Policy, GSP)'으로 정제합니다.
- **전방 일관성 손실(Forward Consistency Loss) 도입**: 행동 분포의 다중 모드성 문제를 해결하기 위해, GSP가 예측한 행동이 '정확히' 일치하는 것보다 '실제로 동일한 다음 상태'로 이끄는 경우에 페널티를 주지 않는 새로운 손실 함수를 제안합니다. 이는 예측 행동의 결과에 대한 일관성을 중시하여 학습 안정성을 높입니다.
- **목표 인식기(Goal Recognizer) 동시 최적화**: 목표 도달 여부를 판단하는 네트워크를 함께 학습시켜 에이전트가 가변적인 단계 수를 통해 목표를 달성하도록 합니다.
- **다양한 실제/시뮬레이션 환경 평가**: Baxter 로봇을 이용한 복잡한 밧줄 조작(매듭 묶기 등), TurtleBot을 이용한 미개척 사무실 환경 내비게이션, VizDoom 시뮬레이션 3D 내비게이션 등에서 제안된 방법의 효과와 일반화 능력을 입증했습니다.
- **탐색 메커니즘의 중요성 확인**: 호기심 기반 탐색(Curiosity-driven exploration)이 GSP의 역량을 강화하고 최종 작업 성능을 향상시키는 데 기여함을 시사했습니다.

## 📎 Related Works

- **모방 학습(Imitation Learning)**: 행동 클로닝(Behavioral Cloning, Argall et al., 2009; Pomerleau, 1989) 및 역강화 학습(Inverse Reinforcement Learning, Abbeel & Ng, 2004; Ho & Ermon, 2016; Levine et al., 2016; Ng & Russell, 2000; Ziebart et al., 2008) 연구와 연관되나, 본 연구는 전문가 행동을 요구하지 않습니다.
- **시각 시연(Visual Demonstration)**: Nair et al. (2017)의 밧줄 조작 연구, Sermanet et al. (2017; 2018)의 자율 학습 기반 모방, Stadie et al. (2017) 및 Liu et al. (2018)의 3인칭 모방 학습 등 시각 정보만을 활용하는 연구들과 맥을 같이 합니다.
- **전방/역동학 및 일관성(Forward/Inverse Dynamics and Consistency)**: Ebert et al. (2017); Oh et al. (2015); Watter et al. (2015)의 전방 동학 모델 학습 및 Agrawal et al. (2016); Jordan & Rumelhart (1992); Pathak et al. (2017); Wolpert et al. (1995)의 전방/역동학 공동 학습 연구들과 관련이 있으나, 본 연구는 동학 간의 명시적인 일관성 최적화를 추가합니다.
- **목표 조건화(Goal Conditioning)**: Agrawal et al. (2016); Andrychowicz et al. (2017); Nair et al. (2017); Schaul et al. (2015) 등 목표 조건부 정책 학습 아이디어의 연장선에 있습니다.

## 🛠️ Methodology

본 연구는 세 가지 주요 구성 요소로 이루어진 제로샷 시각 모방 프레임워크를 제안합니다.

1. **목표 조건부 기술 정책(Goal-Conditioned Skill Policy, GSP) 학습**:

   - **자율 탐색 데이터 수집**: 에이전트 $\pi_E(s)$는 환경을 탐색하여 관찰-행동 시퀀스 $S:\{x_1, a_1, x_2, a_2, ..., x_T\}$를 수집합니다.
   - **GSP 정의**: GSP $\pi$는 현재 관찰 $x_i$와 목표 관찰 $x_g$를 입력으로 받아, $x_i$에서 $x_g$에 도달하는 데 필요한 행동 시퀀스 $\tilde{a}_{\tau}$를 출력하는 함수입니다.
   - **전방 일관성 손실(Forward Consistency Loss)**:
     - 에이전트의 행동 $a_t$에 대한 다음 관찰 $\tilde{x}_{t+1}$을 예측하는 전방 동학 모델 $f(x_t, a_t; \theta_f)$을 학습합니다.
     - GSP가 예측한 행동 $\hat{a}_t = \pi(x_t, x_{t+1}; \theta_\pi)$에 대한 다음 관찰 $\hat{x}_{t+1} = f(x_t, \hat{a}_t; \theta_f)$을 계산합니다.
     - GSP는 다음 상태 예측 오차를 최소화하도록 훈련됩니다. 이는 예측된 행동 $\hat{a}_t$이 실제 행동 $a_t$과 다르더라도 동일한 다음 상태로 이끈다면 페널티를 주지 않아 다중 모드 행동 분포 문제를 완화합니다.
     - GSP와 전방 동학 모델은 공동으로 다음 목적 함수를 최소화하도록 학습됩니다:
       $$ \min*{\theta*\pi, \theta*f} ||x*{t+1} - \tilde{x}_{t+1}||\_2^2 + \lambda||x_{t+1} - \hat{x}\_{t+1}||\_2^2 + L(a_t, \hat{a}\_t) $$
            여기서 $L(a_t, \hat{a}_t)$는 GSP의 행동 예측 손실입니다. 학습 초기 불안정성을 피하기 위해 전방 모델과 GSP를 각각 사전 훈련한 후 공동 미세 조정을 수행합니다.
   - **다단계 GSP 및 특징 공간 동학**:
     - GSP $\pi_m$을 재귀 신경망(recurrent network)으로 구현하여 가변 길이 행동 시퀀스를 생성합니다.
     - $\pi_m$은 현재 상태 특징 $\phi(x_t)$, 목표 상태 특징 $\phi(x_T)$, 이전 행동 $a_{t-1}$ 및 내부 은닉 표현 $h_{t-1}$을 입력으로 받아 $\hat{a}_t$를 예측합니다.
     - 전방 일관성 손실은 각 단계에서 계산되며 전체 궤적에 대한 행동 예측 손실과 함께 공동으로 최적화됩니다. 특히, 관찰 공간 대신 특징 공간 $\phi(x)$에서 동학을 학습하여 일반화 성능을 높입니다:
       $$ \min*{\theta*\pi, \theta*f, \theta*\phi} \sum*{t=i}^{T} \left( ||\phi(x*{t+1}) - \tilde{\phi}(x*{t+1})||\_2^2 + \lambda||\phi(x*{t+1}) - \hat{\phi}(x\_{t+1})||\_2^2 + L(a_t, \hat{a}\_t) \right) $$

2. **목표 인식기(Goal Recognizer) 학습**:

   - 현재 관찰 $x_i$와 목표 $x_g$가 '가까운지' 여부를 이진 분류 문제로 정의하여 학습합니다.
   - 탐색 중 수집된 데이터를 활용하여, 몇 스텝 이내의 관찰은 긍정 샘플로, 일정 거리 이상 떨어진 관찰은 부정 샘플로 레이블링합니다.
   - 에이전트가 목표에 도달했는지 판단하여 다음 중간 목표로 넘어갈지 결정하게 함으로써 시연을 따르는 데 중요한 역할을 합니다.

3. **추론(Inference)**:
   - 전문가는 시퀀스 이미지 $D:\{x_d^1, x_d^2, ..., x_d^N\}$로 시연을 제공합니다 (행동 정보 없음).
   - 에이전트는 학습된 GSP와 목표 인식기를 사용하여 $x_0$에서 $x_d^1$까지 이동한 후, $x_d^1$에 도달하면 다음 목표 $x_d^2$를 설정하고 이 과정을 반복하여 시연을 모방합니다.

## 📊 Results

- **밧줄 조작 (Baxter 로봇)**:
  - 'S'자 모양 밧줄 조작 및 매듭 묶기 작업을 수행했습니다. 매듭 모양은 자율 탐색 데이터에 포함되지 않았습니다.
  - 매듭 묶기 작업에서 제안된 전방 일관성 GSP는 60%의 성공률을 달성하여, 기존 역모델 베이스라인(36%) 및 전방 정규화 GSP(44%)를 크게 능가했습니다.
- **실내 사무실 내비게이션 (TurtleBot)**:
  - 미개척 사무실 환경에서 단일 목표 이미지를 찾거나 스파스한 랜드마크 이미지 시퀀스를 따르는 작업을 평가했습니다.
  - **목표 찾기**: 초기 관찰과 목표 관찰 사이에 시각적 중첩이 전혀 없는 상황에서, GSP는 8번의 시도 중 6번 성공하여 탐색적 회전 행동을 학습했음을 보여주었습니다 (베이스라인 0번 성공).
  - **시각 모방**: 스파스한 랜드마크 이미지 시연을 따르는 미로 및 루프 작업에서 SIFT 기반 방법 및 다른 GSP 변형들보다 우수한 성능을 보였습니다.
- **3D 내비게이션 (VizDoom 시뮬레이션)**:
  - **탐색 데이터의 역할**: 호기심 기반 탐색으로 수집된 데이터가 모든 방법에서 무작위 탐색 데이터보다 최종 모방 성능을 유의미하게 향상시켰습니다.
  - **전방 일관성 손실의 효과**: 제안된 전방 일관성 GSP (특히 특징 공간 동학 사용 시)는 새로운 지도와 새로운 텍스처에 대한 일반화 능력이 다른 베이스라인들보다 우수했습니다. 이는 전방 일관성이 단순히 정책 특징을 정규화하는 것 이상의 역할을 함을 시사합니다.

## 🧠 Insights & Discussion

- **주요 시사점**: 전방 일관성 손실은 행동 공간의 다중 모드성에도 불구하고 에이전트가 목표 상태를 달성하는 데 초점을 맞춰 견고한 정책을 학습하는 데 핵심적인 역할을 합니다. 특징 공간에서 동학을 학습하는 것이 일반화 성능 향상에 기여합니다.
- **한계점**:
  - 시연이 1인칭 시점에만 국한됩니다. 3인칭 시연으로의 확장이 필요합니다.
  - 전문가 시연과 에이전트 실행 시의 시각적 관찰 통계(예: 조명 조건)가 유사하다는 암묵적 가정이 있습니다. 환경 변화에 강건하려면 도메인 적응(domain adaptation)이 필요합니다.
  - 학습된 GSP의 품질은 자율 탐색 데이터의 품질과 다양성에 의해 제한됩니다. 무작위 탐색은 복잡한 궤적을 학습하기 어렵고, 호기심 기반 탐색이 이를 보완합니다.
  - 현재 프레임워크는 전문가 시연을 단순히 '모방'할 뿐, 전문가로부터 '학습'하여 에이전트의 탐색을 유용한 부분으로 유도하지는 않습니다.
- **향후 연구**: 3인칭 시연으로의 확장, 환경 변화에 강건한 GSP 개발, 전문가 시연을 통해 에이전트의 탐색을 유도하는 방법 연구, 시각적 목표 외에 자연어 명령을 목표 특징 공간으로 매핑하여 활용하는 방안 등이 있습니다.

## 📌 TL;DR

- **문제**: 기존 로봇 모방 학습은 전문가의 행동 데이터에 대한 강한 감독이 필수적이며, 행동 분포의 다중 모드성 문제가 학습을 어렵게 합니다.
- **제안 방법**: 이 논문은 전문가의 행동 감독 없이 로봇이 자율 탐색을 통해 '목표 조건부 기술 정책(GSP)'을 학습하는 **제로샷 시각 모방** 방법을 제안합니다. 특히, 예측 행동의 '다음 상태' 일치도를 기준으로 학습하는 **전방 일관성 손실**을 도입하여 다중 모드 행동 분포 문제를 효과적으로 해결하고, 목표 인식기를 통해 유연한 다단계 목표 달성을 가능하게 합니다.
- **핵심 결과**: Baxter 로봇 밧줄 조작 (매듭 묶기 60% 성공률), TurtleBot 실내 내비게이션, VizDoom 3D 내비게이션에서 베이스라인 대비 우수한 성능과 새로운 환경에 대한 뛰어난 일반화 능력을 보였습니다. 호기심 기반 탐색이 더욱 유능한 정책 학습에 기여하며, 전방 일관성 손실이 정책의 견고함과 일반화에 핵심적임을 입증했습니다.
