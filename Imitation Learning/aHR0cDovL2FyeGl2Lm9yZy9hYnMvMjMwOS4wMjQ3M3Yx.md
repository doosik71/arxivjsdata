# A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges

Maryam Zare, Parham M. Kebria,Member, IEEE,Abbas Khosravi,Senior Member, IEEE, and Saeid Nahavandi,Fellow, IEEE

## 🧩 Problem to Solve

로봇 공학 및 인공지능(AI) 시스템이 자율 주행, 항공 로봇, 자연어 처리와 같은 복잡하고 비정형적인 환경에서 활용되면서, 이러한 시스템의 행동을 수동으로 프로그래밍하거나 보상 함수(강화 학습에서와 같이)를 통해 정의하는 것이 극도로 어려워졌습니다. 이는 이러한 환경이 높은 수준의 유연성과 적응성을 요구하여 모든 가능한 상황을 설명할 수 있는 최적의 규칙이나 보상 신호를 지정하기 어렵기 때문입니다. 이 논문은 이러한 환경에서 전문가의 시연을 모방하여 원하는 행동을 학습하는 **모방 학습(Imitation Learning, IL)**의 포괄적인 개요를 제공하고, 그 기본 가정, 접근 방식, 최근 발전, 직면한 도전 과제 및 미래 연구 방향을 제시하는 것을 목표로 합니다.

## ✨ Key Contributions

* **IL 알고리즘의 포괄적인 분류:** 행동 복제(Behavioral Cloning, BC), 역 강화 학습(Inverse Reinforcement Learning, IRL), 적대적 모방 학습(Adversarial Imitation Learning, AIL), 관찰 기반 모방 학습(Imitation from Observation, IfO)을 포함한 주요 IL 알고리즘을 소개하고 각 방법의 장점과 한계를 분석합니다.
* **최신 발전 및 신흥 연구 분야 상세 설명:** 각 IL 범주 내의 최근 연구 발전 사항과 새로운 연구 방향을 상세히 기술합니다.
* **주요 도전 과제 및 해결책 논의:** 불완전한 시연(Imperfect Demonstrations) 및 도메인 불일치(Domain Discrepancies)와 같은 IL의 일반적인 도전 과제와 이를 해결하기 위한 연구자들의 접근 방식을 논의합니다.
* **미래 연구 방향 제시:** IL 분야의 향후 연구를 위한 잠재적인 기회와 방향을 제안하여 연구자와 실무자가 분야를 탐색하는 데 도움을 줍니다.
* **광범위한 청중에게 접근성 향상:** 방대한 IL 연구를 통합하여 관련 분야의 연구자들을 포함한 더 넓은 청중이 IL 개념과 기술을 이해하기 쉽게 만듭니다.

## 📎 Related Works

이 논문은 기존 IL 연구의 광범위한 문헌을 참조하며, 특히 IL의 발전과 도전 과제를 다룬 이전 서베이 논문들과 차별점을 명확히 합니다.

* **Schaal [3]:** 초기 IL 서베이로, 휴머노이드 로봇 생성 경로로서의 IL에 중점을 두었습니다.
* **Osa et al. [1]:** IL에 대한 알고리즘적 관점을 제공했습니다.
* **Hussein et al. [12]:** IL 과정의 각 단계에 대한 설계 옵션을 포괄적으로 검토했습니다.
* **Le Mero et al. [7]:** 엔드-투-엔드 자율 주행 시스템을 위한 IL 기반 기술에 대한 포괄적인 개요를 제공했습니다.

본 서베이는 이러한 선행 연구들을 넘어서 빠르게 발전하는 IL 분야의 최신 동향과 도전 과제를 포괄적으로 다루며, 새로운 알고리즘과 기술, 다양한 응용 분야를 업데이트하여 제공합니다.

## 🛠️ Methodology

이 논문은 IL 접근 방식을 역사적, 논리적 이유에 따라 체계적으로 분류하고 논의합니다.

1. **행동 복제 (Behavioral Cloning, BC)**
    * **개념:** 모방 학습 문제를 지도 학습 작업으로 취급하여, 환경 상태를 해당 전문가 행동에 매핑하는 모델을 훈련합니다.
    * **장점:** 환경의 동역학에 대한 지식이 필요 없으며, 계산 효율적입니다.
    * **주요 단점:** **공변량 이동(Covariate Shift)** 문제 (훈련 시 전문가 정책이 생성한 상태에서 학습하지만, 테스트 시 학습 에이전트의 행동으로 인해 발생하는 상태에서 테스트되어 분포 불일치 발생).
    * **공변량 이동 해결 방법:**
        * **상호작용적 IL (Interactive IL):** 훈련 중 온라인 전문가에게 접근하여 데이터를 다시 레이블링하도록 요청합니다 (예: DAgger [14], Human-gated [21], Robot-gated [20]).
        * **전문가 점유 측정(Expert Occupancy Measure) 추정 및 RL 사용:** 전문가 정책의 지원을 추정하고 에이전트가 그 지원 범위 내에 머무르도록 유도하는 보상을 정의합니다 (예: SQIL [17]).
        * **제약 조건부 IL (Constrained IL):** 에이전트를 시연이 커버하는 공간의 알려진 영역으로 제한합니다 (예: 나쁜 운전을 막기 위한 손실 추가 [31]).
    * **인과 관계 오인식(Causal Misidentification):** 전문가 행동의 근본 원인을 식별하지 못하고 불필요한 상관 관계를 모방하는 문제. "따라쟁이 문제(Copycat Problem)" [36]를 다루기 위해 과거 행동에 대한 정보를 무시하는 특징 표현을 학습합니다.
    * **내재적 BC (Implicit BC):** 에너지 기반 모델링을 통해 불연속성을 더 잘 표현하여 전통적인 명시적 모델의 한계를 극복합니다 [38].

2. **역 강화 학습 (Inverse Reinforcement Learning, IRL)**
    * **개념:** 최적으로 행동하는 전문가의 관찰된 시연으로부터 그 기저에 있는 보상 함수를 추론하고, 이 보상 함수를 최적화하여 학습 정책을 훈련합니다.
    * **장점:** 강화 학습 에이전트는 환경과 지속적으로 상호작용하며 실수를 복구할 수 있어 BC보다 공변량 이동에 덜 민감합니다.
    * **주요 단점:**
        * **계산 비용 및 자원 집약적:** 보상 추정 및 정책 훈련을 반복하는 과정이 샘플 효율성이 낮습니다.
        * **본질적인 모호성:** 단일 정책이 무한한 수의 보상 함수에 대해 최적일 수 있습니다.
    * **모호성 해결 방법:**
        * **최대 마진 방법 (Maximum-margin Methods):** 주어진 정책이 다른 모든 정책보다 마진을 두고 더 철저히 설명하는 보상 함수를 추론합니다 (예: Ng et al. [50], MMP [54]).
        * **최대 엔트로피 방법 (Maximum-entropy Methods):** 결과 정책의 엔트로피를 최대화하여 모호성을 해결하며, 전문가의 비최적성 및 확률성을 처리할 수 있습니다 (예: MaxEntIRL [47], 심층 신경망을 활용한 Maximum Entropy Deep IRL [61], GCL [62]).
        * **베이즈 알고리즘 (Bayesian Algorithms):** 전문가의 행동을 증거로 사용하여 보상 함수 추정치의 사후 분포를 업데이트합니다 (예: BIRL [63], MAP 추론 [64]). 샘플링 효율성 및 확장성 문제를 해결하기 위해 시연 선호도 레이블 [66] 또는 변분 추론 [67]을 활용합니다.

3. **적대적 모방 학습 (Adversarial Imitation Learning, AIL)**
    * **개념:** IRL의 계산적 한계를 해결하기 위해 매 반복마다 RL 하위 문제를 완전히 해결하지 않고 최적 정책을 찾는 방법입니다. 생성적 적대 신경망(GAN) [80]과 유사하게, 에이전트(생성자)와 판별자(adversary) 간의 2인 게임을 통해 에이전트가 전문가 궤적과 유사한 궤적을 생성하도록 학습합니다.
    * **GAIL (Generative Adversarial Imitation Learning) [46]:** 판별자 네트워크가 전문가와 에이전트의 궤적을 구별하도록 훈련되며, 판별자의 혼란도를 기반으로 보상 신호를 파생합니다.
    * **발전:** 샘플 효율성, 확장성, 견고성 개선을 위해 판별자의 손실 함수 변경 [76], 온-정책(on-policy) 에이전트에서 오프-정책(off-policy) 에이전트로 전환 [77], Wasserstein 거리 [78]와 같은 새로운 유사성 측정 활용 [29] 등의 연구가 진행되었습니다.

4. **관찰 기반 모방 학습 (Imitation from Observation, IfO)**
    * **개념:** 학습자가 상태 정보만 가지고 있고 전문가의 행동 정보는 알 수 없는 상황에서 모방 학습을 수행합니다. 이는 인간이 다른 사람의 저수준 행동을 모르는 채 관찰을 통해 학습하는 방식과 유사합니다.
    * **장점:** 온라인 비디오와 같은 행동 정보가 없는 방대한 자료를 활용할 수 있으며, 다른 신체 구조를 가진 에이전트로부터 학습할 가능성을 엽니다.
    * **주요 접근 방식:**
        * **맥락 인식 번역 (Context-aware Translation) [83]:** 전문가의 맥락(예: 3인칭 시점)에서 에이전트의 맥락(예: 1인칭 시점)으로 시연을 변환하여 에이전트의 맥락에서 전문가 행동을 예측합니다.
        * **시간 대조 네트워크 (Time-Contrastive Networks, TCN) [88]:** 다른 시점과 신체 구조에 불변하는 특징 임베딩을 학습합니다.
        * **관찰 기반 행동 복제 (Behavioral Cloning from Observation, BCO) [89]:** 역 동역학 모델을 학습하여 전문가 시연의 누락된 행동을 추론한 다음, 일반적인 BC를 수행합니다.
        * **관찰 기반 생성적 적대적 모방 (Generative Adversarial Imitation from Observation, GAIfO) [84]:** GAIL 목표를 IfO에 적용하여 전문가와 에이전트의 상태-전이 분포를 일치시킵니다.
        * **비적대적 관찰 기반 IRL [91]:** 조건부 상태 전이 확률을 일치시켜 전문가와 학습자 설정 간의 관련 없는 차이에 덜 민감하게 반응합니다.
        * **크로스 도메인 IfO [87]:** 쌍을 이루지 않고 정렬되지 않은 시연으로부터 소스 및 타겟 도메인 간의 상태 맵을 학습하여 신체 구조, 시점, 동역학 불일치를 해결합니다.

## 📊 Results

본 서베이 논문은 새로운 실험 결과를 제시하기보다는, 모방 학습 분야의 다양한 알고리즘과 접근 방식이 달성한 성과를 요약하고 평가합니다.

* **BC의 실용성:** 간단하고 효율적이지만, 공변량 이동 문제를 해결하기 위한 DAgger [14], SQIL [17], 제약 조건부 IL [31] 등의 개선을 통해 견고성이 향상되었습니다. 내재적 BC [38]는 로봇 조작 작업에서 기존의 명시적 BC 모델을 능가하는 성능을 보여줍니다.
* **IRL의 강점:** RL의 장점(오류 복구 능력)을 활용하여 BC보다 공변량 이동에 덜 민감합니다. 최대 마진 [50], 최대 엔트로피 [47], 베이즈 [63] 방법들은 보상 함수의 모호성을 해결하며, 딥러닝과 결합된 MaxEntIRL [61] 및 GCL [62]은 복잡한 비선형 보상 함수 학습 및 샘플 효율성 개선을 가능하게 합니다.
* **AIL의 효율성:** IRL의 계산적 한계를 극복하며, GAIL [46]을 필두로 로봇 공학, 자율 주행, 게임 플레이와 같은 다양한 벤치마크 환경에서 기존 방법보다 통계적으로 유의미한 성능 향상을 입증했습니다. Wasserstein 거리를 활용한 방법 [77, 79]은 훈련 안정성을 높입니다.
* **IfO의 잠재력:** 전문가의 행동 정보 없이 상태 관찰만으로 학습하는 IfO [83, 88, 89]는 YouTube 비디오와 같은 방대한 온라인 자원을 활용할 수 있는 가능성을 열어주며, 다른 신체 구조를 가진 에이전트로부터 학습하는 데 중요한 발전을 이루고 있습니다. 특히, 최적 이하의 순위가 매겨진 시연으로부터 전문가의 의도를 추론하는 IRL [97]은 전문가보다 나은 성능을 달성할 수 있음을 보여줍니다.
* **도전 과제 해결:** 불완전한 시연 [99, 100, 101, 103] 및 동역학 [104], 시점 [88, 105], 신체 구조 [106, 107]와 같은 도메인 불일치를 해결하기 위한 다양한 연구가 진행되어 IL의 실세계 적용 가능성을 확장하고 있습니다.

## 🧠 Insights & Discussion

* **IL의 중요성:** 복잡하고 비정형적인 환경에서 AI 에이전트의 행동을 정의하는 데 있어 수동 프로그래밍이나 강화 학습의 보상 함수 설계의 한계를 극복하는 핵심적인 대안입니다. 특히 전문가의 시연을 통해 학습하는 능력은 로봇 공학 및 AI 시스템의 자율성 및 적응성 향상에 필수적입니다.
* **BC, IRL, AIL, IfO의 상호 보완성:** 각 접근 방식은 고유한 장점과 한계를 가지며, 서로의 단점을 보완하며 발전해왔습니다. 예를 들어, BC의 공변량 이동 문제를 해결하기 위해 IRL이 등장했고, IRL의 계산 복잡성을 줄이기 위해 AIL이 개발되었습니다. 또한, 실제 세계의 다양한 데이터 소스를 활용하기 위해 IfO가 중요해지고 있습니다.
* **주요 한계 및 도전 과제:**
  * **불완전한 시연 (Imperfect Demonstrations):** 대규모의 고품질 시연을 얻는 것이 어렵고, 인간 전문가의 실수나 다양한 전문성 수준으로 인해 비최적 시연이 발생할 수 있습니다. 이를 해결하기 위해 신뢰도 점수 부여, 샘플 재가중치, 전문가의 전문성 추정 [103] 등의 방법이 연구되고 있습니다.
  * **도메인 불일치 (Domain Discrepancies):** 전문가와 에이전트 간의 상태-행동 공간, 동역학, 시점, 신체 구조의 차이로 인해 학습된 정책을 다른 도메인에 적용하기 어렵습니다. 이 문제를 해결하기 위해 도메인 불변 특징 공간 학습 [88], 적대적 프레임워크 [105], 목표 지향적 접근 방식 [106], 최적 수송(Optimal Transport) [107] 등의 연구가 활발히 진행 중입니다.
* **미래 연구 방향 및 기회:**
  * **불완전한 시연으로부터의 학습:** 비최적 또는 노이즈가 있는 시연에서 유용한 정보를 추출하고, 다양한 전문성 수준을 가진 시연자로부터 효과적으로 학습하는 알고리즘 개발이 중요합니다.
  * **크로스 도메인 학습:** 동역학, 시점, 신체 구조가 다른 도메인 간에 지식을 전이할 수 있는 일반화된 IL 알고리즘 개발이 필수적입니다.
  * **다양하고 대규모 시연 수집:** 일반화 가능한 정책 훈련을 위해 대규모의 다양하고 고품질 시연 데이터를 효과적으로 수집하고 활용하는 방법론이 필요합니다.
  * IL은 AI가 새로운 작업과 환경에 적응하고 궁극적으로 더 높은 수준의 지능을 달성하여 실제 세계 응용을 위한 길을 여는 데 결정적인 역할을 할 것입니다.

## 📌 TL;DR

* **문제:** 복잡한 환경에서 AI/로봇의 행동을 수동으로 프로그래밍하거나 보상 함수를 정의하기 어렵습니다. 모방 학습(IL)은 전문가 시연을 통해 행동을 학습하는 대안을 제시합니다.
* **방법:** 이 서베이는 IL을 크게 행동 복제(BC, 지도 학습), 역 강화 학습(IRL, 보상 함수 추론), 적대적 모방 학습(AIL, GAN 기반 효율성), 관찰 기반 모방 학습(IfO, 상태만으로 학습)으로 분류하고 설명합니다. 각 방법은 공변량 이동, 인과 관계 오인식, 계산 효율성 등의 문제를 해결하기 위한 고유한 접근 방식을 가집니다.
* **발견:** IL은 복잡한 행동 학습에 효과적이지만, 불완전하거나 비최적적인 시연 및 전문가-학습자 간의 도메인 불일치(동역학, 시점, 신체 구조)라는 두 가지 주요 도전 과제에 직면해 있습니다. 현재 연구는 시연 재가중치, 전문가 전문성 추정, 크로스 도메인 매핑, 자율 지도 특징 추출 등을 통해 이러한 한계를 극복하려 합니다. IL은 미래 AI 시스템의 핵심적인 학습 패러다임으로, 실제 세계 응용을 위한 중요한 기회를 제공합니다.
