# An Algorithmic Perspective on Imitation Learning

Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, Jan Peters

## 🧩 Problem to Solve

로봇 및 기타 지능형 에이전트가 단순한 환경에서 복잡하고 비정형적인 환경으로 이동함에 따라, 이들의 행동을 수동으로 프로그래밍하는 것이 점점 더 어려워지고 비용이 많이 들게 되었습니다. 종종 원하는 행동을 수동으로 설계하려는 시도보다 교사가 시연하는 것이 더 쉽습니다. 이러한 시연을 통한 학습, 그리고 이를 수행하는 알고리즘 연구를 **모방 학습(Imitation Learning)**이라고 합니다. 이 논문은 모방 학습의 기본 가정, 접근 방식, 상호 관계, 그리고 이를 해결하기 위해 개발된 다양한 알고리즘을 소개하며, 특히 로봇 공학 분야에서 발생하는 도전 과제와 통찰력을 제공합니다.

## ✨ Key Contributions

- **포괄적인 소개**: 모방 학습의 기본 개념, 가정, 접근 방식, 그리고 지도 학습이나 강화 학습과 같은 다른 기계 학습 프레임워크와의 차이점을 명확하게 제시합니다.
- **알고리즘 분류**: 모방 학습을 크게 **행동 복제(Behavioral Cloning, BC)**와 **역 강화 학습(Inverse Reinforcement Learning, IRL)**의 두 가지 주요 범주로 나누어 설명합니다.
- **세부적인 알고리즘 분석**: 각 범주 내에서 모델 기반(model-based) 및 모델 프리(model-free) 방법론을 포함하여 다양한 알고리즘의 세부 사항과 설계 선택 사항을 깊이 있게 다룹니다.
- **정보 이론적 이해**: M-투영(M-projection) 및 최대 엔트로피 원리(Maximum Entropy Principle)와 같은 정보 이론적 관점에서 모방 학습 알고리즘을 분석하여 상호 관계를 밝힙니다.
- **로봇 공학 적용 사례**: RC 헬리콥터 곡예 비행, 수술 로봇 매듭 묶기, 자율 주행, 발걸음 계획 등 다양한 로봇 응용 사례를 통해 각 방법론의 특성과 동기를 설명합니다.
- **미해결 과제 및 미래 연구 방향 제시**: 모방 학습 연구의 현재 한계점과 미래 연구를 위한 다양한 미해결 질문들을 제시합니다.

## 📎 Related Works

- **ALVINN [Pomerleau, 1988]**: 신경망을 활용한 초기 자율 주행 시스템으로, 모방 학습의 초기 성공 사례 중 하나입니다.
- **Learn from Demonstration (LfD) [Schaal, 1997; Atkeson and Schaal, 1997]**: 1990년대 후반 로봇 공학에서 작업 수준 계획에서 궤적 수준 계획으로 초점을 이동하는 데 기여했습니다.
- **AlphaGo [Silver et al., 2016]**: 인간 전문가 시연을 통해 딥 신경망 정책을 초기화하여 바둑에서 뛰어난 성능을 달성했습니다.
- **DA$GGER$ (Dataset Aggregation) [Ross et al., 2011]**: 학습자의 정책에 의해 유도된 상태 분포에서 전문가 시연을 수집하여 연속 의사 결정에서 오류 전파(cascading errors) 문제를 완화하는 반복적 접근 방식입니다.
- **Dynamic Movement Primitives (DMPs) [Schaal et al., 2004; Ijspeert et al., 2013]**: 비선형 동적 시스템을 사용하여 로봇 움직임을 표현하며, 부드러운 수렴과 안정성을 보장합니다.
- **Probabilistic Movement Primitives (ProMPs) [Paraschos et al., 2013]**: 궤적에 대한 확률 분포로 움직임을 표현하여 시연의 확률적 특성을 모델링하고, 가우시안 조건화를 통해 새로운 상황에 일반화합니다.
- **Maximum Margin Planning (MMP) [Ratliff et al., 2006b]**: 최적 정책과 다른 대안 정책 간의 마진을 최대화하는 비용 함수를 찾는 역 강화 학습 방법론입니다.
- **Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) [Ziebart et al., 2008]**: 특징 기대를 일치시키면서 엔트로피를 최대화하는 궤적 분포를 찾아 보상 함수를 복구하는 방법입니다.

## 🛠️ Methodology

이 논문은 모방 학습 알고리즘을 크게 두 가지 범주인 행동 복제(Behavioral Cloning, BC)와 역 강화 학습(Inverse Reinforcement Learning, IRL)으로 나누어 설명하며, 각각 모델 프리(model-free) 및 모델 기반(model-based) 접근 방식을 다룹니다.

### 1. 행동 복제 (Behavioral Cloning, BC)

전문가의 행동을 직접 복제하기 위해 입력에서 행동/궤적으로의 직접 매핑을 학습합니다. 일반적으로 지도 학습 문제로 정식화됩니다.

- **문제 정식화**:
  - **궤적 학습**: 주어진 컨텍스트 $s$에 대해 원하는 궤적 $\tau_d = \pi(s)$를 생성하는 정책을 학습합니다.
  - **상태-행동 공간 학습**: 주어진 상태 $x_t$ 및/또는 컨텍스트 $s$에 대해 제어 입력 $u_t = \pi(x_t, s)$를 생성하는 정책을 학습합니다.
- **기본 절차 (Algorithm 1)**:
  1. 전문가 시연 궤적 데이터셋 $D$를 수집합니다.
  2. 적절한 정책 표현 $\pi_\theta$를 선택합니다.
  3. 시연된 행동과 학습자의 정책 간의 유사성을 나타내는 목적 함수 $L$을 선택합니다.
  4. 수집된 $D$를 사용하여 정책 매개변수 $\theta$를 최적화합니다.
- **설계 선택**:
  - **대리 손실 함수**: 이차 손실($\ell_2$-loss), $\ell_1$-loss, 로그 손실(교차 엔트로피), 힌지 손실, 쿨백-라이블러(KL) 발산 등이 사용될 수 있습니다.
  - **회귀 방법**: 선형 회귀, 신경망, 가우시안 혼합 회귀(GMR), 가우시안 프로세스 회귀(GPR) 등이 사용됩니다.
- **주요 도전 과제**: 학습자의 정책으로 인해 전문가가 겪지 않은 새로운 상태에 직면하게 되는 **공변량 이동(covariate shift)** 또는 **오류 전파(cascading errors)** 문제입니다.
- **해결 접근 방식**:
  - **DA$GGER$ (Dataset Aggregation) [Ross et al., 2011]**: 학습자의 현재 정책으로 궤적을 실행하고, 새로운 상태에서 전문가의 교정 행동을 요청하여 데이터셋에 추가(집계)하는 반복적인 온라인 학습 프레임워크입니다. 이를 통해 학습자 자신의 상태 분포에 대한 전문가 시연을 수집합니다.
  - **궤적 표현 (Model-Free BC)**:
    - **동적 움직임 원형(DMPs)**: 비선형 힘 항과 끌림 힘 항의 조합으로 움직임을 모델링하여 안정적이고 부드러운 궤적 생성을 보장합니다.
    - **확률적 움직임 원형(ProMPs)**: 궤적에 대한 확률 분포를 학습하여 시연의 확률적 특성을 모델링하고, 가우시안 조건화를 통해 새로운 상황에 일반화할 수 있습니다.
    - **시간 불변 동적 시스템(SEDS)**: 시스템의 상태와 움직임의 공동 분포를 학습하여 안정적인 목표 수렴을 보장하는 시간 불변 동적 시스템으로 움직임을 표현합니다.
  - **Model-Based BC**: 시스템 동역학 모델 $x_{t+1} = f(x_t, u_t)$를 명시적으로 학습하고, 이를 활용하여 학습자의 신체(embodiment)와 다른 전문가의 시연을 적응시키거나 저구동 로봇의 궤적을 계획합니다.

### 2. 역 강화 학습 (Inverse Reinforcement Learning, IRL)

전문가의 시연으로부터 그들이 최적화하려는 숨겨진 보상 함수를 복구합니다. 보상 함수를 복구하는 것이 행동을 설명하는 가장 간결한 방법일 때 유용합니다.

- **문제 정식화**: 전문가 시연 $\mathcal{D} = \{\tau_i\}_{i=1}^N$과 시스템 환경 모델로부터, 전문가의 행동을 (근사적으로) 최적화하는 보상 함수 $R(\tau)$를 찾습니다.
- **주요 도전 과제**: 여러 보상 함수가 동일한 최적 정책을 유도할 수 있어 문제가 **ill-posed**합니다.
- **해결 접근 방식**:
  - **최대 마진 계획(Maximum Margin Planning, MMP) [Ratliff et al., 2006b]**: 최적 정책과 다른 모든 정책 간의 "마진"을 최대화하는 보상 함수를 찾습니다.
  - **최대 엔트로피 IRL [Ziebart et al., 2008]**: 주어진 특징 기대를 만족하는 궤적 분포 중에서 엔트로피를 최대화하는 분포를 찾아 유일한 보상 함수를 복구합니다. 보상 함수 $R(\tau)$는 궤적 특징 $\phi(\tau)$에 선형으로 가정됩니다: $p(\tau) \propto \exp(R(\tau))$.
  - **최대 인과 엔트로피 IRL [Ziebart, 2010]**: 확률적 동역학 환경에서 "인과적 엔트로피"를 최대화하여 행동 선택이 미래 상태와 독립적으로 이루어지도록 하며, 환경 자체의 확률적 동역학으로 인한 "보너스 엔트로피"를 제거합니다.
  - **비선형 보상 함수 학습**: 부스팅(boosting) 방법, 딥 신경망, 가우시안 프로세스 IRL(GPIRL) [Levine et al., 2011] 등을 사용하여 복잡한 작업에 대한 비선형 보상 함수를 학습합니다.
  - **유도 비용 학습(Guided Cost Learning, GCL) [Finn et al., 2016b]**: 신경망으로 비용 함수를 표현하고, 샘플링된 궤적과 시연을 기반으로 비용 함수를 업데이트하며, 새로운 비용 함수에 따라 정책을 업데이트하는 반복적인 접근 방식입니다. 생성적 적대 신경망(GANs) [Goodfellow et al., 2014]과의 깊은 관련성을 가집니다.
  - **Model-Free IRL**: 시스템 동역학에 대한 사전 지식 없이 샘플링 기반 방법을 사용하여 궤적 분포를 추정하고 보상 함수를 학습합니다. **상대 엔트로피 IRL [Boularias et al., 2011]** 및 **생성적 적대적 모방 학습(GAIL) [Ho and Ermon, 2016]**이 이 범주에 속합니다. GAIL은 판별자를 훈련하여 학습자의 궤적과 전문가의 시연을 구별하도록 하고, 이 판별자를 사용하여 전문가 행동을 모방하는 정책을 훈련합니다.

## 📊 Results

이 논문은 다양한 로봇 공학 애플리케이션에서 행동 복제(BC)와 역 강화 학습(IRL) 방법론의 성공적인 적용 사례들을 제시합니다.

- **모델 프리 행동 복제(Model-Free BC)의 응용**:
  - **DMP를 사용한 볼 히팅**: DMPs [Ijspeert et al., 2002b]와 리드믹 DMP [Kober and Peters, 2009]를 사용하여 테니스 스윙, 볼 패들링(Ball-Paddling)과 같은 점대점(point-to-point) 및 주기적인 움직임을 학습하고, 이를 휴머노이드 로봇에 성공적으로 재현합니다.
  - **ProMP를 사용한 핸드오버 작업**: Interaction ProMPs [Maeda et al., 2016]를 활용하여 인간-로봇 협업 환경에서 로봇이 인간 작업자의 움직임에 반응하여 접시나 나사를 건네는 것과 같은 결합된 움직임을 학습합니다.
  - **가우시안 프로세스를 사용한 매듭 묶기**: [Osa et al., 2017b]는 가우시안 프로세스로 궤적 분포를 모델링하여 수술 로봇이 매듭 묶기 작업에서 실시간으로 상황에 맞는 궤적을 계획하고 접촉 힘을 제어하도록 학습시킵니다.
- **모델 기반 행동 복제(Model-Based BC)의 응용**:
  - **곡예 헬리콥터 비행**: [Abbeel et al., 2010]은 반복 LQR 제어기와 모델 기반 BC를 결합하여 RC 헬리콥터가 인플립, 롤, 루프, 오토-로테이션 랜딩 등 고도로 비선형적인 곡예 비행을 수행하도록 학습시킵니다.
  - **저구동 로봇으로 공 치기**: [Englert et al., 2013]은 가우시안 프로세스를 사용하여 시스템의 순방향 모델을 학습하고, 이를 통해 저구동 로봇이 볼 히팅 작업을 수행하도록 모델 기반 모방 학습을 적용합니다.
  - **DA$GGER$를 사용한 제어 학습**: [Ross et al., 2011]은 DA$GGER$ 알고리즘을 통해 비디오 게임 플레이를 학습하고, [Ross et al., 2013]은 실제 숲 환경에서 나무를 회피하는 UAV 자율 비행 제어기를 학습하는 데 성공했습니다.
- **역 강화 학습(IRL)의 응용**:
  - **자동차 시뮬레이터에서 운전 학습**: [Abbeel and Ng, 2004]는 모델 기반 IRL을 사용하여 자동차 시뮬레이터에서 다양한 운전 스타일을 학습하고 모방하는 능력을 시연합니다.
  - **MMP/LEARCH를 사용한 경로 계획**: [Ratliff et al., 2006b; Silver et al., 2010; Zucker et al., 2011]은 MMP 및 LEARCH를 적용하여 시각/라이다 입력과 같은 원시 지각 데이터로부터 사람과 유사한 거친 지형 내비게이션 및 발걸음 계획 전략을 학습합니다.
  - **딥 유도 비용 학습을 사용한 모션 계획**: [Finn et al., 2016b]은 딥 신경망으로 보상 함수를 표현하는 유도 비용 학습을 PR2 로봇의 집안일(설거지, 물 따르기)과 같은 조작 작업에 적용하여 비선형 보상 함수가 필요한 복잡한 작업을 학습합니다.
  - **상대 엔트로피 IRL을 사용한 Ball-in-a-Cup**: [Boularias et al., 2011]은 모델 프리 상대 엔트로피 IRL을 저구동 로봇의 Ball-in-a-Cup 작업에 적용하여 시스템 동역학에 대한 사전 지식 없이 성공적으로 모방 학습을 수행합니다.

## 🧠 Insights & Discussion

- **행동 복제(BC) vs. 역 강화 학습(IRL) 선택**: 문제의 특성에 따라 BC와 IRL 중 무엇이 더 적합한지 결정하는 것이 중요합니다.
  - **IRL의 장점**: 전문가의 의도를 추론해야 하는 상황(예: 시각 정보만으로 학습)이나, 기술의 일반화 가능성이 보상 함수에 더 간결하게 인코딩될 수 있는 경우(예: 다양한 지형에 대한 발걸음 계획) IRL이 유용합니다.
  - **BC의 장점**: 보상 함수를 추론할 필요 없이 행동의 분포가 직접적으로 간결한 설명인 경우(예: 조작 작업에서 필요한 궤적 분포 예측) BC가 더 효율적입니다.
  - **수렴**: 최대 엔트로피 가정 하에 IRL은 상태-행동 기대치를 일치시키는 BC 방법의 이중(dual) 문제로 해석될 수 있어, 특정 가정 하에서는 BC와 IRL이 동등할 수 있습니다.
- **시연 데이터 관련 미해결 과제**:
  - **다중 전문가로부터의 학습**: 여러 전문가의 시연을 효과적으로 통합하는 방법.
  - **시연 내 바람직하지 않은 행동 처리**: 시연에 포함된 불필요하거나 비최적적인 움직임을 감지하고 제거하는 방법.
  - **원시 감각 입력(예: 시각)으로부터 학습**: 로봇의 신체 정보 없이 원시 시각 정보만으로 작업을 학습하는 방법(3인칭 모방 학습 포함).
  - **관련 작업 간 지식 전이**: 다른 작업의 과거 시연을 활용하여 현재 작업을 더 빠르게 학습하는 방법.
- **설계 선택 관련 미해결 과제**:
  - **최적의 정책 유사성 측정**: KL 발산, 유클리드 거리 외에 Wasserstein 발산 등 새로운 유사성 측정 방법 탐색.
  - **다양한 지시 유형 통합**: 교정 동작, 선호도, 평가 등 다양한 형태의 인간 지시를 학습에 효과적으로 통합하는 방법.
  - **사전 지식 명시적 통합**: 로봇의 운동학, 질량 등 시스템이나 환경에 대한 사전 지식을 모방 학습에 명시적으로 통합하여 데이터 효율성 개선.
  - **다양한 센서 정보 융합**: 촉각, RGB-D 이미지, 오디오 등 여러 감각 정보를 융합하여 더 강건하고 적응적인 행동 학습.
  - **인간을 능가하는 작업 학습**: 인간이 시연하기 어렵거나 로봇이 물리적으로 이점을 가지는 작업을 학습하고 성능을 개선하는 방법.
  - **궤적 표현 선택**: 특정 애플리케이션에 가장 적합한 궤적 표현을 선택하는 기준(모델 선택 문제와 유사).
- **알고리즘 관련 미해결 과제**:
  - **복잡한 조건에서의 기술 일반화**: 고차원 입력(예: 시각 정보)과 복잡한 상황에 대한 궤적 일반화 스케일링.
  - **성능 보장**: 모방 학습 방법론에 대한 안정성, 수렴성 등 이론적 성능 보장 확립.
  - **차원 수에 따른 확장**: 고차원 상태 및 행동 공간(예: 다관절 휴머노이드 로봇)에서의 효율적인 학습 방법.
  - **고차원 공간에서 전역 최적해 탐색**: 지역 최적해를 넘어 전역 최적해를 찾고 이를 효율적으로 수행하는 방법.
  - **다중 에이전트 모방**: 다중 에이전트 환경에서 다른 에이전트의 행동을 고려하여 모방하는 방법.
  - **IRL에서의 점진적/능동적 학습**: 시연이 부족할 경우, 정책을 점진적으로 개선하거나 능동적으로 추가 시연을 요청하는 IRL 방법.
- **성능 평가**:
  - **벤치마크 문제 확립**: 모방 학습 연구를 위한 널리 인정되는 표준 벤치마크 문제 개발.
  - **평가 메트릭 확립**: 모방 학습 방법론의 성능을 객관적으로 비교하고 대조하기 위한 효과적인 메트릭 개발.

## 📌 TL;DR

이 논문은 로봇 공학을 위한 모방 학습에 대한 포괄적인 알고리즘적 관점을 제공합니다. 수동 프로그래밍의 한계를 극복하기 위해, 전문가 시연으로부터 행동을 학습하는 **행동 복제(BC)**와 숨겨진 보상 함수를 추론하는 **역 강화 학습(IRL)**의 두 가지 주요 접근 방식을 분류하고 상세히 설명합니다. 특히, DA$GGER$와 같은 대규모 상호작용 지도 학습 방법과 DMP, ProMP와 같은 궤적 표현, 그리고 최대 엔트로피 IRL과 GCL 같은 보상 학습 방법을 다룹니다. 또한, 모델 프리 및 모델 기반 방법론을 비교하고, 풍부한 로봇 애플리케이션 사례를 통해 이론을 뒷받침합니다. 마지막으로, 데이터 수집, 알고리즘 설계, 성능 평가 등 모방 학습의 다양한 미해결 과제와 미래 연구 방향을 제시하여 분야 발전을 위한 지침을 제공합니다.
