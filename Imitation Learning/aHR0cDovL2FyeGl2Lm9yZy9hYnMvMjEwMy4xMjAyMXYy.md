# Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism

Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, Stuart Russell

---

## 🧩 Problem to Solve

기존 오프라인 강화 학습(Offline Reinforcement Learning, Offline RL) 알고리즘은 크게 두 가지 데이터 구성에 초점을 맞추고 있습니다. 하나는 전문가 정책(expert policy)이 수집한 데이터에 적합한 모방 학습(Imitation Learning)이며, 다른 하나는 균일한 커버리지(uniform coverage)를 갖는 데이터에 적합한 일반적인 오프라인 RL입니다. 그러나 실제 환경에서는 데이터가 이 두 극단적인 경우 사이에 놓여있거나, 데이터 구성이 사전에 알려지지 않는 경우가 많습니다.

이 논문은 다음과 같은 두 가지 주요 질문을 해결하고자 합니다:

1. **정형화 (Formulation):** 전체 데이터 구성 범위를 수용할 수 있는 오프라인 RL 프레임워크를 제안할 수 있는가?
2. **적응적 알고리즘 설계 (Adaptive algorithm design):** 다양한 데이터 구성($C^{\star}$ 값)에 관계없이 최소한의 준최적성(sub-optimality)을 달성하고, $C^{\star}$ 값을 사전에 알지 못해도 적응적으로 작동하는 알고리즘을 설계할 수 있는가?

## ✨ Key Contributions

이 논문의 핵심 기여는 다음과 같습니다:

* **새로운 오프라인 RL 프레임워크 제안:** 최적 정책과 행동 정책 간의 편차를 측정하는 약한 형태의 집중도 계수($C^{\star}$)를 기반으로 모방 학습과 오프라인 RL을 통합하는 새로운 프레임워크를 제시합니다. $C^{\star}=1$은 전문가 데이터, $C^{\star} > 1$은 일반 오프라인 RL 데이터 구성을 나타냅니다.
* **비관주의 기반 하한 신뢰 경계(Lower Confidence Bound, LCB) 알고리즘 분석:** 불확실성에 대한 비관주의 원칙에 기반한 LCB 알고리즘을 다중 팔 밴딧(Multi-Armed Bandits, MAB), 문맥적 밴딧(Contextual Bandits, CB), 마르코프 결정 과정(Markov Decision Processes, MDPs)의 세 가지 설정에서 연구합니다.
* **놀라운 최적성 속도 발견:**
  * MAB 및 RL에서 LCB는 거의 전문가 데이터셋($C^{\star} \approx 1$)에 대해 일반적인 오프라인 RL의 $\mathcal{O}(1/\sqrt{N})$ 속도보다 빠른 $\mathcal{O}(1/N)$ 속도를 달성함을 밝힙니다.
  * **문맥적 밴딧(CB)에서의 적응적 최적성:** 두 개 이상의 문맥(context)이 있는 CB에서 LCB는 전체 데이터 구성 범위에 걸쳐 적응적으로 최적이며, 모방 학습($\mathcal{O}(1/N)$)에서 오프라인 RL($\mathcal{O}(1/\sqrt{N})$)로 부드럽게 전환하는 속도를 달성합니다.
  * **MDP에서의 준 적응적 최적성:** MDP에서도 LCB가 거의 적응적으로 최적임을 보여주며, 특정 범위에서는 최적성 격차가 존재하지만, 이를 좁힐 수 있음을 시사합니다.
  * **MAB에서의 비적응성:** MAB에서는 LCB가 $C^{\star} \in [1,2)$ 범위에서 적응적으로 최적이지 않음을 증명합니다. 이 범위에서는 "가장 많이 플레이된 팔(most played arm)"이 LCB보다 더 빠른 지수적 속도를 달성할 수 있습니다.
* **정보 이론적 하한(Information-theoretic lower bounds) 수립:** 각 설정에서 LCB의 성능을 평가하기 위한 정보 이론적 하한을 제시하여 LCB 알고리즘의 최적성을 확립합니다.

## 📎 Related Works

이 연구는 기존 오프라인 RL 분야의 다음과 같은 연구들과 관련이 있습니다:

* **배치 데이터셋 가정:**
  * **집중도 계수(Concentrability coefficients):** 이전 연구들은 종종 모든 정책에 대한 밀도 비율의 상한을 요구하는 강한 집중도 계수(예: $\max_{\pi} C_{\pi}$)를 사용했습니다. 이와 달리 본 논문은 최적 정책에 대해서만 밀도 비율을 제한하는 약한 집중도 계수($C^{\star}$)를 사용하며, 이는 훨씬 약한 가정을 통해 모방 학습과의 연결을 가능하게 합니다.
  * **균일 하한 데이터 분포:** 일부 연구는 모든 상태-행동 쌍에 대한 데이터 분포가 균일하게 하한을 가진다고 가정하며, 이는 $C^{\star}$가 제한된다는 가정보다 훨씬 강력합니다.
* **오프라인 RL의 보수적인 방법론:**
  * **정책 정규화/제약:** 학습된 정책과 행동 정책 간의 유사성을 보장하는 방법들(예: Fujimoto et al., 2019b)은 주로 전문가 데이터에 적합합니다.
  * **가치 기반 방법:** Q-함수 정규화(Kumar et al., 2020)나 보수적 업데이트를 사용하는 방법(Liu et al., 2020) 등이 있으며, 데이터 분포에 대한 강한 가정이 필요하거나 Horizon 의존성이 높습니다.
  * **비관적 모델 학습:** 모델 기반 방법(Kidambi et al., 2020; Yu et al., 2020)은 비관적 MDP를 학습합니다. Jin et al. (2020)은 에피소드 MDP 및 함수 근사에서 비관주의를 연구했지만, 본 연구는 $C^{\star}$에 대한 적응성을 명확히 다룹니다.
* **정보 이론적 하한:** 다양한 RL 설정에서 하한을 제시한 기존 연구들(예: Dann and Brunskill, 2015; Lattimore and Hutter, 2012)이 있지만, $C^{\star}$에 의존하는 전체 데이터 스펙트럼에 대한 하한은 본 연구에서 처음으로 다루어집니다. 모방 학습 설정의 하한은 Rajaraman et al. (2020)의 연구를 기반으로 확장됩니다.

## 🛠️ Methodology

이 연구의 핵심 방법론은 오프라인 RL의 불확실성에 직면하여 **비관주의(pessimism)** 원칙을 적용하는 하한 신뢰 경계(LCB) 알고리즘입니다.

* **LCB 일반 원칙:**
  * 각 행동 또는 상태-행동 쌍에 대해 **경험적 보상 추정치($\hat{r}$)**를 계산합니다.
  * 데이터에서 해당 쌍의 관측 횟수 $N(s,a)$에 기반한 **페널티 함수($b(s,a)$)**를 정의합니다. 관측 횟수가 적으면 페널티가 커집니다.
  * 최종적으로 $\hat{r}(s,a) - b(s,a)$ 값을 최대화하는 행동을 선택합니다. 이는 참 보상에 대한 "하한 신뢰 경계"를 최대화하는 비관적인 접근 방식입니다. 페널티 함수는 호프딩 부등식(Hoeffding's inequality)에서 파생됩니다.

* **다중 팔 밴딧(MAB)을 위한 LCB (Algorithm 1):**
  * 각 팔 $a$에 대해 경험적 평균 보상 $\hat{r}(a)$와 페널티 $b(a) = \sqrt{\log(2|A|/\delta) / (2N(a))}$를 계산합니다.
  * $\hat{r}(a) - b(a)$를 최대화하는 팔을 선택합니다.
* **문맥적 밴딧(CB)을 위한 LCB (Algorithm 2):**
  * MAB와 유사하게, 각 상태 $s$에 대해 모든 행동 $a$에 대해 $\hat{r}(s,a)$와 페널티 $b(s,a) = \sqrt{2000 \log(2S|A|/\delta) / N(s,a)}$를 계산합니다.
  * 각 상태 $s$에서 $\hat{r}(s,a) - b(s,a)$를 최대화하는 행동 $a$를 선택하여 정책 $\hat{\pi}(s)$를 결정합니다.
  * **증명 아키텍처:** CB의 준최적성 분석에서는 $\mathcal{O}(\sqrt{S(C^{\star}-1)/N} + S/N)$와 같은 엄밀한 $C^{\star}-1$ 의존성을 얻기 위해 준최적성을 다단계로 분해하는 복잡한 기술이 사용됩니다. 이는 단순히 가치 기반 분석으로는 달성할 수 없는 결과입니다.
* **마르코프 결정 과정(MDPs)을 위한 LCB (Algorithm 3, VI-LCB):**
  * 고전적인 가치 반복(Value Iteration) 알고리즘에 LCB 원칙을 결합합니다.
  * Q-함수 업데이트 규칙: $\hat{Q}(s,a) \leftarrow \hat{r}(s,a) - b(s,a) + \gamma \hat{P}_{s,a} \cdot \hat{V}$ (여기서 $\hat{P}_{s,a}$는 경험적 전이 확률, $\hat{V}$는 이전 단계의 가치 함수).
  * **데이터 분할(Data splitting):** 각 가치 반복 단계에서 데이터셋을 무작위로 분할하여 사용함으로써 분석의 독립성 문제를 완화하고 상태 개수 $S$에 대한 의존성을 최적화합니다.
  * **단조 업데이트(Monotonic update):** 이전 반복보다 가치 함수가 증가할 때만 업데이트하는 단계를 포함하여 샘플 복잡성에서 $1/(1-\gamma)$ 인자를 개선합니다.

## 📊 Results

주요 결과는 다음 표에 요약되어 있습니다 (로그 인자 무시). $N$은 샘플 수, $S$는 상태 수, $\gamma$는 할인 인자입니다.

| 설정            | $C^{\star}$ 범위                      | LCB 준최적성                                      | 정보 이론적 하한                                                           |
| :-------------- | :------------------------------------ | :------------------------------------------------ | :------------------------------------------------------------------------- |
| **MAB**         | $[1,2)$                               | $\mathcal{O}(\sqrt{C^{\star}/N})$                 | $\exp(-\mathcal{O}((2-C^{\star})N))$                                       |
|                 | $[2,\infty)$                          | $\mathcal{O}(\sqrt{C^{\star}/N})$                 | $\mathcal{O}(\sqrt{C^{\star}/N})$                                          |
| **CB**          | $[1,\infty)$                          | $\mathcal{O}(\sqrt{S(C^{\star}-1)/N} + S/N)$      | $\mathcal{O}(\sqrt{S(C^{\star}-1)/N} + S/N)$                               |
| **MDP**         | $[1, 1+1/N)$                          | $\mathcal{O}(S/((1-\gamma)^4 N))$                 | $\mathcal{O}(S/((1-\gamma)^2 N) + \sqrt{S(C^{\star}-1)/((1-\gamma)^3 N)})$ |
|                 | $[1+1/N, \infty)$                     | $\mathcal{O}(\sqrt{SC^{\star}/((1-\gamma)^5 N)})$ | $\mathcal{O}(S/((1-\gamma)^2 N) + \sqrt{S(C^{\star}-1)/((1-\gamma)^3 N)})$ |

**주요 발견:**

* **MAB:** LCB는 $C^{\star} \ge 2$ 범위에서 정보 이론적 하한에 근접하지만, $C^{\star} \in [1,2)$ 범위에서는 LCB가 $\mathcal{O}(\sqrt{C^{\star}/N})$인 반면, "가장 많이 플레이된 팔" 전략은 더 우수한 지수적 속도를 달성할 수 있어 LCB가 적응적으로 최적이지 않음을 보여줍니다.
* **CB:** LCB는 $S \ge 2$인 문맥적 밴딧에서 전체 $C^{\star}$ 범위에 걸쳐 미니맥스(minimax) 최적 속도를 달성합니다. 이는 $C^{\star} \approx 1$인 경우 $\mathcal{O}(S/N)$에서 $C^{\star} \gg 1$인 경우 $\mathcal{O}(\sqrt{S(C^{\star}-1)/N})$으로 부드럽게 전환하는 속도를 보여주며, "가장 많이 플레이된 팔" 전략은 이 경우 실패합니다.
* **MDP:** VI-LCB는 $C^{\star} \approx 1$일 때 $\mathcal{O}(S/((1-\gamma)^4 N))$ 속도를, $C^{\star} \ge 1.1$일 때 $\mathcal{O}(\sqrt{SC^{\star}/((1-\gamma)^5 N)})$ 속도를 달성합니다. 이 결과는 해당 $C^{\star}$ 범위에서 미니맥스 최적에 가깝습니다. 하지만 $C^{\star} \in (1 + 1/N, 1.1)$ 중간 범위에서는 상한과 하한 사이에 격차가 존재합니다. 또한, LCB의 샘플 복잡성에서 $1/(1-\gamma)^2$ 인자의 여유가 있습니다.

## 🧠 Insights & Discussion

* **새로운 프레임워크의 실용적 의미:** $C^{\star}$를 사용한 새로운 프레임워크는 모방 학습과 오프라인 RL을 통합하여 실제 데이터의 다양한 구성을 더 잘 포괄할 수 있게 합니다. 이는 데이터 구성에 대한 사전 지식 없이도 단일 알고리즘이 잘 작동할 수 있음을 의미하여 실용적 가치가 큽니다.
* **LCB의 놀라운 성능:** LCB 접근 방식은 비교적 단순함에도 불구하고, 문맥적 밴딧에서 데이터 구성에 관계없이 적응적으로 미니맥스 최적의 성능을 달성하는 강력함을 보여줍니다. 이는 LCB가 다양한 데이터 구성에 걸쳐 신뢰할 수 있는 기반 방법론이 될 수 있음을 시사합니다.
* **MAB와 MDP에서의 미묘한 차이:**
  * **MAB:** LCB가 $C^{\star}$ 범위 전체에서 적응적으로 최적이지 않다는 점은 흥미로운 발견입니다. $C^{\star}$가 1에 가까운 "거의 전문가" 상황에서는 "가장 많이 플레이된 팔"과 같은 비LCB 방식이 더 나은 지수적 속도를 달성할 수 있습니다. 이는 데이터 구성에 따라 알고리즘 선택이 달라질 수 있음을 의미합니다.
  * **MDP:** $C^{\star} \in (1 + 1/N, 1.1)$ 중간 범위에서 LCB의 최적성에 대한 격차가 남아있지만, 저자들은 LCB가 MDP에서도 데이터 구성 스펙트럼 전반에 걸쳐 최적일 것이라고 추측합니다. 이 격차는 MDP의 오류 전파(error propagation)와 가치 차이(value gap) 및 정책 선택 간의 복잡한 상호작용으로 인해 발생하며, 분석이 더 까다롭습니다.
* **한계 및 미래 연구:**
  * **MDP 격차 해소:** $C^{\star} \in (1 + 1/N, 1.1)$ 범위에서의 상한을 더 타이트하게 만들 필요가 있습니다.
  * **Horizon 의존성 개선:** MDP 샘플 복잡성에서 $1/(1-\gamma)^2$ 인자 격차를 개선하기 위해 베르누이 유형 페널티와 분산 감소 기법 또는 반복 간 데이터 재사용이 고려될 수 있습니다.
  * **함수 근사 통합:** 현재 연구는 테이블형(tabular) 설정에 국한되어 있습니다. 실제 대규모 문제 해결을 위해 함수 근사(function approximation)로 확장하는 것이 중요합니다.
  * **다른 알고리즘 탐색:** 가치 정규화와 같은 다른 보수적인 오프라인 RL 방법론이 적응성 및/또는 미니맥스 최적성을 달성할 수 있는지 추가 연구가 필요합니다.

## 📌 TL;DR

이 논문은 오프라인 강화 학습과 모방 학습 사이의 격차를 해소하기 위해 **최적 정책 집중도 계수($C^{\star}$)**에 기반한 새로운 통합 프레임워크를 제시합니다. $C^{\star}$는 데이터 구성의 스펙트럼을 설명하며, $C^{\star} \approx 1$은 전문가 데이터, $C^{\star} \gg 1$은 일반적인 오프라인 RL 데이터를 의미합니다.

이 프레임워크 하에서 **하한 신뢰 경계(LCB)** 알고리즘을 분석합니다. LCB는 **문맥적 밴딧**에서 **적응적으로 미니맥스 최적** 성능을 달성하여, 전문가 데이터에 대한 $\mathcal{O}(1/N)$ 속도에서 일반 데이터에 대한 $\mathcal{O}(1/\sqrt{N})$ 속도로 부드럽게 전환함을 보였습니다. 그러나 **다중 팔 밴딧**에서는 LCB가 $C^{\star} \in [1,2)$ 범위에서 적응적으로 최적이 아님을 증명했습니다. **마르코프 결정 과정(MDP)**에서는 LCB가 거의 적응적으로 최적임을 보였으며, $C^{\star}$ 중간 범위에서의 성능 격차는 추후 연구 과제로 남겨두었습니다. 이 연구는 LCB 접근 방식이 다양한 데이터 구성에 걸쳐 강력하고 적응적인 성능을 제공하는 잠재력을 가지고 있음을 보여줍니다.
