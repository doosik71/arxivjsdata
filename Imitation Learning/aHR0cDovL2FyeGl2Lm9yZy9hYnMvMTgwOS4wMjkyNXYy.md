# DISCRIMINATOR-ACTOR-CRITIC: ADDRESSING SAMPLE INEFFICIENCY AND REWARD BIAS IN ADVERSARIAL IMITATION LEARNING

Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson

## π§© Problem to Solve

μ λ€μ  λ¨λ°© ν•™μµ(Adversarial Imitation Learning, AIL) ν”„λ μ„μ›ν¬ κΈ°λ° μ•κ³ λ¦¬μ¦λ“¤μ€ λ‘ κ°€μ§€ μ¤‘μ”ν• λ¬Έμ μ μ„ κ°€μ§€κ³  μμµλ‹λ‹¤:

1. **μƒν” λΉ„ν¨μ¨μ„±**: μ „λ¬Έκ°€ μ‹μ—°(expert demonstrations)μ΄ μ λ”λΌλ„, μ‹¤μ  ν™κ²½κ³Όμ μƒνΈμ‘μ©(policy-environment interaction)μ— ν•„μ”ν• μƒν” μκ°€ μλ°±λ§ λ‹¨κ³„λ¥Ό λ„μ–΄κ°€λ―€λ΅ λ§μ€ μ‹¤μ  λ΅λ΄‡ κ³µν•™ λ° ν„μ‹¤ μ μ©μ—λ” λΉ„μ‹¤μ©μ μ…λ‹λ‹¤.
2. **λ³΄μƒ ν•¨μ νΈν–¥(Reward Bias)**: AIL μ•κ³ λ¦¬μ¦μ—μ„ μ‚¬μ©λλ” λ³΄μƒ ν•¨μμ— μ•”λ¬µμ μΈ νΈν–¥μ΄ μ΅΄μ¬ν•©λ‹λ‹¤. μ΄λ” νΉμ • ν™κ²½μ—μ„λ” μ μ‘λ™ν•  μ μμ§€λ§, λ‹¤λ¥Έ ν™κ²½μ—μ„λ” μµμ ν™”λμ§€ μ•μ€(sub-optimal) ν–‰λ™μ„ μ λ°ν•κ±°λ‚, μ—ν”Όμ†λ“ μΆ…λ£ μƒνƒ(terminal states) μ²λ¦¬μ— λ€ν• λ¶€μ μ ν• λ°©μ‹μΌλ΅ μΈν•΄ μ •μ±… μ„±λ¥μ„ μ €ν•μ‹ν‚¬ μ μμµλ‹λ‹¤.

## β¨ Key Contributions

- **λ³΄μƒ ν•¨μ νΈν–¥ ν•΄κ²°**: λ¨λ°© ν•™μµμ—μ„ νλ³„μ(discriminator) κΈ°λ° λ³΄μƒ μ¶”μ •μ νΈν–¥ λ¬Έμ λ¥Ό μ‹λ³„ν•κ³  μ΄λ¥Ό ν•΄κ²°ν•κΈ° μ„ν• λ°©λ²•μ„ μ μ•ν•©λ‹λ‹¤. νΉν, ν΅μ μƒνƒ(absorbing states)μ— λ€ν• λ³΄μƒμ„ λ…μ‹μ μΌλ΅ ν•™μµν•¨μΌλ΅μ¨ κ³ μ •λ λ³΄μƒ ν•¨μ ν•νƒλ¥Ό μλ™μΌλ΅ μ΅°μ •ν•  ν•„μ”λ¥Ό μ—†μ•΄μµλ‹λ‹¤.
- **μƒν” ν¨μ¨μ„± ν–¥μƒ**: AIL μ•κ³ λ¦¬μ¦μ„ μ„ν• μ¤ν”„-μ •μ±…(off-policy) λ³€ν•μ„ μ κ³µν•μ—¬ μ—μ΄μ „νΈ-ν™κ²½ μƒνΈμ‘μ© νμλ¥Ό νκΈ°μ μΌλ΅ μ¤„μ€μµλ‹λ‹¤ (ν‰κ·  10λ°°).
- **DAC(Discriminator-Actor-Critic) μ•κ³ λ¦¬μ¦ μ μ•**: GAIL λ° AIRL ν”„λ μ„μ›ν¬μ™€ νΈν™λλ©°, λ…μ‹μ μΈ μΆ…λ£ μƒνƒ μ²λ¦¬, μ¤ν”„-μ •μ±… νλ³„μ, μ¤ν”„-μ •μ±… μ•΅ν„°-ν¬λ¦¬ν‹±(actor-critic) κ°•ν™” ν•™μµ μ•κ³ λ¦¬μ¦μ„ ν†µν•©ν• μƒλ΅μ΄ μ•κ³ λ¦¬μ¦μ…λ‹λ‹¤.
- **λ‹¤μ–‘ν• ν™κ²½μ—μ„μ κ°•κ±΄μ„± μ…μ¦**: λ…Έμ΄μ¦κ°€ λ§κ³ , λ‹¤μ¤‘ λ¨λ‹¬(multi-modal)ν•λ©°, μ μ•½λ μ „λ¬Έκ°€ μ‹μ—°(μ: μ‚¬λμ λ΅λ΄‡ μ΅°μ‘ μ‹μ—°)μ— λ€ν•΄μ„λ„ DACμ κ°•κ±΄μ„±μ„ μ…μ¦ν–μµλ‹λ‹¤.

## π“ Related Works

- **λ¨λ°© ν•™μµ(Imitation Learning, IL)**: ν–‰λ™ λ³µμ (Behavioral Cloning, BC)μ™€ μ—­κ°•ν™” ν•™μµ(Inverse Reinforcement Learning, IRL)μΌλ΅ κ΄‘λ²”μ„ν•κ² μ—°κµ¬λμ—μµλ‹λ‹¤.
- **μ—­κ°•ν™” ν•™μµ(IRL)**: μµλ€ μ—”νΈλ΅ν”Ό IRL (Ziebart et al., 2008), μ•λ‚΄ λΉ„μ© ν•™μµ(Guided Cost Learning), μ λ€μ  IRL(Adversarial IRL, AIRL) λ“±μ΄ λ³΄μƒ ν•¨μλ¥Ό μ¶”μ •ν•μ—¬ μ •μ±…μ„ λ³µκµ¬ν•©λ‹λ‹¤.
- **μƒμ„±μ  μ λ€μ  λ¨λ°© ν•™μµ(Generative Adversarial Imitation Learning, GAIL)**: IRLμ λ³΄μƒ ν•¨μ λ³µκµ¬ κ³Όμ •μ„ μ°νν•κ³ , μ „λ¬Έκ°€μ μƒνƒ-ν–‰λ™ μ μ  μΈ΅μ •(occupancy measure)κ³Ό ν•™μµλ μ •μ±…μ μ μ  μΈ΅μ •μ„ μΌμΉμ‹ν‚¤λ” λ¬Έμ λ΅ λ¨λ°© ν•™μµμ„ λ‹¤λ£Ήλ‹λ‹¤. μ£Όλ΅ μ¨-μ •μ±…(on-policy) μ•κ³ λ¦¬μ¦μ„ μ‚¬μ©ν•©λ‹λ‹¤.
- **μ¤ν”„-μ •μ±… ν•™μµ(Off-policy Learning)**: μ‹μ—°μ„ ν™μ©ν•μ—¬ κ°•ν™” ν•™μµμ νƒμƒ‰μ„ κ°€μ†ν™”ν•λ” μ—°κµ¬λ“¤μ΄ μμΌλ‚, λ€λ¶€λ¶„ ν™κ²½μΌλ΅λ¶€ν„°μ λ³΄μƒ μ‹ νΈλ¥Ό μ „μ ν•©λ‹λ‹¤. λ³Έ μ—°κµ¬λ” μ „λ¬Έκ°€ μ‹μ—°λ§μ„ μ”κµ¬ν•©λ‹λ‹¤.
- **GAN κ΄€λ ¨ μ—°κµ¬**: Wasserstein GAN (WGAN) λ° Improved WGAN (WGAN-GP)κ³Ό κ°™μ€ GAN ν›λ ¨ μ•μ •ν™” κΈ°λ²•λ“¤μ΄ νλ³„μ ν›λ ¨μ— ν™μ©λμ—μµλ‹λ‹¤.

## π› οΈ Methodology

DACλ” λ‘ κ°€μ§€ μ£Όμ” κµ¬μ„± μ”μ†λ΅ μ΄λ£¨μ–΄μ Έ μμµλ‹λ‹¤: νΈν–¥ μ—†λ” μ λ€μ  λ³΄μƒ ν•¨μ κ³µμ‹ν™”μ™€ μ¤ν”„-μ •μ±… AIL.

1. **ν΅μ μƒνƒ λ³΄μƒ ν•™μµ(Unbiasing Reward Functions)**:
   - κΈ°μ΅΄ AIL μ•κ³ λ¦¬μ¦λ“¤μ΄ ν΅μ μƒνƒμ— μ•”λ¬µμ μΌλ΅ 0μ λ³΄μƒμ„ ν• λ‹Ήν•κ±°λ‚, κ³ μ •λ λ³΄μƒ ν•¨μ ν•νƒ(μ: ν•­μƒ μ–‘μ λλ” μμ)λ΅ μΈν•΄ νΉμ • ν™κ²½μ—μ„ λ¬Έμ κ°€ λ°μƒν•λ” κ²ƒμ„ ν•΄κ²°ν•©λ‹λ‹¤.
   - **ν•΄κ²°μ±…**: μ „λ¬Έκ°€ μ‹μ—° λ° μ •μ±…μ΄ μƒμ„±ν• κ¶¤μ  λ¨λ‘μ— λ€ν•΄ ν΅μ μƒνƒ($s_a$)μ— λ€ν• λ³΄μƒμ„ λ…μ‹μ μΌλ΅ ν•™μµν•©λ‹λ‹¤.
   - μ΄λ¥Ό μ„ν•΄ ν΅μ μƒνƒ μ—¬λ¶€λ¥Ό λ‚νƒ€λ‚΄λ” μ¶”κ°€μ μΈ μ§€ν‘(indicator) μ°¨μ›μ„ μƒνƒμ— μ¶”κ°€ν•κ³ , νλ³„μ $D$κ°€ μ΄ μƒνƒμ— λ€ν• λ³΄μƒμ„ ν•™μµν•λ„λ΅ ν•©λ‹λ‹¤.
   - μΆ…λ£ μƒνƒ($s_T$)μ— λ€ν• μµμΆ… λ¦¬ν„΄μ€ $R_T = r(s_T, a_T) + \frac{\gamma r(s_a, \cdot)}{1-\gamma}$λ΅ μ •μλ©λ‹λ‹¤.
2. **μ¤ν”„-μ •μ±… μƒν” λΉ„ν¨μ¨μ„± ν•΄κ²°(Addressing Sample Inefficiency)**:
   - κΈ°μ΅΄ GAILμ΄ μ¨-μ •μ±… RLμ„ μ‚¬μ©ν•μ—¬ λ†’μ€ μƒν” λ³µμ΅μ„±μ„ κ°€μ§„λ‹¤λ” λ¬Έμ λ¥Ό ν•΄κ²°ν•©λ‹λ‹¤.
   - **μ¤ν”„-μ •μ±… RL μ•κ³ λ¦¬μ¦ ν™μ©**: TD3(Twin Delayed DDPG)μ™€ κ°™μ€ μ¤ν”„-μ •μ±… μ•΅ν„°-ν¬λ¦¬ν‹±(Actor-Critic) μ•κ³ λ¦¬μ¦μ„ μ •μ±… ν•™μµμ— μ‚¬μ©ν•©λ‹λ‹¤.
   - **μ¤ν”„-μ •μ±… νλ³„μ ν›λ ¨**: νλ³„μλ¥Ό ν›λ ¨μ‹ν‚¬ λ• μ •μ±…μ—μ„ μ§μ ‘ κ¶¤μ μ„ μƒν”λ§ν•λ” λ€μ‹ , μ¤ν”„-μ •μ±… ν›λ ¨ κ³Όμ •μ—μ„ μμ§‘λ λ¦¬ν”λ μ΄ λ²„νΌ($\mathcal{R}$)μ—μ„ μ „ν™(transitions)μ„ μƒν”λ§ν•μ—¬ μ‚¬μ©ν•©λ‹λ‹¤.
   - νλ³„μ μ—…λ°μ΄νΈμ λ©ν‘ ν•¨μλ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:
     $$\max_D E_{\mathcal{R}}[\log(D(s,a))] + E_{\pi_E}[\log(1-D(s,a))] - \lambda H(\pi)$$
     μ—¬κΈ°μ„ μ¤‘μ”λ„ μƒν”λ§ κ°€μ¤‘μΉλ” μ‹¤μ©μ μΈ μ•μ •μ„±μ„ μ„ν•΄ μƒλµλμ—μµλ‹λ‹¤.
   - λ§¤ λ‹¨κ³„ λ³΄μƒμ€ μµμ‹  νλ³„μλ¥Ό μ‚¬μ©ν•μ—¬ λ¦¬ν”λ μ΄ λ²„νΌμ—μ„ μ „ν™μ„ κ°€μ Έμ¬ λ•λ§λ‹¤ μ—…λ°μ΄νΈλ©λ‹λ‹¤.
3. **ν›λ ¨ μ•μ •μ„±**: νλ³„μμ κ³Όμ ν•©μ„ λ°©μ§€ν•κ³  ν›λ ¨ μ•μ •μ„±μ„ λ†’μ΄κΈ° μ„ν•΄ κ·Έλλ””μ–ΈνΈ νλ„ν‹°(gradient penalty)λ¥Ό μ‚¬μ©ν•©λ‹λ‹¤.

## π“ Results

- **μƒν” ν¨μ¨μ„±**: MuJoCo λ²¤μΉλ§ν¬ νƒμ¤ν¬μ—μ„ DACλ” GAIL (TRPO λλ” PPO κΈ°λ°)λ³΄λ‹¤ ν‰κ· μ μΌλ΅ 10λ°° μ΄μƒ μƒν” ν¨μ¨μ μ…λ‹λ‹¤. 1λ°±λ§ μ¤ν… μ΄λ‚΄μ—μ„ GAIL λ€λΉ„ ν›¨μ”¬ μ μ€ ν™κ²½ μƒνΈμ‘μ©μΌλ΅ μ μ‚¬ν•κ±°λ‚ λ” λ‚μ€ μ„±λ¥μ„ λ‹¬μ„±ν•©λ‹λ‹¤.
- **λ³΄μƒ νΈν–¥ ν•΄κ²°**:
  - `r(s,a) = -log(1-D(s,a))`μ™€ κ°™μ΄ μ—„κ²©ν μ–‘μμΈ λ³΄μƒ ν•¨μλ” ν•™μµλμ§€ μ•μ€ νλ³„μλ¥Ό μ‚¬μ©ν•λ”λΌλ„ Hopper ν™κ²½μ—μ„ μƒλ‹Ήν• μ„±λ¥(μ „λ¬Έκ°€ μ„±λ¥μ μ•½ 1/3)μ„ λ‹¬μ„±ν•μ—¬, μ•”λ¬µμ μΈ μƒμ΅΄ λ³΄λ„μ¤(survival bonus) νΈν–¥μ΄ μμμ„ λ³΄μ—¬μ¤λ‹λ‹¤. μ΄λ¬ν• νΈν–¥μ€ μ „λ¬Έκ°€ μ‹μ—° μ—†μ΄λ„ ν–‰λ™ λ³µμ (BC)λ³΄λ‹¤ λ‚μ€ μ„±λ¥μ„ λ³΄μΌ μ μμµλ‹λ‹¤.
  - μ΄λ‹Ή νλ„ν‹°(per-step penalty)κ°€ μλ” Kuka-Reach λ° Kuka-PushNext λ΅λ΄‡ νƒμ¤ν¬(μΈκ°„ μ‹μ—° μ‚¬μ©)μ—μ„, ν΅μ μƒνƒ μ²λ¦¬κ°€ μ—†λ” `r(s,a) = -log(1-D(s,a))`λ” μ „λ¬Έκ°€ μ •μ±…μ„ λ³µκµ¬ν•λ” λ° μ™„μ „ν μ‹¤ν¨ν–μµλ‹λ‹¤. λ°λ©΄ DACλ” λ…Έμ΄μ¦κ°€ λ§μ€ μΈκ°„ μ‹μ—°μ„ μ‚¬μ©ν–μμ—λ„ λ¶κµ¬ν•κ³  μ „λ¬Έκ°€λ¥Ό λΉ λ¥΄κ² λ¨λ°©ν•λ” λ²•μ„ ν•™μµν–μµλ‹λ‹¤.
  - AIRL νλ³„μλ¥Ό DAC ν”„λ μ„μ›ν¬ λ‚΄μ—μ„ μ‚¬μ©ν•  λ•λ„ ν΅μ μƒνƒμ— λ€ν• λ³΄μƒμ„ ν•™μµν•λ” κ²ƒμ΄ μ •μ±… μ„±λ¥μ— κ²°μ •μ μΌλ΅ μ¤‘μ”ν•¨μ„ μ…μ¦ν–μµλ‹λ‹¤.

## π§  Insights & Discussion

- μ΄ λ…Όλ¬Έμ€ κΈ°μ΅΄ μ λ€μ  λ¨λ°© ν•™μµ(AIL) μ•κ³ λ¦¬μ¦μ μ£Όμ” λ¬Έμ μ μΈ μƒν” λΉ„ν¨μ¨μ„±κ³Ό λ³΄μƒ ν•¨μμ μ•”λ¬µμ  νΈν–¥μ„ λ…ν™•ν μ‹λ³„ν•κ³  ν¨κ³Όμ μΈ ν•΄κ²°μ±…μ„ μ μ‹ν–μµλ‹λ‹¤.
- **μƒν” ν¨μ¨μ„±**: μ¤ν”„-μ •μ±… RLκ³Ό μ¤ν”„-μ •μ±… νλ³„μ ν›λ ¨μ κ²°ν•©μ€ μ‹¤μ„Έκ³„ λ΅λ΄‡ κ³µν•™ μ• ν”λ¦¬μΌ€μ΄μ… λ“± ν™κ²½κ³Όμ μƒνΈμ‘μ© λΉ„μ©μ΄ λ†’μ€ λ¶„μ•Όμ—μ„ λ¨λ°© ν•™μµμ μ μ© κ°€λ¥μ„±μ„ ν¬κ² ν™•μ¥μ‹ν‚µλ‹λ‹¤.
- **λ³΄μƒ ν•¨μ νΈν–¥**: ν΅μ μƒνƒμ— λ€ν• λ³΄μƒμ„ λ…μ‹μ μΌλ΅ ν•™μµν•λ” κ°„λ‹¨ν•μ§€λ§ ν¨κ³Όμ μΈ λ©”μ»¤λ‹μ¦μ€ λ³΄μƒ ν•¨μμ ν•νƒλ¥Ό μλ™μΌλ΅ μ΅°μ‘ν•λ” λ¶€λ‹΄μ„ μ¤„μ—¬μ¤λ‹λ‹¤. μ΄λ” μ•κ³ λ¦¬μ¦μ΄ λ‹¤μ–‘ν• νƒμ¤ν¬μ— λ”μ± μΌλ°μ μ΄κ³  κ°•κ±΄ν•κ² μ μ©λ  μ μμμ„ μλ―Έν•©λ‹λ‹¤. νΉν, μ•”λ¬µμ μΈ μƒμ΅΄ λ³΄λ„μ¤λ‚ νλ„ν‹°κ°€ νΉμ • νƒμ¤ν¬μ— λ¶€μ ν•©ν•  λ• λ°μƒν•  μ μλ” μµμ ν™”λμ§€ μ•μ€ ν–‰λ™μ„ λ°©μ§€ν•λ” λ° μ¤‘μ”ν•©λ‹λ‹¤.
- **κ°•κ±΄μ„±**: λ…Έμ΄μ¦κ°€ λ§κ±°λ‚, λ‹¤μ¤‘ λ¨λ‹¬ν•λ©°, μµμ ν™”λμ§€ μ•μ„ μ μλ” μΈκ°„ μ‹μ—°μ— λ€ν•΄μ„λ„ DACκ°€ μ„±κ³µμ μΌλ΅ μ‘λ™ν•λ‹¤λ” κ²ƒμ€ μ‹¤μ  ν™κ²½μ—μ„ AILμ„ μ μ©ν•  λ• μ¤‘μ”ν• μ¥μ μ…λ‹λ‹¤.
- **ν•κ³„μ **: λ³Έ λ…Όλ¬Έμ—μ„ λ…μ‹μ μΌλ΅ μ–ΈκΈ‰λμ§€λ” μ•μ•μ§€λ§, μ¤ν”„-μ •μ±… ν•™μµμ—μ„ μ¤‘μ”λ„ μƒν”λ§ κ°€μ¤‘μΉλ¥Ό μƒλµν• κ²ƒμ€ μ΄λ΅ μ μΈ μ—„λ°€μ„±μ„ ν¬μƒν• μ‹¤μ©μ μΈ μ ‘κ·Όμ΄λ©°, κ²½μ°μ— λ”°λΌ λ¶„ν¬ λ¶μΌμΉ λ¬Έμ λ¥Ό μ•ΌκΈ°ν•  κ°€λ¥μ„±μ΄ μμµλ‹λ‹¤.

## π“ TL;DR

λ³Έ λ…Όλ¬Έμ€ μ λ€μ  λ¨λ°© ν•™μµ(AIL) μ•κ³ λ¦¬μ¦μ μ‹¬κ°ν• μƒν” λΉ„ν¨μ¨μ„±κ³Ό λ³΄μƒ ν•¨μμ μ•”λ¬µμ  νΈν–¥ λ¬Έμ λ¥Ό λ‹¤λ£Ήλ‹λ‹¤. μ €μλ“¤μ€ ν΅μ μƒνƒμ— λ€ν• λ³΄μƒμ„ λ…μ‹μ μΌλ΅ ν•™μµν•μ—¬ λ³΄μƒ νΈν–¥μ„ μ κ±°ν•κ³ , μ¤ν”„-μ •μ±… κ°•ν™” ν•™μµ(TD3) λ° μ¤ν”„-μ •μ±… νλ³„μ ν›λ ¨μ„ ν†µν•΄ μƒν” ν¨μ¨μ„±μ„ νκΈ°μ μΌλ΅ κ°μ„ ν• DAC(Discriminator-Actor-Critic) μ•κ³ λ¦¬μ¦μ„ μ μ•ν•©λ‹λ‹¤. μ‹¤ν— κ²°κ³Ό, DACλ” κΈ°μ΅΄ AIL μ•κ³ λ¦¬μ¦λ³΄λ‹¤ μµλ€ 10λ°° λ” μƒν” ν¨μ¨μ μ΄λ©°, λ³΄μƒ νΈν–¥ λ¬Έμ λ¥Ό ν•΄κ²°ν•μ—¬ λ‹¤μ–‘ν• MuJoCo λ²¤μΉλ§ν¬ λ° λ΅λ΄‡ νƒμ¤ν¬μ—μ„ κ°•κ±΄ν•κ³  μ°μν• μ„±λ¥μ„ λ‹¬μ„±ν•¨μ„ μ…μ¦ν–μµλ‹λ‹¤.
