# Accelerating Reinforcement Learning through Implicit Imitation

Bob Price, Craig Boutilier

## 🧩 Problem to Solve

강화 학습(Reinforcement Learning, RL) 에이전트는 환경을 탐색하여 최적의 행동 정책을 학습하지만, 이는 특히 상태 공간이 크거나 보상이 희박한 경우 매우 느릴 수 있습니다. 이 연구는 명시적인 통신이나 목표 공유 없이, 에이전트가 전문가(또는 경험 많은) 멘토의 행동을 관찰함으로써 자신의 학습 과정을 **가속화**할 수 있는 방법을 찾는 것을 목표로 합니다.

## ✨ Key Contributions

- **암묵적 모방(Implicit Imitation) 모델 제시**: 멘토의 행동을 관찰하여 RL 에이전트의 학습을 가속화하는 정형화된 모델을 제안합니다.
- **두 가지 설정에 대한 구체화**:
  - **동질적 설정(Homogeneous Settings)**: 학습 에이전트와 멘토의 행동 능력이 동일한 경우.
  - **이질적 설정(Heterogeneous Settings)**: 학습 에이전트와 멘토의 행동 세트가 다른 경우.
- **새로운 기법 개발**: 모델 추출(model extraction), 증강 벨만 백업(augmented Bellman backups), 신뢰도 테스트(confidence testing), 초점화(focusing), 실행 가능성 테스트(feasibility testing), k-단계 복구(k-step repair) 등을 도입하여 암묵적 모방을 효과적으로 구현합니다.
- **실증적 효과 입증**: 제안된 기법들이 학습 속도, 수렴 성능, 노이즈에 대한 견고성, 다중 멘토 정보 통합, 이질적인 환경 적응 능력 등에서 상당한 이점을 제공함을 보여줍니다.
- **"골절 지표(Fracture Metric)" 도입**: 멘토가 관찰자에게 제공하는 정보의 유용성을 예측하는 새로운 지표를 제안합니다.

## 📎 Related Works

- **명시적 가르침 및 시연 기반 학습**: Atkeson & Schaal (1997), Lin (1992), Whitehead (1991a) 등의 연구는 멘토가 명시적으로 행동을 보여주거나 정보를 공유하는 방식에 중점을 둡니다.
- **행동 복제(Behavioral Cloning)**: Sammut et al. (1992), Urbancic & Bratko (1994) 등은 멘토의 행동-상태 쌍을 직접 모방하여 정책을 학습합니다.
- **가치 함수 제약 추론**: Utgoff & Clouse (1991), Šuc & Bratko (1997)는 멘토의 행동 선호도에서 가치 함수에 대한 제약을 추론합니다.
- **역강화학습(Inverse Reinforcement Learning)**: Ng & Russell (2000)은 관찰된 행동으로부터 보상 함수를 재구성합니다.
- **MDP 추상화 및 근사 계획**: Dearden & Boutilier (1997), Dean & Givan (1997)은 MDP 모델의 추상화 기법을 다룹니다.
- **우선순위 스위핑(Prioritized Sweeping)**: Moore & Atkeson (1993)의 모델 기반 RL 알고리즘을 기반으로 합니다.
- **베이즈 탐색(Bayesian Exploration)**: Dearden et al. (1999)는 베이즈적 관점에서 탐색의 가치를 계산합니다.

## 🛠️ Methodology

이 연구는 멘토와 관찰자가 **상호작용하지 않는 확률 게임(noninteracting stochastic games)** 내에서 각자 자신의 MDP $M_m$과 $M_o$를 가진다고 가정합니다. 관찰자는 멘토의 상태 전이를 완전히 관찰할 수 있지만, 멘토의 행동 자체는 관찰할 수 없으며, 자신의 보상 함수 $R_o$는 알고 있지만 전이 함수 $Pr_o$는 알지 못합니다.

### 동질적 설정(Homogeneous Settings)

관찰자와 멘토가 동일한 상태 공간 $S$를 공유하며, 멘토의 행동이 관찰자의 행동으로 복제될 수 있다고 가정합니다.

1. **모델 추출(Model Extraction)**: 관찰자는 멘토의 상태 전이 $s \to t$를 관찰하여 멘토 정책 $\pi_m$이 유도하는 마르코프 연쇄 $\hat{Pr}_m(s, t)$를 추정합니다.
2. **증강 벨만 백업(Augmented Bellman Backups)**: 관찰자는 자신의 최적 가치 함수를 추정할 때, 자신의 행동 모델 $Pr_o(s, a, t)$뿐만 아니라 멘토의 행동 모델 $Pr_m(s, t)$도 고려합니다.
   $$V(s) = R_o(s) + \gamma \max \left\{ \max_{a \in A_o} \left\{ \sum_{t \in S} Pr_o(s, a, t)V(t) \right\}, \sum_{t \in S} Pr_m(s, t)V(t) \right\}$$
   여기서 두 번째 항은 멘토의 (관찰되지 않은) 행동을 복제했을 때 예상되는 가치를 나타냅니다.
3. **모델 신뢰도 테스트(Model Confidence Testing)**: 멘토 모델의 추정치가 부정확하여 관찰자를 잘못된 판단으로 이끌 수 있는 경우를 방지합니다. 디리클레 사전 분포(Dirichlet prior distribution)와 체비쇼프 부등식(Chebychev's inequality)을 사용하여 관찰자 자신의 모델과 멘토 모델의 가치 추정치에 대한 하한 $V_o^-(s)$와 $V_m^-(s)$을 계산합니다. 만약 $V_o^-(s) > V_m^-(s)$라면, 관찰자는 자신의 경험 기반 모델을 선호하고 멘토의 정보를 사용하지 않습니다.
4. **초점화(Focusing)**: 멘토가 방문하는 상태 $s$에서 벨만 백업을 수행하여 계산 노력을 관련성 높은 상태 공간 영역에 집중시킵니다.
5. **행동 선택(Action Selection)**: $\epsilon$-탐욕(greedy) 전략을 사용하며, 멘토의 모델이 선호될 경우, 멘토의 관찰된 전이 분포와 쿨백-라이블러 발산(Kullback-Leibler divergence)이 최소화되는 관찰자의 행동 $\kappa_m(s)$을 선택합니다.
6. **우선순위 스위핑(Prioritized Sweeping)과의 통합**: 위에 설명된 메커니즘들을 효율적인 모델 기반 RL 알고리즘인 우선순위 스위핑에 통합하여 학습을 가속화합니다.

### 이질적 설정(Heterogeneous Settings)

관찰자가 멘토의 행동을 직접 복제할 수 없는 경우를 다룹니다.

1. **실행 가능성 테스트(Feasibility Testing)**: 멘토의 행동 $a_m$이 관찰자에게 실행 가능한지 여부를 명시적으로 테스트합니다. 관찰자의 현재 추정 모델을 기반으로 멘토의 행동 전이 분포와 관찰자 각 행동의 전이 분포를 비교합니다. 디리클레 분포와 본페로니 테스트(Bonferroni test)를 사용하여 유의미한 차이가 있는지 판단하며, 차이가 있다면 증강 벨만 백업을 억제합니다.
   $$\frac{|Pr_o(s, a_o, t) - Pr_m(s, t)|}{\sqrt{\frac{n_o(s,a_o,t)\sigma^2_{omodel}(s,a_o,t)+n_m(s,t)\sigma^2_{mmodel}(s,t)}{n_o(s,a_o,t)+n_m(s,t)}}} > Z_{\alpha/2r}$$
   여기서 $r$은 가능한 후속 상태의 수입니다.
2. **k-단계 유사성 및 복구(k-step Similarity and Repair, 브리징)**: 멘토의 특정 행동이 실행 불가능하더라도, 관찰자가 멘토의 궤적을 약 $k$ 단계 이내의 자신의 행동 시퀀스로 "모방"할 수 있는 경우를 처리합니다. 관찰자는 실행 불가능한 멘토의 행동을 우회하여 멘토 궤적의 "하류" 상태에 도달할 수 있는 짧은 경로(bridge)를 찾으려 시도합니다. $k$ 단계 내에 브리징 경로를 찾지 못하면, 해당 상태에서의 멘토 가치 전파를 억제합니다.

## 📊 Results

- **동질적 환경**:
  - **학습 가속화**: 기본 그리드 월드에서 멘토를 모방한 에이전트(Observer)가 대조군(Control)보다 훨씬 빠르게 목표를 달성하고 높은 누적 보상을 얻었습니다.
  - **확장성 및 노이즈**: 상태 공간이 커질수록 모방의 이득이 증가했으며, 노이즈가 증가해도 성능 저하가 점진적으로 이루어졌습니다.
  - **신뢰도 테스트의 중요성**: 오도하는 보상 구조(예: 접근 불가능한 고가치 섬)가 있는 환경에서 신뢰도 테스트를 사용한 에이전트는 함정에 빠지지 않고 최적 경로를 찾았습니다.
  - **복잡한 문제**: 25x25 미로와 같이 복잡한 환경에서 모방 에이전트는 20,000단계 내에 안정적인 가치 함수를 구축하여 목표 달성률을 최적화한 반면, 대조군은 200,000단계 이상이 소요되었습니다.
  - **준최적 정책 개선**: 멘토와 보상 구조가 완벽하게 일치하지 않더라도, 관찰자가 먼저 준최적 경로를 찾은 후 멘토의 관찰을 통해 더 짧은 최적 경로(지름길)를 발견하고 성능을 크게 향상시킬 수 있었습니다.
  - **다중 멘토 활용**: 여러 멘토의 정보를 성공적으로 통합하여 대조군보다 훨씬 빠르게 학습할 수 있음을 보여주었습니다.
- **이질적 환경**:
  - **실행 가능성 테스트의 필요성**: 멘토와 다른 행동 세트(예: NEWS vs. Skew)를 가진 경우, 실행 가능성 테스트 없이는 에이전트가 실행 불가능한 멘토 행동을 반복적으로 시도하며 학습이 지연되거나 갇히는 현상이 발생했습니다. 테스트를 사용한 에이전트는 훨씬 빠르게 수렴했습니다.
  - **상태 공간 변화에 대한 대처**: 멘토 경로에 장애물이 있는 경우에도 실행 가능성 테스트가 로컬 상태 차이를 잘 처리함을 보여주었습니다. 완전히 실행 불가능한 멘토 궤적에 대해서도 관찰자는 평행한 자체 궤적을 생성하여 목표에 도달하는 "평행 일반화(Parallel generalization)" 능력을 보였습니다.
  - **브리징의 유용성**: 강을 건너야 하는 시나리오에서, 멘토 행동이 실행 불가능할 때 k-단계 복구(브리징)를 통해 관찰자가 강 반대편의 보상까지 가치 정보를 전파하고, 결국 최적 경로를 찾아낼 수 있었습니다.

## 🧠 Insights & Discussion

- **학습 가속화의 원리**: 암묵적 모방은 멘토의 관찰된 행동을 통해 관찰자 자신의 환경 모델과 가치 추정치를 미리 학습하여 탐색 부담을 줄이고, 높은 가치 영역으로의 편향된 탐색을 유도함으로써 학습을 가속화합니다.
- **광범위한 적용 가능성**: 명시적인 통신, 공유된 보상 함수, 또는 멘토의 행동 관찰 없이 작동하므로, 기존 모방 모델보다 더 넓은 범위의 다중 에이전트 시나리오에 적용될 수 있습니다.
- **멘토 정보의 신뢰성 관리**: 모델 신뢰도 테스트, 실행 가능성 테스트, k-단계 복구는 멘토 모델의 부정확성이나 멘토와 관찰자의 능력 차이로 인해 발생할 수 있는 오도된 정보로부터 관찰자를 보호하는 핵심 메커니즘입니다.
- **탐색-활용 트레이드오프 재해석**: 암묵적 모방은 탐색률을 멘토 정책의 충분성에 대한 관찰자의 믿음과 연결하여, 전통적인 탐색/활용 트레이드오프에 새로운 해석을 제공합니다.
- **골절 지표(Fracture Metric)**: 멘토와 관찰자 정책이 불일치하는 상태와 일치하는 상태 사이의 평균 최소 거리를 측정하는 "골절 지표"는 도메인 특성(연결성, 이질성)이 모방 성능에 미치는 영향을 정량적으로 예측하는 데 유용합니다.
- **준최적성 및 편향**: 멘토가 최적이 아니거나 탐색률이 너무 낮으면 관찰자가 준최적 정책에 수렴할 수 있지만, 그럼에도 불구하고 많은 실제 문제에서 "아무것도 없는 것"보다는 실행 가능한 해결책을 제공하는 것이 바람직할 수 있습니다.
- **확장 가능성**: 보상 함수 미지, 에이전트 간 상호작용, 부분 관찰 가능 도메인, 연속적 및 모델 없는 학습 등 다양한 복잡한 문제 클래스로의 확장이 논의됩니다.

## 📌 TL;DR

**Problem**: 강화 학습(RL) 에이전트의 느린 학습 속도를 가속화하기 위해, 명시적인 통신 없이 다른 에이전트의 행동 관찰을 활용하는 방법이 필요합니다.

**Proposed Method**: **Implicit Imitation** 모델을 제안합니다. 이 모델은 멘토의 상태 전이를 관찰하여 멘토의 행동 모델을 추출하고(Model Extraction), 이를 관찰자의 가치 함수 추정에 반영하는 **증강 벨만 백업(Augmented Bellman Backup)**을 사용합니다. 멘토와 관찰자의 능력 차이(동질적/이질적)를 다루기 위해 **모델 신뢰도 테스트(Model Confidence Testing)**, **실행 가능성 테스트(Feasibility Testing)**, **k-단계 복구(k-step Repair, 브리징)** 기법을 도입하여 정보의 오용을 방지하고 활용도를 높입니다.

**Key Findings**: Implicit Imitation은 강화 학습의 수렴 속도를 크게 가속화하며, 특히 큰 상태 공간, 복잡한 환경, 또는 여러 멘토가 있는 경우에 효과적입니다. 이질적인 능력이나 환경 차이가 있는 상황에서도 실행 가능성 테스트와 k-단계 복구를 통해 견고하게 작동하며, 최적의 정책이 아닌 멘토로부터도 유용한 정보를 얻을 수 있음을 보여줍니다. "골절 지표(Fracture Metric)"는 멘토 관찰의 유효성을 예측하는 데 사용될 수 있습니다.
