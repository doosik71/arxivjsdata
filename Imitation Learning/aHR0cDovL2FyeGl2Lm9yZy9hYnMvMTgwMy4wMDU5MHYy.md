# Hierarchical Imitation and Reinforcement Learning

Hoang M. Le, Nan Jiang, Alekh Agarwal, Miroslav Dudík, Yisong Yue, Hal Daumé III

## 🧩 Problem to Solve

강화 학습(RL)은 보상이 희소(sparse)하고 계획 범위(planning horizon)가 긴 문제에서 상당한 어려움을 겪습니다. 모방 학습(IL)은 이러한 문제를 완화할 수 있지만, 장기적인 문제의 경우 많은 양의 전문가 시범(demonstration) 데이터가 필요할 수 있습니다. 이 논문은 전문가가 제공될 때, 계층적 문제 구조를 활용하여 전문가 피드백을 가장 효과적으로 활용하고, 전문가의 노력과 탐색 비용을 줄이는 방법을 탐구합니다.

## ✨ Key Contributions

- **계층적 가이던스(Hierarchical Guidance) 프레임워크 제안**: 계층적 문제 구조를 활용하여 다양한 모드의 전문가 상호작용을 통합하는 알고리즘 프레임워크를 제시합니다.
- **전문가 노력 및 탐색 비용의 획기적인 절감**: 이 프레임워크는 모방 학습(IL)과 강화 학습(RL)의 다양한 조합을 여러 수준에서 통합하여 전문가의 노력과 탐색 비용을 극적으로 줄입니다.
- **알고리즘 인스턴스 개발**: 수동적인 모방 학습(h-BC) 및 대화형 모방 학습(hg-DAgger), 그리고 하이브리드 IL/RL 설정(hg-DAgger/Q)을 위한 계층적 가이던스 변형을 제안합니다.
- **이론적 분석**: hg-DAgger의 라벨링 비용을 분석하여, 평면(flat) DAgger 대비 계층적 가이던스가 전문가 비용을 크게 절감할 수 있음을 이론적으로 증명합니다. 특히 전체 계획 범위 $H_{\text{FULL}}$이 길 때, 라벨링 비용을 $\sqrt{H_{\text{FULL}}}$ 요인만큼 줄일 수 있습니다.
- **실험적 검증**: Montezuma's Revenge를 포함한 장기적인 벤치마크에서 기존 계층적 RL보다 훨씬 빠르게 학습하고, 표준 IL보다 라벨 효율성이 훨씬 높음을 입증합니다.

## 📎 Related Works

- **모방 학습(Imitation Learning)**: 행동 복제(behavioral cloning)와 DAgger(Dataset Aggregation)와 같은 대화형 모방 학습을 포함하며, 본 연구는 이들을 계층적 설정으로 확장합니다.
- **계층적 강화 학습(Hierarchical Reinforcement Learning, HRL)**: 옵션(options) 프레임워크, MAXQ, h-DQN 등 계층적 정책 학습에 대한 기존 RL 접근 방식과 비교하며, 본 연구는 HRL이 전문가 피드백 없이 서브골 정책을 학습해야 하는 한계를 지적합니다.
- **RL과 IL의 결합**: 기존 연구들은 주로 IL을 전처리(pre-training) 단계로 사용하는 데 초점을 맞춘 반면(예: 데모로 리플레이 버퍼 채우기), 본 연구는 계층적 정책 클래스의 여러 수준에서 다양한 유형의 피드백(한 수준에서는 모방, 다른 수준에서는 강화)을 고려합니다.
- **약한 피드백을 통한 학습**: 선호도 기반 피드백(preference-based feedback)이나 그래디언트 스타일 피드백(gradient-style feedback)을 사용하는 약한 전문가 피드백 학습과도 연관됩니다.

## 🛠️ Methodology

이 논문은 두 가지 수준의 계층을 가진 환경을 가정합니다: 상위(HI) 수준에서는 서브태스크(subtask)를 선택하고, 하위(LO) 수준에서는 해당 서브태스크를 실행합니다. 에이전트는 메타 컨트롤러 $\mu$와 각 서브골 $g$에 대한 서브폴리시 $\pi_g$를 동시에 학습합니다. 전문가는 다음과 같은 다양한 유형의 감독을 제공할 수 있습니다:

- **HierDemo(s)**: 계층적 시범(expert's hierarchical policy 실행).
- **Label${}_{\text{HI}}(\tau_{\text{HI}})$**: 상위 수준 라벨링(주어진 HI-수준 궤적에 대한 다음 서브골 제공).
- **Label${}_{\text{LO}}(\tau; g)$**: 하위 수준 라벨링(주어진 LO-수준 궤적에 대한 다음 원시 동작 제공).
- **Inspect${}_{\text{LO}}(\tau; g)$**: 하위 수준 검사(서브골 $g$가 달성되었는지 Pass/Fail만 확인).
- **Label${}_{\text{FULL}}(\tau_{\text{FULL}})$**: 전체 라벨링(계층 구조 무시하고 전체 궤적 라벨링).
- **Inspect${}_{\text{FULL}}(\tau_{\text{FULL}})$**: 전체 검사(전체 목표 달성 여부 Pass/Fail만 확인).

**계층적 가이던스(Hierarchical Guidance)**는 다음과 같은 두 가지 방식으로 작동합니다:

1. **필요할 때만 질의**: 상위 수준 전문가가 서브태스크가 아직 숙달되지 않았을 때만 하위 수준 전문가에게 질의합니다.
2. **관련 상태 공간으로 제한**: 하위 수준 학습을 관련 상태 공간으로 제한합니다.

**알고리즘 구현:**

- **계층적 행동 복제(h-BC, Algorithm 1)**: 전문가로부터 계층적 시범 $\sigma^*$를 수집하고, 이를 HI-수준 및 LO-수준 데이터 버퍼에 추가하여 메타 컨트롤러 $\mu$와 서브폴리시 $\pi_g$를 학습합니다.
- **계층적 가이던스 DAgger(hg-DAgger, Algorithm 2)**:
  1. 웜 스타트(warm-start)를 위해 h-BC를 실행합니다.
  2. 에이전트가 계층적 정책을 실행하여 궤적 $\sigma$를 생성합니다.
  3. `Inspect_FULL($\tau_{\text{FULL}}$)`이 Fail을 반환하면, `Label_HI($\tau_{\text{HI}}$)`를 요청합니다.
  4. 메타 컨트롤러의 서브골 선택이 전문가와 일치하는 동안, `Inspect_LO($\tau_h; g_h$)`가 Fail을 반환할 때만 `Label_LO($\tau_h; g_h$)`를 요청하여 LO-수준 라벨링 비용을 절감합니다.
- **계층적 가이던스 DAgger/Q-러닝 (hg-DAgger/Q, Algorithm 3)**:
  1. HI-수준은 DAgger, LO-수준은 Q-러닝을 사용합니다.
  2. `pseudo(s; g)` 함수와 `terminal(s; g)` 조건이 있다고 가정합니다.
  3. `Inspect_FULL`이 Fail일 때 HI-수준 라벨링을 요청하고, 메타 컨트롤러가 올바른 서브골을 선택할 때만 LO-수준 Q-러닝을 위한 경험을 축적합니다. 이는 경험 버퍼에 관련 데이터만 포함되도록 합니다.

## 📊 Results

- **미로 탐색 도메인**:
  - hg-DAgger는 평면 IL 알고리즘(flat DAgger, flat 행동 복제) 및 h-BC보다 일관되게 가장 높은 성공률(1000 에피소드 미만에서 100%에 근접)을 달성했습니다.
  - hg-DAgger는 계층적 가이던스를 통해 LO-수준 전문가의 효율적인 사용 덕분에 다른 IL 알고리즘에 비해 전문가 비용을 크게 절감했습니다. 특히 학습 초기에 대부분의 LO-수준 라벨을 요구하고, 이후에는 주로 HI-수준 라벨을 요청했습니다.
  - hg-DAgger/Q는 LO-수준에서 훨씬 더 많은 샘플을 사용했음에도 h-DQN(계층적 RL)보다 훨씬 높은 성공률을 보였으며, h-DQN은 낮은 성공률에 머물렀습니다.
- **몬테주마의 복수(Montezuma's Revenge)**:
  - hg-DAgger/Q는 h-DQN에 비해 외부 보상에서 훨씬 높은 성능을 달성했으며, 서브골 학습에 필요한 LO-수준 샘플 수가 현저히 적었습니다.
  - 특히 hg-DAgger/Q는 전문가의 HI-수준 가이던스 덕분에 LO-수준 강화 학습자가 '나쁜 경험(bad experience)'을 축적하는 것을 방지하여 학습 속도와 안정성을 높였습니다.
  - h-DQN은 서브골을 2개로 제한했음에도 불구하고 두 번째 서브골(열쇠 획득)을 마스터하는 데 실패하는 등 저조한 성능을 보였습니다.

## 🧠 Insights & Discussion

- **전문가 피드백의 효율성**: 계층적 가이던스 프레임워크는 장기적인 순차적 의사 결정 문제에서 전문가 피드백을 활용하여 학습 속도를 가속화하고 전문가의 노력을 줄이는 효과적인 방법임을 보여줍니다.
- **하이브리드 접근 방식의 강점**: 하이브리드 IL/RL(hg-DAgger/Q)은 LO-수준 전문가 라벨을 얻기 어렵거나 비용이 많이 드는 환경에서 특히 유용합니다. HI-수준의 전문가 지도를 통해 LO-수준 RL의 탐색 효율성을 크게 개선할 수 있습니다.
- **상태 공간의 관련성**: LO-수준 학습을 관련 상태 공간으로 제한함으로써 경험 리플레이 버퍼가 '오염'되는 것을 방지하고, 이는 특히 불안정한 딥 RL 알고리즘의 수렴 속도와 안정성에 중요한 영향을 미칩니다.
- **한계 및 미래 연구**: 현재 연구는 `pseudo` 보상 함수와 `terminal` 예측기가 주어진다고 가정합니다. 이러한 서브골 종료 예측기를 강화 학습 피드백으로부터 학습하는 방법은 향후 연구 과제로 남아 있습니다. 또한, 선호도 기반 피드백과 같은 더 약한 형태의 전문가 피드백을 통합하는 것도 가능할 것입니다.

## 📌 TL;DR

이 논문은 보상이 희소하고 계획 범위가 긴 계층적 순차적 의사 결정 문제에서 효과적인 정책 학습을 위해 **계층적 가이던스(Hierarchical Guidance)**라는 새로운 프레임워크를 제안한다. 이 프레임워크는 상위 수준(HI-level)과 하위 수준(LO-level)에서 모방 학습(IL)과 강화 학습(RL)을 통합하여, 전문가 피드백을 효과적으로 활용하고 전문가의 노력과 탐색 비용을 획기적으로 줄인다. **hg-DAgger**와 **hg-DAgger/Q** 알고리즘은 HI-수준 전문가의 지시를 통해 LO-수준 학습을 필요한 부분과 관련 상태 공간에만 집중시켜, 기존 평면(flat) IL 및 계층적 RL(h-DQN)보다 훨씬 빠르고 라벨 효율적으로 Montezuma's Revenge와 같은 도전적인 벤치마크에서 높은 성능을 달성함을 이론적 분석 및 실험을 통해 입증했다.
