# Task-Relevant Adversarial Imitation Learning

Konrad Zołna, Scott Reed, Alexander Novikov, Sergio Gómez Colmenarejo, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, Ziyu Wang

## 🧩 Problem to Solve

적대적 모방 학습(Adversarial Imitation Learning, GAIL)의 주요 취약점은 판별자(discriminator) 네트워크가 시각적 특징과 전문가 라벨 간의 의사 연관성(spurious associations)을 학습하는 경향입니다. 판별자가 작업과 무관한 특징(task-irrelevant features)에 집중하게 되면, 유익한 보상 신호(informative reward signal)를 제공하지 못하여 작업 성능 저하로 이어집니다. 특히 로봇 조작과 같은 제어 애플리케이션에서 픽셀 기반의 강건한 적대적 모방 학습은 여전히 어려운 과제로 남아있습니다. 에이전트의 성능이 향상될수록 작업 관련 특징만으로는 전문가와 에이전트의 행동을 구분하기 어려워지며, 이때 판별자가 제어할 수 없는 의사 특징(예: 초기 조건, 밝기 등)에 의존하게 되어 보상 신호가 무의미해지는 문제가 발생합니다.

## ✨ Key Contributions

1. **GAIL의 근본적인 문제점 강조**: GAIL 판별자가 실제로 의사 연관성을 활용하여 작업 성능을 저하시킨다는 것을 입증했습니다.
2. **TRAIL(Task-Relevant Adversarial Imitation Learning) 제안**: 판별자가 작업 관련 패턴에 집중하도록 효과적으로 제약하는 새로운 방법을 제시했습니다.
3. **향상된 로봇 조작 성능**: 시각 기반 로봇 조작 작업에서 TRAIL이 기존 방법들보다 우수한 성능을 보여주었습니다.

## 📎 Related Works

- **행동 복제(Behavioral Cloning, BC)**: 많은 제어 문제를 해결하지만, 오류 누적에 취약하고 대량의 데모가 필요하며 전문가 성능을 뛰어넘을 수 없다는 한계가 있습니다.
- **역강화 학습(Inverse Reinforcement Learning, IRL)**: 데모로부터 보상 함수를 학습하는 방법으로, GAIL과 밀접한 관련이 있는 최대 엔트로피 IRL 알고리즘도 유사한 단점을 가집니다.
- **데모로부터 학습(Learning from Demonstrations, DQfD)**: 전문가 궤적을 경험 리플레이에 추가하여 사용하지만, 학습을 위해 여전히 보상에 접근해야 합니다.
- **교란 요인과 인과 관계(Confounders and Causality)**: 의사 연관성 및 인과 관계 연구는 광범위하며, 일반적인 정규화만으로는 식별 불가능성 문제를 해결하기 어렵다는 점에서 TRAIL의 접근 방식과 차이가 있습니다.
- **GAIL 변형 연구**: GAN의 성공에 따라 적대적 학습을 모방 학습에 적용했지만, 고차원 입력(예: 원시 픽셀)에서는 여전히 어려움이 있습니다. 판별자의 과적합 문제와 유사하지만, 비구조적인 정규화로는 의사 특징 활용을 막을 수 없습니다.
- **Stadie et al.의 3인칭 모방 학습**: 다른 시점에서 오는 데모와 에이전트 관찰에 대해 도메인 불변 특징 학습을 시도했지만, 본 연구의 목표(단일 도메인 내 의사 연관성 로버스트ness)와 다릅니다.

## 🛠️ Methodology

TRAIL은 판별자가 의사 연관성을 형성하는 것을 방지하기 위해 판별자 최적화에 제약을 가합니다.

- **제약 세트($I_E$, $I_A$) 구성**:
  - 전문가 데모에서 얻은 관찰($I_E$)과 에이전트 에피소드에서 얻은 관찰($I_A$)로 구성됩니다.
  - 이 제약 세트의 원소들은 오직 **의사 특징**을 통해서만 전문가 또는 에이전트에 속하는 것으로 식별될 수 있도록 합니다.
  - **구성 방법**:
    1. **초기 프레임(Early frames)**: 에피소드의 초반 프레임에는 작업 행동이 거의 없으므로, 이를 통해 작업과 무관한 특징만으로 구별될 수 있는 세트를 만듭니다. (기본값)
    2. **무작위 정책(Random policy)**: 목적 없는 행동을 수행하는 무작위 정책으로 수집된 관찰을 사용합니다.
- **판별자 목적 함수 수정**:
  - GAIL의 교차 엔트로피 목적 함수와 동일하게 시작하지만, $I_E \cup I_A$에서 얻은 관찰에 대해 정확도 제약(accuracy constraint)을 적용합니다.
  - 수정된 판별자 목적 함수는 다음과 같습니다:
    $$ L*{\psi}(s_E, s_A, \hat{s}\_E, \hat{s}\_A) = G*{\psi}(s*E, s_A) - \mathbb{1}*{\text{accuracy}(\hat{s}_E, \hat{s}\_A) \ge \frac{1}{2}} G_{\psi}(\hat{s}_E, \hat{s}\_A) $$
    여기서 $G_{\psi}(s*E, s_A) = \sum*{i=1}^{N} \log D*{\psi}(s*{E}^{(i)}) + \log[1-D_{\psi}(s_{A}^{(i)})]$ 이고, $\mathbb{1}_{\text{accuracy}(\hat{s}_E, \hat{s}_A) \ge \frac{1}{2}}$ 는 제약 세트에 대한 판별자의 정확도가 $\frac{1}{2}$ 이상일 때 1이 되는 지시 함수입니다. 즉, 판별자가 의사 특징을 통해 제약 세트를 잘 구별하면 페널티를 부여하여 이를 '잊게' 만듭니다.
- **액터 조기 중단(Actor Early Stopping, AES)**:
  - 에이전트가 발전하여 전문가와 유사한 행동을 생성하면 판별자가 의사 특징에 의존하게 되는 문제를 방지합니다.
  - 에이전트 에피소드가 일정 수의 스텝 후에 중단되고 재시작되어, 성공적인 행동이 에이전트 데이터에 너무 자주 나타나지 않도록 합니다.
  - **적응형 기준**: 현재 스텝의 판별자 점수가 에피소드 현재까지의 중간 점수를 $T_{\text{patience}}$ 스텝 연속으로 초과하면 에피소드를 재시작합니다.

## 📊 Results

- **다양한 조작 작업 평가**: TRAIL은 '들어 올리기(lift)', '쌓기(stack)', '바나나 쌓기(stack banana)', '삽입(insertion)'과 같은 4가지 로봇 조작 작업에서 전문가 수준의 성능을 달성했습니다. 행동 복제(BC)와 표준 GAIL은 실패하거나 저조한 성능을 보였고, GAIL+AES(액터 조기 중단 포함)도 TRAIL보다 성능이 떨어졌습니다. 희소한(sparse) 실제 보상을 사용하는 D4PGfD는 '들어 올리기'만 해결하여, 밀집된(dense) 판별자 기반 보상이 탐색에 중요함을 시사했습니다.
- **다른 외관의 전문가 환경**: 그리퍼 색상 변경을 통해 시각적 의사 특징을 의도적으로 도입한 환경에서, GAIL+AES는 성능 저하를 보인 반면, TRAIL은 작업을 해결하고 전문가보다 나은 성능을 달성하여 의사 특징에 대한 강건함을 입증했습니다.
- **방해물(distractors)이 있는 들어 올리기**: 두 개의 추가 블록을 방해물로 도입한 'lift distracted' 작업에서 모든 GAIL 기반 기준 모델은 방해물에 영향을 받았지만, TRAIL만 성공적으로 작업을 수행했습니다. 이는 GAIL 판별자가 방해물의 초기 위치와 같은 의사 특징을 학습하여 완벽한 정확도를 달성했기 때문입니다.
- **제약 세트($I_E$, $I_A$) 구성**: '초기 프레임'과 '무작위 정책'으로 구성된 TRAIL 변형 모두 가장 어려운 'lift distracted with different body appearance' 작업에서 유사하게 우수한 성능을 보였으며, GAIL을 압도했습니다. '초기 프레임' 방법은 추가 데이터 수집이 필요 없어 실용적입니다.
- **액터 조기 중단(AES) 연구**: 적응형 AES는 강건하게 인간 수준의 성능을 달성했으며, 조기 중단이 에이전트 성능에 도움이 되지만, 어려운 문제에서는 제약 세트만큼 효과적이지 않음을 보여주었습니다.
- **TRAIL의 일반화 능력**: TRAIL 판별자는 훈련 및 홀드아웃 데모에 대해 유사한 예측 값을 할당하여 뛰어난 일반화 능력을 보였습니다. 반면 GAIL+AES는 훈련 데모에 빠르게 의사 연관성을 형성하여 홀드아웃 데모에는 일반화되지 못했습니다.
- **오라클 판별자를 통한 학습**: 의사 연관성을 완벽하게 악용하는 고정된 오라클 판별자로 학습한 에이전트는 TRAIL보다 성능이 낮았으며, 방해물이 있는 작업에서는 완전히 실패했습니다. 이는 학습된 보상 함수의 중요성을 강조합니다.

## 🧠 Insights & Discussion

GAIL에서 판별자가 시각적 특징과 전문가 라벨 간의 의사 연관성을 형성하는 경향은 핵심적인 취약점이며, 이는 보상 신호를 무의미하게 만들어 성능 저하를 초래합니다. TRAIL은 이 문제를 직접적으로 해결하기 위해 판별자 최적화에 제약을 가하는 새로운 접근 방식을 제안합니다. 이 방법은 의사 연관성에 대한 사전 지식 없이도 자동으로 이를 중화시킬 수 있습니다.

TRAIL은 전문가 행동이나 환경 보상에 대한 접근 없이도 다양한 로봇 조작 작업에서 전문가 수준의 성능을 달성했으며, 기존의 GAIL, 행동 복제 및 D4PGfD 에이전트를 능가했습니다. 이는 판별자가 작업 관련 특징에 집중하도록 강제함으로써 유익한 보상을 제공할 수 있음을 보여줍니다. 하지만 모든 가능한 작업에 대해 제약 세트의 선택을 자동화하지는 않으며, 효과적인 제약 세트 정의에는 여전히 어느 정도의 인간 통찰력이나 작업별 설계가 필요할 수 있다는 한계점이 있습니다.

## 📌 TL;DR

- **문제**: 표준 GAIL은 판별자가 작업과 무관한 시각적 특징에서 의사 연관성을 학습하여 로봇 조작과 같은 픽셀 기반 모방 학습에서 비효율적인 보상과 낮은 성능을 야기합니다.
- **방법**: TRAIL(Task-Relevant Adversarial Imitation Learning)은 판별자 최적화에 제약을 가하여 이 문제를 해결합니다. '제약 세트'($I_E$, $I_A$)를 사용하여 (주로 에피소드의 초기 프레임에서 가져옴) 의사 특징으로만 전문가와 에이전트 관찰을 구별할 수 있도록 합니다. 판별자가 이 세트들을 잘 구별하면 페널티를 부과하여 의사 특징에 집중하는 것을 방지합니다. 또한, 에이전트가 전문가와 너무 빨리 유사해지는 것을 막기 위해 '액터 조기 중단' 기법을 사용합니다.
- **주요 발견**: TRAIL은 시각적 방해물이나 외관 변화가 있는 환경에서도 강건함을 보이며, 다양한 도전적인 픽셀 기반 로봇 조작 작업에서 전문가 수준의 성능을 지속적으로 달성합니다. 이는 기존 GAIL, 행동 복제 및 D4PGfD보다 월등히 우수한 결과로, 의사 연관성 문제에 대한 효과적인 해결책을 제시합니다.
