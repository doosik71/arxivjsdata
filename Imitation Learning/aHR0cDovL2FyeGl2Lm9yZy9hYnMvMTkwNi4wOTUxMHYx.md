# Learning Belief Representations for Imitation Learning in POMDPs

Tanmay Gangwani, Joel Lehman, Qiang Liu, Jian Peng

## 🧩 Problem to Solve

본 논문은 부분 관측 마르코프 결정 과정(POMDPs)에서 전문가 데모를 활용한 모방 학습 문제를 다룹니다. 기존 강화 학습(RL)은 관측 공간의 정보가 불완전하고 노이즈가 많거나 보상이 희소한 실제 환경에 적용하기 어렵습니다. 또한, POMDPs에서의 신념(belief) 표현 학습은 종종 정책 학습과 분리되어 이루어졌으며, 모방 학습 연구는 대부분 완전 관측 마르코프 결정 과정(MDPs)에 초점을 맞추었습니다.

## ✨ Key Contributions

- **작업 인지 신념 학습(Task-aware Belief Learning):** 신념 모듈(belief module)과 정책을 작업 인지 모방 손실(task-aware imitation loss)을 사용하여 공동으로 학습합니다. 이는 신념 표현이 정책의 목표에 더 잘 부합하도록 하여 기존의 분리된 학습 방식의 한계를 극복합니다.
- **정보적 신념 정규화 기법 도입:** 신념 표현의 견고성을 향상시키고 신념 퇴화(belief degeneration)를 방지하기 위해, 다단계 동역학 및 행동 시퀀스 예측을 포함한 여러 정규화 기법을 제시합니다.
- **Belief-Module Imitation Learning (BMIL) 프레임워크 제안:** POMDPs에서 강건한 모방 학습을 위한 통합 아키텍처를 제안하며, 이는 신념, 정책, 판별자 네트워크를 적대적 모방의 최소-최대 목표로 공동 학습합니다.
- **우수한 성능 입증:** 부분 관측 연속 제어 이동 작업에서 기존 GAIL 알고리즘 및 작업 불인지 신념 학습(task-agnostic belief learning) 알고리즘을 포함한 여러 기준선보다 훨씬 뛰어난 성능을 보였습니다.
- **심층적인 기여 분석:** 광범위한 절제 연구(ablation analysis)를 통해 작업 인지 신념 학습 및 신념 정규화 기법의 효과를 검증했습니다.

## 📎 Related Works

- **POMDPs에서의 강화 학습:** DRQN (Hausknecht & Stone, 2015), 정책 경사법 (Igl et al., 2018), 부분 관측 안내 정책 탐색 (Zhang et al., 2016) 등 사전 정의된 보상 함수에 의존하는 방법들.
- **신념 상태 학습:** 이력 $h_t = (o_{\le t}, a_{\lt t})$로부터 신념을 학습하는 접근 방식들.
  - **Contrative Predictive Coding (CPC):** Guo et al. (2018)은 미래 관측치에 대한 CPC 손실을 사용하여 신념 표현을 학습하는 방법을 제시했습니다.
  - **Predictive State Representations (PSRs):** Littman & Sutton (2002), Venkatraman et al. (2017), Hefny et al. (2018)은 미래 관측치를 예측하여 표현을 개선했습니다. 본 논문은 과거 및 행동 공간 예측을 추가로 사용합니다.
- **상태 공간 모델 (SSMs):** Fraccaro et al. (2016), Goyal et al. (2017), Buesing et al. (2018) 등 잠재 변수를 사용하여 관측되지 않는 환경 상태를 나타내는 모델들. Igl et al. (2018)은 VAE와 파티클 필터링을 사용했으며, Gregor & Besse (2018)는 TD-VAE를 제안했습니다.

## 🛠️ Methodology

본 논문은 신념 모듈, 정책 네트워크, 판별자 네트워크로 구성된 Belief-Module Imitation Learning (BMIL) 프레임워크를 제안합니다.

1. **POMDP를 위한 모방 학습 목표:**

   - 기존 GAIL의 상태-행동 방문 분포 일치 목표 $D_{JS}[\rho_{\pi}(s,a) || \rho_E(s,a)]$를 POMDP 환경에 맞게 신념-행동 방문 분포 일치 $D_{JS}[\rho_{\pi}(b,a) || \rho_E(b,a)]$로 수정합니다.
   - 이 목표는 생성적 적대 신경망(GAN)의 최소-최대(min-max) 목표로 근사화됩니다:
     $$ \min*{\phi,\theta} \max*{\omega} \tilde{E}_{(b,a) \sim M_E} [\log D_{\omega}(b,a)] + \tilde{E}_{(b,a) \sim \pi,T} [\log(1-D_{\omega}(b,a))] $$
        여기서 $b = B_{\phi}(h)$는 신념 모듈 $B_{\phi}$가 생성한 신념 표현입니다.

2. **정책 모듈 $\pi_{\theta}(a_t|b_t)$:**

   - 신념 $b_t$에 조건화된 행동 분포를 학습합니다.
   - 정책은 판별자 $D_{\omega}$로부터 얻은 보상 $r(b,a) = -\log(1-D^*(b,a))$를 사용하여 A2C(Actor-Critic)와 같은 정책 경사(policy gradient) 알고리즘으로 업데이트됩니다.
     $$ \nabla*{\theta} D*{JS}(\theta;\phi) \approx \tilde{E}_{(b,a) \sim \pi,T} [\nabla_{\theta} \log \pi*{\theta}(a|b) \hat{Q}*{\pi}(b,a)] $$

3. **신념 모듈 $B_{\phi}$:**

   - 이력 $h_t = (o_{\le t}, a_{\lt t})$를 GRU 기반의 순환 신경망으로 처리하여 신념 표현 $b_t$를 생성합니다.
   - **작업 인지 학습:** 정책과 동일한 모방 학습 손실 $L_{IM}(\phi) := D_{JS}(\theta,\phi)$를 사용하여 공동으로 학습됩니다. 이는 신념 표현이 정책 학습 목표에 직접적으로 기여하도록 합니다.
   - **신념 정규화:** 신념 퇴화를 방지하고 강건성을 높이기 위해 상호 정보량 극대화를 기반으로 세 가지 보조 손실을 추가합니다:
     - **순방향 정규화 ($L_f$):** 미래 관측치 $o_{t+k}$를 예측합니다.
       $$ L*f(\phi) = E_R ||o*{t+k} - g(b*{\phi_t}, a*{t:t+k-1})||^2_2 $$
     - **역방향 정규화 ($L_i$):** 과거 관측치 $o_{t-k}$를 예측합니다.
       $$ L*i(\phi) = E_R ||o*{t-k} - g(b*{\phi_t}, a*{t-k:t-1})||^2_2 $$
     - **행동 정규화 ($L_a$):** 미래 행동 시퀀스 $a_{t:t+k-1}$를 예측합니다.
       $$ L*a(\phi) = E_R ||(a*{t:t+k-1}) - g(b*{\phi_t}, o*{t+k})||^2_2 $$
   - 신념 모듈의 최종 손실은 $L(\phi) = L_{IM} + \lambda_1 L_f + \lambda_2 L_i + \lambda_3 L_a$ 입니다. (여기서 $k$는 다단계 예측의 시간 오프셋을 나타내며, $k=1$ 및 $k=5$를 사용했습니다.)

4. **판별자 $D_{\omega}(b_t,a_t)$:**
   - 에이전트와 전문가 데모로부터의 $(b_t, a_t)$ 튜플을 이진 분류하여 $D_{JS}$ 목표를 최적화합니다.

## 📊 Results

- **기준선 대비 우수한 성능:** GAIL 및 관측치 스택킹(GAIL+Obs. stack)을 사용한 GAIL은 부분 관측성으로 인해 전문가 행동을 성공적으로 모방하지 못했습니다. 반면, BMIL은 전문가와 매우 유사한 성능을 달성했습니다 (표 1).
- **순환 네트워크 기반 GAIL 대비 우수성:** GAIL-RF (순환 정책, 피드포워드 판별자) 및 GAIL-RR (순환 정책, 순환 판별자)과 비교했을 때, BMIL은 지속적으로 우수한 성능을 보였으며, 특히 Humanoid 작업에서 1.6배 높은 점수를 기록했습니다. 이는 신념 모듈의 분리된 아키텍처와 BMIL의 설계 결정이 효과적임을 시사합니다 (표 2).
- **신념 정규화의 효과:** 신념 정규화(BMIL w/ Reg.)를 포함하면 대부분의 작업에서 더 나은 에피소드 보상과 샘플 효율성을 얻을 수 있음을 확인했습니다 (그림 3).
- **작업 인지 신념 학습의 중요성:** 신념 모듈을 작업 불인지 방식으로 학습한 경우 (Task-Agnostic)는 성능이 혼합되어 있었지만, BMIL의 작업 인지 학습 방식은 지속적으로 더 나은 성능을 보였습니다 (그림 3).
- **개별 정규화 기법의 기여:** $L_f, L_i, L_a$ 각각이 정규화 없는 기준선보다 성능 향상에 기여하며, 이들을 결합한 BMIL이 최상의 성능을 달성했습니다 (그림 5).
- **다단계 예측의 유용성:** 다단계($k=\{1,5\}$) 예측이 단일 단계($k=\{1\}$) 예측보다 더 나은 샘플 효율성과 높은 최종 에피소드 보상을 제공하여 표현 학습에 긍정적으로 기여함을 확인했습니다 (그림 6).
- **가중 부분 관측 환경에서의 강건성:** 신념 추론이 더 어려운 변형된 POMDP 환경(예: Inv.DoublePend. 및 Walker를 속도 센서만으로 구성)에서도 BMIL이 가장 강력한 기준선인 GAIL-RF보다 뛰어난 성능을 보였습니다 (표 5).

## 🧠 Insights & Discussion

- **POMDPs에서의 모방 학습 도전 과제:** 본 연구는 POMDPs 환경에서 모방 학습의 어려움, 특히 불완전한 관측 정보로 인한 정책 및 판별자의 학습 한계를 명확히 보여줍니다.
- **통합적 신념 학습의 중요성:** 정책과 신념 표현 학습을 최소-최대 목표로 공동 학습하는 방식은 신념 표현이 작업 목표에 직접적으로 부합하도록 유도하여 기존의 분리된 학습 방식보다 훨씬 효과적임을 입증했습니다.
- **신념 정규화의 역할:** 순방향/역방향 관측치 및 행동 시퀀스 예측을 통한 신념 정규화는 신념 표현의 퇴화를 방지하고, 장기적인 시간 의존성을 포착하는 데 필수적인 역할을 합니다. 특히 다단계 예측은 시간적 추상화를 통해 더 유용한 전역 정보를 신념 표현에 주입하는 데 기여합니다.
- **확장 가능성:** 본 프레임워크는 다양한 네트워크 아키텍처 (예: CPC, Z-forcing, 상태 공간 잠재 변수 모델) 또는 GAN 및 RL 분야의 새로운 발전을 통합하여 미래 연구를 확장할 수 있는 유연성을 제공합니다.

## 📌 TL;DR

본 논문은 부분 관측 마르코프 결정 과정(POMDPs)에서 전문가 데모를 모방 학습하는 문제를 다룹니다. 저자들은 신념 모듈(belief module), 정책, 판별자 네트워크를 적대적 모방의 최소-최대 목표 하에 공동으로 학습하는 **Belief-Module Imitation Learning (BMIL)** 프레임워크를 제안합니다. 특히, 신념 모듈은 **작업 인지 모방 손실**을 통해 정책 목표와 정렬되며, **다단계 관측치 및 행동 시퀀스 예측**과 같은 정규화 기법으로 강건성을 높입니다. BMIL은 부분 관측 MuJoCo 연속 제어 작업에서 기존 GAIL 및 다른 기준선들을 뛰어넘는 탁월한 성능을 보였으며, 신념 정규화와 작업 인지 신념 학습의 효과를 성공적으로 입증했습니다.
