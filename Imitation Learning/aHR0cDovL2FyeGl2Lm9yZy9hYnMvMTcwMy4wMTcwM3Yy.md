# Third-Person Imitation Learning

Bradly C. Stadie, Pieter Abbeel, Ilya Sutskever

## 🧩 Problem to Solve

강화 학습(Reinforcement Learning, RL)의 주요 난점 중 하나는 에이전트가 최적화할 보상 함수를 수동으로 지정해야 한다는 것입니다. 이 문제는 전통적으로 모방 학습(Imitation Learning)을 통해 해결되어 왔습니다. 그러나 기존 모방 학습 방법은 시연이 에이전트 자신의 시점, 즉 '1인칭' 시점(에이전트에게 상태 시퀀스 및 취해야 할 행동 명세 제공)으로 제공되어야 한다는 한계가 있습니다. 1인칭 시연을 수집하는 것은 어려운 문제인 반면, 인간은 다른 사람이 작업을 수행하는 것을 관찰하고 작업을 추론한 뒤 스스로 동일한 작업을 수행하는 '3인칭' 시연을 통해 학습합니다.

본 논문은 이러한 한계를 극복하고, 다른 시점에서 주어진 교사의 시연(demonstration)을 통해 학생 에이전트(student agent)가 동일한 목표를 성공적으로 달성하도록 훈련하는 **비지도(unsupervised) 3인칭 모방 학습** 방법론을 개발하는 것을 목표로 합니다. 특히, 교사의 상태와 학생의 상태 간에 명시적인 대응 정보가 주어지지 않는 상황에서 학습하는 것이 핵심 도전 과제입니다.

## ✨ Key Contributions

* **비지도 3인칭 모방 학습 프레임워크 제안**: 교사의 시연(다른 시점)만으로, 그리고 교사 상태와 학생 상태 간의 직접적인 대응 정보 없이 학생 에이전트가 동일한 목표를 달성하도록 훈련하는 새로운 방법론을 제시했습니다.
* **도메인 불변 특징 추출(Domain-Agnostic Feature Extraction) 도입**: 도메인 혼동(domain confusion) 기술을 활용하여, 교사와 학생 환경 간의 시각적 차이에도 불구하고 작업에 본질적인 도메인 불변 특징을 추출하는 메커니즘을 개발했습니다. 이는 Generative Adversarial Networks (GANs)의 아이디어를 기반으로 합니다.
* **다중 시간 단계 입력(Multi-Time Step Input) 활용**: 판별자(discriminator)가 에이전트 행동의 환경 변화 영향을 더 잘 이해하도록 현재 상태뿐만 아니라 미래 특정 시점($t+n$)의 상태 특징도 입력으로 사용합니다.
* **다양한 환경에서의 성공적인 검증**: `pointmass`, `reacher`, `inverted pendulum`의 세 가지 시뮬레이션 환경에서 제안된 방법이 성공적으로 3인칭 모방 학습을 수행함을 입증했습니다.
* **도메인 혼동의 중요성 입증**: 실험을 통해 도메인 혼동 손실(domain confusion loss)이 3인칭 모방 학습에서 강력한 성능을 달성하는 데 필수적임을 확인했습니다.

## 📎 Related Works

* **모방 학습(Imitation Learning)**: 행동 복제(Behavioral Cloning)와 역강화 학습(Inverse Reinforcement Learning, IRL)의 두 가지 주요 흐름이 있습니다.
  * **행동 복제**: 시연을 사용하여 관찰에서 행동으로 직접 매핑을 학습합니다 (e.g., Pomerleau (1989), Ross et al. (2011)).
  * **역강화 학습(IRL)**: 전문가 시연을 통해 보상 함수를 추정합니다 (e.g., Abbeel & Ng (2004), Ho & Ermon (2016), Finn et al. (2016)).
* **3인칭 모방 학습의 도전**: 기존 모방 학습은 1인칭 시점에 한정되며, 3인칭 시점에서는 시연자와 모방자의 관찰 및 행동이 다르기 때문에 직접 적용하기 어렵습니다. 특히 원시 센서 데이터로부터 도메인 불변 특징을 추출하는 것이 중요합니다.
* **원시 센서 데이터 기반 IRL**: Finn et al. (2016), Ho & Ermon (2016), Wulfmeier et al. (2015) 등은 원시 센서 데이터로부터 직접 IRL을 수행했지만, 이들은 1인칭 설정에 국한됩니다.
* **GANs(Generative Adversarial Networks)**: 본 연구의 최적화 공식은 Generative Adversarial Networks (Goodfellow et al., 2014) 및 Ho & Ermon (2016)의 Generative Adversarial Imitation Learning (GAIL)에 크게 기반을 둡니다.
* **도메인 적응(Domain Adaptation)**: 컴퓨터 비전 분야에서 한 도메인에서 학습된 지식을 다른 도메인에 적용하는 연구 (e.g., Tzeng et al. (2014; 2015), Ganin & Lempitsky (2014)). 특히, 도메인 혼동 손실을 사용하여 도메인 불변 특징을 학습하는 아이디어가 활용되었습니다.
* **심층 강화 학습(Deep Reinforcement Learning)**: Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (TRPO, Schulman et al., 2015a), A3C (Mnih et al., 2016) 등 최근 심층 강화 학습의 발전이 본 연구를 가능하게 했으며, 본 논문에서는 TRPO를 사용합니다.

## 🛠️ Methodology

본 논문은 Ho & Ermon (2016)의 1인칭 RL-GAN 프레임워크를 3인칭 모방 학습 문제에 맞게 확장합니다. 핵심은 도메인 불변 특징 추출기와 도메인 혼동 손실을 도입하는 것입니다.

1. **게임 공식화**:
    * **확장된 판별자 구조**: 기존 GAN 기반 모방 학습의 판별자($D_R$)는 전문가와 비전문가 정책이 생성한 상태를 구별합니다. 3인칭 설정의 도메인 차이를 다루기 위해 $D_R$을 다음과 같이 분할합니다:
        * **특징 추출기($D_F$)**: 관찰($o_t$)에서 특징 벡터($\sigma_t = D_F(o_t)$)를 추출합니다.
        * **클래스 분류기($D_R$)**: $D_F$가 추출한 특징을 입력받아 해당 관찰이 전문가 궤적에서 왔는지($c'_i$)를 분류합니다.
        * **도메인 판별자($D_D$)**: $D_F$가 추출한 특징($\sigma_t$)을 입력받아 해당 특징이 전문가 도메인($d'_i = 1$)에서 왔는지, 학생 도메인($d'_i = 0$)에서 왔는지를 판별합니다.
    * **도메인 불변 특징 학습**: $D_F$가 도메인에 불가지론적인(domain-agnostic) 특징을 생성하도록 유도하기 위해 $D_D$를 혼동시키는 목표를 추가합니다. 이는 $D_R$의 분류 정확도를 최소화하면서 $D_D$의 분류 정확도를 최대화하는 적대적 학습으로 이루어집니다.
    * **수학적 공식화**:
        $$ \max_{\pi_{\theta}} \min_{D_R} \max_{D_D} L_R + L_D = \sum_i \text{CE}(D_R(\sigma_i, \sigma_{i+n}), c'_i) + \text{CE}(D_D(\sigma_i), d'_i) $$
        여기서 $\text{CE}$는 교차 엔트로피 손실, $c'_i$는 전문가/비전문가 클래스 레이블, $d'_i$는 도메인 레이블입니다.
    * **그래디언트 뒤집기(Gradient Flipping)**: $D_F$가 $D_D$의 손실을 최대화하도록 직접 훈련하는 대신, Ganin & Lempitsky (2014)의 기법을 차용하여 $D_D$ 손실에 대한 $D_F$의 그래디언트 부호를 뒤집습니다. 이로써 $D_F$는 도메인 분류에 유용한 정보를 파괴하여 도메인 불변 특징을 생성하게 됩니다. 최종 공식은 다음과 같습니다.
        $$ \max_{\pi_{\theta}} \min_{D_R, D_D, D_F} L_R + L_D = \sum_i \text{CE}(D_R(\sigma_i, \sigma_{i+n}), c'_i) + \lambda \text{CE}(D_D(G(\sigma_i)), d'_i) $$
        여기서 $G$는 역전파 시 그래디언트 부호를 뒤집는 함수이며, $\lambda$는 도메인 혼동 손실의 가중치 하이퍼파라미터입니다.
    * **다중 시간 단계 입력**: $D_R$이 에이전트의 행동으로 인한 환경 변화를 더 잘 판단할 수 있도록, 현재 관찰($o_t$)뿐만 아니라 미래 특정 시점($o_{t+n}$)의 관찰로부터 추출된 특징($\sigma_t, \sigma_{t+n}$)을 함께 사용합니다. 본 실험에서는 $n=4$를 사용합니다.

2. **알고리즘 절차 (Algorithm 1)**:
    * **초기화**: 전문가($E$) 및 학생($N$) 도메인, 전문가 성공/실패 궤적 메모리 뱅크($\Omega$), 도메인 불변 판별자 ($D_F, D_R, D_D$), 학생 정책($\pi_{\theta}$)을 초기화합니다.
    * **반복 학습**:
        * **판별자($D_F, D_R, D_D$) 업데이트 단계**:
            * $\Omega$에서 전문가 궤적($\omega_E$)과 현재 학생 정책($\pi_{\theta}$)으로 수집된 학생 궤적($\omega_N$)을 수집합니다.
            * 이 궤적들을 사용하여 $D_F, D_R, D_D$를 훈련합니다. 각 관찰 쌍($o_t, o_{t+4}$)에 대해 특징($\sigma_t, \sigma_{t+4}$)을 추출하고, 클래스 손실($L_R$)과 도메인 혼동 손실($L_D$)을 계산합니다.
            * 총 손실 $L = \lambda \cdot L_D + L_R$을 ADAM 최적화기를 사용하여 최소화하며, 이때 $D_F$에 대한 $L_D$의 그래디언트는 부호가 뒤집힙니다.
        * **정책($\pi_{\theta}$) 업데이트 단계**:
            * 현재 학생 정책($\pi_{\theta}$)으로 새로운 궤적($\omega_N$)을 수집합니다.
            * 각 관찰 쌍($o_t, o_{t+4}$)에 대해 훈련된 $D_F$와 $D_R$을 사용하여, 해당 궤적이 전문가 궤적일 확률($\hat{c}'$)을 보상 $r = \hat{c}'[0]$로 사용합니다.
            * 이 보상($r$)을 기반으로 Trust Region Policy Optimization (TRPO)를 사용하여 학생 정책($\pi_{\theta}$)을 업데이트합니다.
    * 이 과정을 반복하여 최적화된 정책($\pi_{\theta}$)을 반환합니다.

## 📊 Results

* **3인칭 모방 학습 문제 해결 가능성**: `pointmass`, `reacher`, `inverted pendulum` 세 가지 환경 모두에서 제안된 알고리즘이 합리적인 정책을 성공적으로 학습함을 확인했습니다 (Figure 3). 학습 초기에는 불안정하지만, 반복 학습 후에는 안정적인 성능을 보이며 도메인 불변 특징을 성공적으로 추출했습니다 (Figure 9).
* **도메인 혼동 및 다중 시간 단계 입력의 이점**:
  * **도메인 혼동**: 모든 실험에서 도메인 혼동 손실(domain confusion loss)을 추가하는 것이 강력한 성능을 얻는 데 **필수적**임을 확인했습니다 (Figure 5). 도메인 혼동이 없으면 판별자가 단순히 환경 차이를 분류 신호로 사용해 모방 학습에 실패했습니다 (Figure 4).
  * **다중 시간 단계 입력**: 현재 상태($o_t$)와 미래 상태($o_{t+n}$)를 함께 사용하는 것은 결과를 **미미하게 개선**시키는 것으로 나타났습니다 (Figure 5, 7).
* **하이퍼파라미터 민감도**:
  * **도메인 혼동 계수($\lambda$)**: $\lambda$ 값이 너무 낮거나 높으면 최종 정책 성능이 저하되었습니다 (Figure 6). 적절한 $\lambda$ 값 설정이 중요하며, 너무 낮으면 도메인 불변 특징 학습이 부족하고, 너무 높으면 너무 많은 정보가 파괴되어 정확한 비용 복구가 어려워집니다.
  * **미래 탐색 프레임 수($n$)**: `n=4`가 모든 작업에서 일관적으로 좋은 성능을 보였습니다. 너무 작은 $n$은 환경 변화를 반영하기 어렵고, 너무 큰 $n$은 인과 관계를 파악하기 어렵게 만들었습니다 (Figure 7).
* **카메라 앵글 변화에 대한 민감도**: `pointmass` 환경에서는 카메라 앵글 차이에 따라 최종 보상이 선형적으로 감소하는 경향을 보였으나, `reacher` 환경에서는 더 무작위적인 변화를 보였습니다 (Figure 8). `inverted pendulum`은 카메라 앵글 대신 폴 색상만 변경하여 실험했습니다.
* **다른 기준선(Baselines)과의 비교**:
  * **참 보상을 사용한 표준 RL**: 최상의 성능을 보였으며, 다른 모방 학습 접근 방식들의 성능을 평가하는 기준선 역할을 했습니다 (Figure 9).
  * **1인칭 모방 학습(GAIL)**: 3인칭 모방 학습보다 더 간단한 문제에 직면하므로 성능이 더 좋았지만, 3인칭 모방 학습도 경쟁력 있는 결과를 보였습니다 (Figure 9).
  * **1인칭 정책을 3인칭 환경에 적용**: 이 접근 방식은 처참하게 실패하여, 3인칭 모방 학습을 명시적으로 고려하는 것이 중요함을 시사했습니다 (Figure 9).

## 🧠 Insights & Discussion

* **3인칭 모방 학습의 중요성**: 1인칭 시연 데이터 수집 비용이 높은 상황에서, 3인칭 시연을 통한 학습은 로봇공학 등 다양한 분야의 발전에 핵심적인 역할을 할 것입니다. 인간이 다른 인간을 관찰하며 학습하는 방식과 유사하게, 로봇이 쉽게 새로운 기술을 습득할 수 있는 잠재력을 제공합니다.
* **도메인 불변 특징의 중요성**: 제안된 방법론의 핵심은 도메인 혼동(domain confusion)을 통해 시점이나 환경 설정이 다른 상황에서도 작업의 본질적인 특징을 추출하는 능력입니다. 이는 시각적 입력만으로도 다양한 도메인에 걸쳐 일반화될 수 있는 학습 능력을 제공합니다.
* **GAN 기반 프레임워크의 잠재력**: Generative Adversarial Imitation Learning (GAIL) 프레임워크의 확장 가능성을 보여주며, GAN 훈련 기법의 안정성과 성능이 향상될수록 3인칭 모방 학습 알고리즘도 더 복잡한 작업을 비지도 방식으로 해결할 수 있을 것으로 기대됩니다.
* **다중 시간 단계 입력의 활용**: 현재 및 미래 시점의 특징을 함께 사용하는 것이 에이전트 행동의 환경 변화 영향을 포착하는 데 유용했지만, 최적의 시간 간격($n$)은 작업마다 다를 수 있다는 한계가 있습니다. 이는 에이전트 행동의 즉각적인 영향과 장기적인 영향 사이의 균형을 찾는 문제로 볼 수 있습니다.
* **미래 연구 방향**:
  * 픽셀 수준에서 정책 특징(policy features)과 비용 특징(cost features)을 공동으로 훈련하여 이미지 특징 재사용을 극대화하는 방안.
  * 더 복잡하고 다양한 3인칭 모방 작업에 대한 적용 가능성 탐구.
  * GAN 훈련의 안정성 및 효율성 개선.
  * 더 다양한 형태의 도메인 변화(예: 물리적 속성, 환경 동역학)에 대한 강건성 확보.
* **제한 사항**: 현재는 간단한 환경에서 성공을 보였으며, 복잡한 현실 환경에서의 적용은 추가 연구가 필요합니다. 특히 `inverted pendulum`의 경우, 카메라 앵글 변경 시 실패하여 색상만 변경했다는 점은 아직 도메인 불변 특징 추출의 한계를 보여줍니다.

## 📌 TL;DR

본 논문은 강화 학습의 어려운 보상 함수 설계 문제를 해결하기 위한 **비지도 3인칭 모방 학습** 방법론을 제시합니다. 기존 1인칭 모방 학습의 한계를 넘어, 에이전트가 다른 시점의 전문가 시연을 관찰하여 보상 함수 없이 동일한 작업을 수행하도록 학습합니다. 핵심은 **도메인 혼동(domain confusion)** 기술을 활용하여 교사와 학생 환경 간의 시각적 차이에도 불구하고 작업에 본질적인 **도메인 불변 특징**을 추출하는 것입니다. GAN 기반의 프레임워크를 사용하며, 현재 및 미래 시점의 특징을 함께 활용하여 에이전트 행동의 영향을 더 잘 포착합니다. `pointmass`, `reacher`, `inverted pendulum` 환경에서 성공적으로 검증되었으며, 도메인 혼동이 3인칭 모방 학습에 필수적임을 입증했습니다. 이는 로봇이 인간으로부터 쉽게 새로운 기술을 학습할 수 있는 가능성을 열어줍니다.
