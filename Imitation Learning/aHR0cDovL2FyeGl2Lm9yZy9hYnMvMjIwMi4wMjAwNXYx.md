# BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning

Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn

## 🧩 Problem to Solve

시각 기반 로봇 조작 시스템이 이전에 본 적 없는 새로운 작업에 대해 일반화하는 것은 로봇 학습 분야의 오랜 난제입니다. 특히, 다양한 객체와 스킬(예: 닦기, 밀기, 집어 놓기)을 아우르는 시각 기반 조작 작업에서 *새로운 작업*에 대한 제로샷(zero-shot) 일반화는 여전히 어려운 문제로 남아있습니다. 이는 데이터 수집을 확장하고 다양한 데이터에 대한 학습 알고리즘을 개발하는 데 관련된 도전 과제들을 해결해야 함을 의미합니다.

## ✨ Key Contributions

* **대규모 대화형 모방 학습 시스템(BC-Z) 개발:** 데모와 인간의 개입(intervention)을 통해 학습하며, 자연어 임베딩 또는 인간의 작업 수행 비디오 등 다양한 형태의 정보로 태스크를 조건화할 수 있는 유연한 시스템을 개발했습니다.
* **대규모 데이터셋 구축 및 공개:** 12대의 로봇, 7명의 오퍼레이터가 100가지의 다양한 조작 작업을 수행하는 25,877개의 로봇 데모(125시간)와 18,726개의 인간 비디오를 수집했습니다. 데이터셋을 오픈소스로 공개했습니다.
* **제로샷 작업 일반화 입증:** 훈련 과정에서 보지 못한 24가지의 새로운 조작 작업을 평균 44%의 성공률로 수행하는 능력을 입증했습니다. 이는 해당 작업에 대한 로봇 데모 없이 달성된 결과입니다.
* **주요 방법론의 중요성 확인:** HG-DAgger 기반의 데이터 수집 방식과 적응형 상태 차분(adaptive state diffs), 그리고 미리 학습된 언어 임베딩이 로봇의 높은 일반화 성능에 중요함을 실험적으로 보여주었습니다.

## 📎 Related Works

* **모방 학습(Imitation Learning) 기반 일반화:** 기존 연구들은 새로운 객체($[1, 2, 3, 4, 5, 18]$), 새로운 객체 구성($[19]$), 또는 새로운 목표 구성($[6, 7, 20, 22]$)에 대한 원샷(one-shot) 또는 제로샷 일반화를 달성했습니다. 이는 로봇 데모($[1, 2]$), 인간 비디오($[3, 4]$), 언어 지시($[23, 24]$), 또는 목표 이미지($[21]$)를 조건으로 사용했습니다.
* **데이터 수집 방법:** 텔레오퍼레이션($[25]$)이나 운동 학습($[10]$)을 통한 데모 수집이 일반적입니다. DAgger($[26]$)와 같은 액티브 러닝 방법이 분포 이동(distribution shift)을 줄이는 데 도움이 되지만, 로봇 조작에는 적용하기 어렵다는 단점이 있습니다. 본 논문은 HG-DAgger($[29]$) 및 EIL($[30]$)에서 영감을 받은 인간 개입 방식을 사용합니다.
* **다른 로봇 학습 분야의 일반화:** 객체($[31, 32, 33, 34, 35]$), 환경($[36]$), 시뮬레이션-현실($[37, 38, 39, 40, 41]$)에 대한 스킬 일반화와 새로운 조작 스킬 및 객체에 대한 일반화($[42, 43, 44, 45]$) 연구들이 있습니다. 본 논문은 7-자유도(DoF) 제어와 에피소드당 100개 이상의 의사결정을 포함하는 100가지의 도전적인 태스크에 대한 일반화에 초점을 맞춥니다.

## 🛠️ Methodology

1. **시스템 구성:**
    * 로봇은 Oculus VR 헤드셋과 컨트롤러를 이용한 텔레오퍼레이션 시스템으로 제어됩니다. 10Hz의 비실시간 제어 루프를 통해 7-DoF(말단 장치 6-DoF + 그리퍼 1-DoF) 조작이 가능합니다.
2. **데이터 수집 및 공유 자율성(Shared Autonomy):**
    * 100가지의 사전에 정의된 조작 작업에 대해 데모와 인간 비디오를 수집합니다.
    * **공유 자율성 데이터 수집:**
        * 초기에는 전문가만이 작업을 시연합니다.
        * 이후 학습된 정책을 로봇에 배포하고, 인간 오퍼레이터가 로봇의 실수를 바로잡기 위해 일시적으로 완전한 제어권을 넘겨받아 개입(intervention)합니다. 이 과정은 HG-DAgger($[29]$) 알고리즘과 유사합니다.
        * 총 25,877개의 로봇 데모와 18,726개의 인간 비디오가 수집되었습니다.
    * 개입률(intervention rate)은 정책 성능을 추적하는 지표로 활용될 수 있음을 확인했습니다.
3. **정책 학습 알고리즘:**
    * **조건부 정책($\mu: S \times W \to A$):** RGB 이미지 $S$와 작업 명령어 $W$를 입력받아 액션 $A$를 출력합니다.
    * **인코더($q(z|w)$) 및 제어 레이어($\pi(a|s,z)$) 분리:**
        * 인코더 $q(z|w)$는 명령어 $w$를 512차원 태스크 임베딩 $z$로 변환합니다.
        * 제어 레이어 $\pi(a|s,z)$는 이미지 $s$와 태스크 임베딩 $z$를 바탕으로 액션 $a$를 생성합니다.
    * **언어 및 비디오 인코더:**
        * **언어 명령어:** 미리 학습된 다국어 문장 인코더($[46]$)를 사용하여 512차원 언어 벡터를 생성합니다.
        * **인간 비디오:** ResNet-18 기반 CNN을 사용하며, 행동 복제(behavior cloning) 손실과 보조 **언어 회귀 손실(language regression loss)**을 결합하여 학습됩니다.
            $$ \min \sum_{\text{task }i} \sum_{(s,a)\sim D^e_i, w_h \sim D^h_i \cup D^e_i} -\log\pi(a|s,z_i) + D_{\cos}(z^h_i, z^l_i) $$
            여기서 $D_{\cos}$는 코사인 거리를 나타내며, $z^h_i$는 비디오 인코더에서 나온 임베딩, $z^l_i$는 언어 인코더에서 나온 임베딩입니다. 이는 비디오 임베딩을 의미론적으로 정렬하는 데 중요합니다.
    * **정책 학습:** XYZ 및 축-각도 예측에는 Huber 손실($[49]$), 그리퍼 각도 예측에는 로그 손실을 사용합니다.
    * **개방 루프 보조 예측(Open-Loop Auxiliary Predictions):** 정책은 다음 10개의 액션 궤적을 예측하지만, 첫 번째 액션만 실행하여 폐쇄 루프(closed-loop) 제어를 수행합니다. 이는 보조 학습 목표를 제공합니다.
    * **액션으로서의 상태 차분(State Differences as Actions):** 액션을 $N>1$ 스텝 미래의 목표 포즈까지의 상태 차분으로 정의합니다. 암(arm)과 그리퍼의 움직임에 따라 $N$을 적응적으로 선택하여 "작은 액션" 문제를 완화합니다.
4. **네트워크 아키텍처:** ResNet-18 기반의 비전 특징 추출기를 사용하고, 태스크 임베딩 $z$는 FiLM 레이어($[47]$)를 통해 정책에 조건화됩니다.

## 📊 Results

* **단일 작업 성능 검증:**
  * `bin-emptying` 작업에서 인간 텔레오퍼레이터 속도의 절반 이상인 3.4 picks/분을 달성했습니다.
  * `door opening` 작업에서 훈련 도어 씬에서 87%, 홀드아웃(held-out) 도어 씬에서 94%의 성공률을 보였습니다.
* **제로샷 및 퓨샷(Few-Shot) 작업 일반화:**
  * 29개의 홀드아웃 태스크(새로운 객체 조합 포함)에 대해 평가했습니다.
  * **언어 조건부 BC-Z (제로샷):** 전체적으로 32%의 성공률을 보였으며, 24개의 태스크에서 0이 아닌 성공률(평균 44%)을 달성했습니다. 로봇은 종종 올바른 객체로 이동했지만, "마지막 센티미터(last-centimeter)" 오류(그리퍼를 닫거나 놓는 데 실패)가 주요 실패 원인이었습니다.
  * **비디오 조건부 BC-Z (퓨샷):** 일반화가 더 어려웠으나, 9가지 새로운 태스크에서 0이 아닌 성공률을 보였습니다(전체 4% 성공률). 특히 새로운 객체 조합이 없는 태스크에서 성능이 더 좋았습니다.
* **인코더 대 정책 병목 현상:**
  * 훈련 태스크에서 원-핫(one-hot) 조건화(42%)와 언어 조건화(40%)의 유사한 성능은 언어 잠재 공간이 충분하며, 일반화 병목 현상이 임베딩보다는 제어 레이어에 있음을 시사합니다.
  * 비디오 조건부 정책의 성능 저하(훈련 태스크에서 24%)는 비디오를 통한 태스크 추론이 더 어렵다는 것을 나타냅니다.
* **결과 제거 연구(Ablation Studies):**
  * **멀티태스크 학습의 중요성:** 단일 태스크 학습(5%)에 비해 멀티태스크 학습(52%)이 훨씬 우수한 성능을 보여, 여러 태스크 간의 데이터 풀링이 중요함을 확인했습니다.
  * **적응형 상태 차분(Adaptive State Diffs)의 중요성:** 이 기능이 없을 경우 성능이 크게 저하되었습니다(3%).
  * **HG-DAgger의 효과:** HG-DAgger를 사용한 데이터가 전문가 데모만 사용한 경우보다 작업 성능을 크게 향상시켰습니다(1-태스크에서 27% vs 53%, 8-태스크에서 23% vs 47%).
  * **개입률(Intervention Rate)과 정책 성공률의 음의 상관관계**를 확인하여, 개입률이 정책 성능의 좋은 지표로 활용될 수 있음을 보였습니다.

## 🧠 Insights & Discussion

* **단순한 모방 학습 접근 방식의 확장성:** 본 연구의 핵심 결론은 단순한 모방 학습 접근 방식도 대규모의 다양한 데이터셋과 유연한 태스크 조건화를 통해 새로운 태스크에 대한 제로샷 일반화를 달성할 수 있다는 것입니다. 복잡한 접근 방식이 반드시 필요한 것은 아닙니다.
* **데이터 스케일의 중요성:** 100개의 훈련 태스크가 새로운 태스크로의 일반화를 가능하게 하기에 충분하다는 것을 보여주었습니다.
* **HG-DAgger의 효과적인 역할:** 효율적인 데이터 수집과 정책 개선에 HG-DAgger 방식이 중요합니다.
* **사전 학습된 언어 임베딩의 우수성:** 추가적인 훈련 없이도 태스크 조건화에 매우 효과적이며, 의미론적 태스크 정보를 잘 포착함을 보여줍니다.
* **제한 사항 및 향후 연구:**
  * 새로운 태스크에서의 성능은 다양하지만, 로봇이 최소한 태스크의 일부를 이해하고 있음을 시사하는 동작을 자주 보입니다. 이는 자율 강화 학습(RL) 등을 통한 미세 조정(finetuning)의 초기화로 정책을 활용하는 흥미로운 방향을 제시합니다.
  * 현재 언어 명령어 구조가 단순합니다. 향후에는 다양한 인간 주석을 통해 언어 구조의 가변성을 더 잘 처리할 수 있도록 발전시킬 수 있습니다.
  * 비디오 조건부 정책의 낮은 성능은 비디오 기반 태스크 표현의 일반화 개선 및 모방 학습 알고리즘 전반의 성능 향상에 대한 연구가 필요함을 시사합니다.

## 📌 TL;DR

**문제:** 로봇이 시각 기반 조작 작업에서 새로운 작업을 처음 보거나(zero-shot) 적은 수의 데모로(few-shot) 일반화하는 것은 어려운 문제입니다.
**제안 방법:** BC-Z는 대규모 대화형 모방 학습 시스템으로, 자연어 임베딩 또는 인간 비디오를 통해 작업을 유연하게 조건화하며, 전문가 데모와 인간 개입(HG-DAgger 방식)을 통해 데이터를 수집합니다. 총 100가지의 다양한 작업에 대한 25,877개의 로봇 데모와 18,726개의 인간 비디오 데이터셋을 구축하고, 비디오 인코더에는 보조 언어 회귀 손실을 적용하여 의미론적 임베딩을 학습시킵니다.
**주요 발견:** BC-Z는 훈련 과정에서 보지 못한 24가지 새로운 조작 작업에서 평균 44%의 성공률을 달성하여 zero-shot 일반화 능력을 입증했습니다. 대규모 데이터, HG-DAgger 방식, 적응형 상태 차분(adaptive state diffs), 그리고 미리 학습된 언어 임베딩이 좋은 성능에 필수적임을 보여주었습니다.
