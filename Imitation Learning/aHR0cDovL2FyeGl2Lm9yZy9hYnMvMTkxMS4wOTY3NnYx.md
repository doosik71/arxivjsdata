# Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller

Pratyusha Sharma, Deepak Pathak, Abhinav Gupta

---

## 🧩 Problem to Solve

본 논문은 로봇이 단 한 번의 **3인칭 시점의 인간 시연 영상**만을 보고, **새로운 객체**와 **미지의 시나리오**에서 조작 작업을 모방하여 수행할 수 있도록 하는 일반화된 모방 학습 설정을 다룹니다. 핵심적인 문제는 다음과 같습니다:

1. **의도 이해 및 맥락 번역**: 3인칭 시연 영상의 의도를 로봇 자신의 환경 맥락에 맞춰 이해하고 번역해야 합니다.
2. **새로운 객체 조작**: 시연에 사용된 것과 다른 새로운 객체들을 조작할 수 있어야 합니다.
3. **원시 이미지 관찰**: 환경의 전체 상태 정보에 접근하지 않고, 오직 원시 이미지 관찰만을 통해 작동해야 합니다.
4. **일반화 및 효율성**: 복잡한 End-to-End 모델은 방대한 데이터가 필요하며 새로운 상황에 대한 일반화가 어렵습니다.

## ✨ Key Contributions

- **디커플링된 계층적 제어 구조 제안**: "무엇을 달성할 것인가"(의도/목표)와 "어떻게 수행할 것인가"(제어)를 분리하는 계층적 접근 방식을 제안하여, 학습 효율성과 일반화 능력을 향상시킵니다.
- **고수준 목표 생성기 (Goal Generator)**: 3인칭 인간 시연 영상과 로봇의 현재 관찰을 기반으로 1인칭 시점의 시각적 하위 목표를 생성하는 모듈을 개발했습니다.
- **저수준 역 제어기 (Inverse Controller)**: 생성된 시각적 하위 목표를 달성하기 위한 행동(로봇 관절 각도)을 예측하는 모듈입니다. 이 제어기는 작업에 독립적으로 설계되어 다양한 작업에 공유될 수 있습니다.
- **GAN 기반의 현실적인 하위 목표 생성**: 현실적인 시각적 하위 목표 생성을 위해 조건부 GAN과 U-Net 아키텍처를 활용했습니다.
- **실제 로봇 플랫폼 검증**: Baxter 로봇을 사용하여 붓기(pouring) 및 물체 놓기(placing) 작업에서 제안된 방식의 효과와 새로운 객체 및 위치, 심지어 새로운 작업에 대한 뛰어난 일반화 능력을 입증했습니다.
- **모듈화의 장점**: End-to-End 방식 대비 데이터 효율성 향상, 오버피팅 방지, 모델 해석 가능성 증대 등을 제공합니다.

## 📎 Related Works

- **Learning from Demonstrations (LfD) [2,3,16,20,24,31]**: 로봇 조작을 위해 키네스틱 시연, 텔레오퍼레이션 등으로 얻은 궤적을 학습합니다. 본 연구는 단일 3인칭 시연 영상에서 학습하는 더 일반적인 설정을 목표로 합니다.
- **Domain Adaptation [12,32,5,17,23,28,26]**: 인간과 로봇 시연 사이의 도메인 시프트(외모, 시점 등) 문제를 다루는 연구들입니다.
- **Explicitly Inferring Rewards (Inverse Reinforcement Learning) [21,25,16]**: 시연을 통해 보상 함수를 추론하고 강화 학습으로 로봇을 훈련시키지만, 상당한 양의 실제 데이터와 작업별 재훈련이 필요합니다.
- **Visual Foresight [6-8,29]**: 자체 감독(self-supervised) 로봇 조작을 위한 시각적 예측 기술이지만, 이미지 공간의 점 형태와 같은 수동 지정된 목표에 의존합니다.
- **End-to-End Visual Imitation Learning [27,30]**: 시연 영상으로부터 직접 행동을 예측하는 단일 네트워크 접근 방식입니다.

## 🛠️ Methodology

본 논문은 "무엇을 할 것인가"를 정의하는 고수준 모듈과 "어떻게 할 것인가"를 정의하는 저수준 모듈로 구성된 2단계 계층적 접근 방식을 제안합니다.

- **고수준 모듈: 목표 생성기 (Goal Generator, $\pi_H$)**

  - **역할**: 인간 시연 영상($h_t, h_{t+n}$)과 로봇의 현재 시각 상태($s_t$)를 입력받아, 로봇이 달성해야 할 다음 시간 단계($t+n$)의 시각적 하위 목표($\hat{s}_{t+n}$)를 로봇의 1인칭 시점으로 생성합니다.
  - **접근 방식**: 인간 시연 이미지의 *변화*를 로봇 관찰 이미지의 *변화*로 번역(재렌더링)하는 방식입니다. 이는 이미지를 처음부터 생성하는 것보다 훨씬 단순하고 효과적입니다.
  - **아키텍처**: U-Net 기반의 조건부 생성적 적대 신경망(Conditional GAN)을 사용하여 현실적인 이미지를 생성합니다.
  - **최적화**: 실제 로봇 관찰($s$)에 대한 판별자(D)의 로그 확률을 최대화하고, 목표 생성기가 생성한 이미지에 대한 판별자의 로그 확률을 최소화하며, 생성된 이미지와 실제 목표 이미지($s_{t+n}$) 사이의 L1 거리($\lambda \Vert \pi_H(h_t, h_{t+n}, s_t) - s_{t+n} \Vert_1$)를 최소화합니다.

- **저수준 모듈: 역 제어기 (Inverse Controller, $\pi_L$)**

  - **역할**: 로봇의 현재 시각 상태($s_t$)와 목표 생성기가 예측한 시각적 하위 목표($\hat{s}_{t+n}$)를 입력받아, 로봇이 목표 상태로 전환하기 위해 수행해야 할 행동($a_t$, 14차원 관절 각도)을 예측합니다.
  - **아키텍처**: ImageNet으로 사전 학습된 ResNet-18 모델과 세 개의 완전 연결 계층을 사용합니다.
  - **특징**: 특정 작업에 구애받지 않는(task-agnostic) 방식으로 설계되어 다양한 작업에 걸쳐 공유될 수 있습니다.

- **추론 단계**:

  - 로봇의 현재 관찰 $s_t$와 인간 시연 $I_H$가 주어지면, 목표 생성기 $\pi_H$가 하위 목표 $\hat{s}_{t+n}$을 생성합니다.
  - 이후 역 제어기 $\pi_L$가 $\hat{s}_{t+n}$에 도달하기 위한 일련의 로봇 관절 각도 행동을 출력합니다.
  - 이 과정은 인간 시연의 최종 이미지에 도달할 때까지 반복됩니다.

- **훈련 데이터**: MIME 데이터셋 [27]을 사용하며, 인간 시연 영상, 로봇 시연 영상, 로봇의 관절 각도 상태가 쌍으로 존재하지만 시간적으로는 정렬되지 않은 데이터를 활용합니다.

## 📊 Results

제안하는 계층적 디커플링 제어기는 실제 Baxter 로봇에서 붓기 및 물체 놓기 작업을 통해 평가되었습니다.

- **개별 모듈의 일반화 성능**:

  - **목표 생성기**: L1/L2 loss, CycleGAN과 같은 다른 접근 방식보다 더 선명하고 정확한 하위 목표를 생성하여 L1, L2, PSNR, SSIM 지표에서 우수함을 보였습니다. 새로운 객체 위치와 새로운 객체에 대한 강력한 일반화 능력을 입증했습니다 (Table 1, Figure 4a, 4b).
  - **역 제어기**: End-to-End [27], DAML [30] 및 특징 매칭 기반(trajectory-based, SIFT) 모델 대비 낮은 RMSE를 달성하여 새로운 객체 및 위치에 대한 뛰어난 일반화 능력을 보였습니다 (Table 2). 특히, 새로운 작업에 대한 일반화 능력도 우수하여, 학습된 작업(15개)과 새로운 작업(5개) 모두에서 기준선 대비 낮은 오차를 보였습니다 (Table 3).

- **계층적 모델의 통합 일반화 및 평가**:
  - **작업 성공률**: 붓기 및 물체 놓기 작업에서 End-to-End [27] 및 DAML [30] 대비 크게 향상된 성공률을 달성했습니다 (Table 4). 예를 들어, 붓기 작업에서 도달(Reaches) 75%, 붓기 수행(Pours) 60% 성공률을 보인 반면, 기준선 모델들은 20-25% / 8-15% 수준이었습니다.
  - **새로운 객체 및 위치 일반화**: 무작위 위치에 놓인 새로운 객체를 사용하여 테스트했을 때, 개별 모듈의 일반화 능력과 더불어 모듈들이 함께 작동할 때도 강력한 성능을 보여주었습니다.

## 🧠 Insights & Discussion

- **디커플링의 효과**: "무엇을 할 것인가" (고수준 목표)와 "어떻게 할 것인가" (저수준 제어)를 명확히 분리함으로써, 3인칭 시연 영상으로부터의 모방 학습에서 탁월한 일반화 능력과 데이터 효율성을 달성했습니다. 이는 End-to-End 방식의 한계를 극복하고 더 해석 가능한 모델을 가능하게 합니다.
- **저수준 제어기의 재사용성**: 저수준 역 제어기가 작업에 독립적으로 학습될 수 있다는 점은 다양한 작업에 걸쳐 재사용될 수 있음을 의미하며, 이는 모방 학습의 효율성을 크게 높입니다.
- **한계**: 현재 고수준 및 저수준 모듈이 각 시간 단계에서 독립적으로 작동하여 예측 궤적이 다소 불안정할 수 있습니다 (시간적 정보의 부족). 또한, 현재 목표 생성기는 작업에 독립적이지 않습니다.
- **향후 연구 방향**: 시간적 정보를 통합하여 궤적의 안정성을 높이는 방법(예: LSTM 활용), 작업에 구애받지 않는 목표 생성기 개발, 저수준 제어기의 견고성을 높이기 위한 추가적인 자체 감독(self-supervised) 데이터 활용 등을 제안합니다.

## 📌 TL;DR

본 논문은 단일 3인칭 시연 영상으로부터 로봇이 새로운 환경에서 새로운 객체를 조작하도록 학습하는 문제를 해결하기 위해 **디커플링된 계층적 제어기**를 제안합니다. 고수준 **목표 생성기**는 3인칭 인간 시연을 1인칭 로봇 시점의 시각적 하위 목표로 변환하고, 저수준 **역 제어기**는 이 목표를 달성하기 위한 로봇 행동을 예측합니다. 이 계층적 접근 방식은 End-to-End 모델 및 다른 기준선 대비 **새로운 객체, 위치, 심지어 새로운 작업에 대한 일반화 능력**을 크게 향상시키며, 실제 로봇 플랫폼에서 붓기 및 놓기 작업 성공률에서 우수한 성능을 입증했습니다.
