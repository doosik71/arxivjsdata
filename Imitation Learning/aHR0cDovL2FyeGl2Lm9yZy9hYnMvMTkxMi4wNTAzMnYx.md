# IMITATION LEARNING VIA OFF-POLICY DISTRIBUTION MATCHING

Ilya Kostrikov, Ofir Nachum, Jonathan Tompson

## 🧩 Problem to Solve

모방 학습(Imitation Learning)은 명시적인 보상 없이 전문가 시연(expert demonstrations)으로부터 정책(policy)을 학습하는 것을 목표로 합니다. 기존의 적대적 모방 학습(Adversarial Imitation Learning, AIL) 방법들은 분포 매칭(distribution matching)을 통해 이를 수행하지만, 다음과 같은 두 가지 주요 한계점을 가지고 있습니다:

* **온-정책(On-policy) 데이터 요구사항**: 분포 비율(distribution ratio)을 추정하기 위해 현재 학습 중인 정책으로부터 얻은 온-정책 샘플이 필요합니다. 이는 환경과의 상호작용 비용이 높은 실제 시나리오에서 데이터 비효율성을 야기하며, 실용성을 떨어뜨립니다. 일부 오프-정책(off-policy) 시도는 원래의 목적 함수를 변경하여, 재연 버퍼(replay buffer)의 분포와 전문가 분포 간의 발산을 측정하게 되어 원하는 전문가 분포와 정확히 일치한다는 보장이 없습니다.
* **별도의 RL 최적화**: 분포 비율을 추정하는 단계와, 이 비율을 보상으로 사용하여 표준 강화 학습(RL) 알고리즘으로 정책을 최적화하는 단계가 번갈아 가며 진행됩니다. 이는 알고리즘의 복잡성을 증가시키고, 추가적인 설계 선택 및 함수 근사기(예: 가치 함수) 학습을 필요로 합니다.

## ✨ Key Contributions

본 논문은 이러한 한계점을 극복하기 위해 ValueDICE 알고리즘을 제안하며 다음과 같은 주요 기여를 합니다:

* **원칙적인 오프-정책 목적 함수**: 원래의 분포 비율 추정 목적 함수를 완전히 오프-정책 방식으로 변환하여 데이터 비효율성 문제를 해결합니다.
* **명시적인 보상 없는 직접적인 정책 학습**: 유도된 목적 함수의 특정 형태를 활용하여 별도의 RL 최적화 없이 정책을 직접 학습할 수 있음을 보여줍니다. 이로써 명시적인 보상 없이도 모방 정책을 배울 수 있습니다.
* **최고 수준의 샘플 효율성 및 성능**: 인기 있는 모방 학습 벤치마크(MuJoCo)에서 최첨단 샘플 효율성과 성능을 달성합니다.
* **견고성**: 희소(sparse)한 전문가 데이터 상황 및 확률적(stochastic) 전문가 환경에서도 효과적인 학습을 보여줍니다.

## 📎 Related Works

* **행동 복제(Behavioral Cloning, BC)**: 전문가 시연을 지도 학습(supervised learning)으로 모방하는 간단한 방법입니다. 샘플 효율성은 좋지만, 전문가 데이터에서 벗어난 상태에 대한 복구 능력이 없어 분포 드리프트(distributional drift)에 취약합니다.
* **적대적 모방 학습(Adversarial Imitation Learning, AIL) / GAIL (Ho & Ermon, 2016)**: 정책의 상태-행동 분포와 전문가의 분포 간의 KL-발산(KL-divergence)을 최소화하는 것을 목표로 합니다. GAN(Generative Adversarial Nets)과 유사한 방식으로 판별자(discriminator)를 훈련하여 분포 비율을 추정하고, 이를 보상으로 RL을 수행합니다. 그러나 판별자 훈련을 위해 온-정책 샘플이 필요합니다.
* **Discriminator-Actor-Critic (DAC) (Kostrikov et al., 2019)**: GAIL의 온-정책 요구사항을 완화하기 위해 재연 버퍼 샘플을 판별자 훈련에 사용합니다. 하지만 이 방법은 목적 함수를 변경하여 전문가 분포와 *재연 버퍼 분포* 간의 발산을 측정하게 되므로, 정책이 전문가 분포와 정확히 일치한다는 보장이 없습니다.
* **DualDICE (Nachum et al., 2019b)**: 오프-정책 상태 분포 보정 추정(off-policy stationary distribution correction estimation)을 위한 기술로, 본 논문의 오프-정책 KL-발산 공식화에 영감을 주었습니다. 하지만 DualDICE는 고정된 정책의 평가에 사용되었으며, 직접적인 정책 학습에는 적용되지 않았습니다. 또한 $f$-발산(f-divergence) 형태를 사용한 반면, ValueDICE는 Donsker-Varadhan 형태를 활용합니다.
* **REPS (Peters et al., 2010)**: 로그-평균-지수(log-average-exp) 항을 사용하는 목적 함수를 가집니다. 그러나 REPS는 이중 수준 최적화(bi-level optimization)를 통해 정책을 학습하며, ValueDICE와는 달리 가치와 정책을 동일한 목적 함수로 직접 훈련하지 않습니다.

## 🛠️ Methodology

ValueDICE는 KL-발산 목적 함수를 원칙적인 오프-정책 방식으로 변환하고, 이를 통해 별도의 RL 알고리즘 없이 정책을 직접 학습합니다.

1. **Donsker-Varadhan 표현 활용**: 정책의 상태-행동 점유 분포 $d_{\pi}$와 전문가 분포 $d_{exp}$ 간의 KL-발산 $-D_{KL}(d_{\pi}||d_{exp})$을 Donsker-Varadhan 이중 표현으로 시작합니다:
    $$ \min_{x:S \times A \to \mathbb{R}} \log E_{(s,a) \sim d_{exp}}[e^{x(s,a)}] - E_{(s,a) \sim d_{\pi}}[x(s,a)] $$
    여기서 최적의 $x^*(s,a)$는 $\log \frac{d_{\pi}(s,a)}{d_{exp}(s,a)} + C$입니다. 기존 형태는 여전히 $d_{\pi}$로부터의 온-정책 샘플을 요구합니다.

2. **오프-정책 변수 변환**: DualDICE에서 영감을 받아 $x(s,a) = \nu(s,a) - B_{\pi}\nu(s,a)$로 변수를 변경합니다. 여기서 $B_{\pi}\nu(s,a) = \gamma E_{s' \sim p(\cdot|s,a), a' \sim \pi(\cdot|s')}[\nu(s',a')]$는 보상이 0인 Bellman 연산자입니다.

3. **ValueDICE 목적 함수**: 이 변수 변환을 통해 두 번째 기댓값 항이 연쇄적으로 소멸되어 초기 상태에 대한 기댓값으로 줄어들고, 최종적으로 완전히 오프-정책 형태의 ValueDICE 목적 함수 $J_{DICE}(\nu)$를 얻습니다:
    $$ J_{DICE}(\pi, \nu) := \log E_{(s,a) \sim d_{exp}}[e^{\nu(s,a) - B_{\pi}\nu(s,a)}] - (1-\gamma)E_{s_0 \sim p_0(\cdot), a_0 \sim \pi(\cdot|s_0)}[\nu(s_0,a_0)] $$
    이 목적 함수는 전문가 시연 $d_{exp}$와 초기 상태 분포 $p_0(\cdot)$로부터의 샘플만 요구하며, $d_{\pi}$로부터의 온-정책 샘플은 필요 없습니다.

4. **명시적 보상 없는 직접 정책 최적화**: ValueDICE의 중요한 특징은 이 목적 함수 $J_{DICE}(\pi, \nu)$의 그래디언트를 정책 $\pi$에 대해 직접 계산하여 $\pi$를 최적화할 수 있다는 것입니다. 이는 별도의 RL 알고리즘이나 명시적인 보상을 정의할 필요 없이, $\nu$ 함수가 분포 비율을 기반으로 한 암묵적인 Q-함수 역할을 수행하게 함으로써 학습 과정을 간소화합니다.

5. **실용적인 고려사항**:
    * **경험적 기댓값**: 실제 환경에서는 전문가 데이터와 초기 상태 분포에 대한 유한한 샘플만을 가지므로, 목적 함수의 기댓값들은 미니 배치(mini-batch)를 사용하여 근사합니다. $B_{\pi}\nu$ 항은 단일 샘플 $s' \sim p(\cdot|s,a)$를 기반으로 편향된 추정치를 사용하지만, 성능에 충분함을 확인했습니다.
    * **재연 버퍼 정규화**: 제한된 전문가 샘플의 다양성을 높이기 위해 재연 버퍼의 경험을 활용하는 정규화 항을 도입합니다 ($J_{mix}^{DICE}$). 이는 $d_{mix}(s,a) = (1-\alpha)d_{exp}(s,a) + \alpha d_{RB}(s,a)$와 같이 전문가 분포와 재연 버퍼 분포를 혼합하여 사용하며, 작은 $\alpha$ 값($\alpha=0.1$)을 사용함으로써 정책의 전역 최적성(global optimality)이 $\pi_{exp}$와 일치하도록 보장합니다.
    * **초기 상태 샘플링**: 모든 궤적(trajectory)의 각 상태를 '가상 초기 상태'로 취급하여 $p_0(\cdot)$의 다양성을 높입니다. 이는 마르코프 환경에서 정책의 최적성에 영향을 미치지 않으면서 샘플 효율성을 향상시킵니다.

## 📊 Results

ValueDICE는 다양한 환경에서 기존 모방 학습 알고리즘 대비 뛰어난 성능을 보였습니다.

* **Ring MDP (합성 환경)**:
  * **희소한 전문가 데이터**: 전문가 데이터가 특정 상태-행동을 충분히 커버하지 못하는 상황에서도 ValueDICE는 관찰된 전문가 상태-행동 점유율에 가장 잘 일치하는 정책을 성공적으로 학습했습니다. 이는 BC와 같은 방법이 실패하는 지점입니다.
  * **확률적 전문가**: 학습 과정에서 $d_{\pi}$와 $d_{exp}$ 간의 실제 KL-발산이 꾸준히 감소함을 보여, 목적 함수의 유효성을 검증했습니다.
* **MuJoCo 벤치마크**:
  * **초저데이터 환경 (전문가 궤적 1개)**: ValueDICE는 Walker2d를 제외한 모든 태스크에서 DAC와 유사하거나 더 나은 성능을 보였으며, GAIL과 BC를 크게 능가했습니다.
  * **고데이터 환경 (전문가 궤적 10개)**: ValueDICE는 모든 태스크에서 강력한 성능과 더 빠른 수렴을 보이며 다른 방법을 능가했습니다. 이 경우 BC도 전문가 수준의 성능을 달성할 수 있지만, ValueDICE의 성능이 여전히 더 우수했습니다.
  * GAIL은 모든 시나리오에서 샘플 효율성이 매우 낮아 의미 있는 진전을 이루지 못했습니다.
* **오프라인 환경**: 학습 환경으로부터 추가적인 상호작용 없이 오프라인 데이터만 주어졌을 때도 ValueDICE는 BC보다 뛰어난 성능을 보였습니다.

## 🧠 Insights & Discussion

* **원칙적이고 견고한 프레임워크**: ValueDICE는 오프-정책 발산 최소화를 위한 강력하고 원칙적인 이론적 기반을 제공합니다. 이는 기존 AIL 방법들의 핵심 한계점인 온-정책 의존성과 목적 함수 변형 문제를 해결합니다.
* **학습 과정의 간소화**: 명시적인 보상 함수를 정의하거나 별도의 강화 학습 알고리즘을 사용할 필요가 없으므로, 모방 학습 알고리즘의 구현 복잡성을 크게 줄였습니다. ValueDICE는 $\nu$ 함수를 분포 매칭 목적 함수로부터 직접 학습하며, 이 $\nu$ 함수는 암묵적으로 'Q-함수' 역할을 수행합니다. 정책은 $\nu^*(s,a)$를 최소화하도록 학습되는데, 이는 곧 미래의 할인된 로그 비율 $\log \frac{d_{exp}(s,a)}{d_{\pi}(s,a)}$을 최대화하는 것과 동일합니다.
* **향상된 샘플 효율성 및 성능**: 오프-정책 데이터 활용 능력과 간소화된 학습 메커니즘 덕분에, 특히 데이터가 부족한 환경에서 SOTA 성능을 달성하여 실제 적용 가능성을 높였습니다.
* **새로운 접근 방식**: 이 논문은 적대적 모방 학습에서 명시적으로 보상을 학습하거나 정의하지 않고, 분포 비율 목적 함수 내에서 Q-함수 역할을 하는 $\nu$를 직접 학습하는 첫 번째 알고리즘입니다.
* **한계점**: 재연 버퍼 정규화 시 $\alpha$ 값이 너무 크면 재연 버퍼의 분포가 전문가 분포와 다를 때 정책이 전문가 행동에서 벗어날 수 있습니다. 또한, 기댓값의 경험적 근사는 일부 편향을 유발할 수 있으나, 실험적으로는 충분한 성능을 보였습니다.

## 📌 TL;DR

**문제:** 기존 적대적 모방 학습(AIL)은 온-정책 데이터 요구사항으로 인해 샘플 비효율적이며, 별도의 RL 최적화로 인해 복잡합니다.
**방법:** ValueDICE는 Donsker-Varadhan 표현과 Bellman 연산자 기반의 변수 변환을 통해 KL-발산 목적 함수를 완전히 오프-정책 방식으로 재구성합니다. 이 덕분에 명시적인 보상이나 별도의 RL 알고리즘 없이도 모방 정책을 직접 최적화할 수 있으며, 학습된 $\nu$ 함수가 암묵적으로 Q-함수 역할을 합니다. 재연 버퍼 정규화 및 포괄적인 초기 상태 샘플링 기법을 통해 견고성을 높였습니다.
**결과:** ValueDICE는 합성 및 MuJoCo 벤치마크에서 기존 방법들을 능가하는 최첨단 샘플 효율성과 성능을 달성하며, 특히 데이터가 부족한 환경에서 오프-정책 방식으로 전문가 분포를 강력하게 모방하는 능력을 입증했습니다.
