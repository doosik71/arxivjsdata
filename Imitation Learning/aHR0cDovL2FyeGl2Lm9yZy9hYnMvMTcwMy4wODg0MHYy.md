# InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations

Yunzhu Li, Jiaming Song, Stefano Ermon

## 🧩 Problem to Solve

강화 학습(RL)은 복잡한 작업(예: 자율 주행)에 대해 보상 함수를 정의하기 어렵다는 한계를 가집니다. 반면 모방 학습(IL)은 전문가 시연을 통해 직접 작업을 학습할 수 있지만, 전문가 시연은 종종 잠재 요인(예: 여러 전문가의 다른 전략, 개인의 운전 습관)으로 인해 상당한 가변성을 보입니다. 기존 GAIL(Generative Adversarial Imitation Learning)은 모든 시연이 단일 전문가로부터 나온다고 가정하여 이러한 가변성을 효과적으로 분리하고 해석 가능한 방식으로 모델링하지 못합니다. 본 논문의 목표는 시각적 시연(raw pixel)으로부터 전문가 행동의 잠재 구조를 비지도 방식으로 자동으로 발견하고 분리하며, 이를 통해 복잡한 행동을 모방하고 의미 있는 표현을 학습하는 새로운 모방 학습 프레임워크를 개발하는 것입니다.

## ✨ Key Contributions

- GAIL에 잠재 공간과 궤적 간의 상호 정보량($I(c; \tau)$)을 최대화하는 구성 요소(InfoGAN [14]과 유사)를 추가하여 **InfoGAIL**을 제안했습니다. 이를 통해 저수준 행동을 추상적인 고수준 잠재 변수를 통해 제어할 수 있는 정책을 학습합니다.
- InfoGAIL을 확장하여 **원시 픽셀(raw pixels)을 유일한 입력으로 사용**하여 복잡한 고차원 동적 환경에서 사람과 유사한 행동을 생성할 수 있도록 만들었습니다.
- TORCS 시뮬레이터를 사용한 자율 주행 애플리케이션을 통해, 학습된 정책이 충돌 없이 트랙을 정확하게 주행하고, 잠재 변수 공간을 탐색하여 다양한 종류의 **인간과 유사한 운전 행동을 재현**할 수 있음을 입증했습니다.

## 📎 Related Works

- **강화 학습(RL)** [1-6]: 복잡한 환경에서 보상 함수 설계의 어려움이 있습니다.
- **모방 학습(IL)** [7-11]: 전문가 시연으로부터 직접 정책을 학습합니다.
  - **행동 복제(Behavior Cloning, BC)** [16]: 지도 학습 방식이며, 오차 누적 및 공변량 변화 문제로 일반화 성능이 좋지 않습니다 [17,18].
  - **도제 학습(Apprenticeship Learning, AL)**: 알 수 없는 보상 함수 하에서 전문가 정책이 최적이라고 가정하고, 보상 함수를 복구하여 정책을 학습합니다 [19-21].
- **GAN(Generative Adversarial Network) 기반 방법**:
  - **GAIL (Generative Adversarial Imitation Learning)** [12]: GAN [13]에서 영감을 받은 모델 프리(model-free) AL 방법으로, 생성된 상태-행동 분포를 전문가 분포와 일치시킵니다.
  - **InfoGAN** [14]: 잠재 코드와 생성된 데이터 간의 상호 정보량을 최대화하여 해석 가능한 표현을 학습하는 방법입니다. 본 논문은 이 아이디어를 GAIL에 적용합니다.
- **시각 기반 자율 주행 시스템**:
  - **매개된 인지(Mediated perception)** [34-36]: 장면 정보 획득 후 운전 결정. 차선 표시, 차량 감지 등 수작업 기능이 필요합니다 [38,33].
  - **행동 반사(Behavior reflex)** [37,16]: 시각 입력에서 운전 행동으로 직접 매핑하는 엔드-투-엔드(end-to-end) 접근 방식입니다.
  - [39,40]: BC 또는 DAgger [18]를 통한 엔드-투-엔드 시각 모방 학습 연구가 있었으나, 한계가 있었습니다.
  - [21,41]: 많은 연구가 LIDAR와 같은 정밀 거리 측정 입력에 의존하는 반면, 본 연구는 **원시 시각 정보만**을 사용합니다.
- **전이 학습(Transfer Learning)** [29,30]: 사전 훈련된 CNN을 활용하여 시각 특징을 추출합니다.
  - [42,9]: 사전 훈련된 심층 신경망을 사용하여 IL 성능을 향상시키는 연구가 있었으며, 주로 보상 함수 학습에 초점을 맞추었습니다. 본 연구는 정책 자체의 시각 특징 추출에 전이 학습을 적용합니다.

## 🛠️ Methodology

InfoGAIL은 GAIL에 잠재 변수 $c$와 상호 정보량(mutual information) 최대화 항을 도입하여 전문가 행동의 잠재 구조를 비지도 방식으로 학습합니다.

1. **잠재 변수 및 상호 정보량 최대화**:

   - 정책을 $\pi(a|s,c)$로 확장하여 상태 $s$와 잠재 변수 $c$에 조건화합니다.
   - GAIL 목적 함수에 잠재 변수 $c$와 생성된 궤적 $\tau$ 간의 상호 정보량 $I(c; \tau)$를 최대화하는 항을 추가합니다. 이는 직접 최대화하기 어렵기 때문에 변분 하한 $L_I(\pi, Q)$를 사용하며, 실제 사후 분포 $P(c|\tau)$를 근사하는 $Q(c|s,a)$를 도입합니다.
   - InfoGAIL의 목적 함수는 다음과 같습니다:
     $$ \min*{\pi,Q} \max*{D} E*{\pi}[\log D(s,a)] + E*{\pi_E}[\log(1-D(s,a))] - \lambda_1 L_I(\pi,Q) - \lambda_2 H(\pi) $$
        여기서 $\lambda_1$은 상호 정보량 정규화 항의 하이퍼파라미터이고, $\lambda_2$는 인과 엔트로피 항 $H(\pi)$의 하이퍼파라미터입니다.

2. **보상 증강(Reward Augmentation)**:

   - 정책에 대한 선험적 지식이나 제약 조건을 반영하기 위해 보상 함수 $\eta(\pi_\theta)$를 추가하여 모방 학습에 추가적인 동기를 부여합니다.
   - 확장된 목적 함수는 다음과 같습니다:
     $$ \min*{\theta,\psi} \max*{\omega} E*{\pi*\theta}[\log D_\omega(s,a)] + E*{\pi_E}[\log(1-D*\omega(s,a))] - \lambda*0 \eta(\pi*\theta) - \lambda*1 L_I(\pi*\theta,Q*\psi) - \lambda_2 H(\pi*\theta) $$
        $\lambda_0$은 보상 증강 항의 하이퍼파라미터입니다.

3. **최적화 개선**:

   - **Wasserstein GAN (WGAN)** [26] 사용: 기존 GAN 목적 함수의 기울기 소실 및 모드 붕괴 문제를 완화하여 복잡한 다중 모드 궤적 분포 모델링에 유리합니다.
     $$ \min*{\theta,\psi} \max*{\omega} E*{\pi*\theta}[D_\omega(s,a)] - E*{\pi_E}[D*\omega(s,a)] - \lambda*0 \eta(\pi*\theta) - \lambda*1 L_I(\pi*\theta,Q*\psi) - \lambda_2 H(\pi*\theta) $$
   - **분산 감소 기법**: TRPO [2]와 함께 베이스라인 [27] 및 리플레이 버퍼(replay buffers) [28]를 사용하여 훈련 안정성을 높입니다.
   - **행동 복제(Behavior Cloning)를 통한 정책 초기화**: 훈련 속도를 높이기 위해 정책을 BC로 초기화합니다.
   - **별도의 네트워크**: 판별자 $D_\omega$와 사후 추정 네트워크 $Q_\psi$는 서로 다른 네트워크 파라미터를 사용합니다. 이는 WGAN 훈련이 $D_\omega$에 대해 가중치 클리핑(weight clipping)을 요구하는데, 이는 $Q_\psi$ 훈련에 방해될 수 있기 때문입니다.

4. **원시 시각 입력 처리**:
   - **전이 학습**: ImageNet [32]으로 사전 훈련된 ResNet [31]에서 추출한 특징을 정책 네트워크의 시각 입력으로 사용합니다. 이를 통해 고차원 시각 정보로부터 의미 있는 특징을 효과적으로 학습합니다.
   - **네트워크 구조**: 정책 네트워크는 시각 특징, 보조 정보(속도, 이전 행동, 차량 손상), 잠재 코드 $c$를 입력으로 받습니다. 판별자와 사후 추정 네트워크는 유사한 CNN 기반 구조를 가집니다.

## 📊 Results

- **합성 2D 환경**:
  - InfoGAIL은 3가지 뚜렷한 원형 궤적 모드를 성공적으로 구별하고 모방했습니다 (Figure 1).
  - BC는 오차 누적으로 인해 전문가 궤적에서 벗어났고, GAIL은 원형 궤적을 생성했지만 모드를 분리하지 못했습니다.
- **TORCS 자율 주행 (시각 입력)**:
  - **'Turning' 및 'Passing' 시나리오**: InfoGAIL은 전문가 시연에서 차선 선택(내부/외부 차선) 및 추월 방향(좌/우)과 같은 두 가지 뚜렷한 행동 모드를 비지도 방식으로 성공적으로 포착했습니다 (Figure 2, 3).
  - 모델은 잠재 코드를 통해 학습된 행동을 재현할 수 있었으며, 잠재 코드의 중간값을 통해 새로운 행동을 생성하는 일반화 능력도 보였습니다.
  - $Q_\psi$를 통한 잠재 코드 추론 정확도는 'Passing' 시나리오에서 81.9%에 달하여, 모델이 의미 있는 잠재 요인을 정확하게 식별했음을 보여주었습니다 (Table 1).
  - GAIL은 모드 분리에 실패하고 'Passing' 시나리오에서 하나의 행동 모드를 모방하면서 예상치 못한 '스윙' 동작을 보였습니다.
- **Ablation 연구**:
  - 제안된 최적화 기법들(WGAN, 보상 증강, 리플레이 버퍼)이 효과적인 정책 학습에 중요함을 입증했습니다 (Table 2).
  - InfoGAIL(본 연구)은 평균 주행 거리 1226.68로 인간 전문가(1203.51)를 능가했으며, 이는 보상 증강의 효과를 나타냅니다.
  - GAIL (914.45) 및 BC (701.83)는 InfoGAIL보다 현저히 낮은 성능을 보였습니다.

## 🧠 Insights & Discussion

- InfoGAIL은 명시적인 감독 없이도 시연 데이터 내에서 중요한 잠재 요인을 발견하고 분리할 수 있음을 보여주었습니다. 이는 인간 행동의 복잡한 다양성을 이해하고 재현하는 데 중요한 통찰력을 제공합니다.
- 원시 시각 입력만을 사용하여 복잡한 환경에서 정책을 학습하는 능력을 입증함으로써, 엔드-투-엔드 자율 주행 시스템의 잠재력을 강조했습니다. 사전 훈련된 CNN을 통한 전이 학습이 이 과정에서 핵심적인 역할을 합니다.
- WGAN 목적 함수, 보상 증강, 리플레이 버퍼와 같은 최적화 개선 기법들이 GAIL 기반의 모방 학습 프레임워크가 고차원 시각 입력에서 성공적으로 작동하는 데 필수적임을 확인했습니다. 특히 WGAN은 다중 모드 분포를 모델링하고 모드 붕괴를 방지하는 데 효과적이었습니다.
- 보상 증강은 정책이 전문가 행동을 모방하는 것을 넘어, 충돌 회피와 같은 추가적인 목표를 달성하도록 유도하여 심지어 전문가보다 더 나은 성능을 달성할 수 있음을 보여주었습니다. 이는 모방 학습과 강화 학습의 하이브리드 접근 방식의 가능성을 시사합니다.
- 제한 사항으로는, 보상 증강이 부적절하게 설정될 경우(예: 충돌 회피에 과도한 보상) 에이전트가 지나치게 소극적인 행동(예: 급격한 감속)을 보이는 등 바람직하지 않은 행동을 유발할 수 있다는 점이 지적되었습니다.

## 📌 TL;DR

**문제:** 기존 모방 학습(GAIL 포함)은 복잡하고 가변적인 전문가 시연, 특히 원시 시각 입력에서 의미 있는 잠재 행동 모드를 식별하고 학습하는 데 어려움을 겪습니다.

**제안 방법:** InfoGAIL은 GAIL에 잠재 변수 $c$와 상호 정보량 최대화 항을 통합하여 비지도 방식으로 행동 모드를 발견합니다. 원시 시각 입력을 위해 사전 훈련된 CNN의 전이 학습을 활용하며, WGAN, 보상 증강, 리플레이 버퍼를 통해 학습 안정성과 성능을 향상시켰습니다.

**주요 결과:** InfoGAIL은 TORCS 시뮬레이터에서 운전 시연(예: 내부/외부 차선 주행, 좌/우 추월)에서 뚜렷한 행동 모드를 성공적으로 분리하고 모방했습니다. 잠재 코드를 통해 해석 가능한 행동을 제어할 수 있었으며, 보상 증강을 통해 인간 전문가보다 더 나은 성능을 달성하는 엔드-투-엔드 해석 가능한 모방 학습의 가능성을 입증했습니다.
