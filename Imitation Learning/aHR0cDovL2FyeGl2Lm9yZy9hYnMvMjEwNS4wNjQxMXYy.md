# Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration

Edward Johns

## 🧩 Problem to Solve

기존 모방 학습(Imitation Learning) 방법들은 로봇이 새로운 작업을 학습하기 위해 다음과 같은 문제에 직면합니다:

1. **대량의 시연 필요**: 행동 복제(Behavioral Cloning)와 같은 방법은 일반화를 위해 많은 인간 시연을 요구합니다.
2. **환경 재설정 및 사전 지식**: 강화 학습(Reinforcement Learning)과 같이 시연을 보완하는 방법은 환경 재설정(수동 개입 또는 특정 장치 필요)을 필요로 하며, 메타 모방 학습(Meta-Imitation Learning)은 유사 작업에 대한 사전 학습이 필요하고, 동적 동작 원시(Dynamic Movement Primitives)는 작업별 상태 공간에 대한 사전 지식이 필요합니다.
   이 논문은 단일 인간 시연만으로, 상호작용할 객체에 대한 사전 지식 없이도 로봇 조작 작업을 학습할 수 있는 방법을 제시하여 위 두 가지 문제(필요한 인간 상호작용 및 알고리즘의 사전 작업 지식)를 최소화하는 것을 목표로 합니다.

## ✨ Key Contributions

- **단일 시연 학습**: 객체에 대한 사전 지식 없이 단 한 번의 인간 시연만으로 다양한 로봇 조작 작업을 학습하는 방법을 제안합니다.
- **Coarse-to-Fine 궤적 모델링**: 로봇의 동작을 거친 접근(coarse approach) 궤적과 미세한 상호작용(fine interaction) 궤적으로 모델링합니다.
- **상태 추정 문제로서의 모방 학습**: 모방 학습을 "병목(bottleneck)" 지점에서의 엔드 이펙터 포즈(end-effector pose)를 추정하는 상태 추정 문제로 재정의합니다.
- **자체 지도 학습(Self-Supervised Learning) 기반 포즈 예측**: 엔드 이펙터 카메라를 객체 주변으로 자동 이동시켜 자체 지도 방식으로 병목 포즈 예측기($f(I)$ 및 $g(I)$)를 훈련합니다. 이를 통해 접근 궤적에 대한 일반화를 달성합니다.
- **간단한 상호작용 궤적 획득**: 병목 포즈에 도달한 후에는 원래 시연의 엔드 이펙터 속도를 간단히 재생(replay)하여 복잡한 상호작용 궤적을 명시적으로 정책 학습 없이 편리하게 획득합니다.
- **안정적이고 해석 가능한 제어기**: 머신러닝을 포즈 추정에만 사용하고 제어기 자체는 분석적(analytical)이므로, 안정적이고 해석 가능한 제어 방식을 제공합니다.
- **순차적 상태 추정(Sequential State Estimation) 도입**: 접근 궤적 중에 여러 이미지로부터의 예측을 융합하여 병목 포즈 추정의 정확도를 향상시킵니다.
- **"마지막 단계(Last-inch)" 보정**: 병목 지점에 근접했을 때 포즈 추정의 정확도를 높이기 위한 두 번째 네트워크($g(I)$)를 사용합니다.

## 📎 Related Works

- **행동 복제(Behavioral Cloning)**: 관측값을 행동으로 매핑하는 지도 학습 기반 방법으로, 많은 수의 시연을 요구하며 주로 엔드-투-엔드(end-to-end) 학습을 사용합니다. 본 연구는 단일 시연으로 일반화를 목표로 합니다.
  - [1] T. Zhang et al. (VR teleoperation)
  - [8] P. Florence et al. (Self-supervised correspondence)
  - [12] S. Paradis et al. (Planning with behavioral cloning)
- **탐색 기반 방법(Exploration-based Methods)**: 시연으로부터 정책 학습을 부트스트랩하기 위해 환경의 자체 탐색을 사용합니다. 환경 재설정, 사람의 개입 또는 작업별 장치가 필요하다는 단점이 있습니다.
  - [2] M. Vecerik et al. (Leveraging demonstrations for DRL)
  - [19] G. Schoettler et al. (Deep RL for insertion tasks)
  - [20] C. Finn et al. (Inverse reinforcement learning)
- **전이 학습 방법(Transfer Learning Methods)**: 유사한 작업에 대한 사전 지식을 활용하여 적은 수의 시연으로 새 작업을 학습합니다. 작업군에 대한 사전 학습이 필요합니다.
  - [3] C. Finn et al. (One-shot visual imitation learning via meta-learning)
  - [28] Y. Duan et al. (One-shot imitation learning)
- **비주얼 서보잉(Visual Servoing)**: 로봇의 현재 이미지 관측을 목표 이미지에 정렬하려 시도합니다. 작업별 제어기 또는 엄격한 초기 정렬을 요구하는 경향이 있습니다.
  - [32] C. Yu et al. (Siamese CNN for pose estimation and visual servoing)
  - [34] M. Argus et al. (Optical flow based visual servoing)

## 🛠️ Methodology

본 방법은 로봇의 동작을 "coarse-to-fine" 궤적으로 모델링하고, 모방 학습을 상태 추정 문제로 접근합니다.

1. **좌표계 정의**:

   - 로봇 베이스($R$), 엔드 이펙터($E$), 객체의 병목($B$) 세 가지 가상 좌표계를 정의합니다. 병목($B$)은 엔드 이펙터가 객체와의 상호작용을 시작해야 하는 지점의 포즈를 나타냅니다.

2. **Coarse-to-Fine 궤적**:

   - **Coarse Approach Trajectory (거친 접근 궤적)**: 객체 병목 지점까지의 선형 경로를 의미합니다. 이 부분은 인간 시연 없이 로봇 카메라를 객체 주변으로 자동 이동시켜 자체 지도 학습을 위한 데이터를 수집합니다.
   - **Fine Interaction Trajectory (미세 상호작용 궤적)**: 병목 지점에서부터 객체와 물리적으로 상호작용하는 궤적을 의미합니다. 이 부분은 단일 인간 시연의 엔드 이펙터 속도(local end-effector velocities)를 그대로 재생합니다.

3. **병목 포즈 추정 (State Estimation)**:

   - **$f(I)$ 네트워크 훈련**: 이미지 $I$로부터 엔드 이펙터와 병목 간의 상대 포즈 $T_{EB}$를 예측하는 컨볼루션 신경망 $f(I)$를 훈련합니다.
     - **데이터 수집**: 단일 시연 후, 로봇은 엔드 이펙터와 카메라를 병목 지점 주변(파란색 영역, Fig. 2)에서 자동 이동하며 이미지 $I$와 해당 $T_{EB}$ 쌍을 데이터셋 $D$로 수집합니다. $T_{EB} = T_{ER} T_{RB}$를 사용하여 계산됩니다.
     - **예측 공간 단순화**: 일반적인 탁상 환경을 가정하여, 엔드 이펙터 포즈를 수직으로 아래를 향하도록 고정하고 수직축을 중심으로만 회전하도록 제한하여 $f(I)$의 예측 공간을 수평 이동 2개와 수직 회전 1개, 총 3개 자유도(3-DOF)로 줄입니다. 시연 시작 시 엔드 이펙터의 원래 6-DOF 포즈와의 회전 차이 $R$은 저장되어 나중에 적용됩니다.

4. **순차적 상태 추정 (Sequential State Estimation)**:

   - 접근 궤적 중 객체는 정지해 있으므로, 여러 이미지로부터의 병목 포즈 예측을 융합하여 정확도를 높입니다.
   - **불확실성 추정**: 예측 $\hat{x}_t$에 대한 가우시안 불확실성 $\hat{\sigma}_t$를 추정하기 위해 세 가지 방법(Dropout, Predicted, Prior)을 탐색합니다. 실험 결과, 검증 오차를 모든 예측에 대한 상수로 사용하는 Prior 방법이 가장 좋은 성능을 보였습니다.
   - **Batch 방법**: 현재까지의 모든 개별 예측 $\hat{x}_0 \dots \hat{x}_t$을 역분산 가중치(inverse-variance weighting)를 사용하여 결합합니다.
     $$ \bar{x}_t = \frac{\sum_{\tau=0}^{t} \hat{x}_{\tau} / \hat{\sigma}^2_{\tau}}{\sum*{\tau=0}^{t} 1 / \hat{\sigma}^2*{\tau}} $$
   - **Filtering 방법**: 이전 시간 단계의 추정 $\bar{x}_{t-1}$과 현재 시간 단계의 예측 $\hat{x}_t$을 결합합니다. 이는 프로세스 노이즈가 없는 칼만 필터(Kalman filter)의 일종입니다.
     $$ \bar{x}_t = \frac{(\bar{x}_{t-1} / \bar{\sigma}^2*{t-1}) + (\hat{x}\_t / \hat{\sigma}^2_t)}{(1 / \bar{\sigma}^2*{t-1}) + (1 / \hat{\sigma}^2*t)} $$
        $$ \bar{\sigma}^2_t = \frac{1}{(1 / \bar{\sigma}^2*{t-1}) + (1 / \hat{\sigma}^2_t)} $$

5. **"마지막 단계(Last-inch)" 보정**:

   - **$g(I)$ 네트워크 훈련**: 병목 지점에 매우 근접했을 때 예측의 정확도를 높이기 위해 두 번째 네트워크 $g(I)$를 훈련합니다. 데이터셋 $E$는 엔드 이펙터의 높이를 병목 높이에 고정하고 수평 이동 및 수직 회전만 허용하여(녹색 영역, Fig. 2) 수집됩니다.

6. **작업 실행 (Task Execution)**:
   - 로봇은 $f(I)$와 순차적 상태 추정을 통해 업데이트된 병목 포즈 추정치로 선형 경로를 따라 이동합니다.
   - 엔드 이펙터가 병목 높이에 도달하면 $g(I)$를 사용하여 최종 병목 포즈를 추정합니다.
   - 로봇은 이 포즈로 이동한 다음, 저장된 회전 $R$을 적용하여 시연 시작 시의 원래 방향으로 조정합니다.
   - 마지막으로, 시연에서 기록된 엔드 이펙터 속도 $U$를 재생하여 상호작용 궤적을 실행합니다.

## 📊 Results

### A. 목표 도달 (Target Reaching) 실험

- **목표**: $f(I)$를 이용한 병목 포즈 추정 방법들을 평가합니다. 7가지 무작위 객체에 대해 50개 궤적을 사용하여 데이터를 수집하고, 각 객체를 5가지 다른 포즈로 테스트했습니다.
- **방법론**:
  - **Oracle**: Ground-truth 병목을 사용하여 위치 제어기의 피할 수 없는 오차를 보여줍니다.
  - **First Image**: 궤적의 첫 번째 이미지에서만 $f(I)$를 사용하여 추정하고 그 추정치를 궤적 내내 사용합니다.
  - **Best Image**: 모든 이미지에 대해 $f(I)$를 지속적으로 사용하고, 지금까지 캡처된 모든 이미지 중 가장 낮은 불확실성을 가진 추정치를 사용합니다.
  - **Visual Servoing**: 모든 이미지에 대해 $f(I)$를 지속적으로 사용하고, 순차적 추정 없이 현재 이미지의 추정치만 사용합니다.
  - **Batch/Filtering (Prior, Dropout, Predicted)**: 순차적 추정 방법을 사용합니다.
- **결과 (Table I)**:
  - **순차적 추정의 우수성**: Batch 및 Filtering과 같은 여러 예측 융합 방법이 기본적인 Visual Servoing보다 우수했습니다. 이는 딥러닝을 통한 암묵적 상태 추정보다 고전적인 명시적 상태 추정의 효과를 보여줍니다.
  - **Prior 불확실성 추정의 강점**: 모든 예측에 대해 일정한 불확실성을 가정하는 Prior 방법(예: Filtering (Prior)의 평균 오차: 위치 5.2mm, 방향 5.7$^\circ$)이 동적으로 불확실성을 추정하는 Dropout 또는 Predicted 방법보다 일반적으로 더 나은 성능을 보였습니다. 이는 가우시안 불확실성을 추정하는 것이 딥러닝에서 어렵기 때문에 가정 자체가 유해한 편향을 도입할 수 있음을 시사합니다.
  - **Filtering의 효율성**: Filtering 방법은 과거 데이터를 버림에도 불구하고 Batch 방법만큼, 혹은 그 이상으로 잘 수행되었습니다. 이는 객체에 더 가까이서 캡처된 최근 이미지에 대한 편향이 더 판별적일 수 있음을 시사합니다.
  - **Filtering (Prior)**가 가장 좋은 평균 오차를 보였습니다 (위치 5.2mm, 방향 5.7$^\circ$).

### B. 모방 학습 (Imitation Learning) 실험

- **목표**: 제안하는 Coarse-to-Fine 프레임워크의 전체 성능을 평가합니다. 각 $D$와 $E$ 데이터셋 수집에 50개 궤적을 사용했습니다.
- **작업 (8가지 일상 작업, Fig. 3)**: Bottle, Plate, Screwdriver, Lid, Knife, Hammer, Scoop, Plug. 다양한 난이도와 상호작용 유형을 포함합니다.
- **방법론**:
  - **Visual Servoing**: 순차적 추정 없이 각 시간 단계에서 현재 이미지만으로 병목 포즈를 추정합니다.
  - **Visual Servoing + Correction**: Visual Servoing에 "마지막 단계(last-inch)" 보정($g(I)$)을 추가합니다.
  - **Filtering**: Filtering (Prior) 방법을 사용합니다.
  - **Filtering + Correction**: Filtering (Prior)에 "마지막 단계(last-inch)" 보정($g(I)$)을 추가합니다.
- **결과 (Table II)**: 각 작업별 20개의 객체 포즈에 대한 성공률(%)을 보여줍니다.
  - **전체 프레임워크의 성공**: Filtering과 마지막 단계 보정(Filtering + Correction)을 모두 사용한 전체 구현은 단일 시연만으로도 다양한 일상 작업을 성공적으로 수행했습니다 (평균 성공률 70%). Bottle, Lid, Hammer 작업은 100% 성공률을 보였습니다.
  - **순차적 상태 추정의 우월성**: Filtering (평균 44.4%)이 Visual Servoing (평균 35.6%)보다 우수하여, 목표 도달 실험과 일관된 결과를 보여줍니다.
  - **마지막 단계 보정의 효과**: 모든 경우에 "마지막 단계" 보정($g(I)$)이 성공률을 크게 향상시켰습니다 (예: Visual Servoing 35.6% $\to$ Visual Servoing + Correction 65%, Filtering 44.4% $\to$ Filtering + Correction 70%).
  - **성능의 작업별 편차**:
    - **Knif**e (10%) 및 **Plug** (45%)는 어려웠습니다. Knife는 랙의 슬롯이 얇고 텍스처가 부족하여 포즈 추정이 어려웠으며, Plug는 접촉이 많고 상당한 힘이 필요했습니다.
    - **Hammer** (100%) 및 **Scoop** (100%)는 높은 속도로 정확한 움직임이 필요한 작업이었는데, 시연 속도를 직접 복제하는 본 방법론에 적합하여 매우 좋은 성능을 보였습니다. 이는 명시적인 정책 학습으로는 어려울 수 있는 부분입니다.
  - **자체 지도 학습의 이점**: 병목 포즈 추정은 자체 지도 방식으로 이루어지므로, 추가적인 인간 시연 없이도 데이터 수집을 통해 성능을 자동으로 향상시킬 수 있습니다.

## 🧠 Insights & Discussion

- **상태 추정 접근 방식의 유효성**: 모방 학습을 상태 추정 문제로 모델링하고, 예측된 상태를 사용하여 제어하는 접근 방식이 복잡한 조작 작업에 효과적임을 입증했습니다. 이는 엔드-투-엔드 정책 학습 방식이 가지는 불안정성과 해석 불가능성 문제를 해결합니다.
- **분석적 제어기의 장점**: 머신러닝을 포즈 추정에만 사용하고 제어기 자체는 분석적(analytical)이므로, 결과적으로 제어기가 안정적이고 해석 가능합니다. 이는 많은 시각 기반 모방 학습 방법에서 찾아보기 어려운 특징입니다.
- **데이터 효율성**: 단일 인간 시연만으로 새로운 작업을 학습할 수 있다는 점은 기존 방법론에 비해 데이터 효율성이 매우 높음을 의미합니다. 특히 "병목" 포즈 추정을 위한 데이터는 자체 지도 방식으로 수집되므로 인간의 수고를 최소화합니다.
- **제한 사항 및 향후 연구**:
  - **정확한 포즈 추정의 중요성**: 특히 Knif나 Plug와 같이 얇거나 텍스처가 부족한 객체에 대한 포즈 추정 정확도는 여전히 개선의 여지가 있습니다. 3D 컴퓨터 비전 기술을 활용하여 병목 포즈 추정기를 개선할 수 있습니다.
  - **상호작용 중 폐루프 제어**: 현재 상호작용 궤적은 개루프(open-loop)로 시연의 속도를 재생하는 방식입니다. 객체 상호작용 중 폐루프(closed-loop) 제어를 도입하면 강한 접촉이 필요한 Plug와 같은 작업의 견고성을 더욱 높일 수 있을 것입니다.
  - **다단계 작업 확장**: 현재 방법은 단일 병목을 가정합니다. 이를 여러 병목을 포함하는 다단계 작업으로 확장하는 연구가 가능합니다.
  - **불확실성 추정의 어려움**: 딥러닝에서 가우시안 불확실성을 정확히 추정하는 것이 어렵다는 점이 드러났습니다. 이에 대한 개선 연구가 필요합니다.

## 📌 TL;DR

이 논문은 로봇이 단일 인간 시연만으로 새로운 조작 작업을 학습할 수 있는 **Coarse-to-Fine Imitation Learning** 프레임워크를 제안합니다. 핵심은 로봇 동작을 거친 접근 궤적과 미세한 상호작용 궤적으로 나누고, 모방 학습을 **엔드 이펙터가 객체와 상호작용을 시작하는 "병목" 포즈를 추정하는 문제**로 정의하는 것입니다. 이 병목 포즈 추정기는 카메라를 객체 주변으로 자동 이동시켜 수집한 데이터로 **자체 지도 학습**됩니다. 테스트 시에는 추정된 병목 포즈로 이동한 후 시연자의 엔드 이펙터 속도를 간단히 **재생**하여 복잡한 상호작용을 수행합니다. 실험 결과, 이 방법은 기존 방법들이 요구하는 다수의 시연, 환경 재설정, 또는 사전 지식 없이도 다양한 실제 작업을 높은 성공률로 학습할 수 있음을 보여주며, **안정적이고 해석 가능한 분석적 제어기**를 제공합니다.
