# MimicPlay: Long-Horizon Imitation Learning by Watching Human Play

Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, Anima Anandkumar

## 🧩 Problem to Solve

로봇이 현실 세계에서 복잡하고 장기적인(long-horizon) 조작 작업을 학습하는 데 필요한 방대한 양의 시연(demonstrations) 데이터는 수집하기 어렵고 비용이 많이 듭니다. 기존 모방 학습(Imitation Learning, IL) 방법은 주로 단기 작업에 머물러 있었으며, 계층적 모방 학습이나 플레이 데이터(play data)를 활용하는 접근법도 여전히 값비싼 로봇 원격 조작 데이터에 의존하는 한계가 있었습니다. 또한, 인간 시연 비디오를 사용할 경우 인간과 로봇의 신체적 형태(embodiment) 차이로 인한 도메인 격차(domain gap)를 극복하는 것이 중요한 과제입니다.

## ✨ Key Contributions

* **새로운 학습 패러다임**: 저렴한 비용으로 수집 가능한 인간 플레이 데이터(human play data)로부터 3D 인지(3D-aware) 잠재 계획(latent plans)을 학습하는 새로운 패러다임을 제안합니다.
* **계층적 프레임워크**: 계획 안내(plan-guided) 다중 작업(multi-task) 로봇 제어기를 훈련하여 도전적인 장기 조작 작업을 샘플 효율적으로(sample-efficiently) 수행할 수 있는 계층적 프레임워크를 개발합니다.
* **우수한 성능**: 14가지 실제 장기 조작 작업에 대한 체계적인 평가에서, MIMICPLAY는 작업 성공률, 일반화 능력 및 교란에 대한 강건성 측면에서 최첨단 모방 학습 방법보다 우수한 성능을 보여줍니다.
* **직관적인 인터페이스**: 인간 비디오를 직접 "프롬프트(prompt)"로 사용하여 로봇 조작 작업의 목표를 지정할 수 있는 인터페이스를 가능하게 합니다.

## 📎 Related Works

* **시연을 통한 모방 학습 (Imitation learning from demonstrations)**: 기존 IL은 높은 샘플 효율성을 제공하지만 고차원 관측 및 폐쇄 루프 제어에는 한계가 있었습니다. 딥러닝 기반 IL은 유연성이 높지만, 많은 양의 시연 데이터가 필요합니다. 본 연구는 로봇 온-로봇 시연 데이터의 필요성을 대폭 줄이기 위해 인간 플레이 데이터를 활용합니다.
* **계층적 모방 학습 (Hierarchical imitation learning)**: 이전 연구들은 계획(planning)과 제어(control) 모두를 단일 유형의(비용이 많이 드는) 데이터(로봇 원격 조작 시연)로 학습했습니다. MIMICPLAY는 고수준 계획 학습에는 저렴한 인간 플레이 데이터를, 저수준 제어 학습에는 소량의 로봇 시연 데이터를 사용하여 시연 데이터 요구량을 낮추면서 모델의 계획 능력을 강화합니다.
* **인간 비디오 학습 (Learning from human videos)**: R3M, MVP와 같은 연구들은 인터넷 규모의 인간 비디오 데이터셋을 활용하여 시각적 표현을 사전 학습하지만, 데이터 소스의 다양성과 큰 도메인 격차로 인해 특정 조작 작업에 전이하기 어렵습니다. 다른 연구들은 도메인 내(in-domain) 인간 비디오를 사용하여 보상 형성(reward shaping)이나 특징 학습에 집중하지만, 로봇 저수준 동작 생성에 직접적인 도움을 주지 않습니다. 본 연구는 인간 플레이 데이터에서 의미 있는 궤적 수준(trajectory-level)의 작업 계획을 추출합니다.
* **플레이 데이터 학습 (Learning from play data)**: LMP, C-BeT와 같은 이전 연구들은 로봇 원격 조작 플레이 데이터(목표 없이 로봇이 환경과 상호작용하는 데이터)를 사용하여 다중 작업 학습을 수행했지만, 이 데이터 수집 역시 매우 노동 집약적이고 시간이 많이 소요됩니다(예: 4.5~6시간). MIMICPLAY는 인간이 자신의 손으로 환경과 자유롭게 상호작용하는 인간 플레이 데이터(약 10분)를 사용하여 로봇 동작 생성에 풍부한 궤적 수준 안내를 제공합니다.

## 🛠️ Methodology

MIMICPLAY는 고수준 계획(high-level planning)은 인간 플레이 데이터에서, 저수준 제어(low-level control)는 로봇 시연 데이터에서 학습하는 계층적 프레임워크입니다. 인간과 로봇 신체 간의 격차를 연결하기 위해 3D 인지 잠재 계획 공간을 학습합니다.

1. **인간 플레이 데이터 수집 (Collecting human play data)**:
    * 인간 작업자가 호기심에 이끌려 한 손으로 장면과 자유롭게 상호작용하는 비디오를 각 환경당 10분씩 수집합니다.
    * 두 대의 보정된(calibrated) 카메라와 상용 핸드 감지기([49])를 사용하여 3D 인간 손 궤적($\tau$)을 추적합니다.

2. **인간 플레이 데이터로부터 3D 인지 잠재 계획 학습 (Learning 3D-aware latent plans from human play data)**:
    * **다중 모드 잠재 계획 학습**: 시각 인코더 $E$ (ResNet-18)가 인간 비디오 $V_h$의 시각적 관측 $o_h^t$와 목표 이미지 $g_h^t$를 저차원 특징으로 인코딩하고, MLP 기반 인코더 네트워크가 이를 잠재 계획 벡터 $p_t$로 변환합니다. MLP 기반 가우시안 혼합 모델(Gaussian Mixture Model, GMM) 디코더는 잠재 계획 $p_t$와 손 위치 $l_t$를 기반으로 3D 손 궤적을 예측하여, 인간 동작의 풍부한 다중 모드 분포를 포착합니다.
        * GMM 손실 함수: $L_{GMM}(\theta) = -\mathbb{E}_{\tau} \log \left( \sum_{k=1}^K \eta_k \mathcal{N}(\tau|\mu_k, \sigma_k) \right)$
    * **인간-로봇 도메인 간 시각적 격차 처리**: 인간 비디오 프레임 $o_h \in V_h$와 로봇 비디오 프레임 $o_r \in V_r$의 특징 임베딩 $Q_h = E(o_h)$ 및 $Q_r = E(o_r)$ 간의 Kullback-Leibler (KL) 발산 손실($L_{KL} = D_{KL}(Q_r || Q_h)$)을 최소화하여 시각적 표현 격차를 줄입니다.
    * 총 손실 함수: $L = L_{GMM} + \lambda \cdot L_{KL}$.

3. **계획 안내 다중 작업 모방 학습 (Plan-guided multi-task imitation learning)**:
    * 인간 플레이 데이터로 사전 훈련된 고수준 플래너 $P$ (고정)는 고차원 입력을 저차원 잠재 계획 벡터 $p_t$로 응축합니다.
    * 저수준 정책 $\pi$는 이 잠재 계획 $p_t$와 로봇 행동 $a_t$ 간의 변환 학습에 집중합니다.
    * **비디오 프롬프팅(Video prompting)**: 단일 샷 비디오($V_h$ 또는 $V_r$)를 목표 지정 프롬프트로 사용하여 사전 훈련된 플래너가 로봇 실행 가능한 잠재 계획 $p_t$를 생성합니다.
    * **트랜스포머 기반 계획 안내 모방 (Transformer-based plan-guided imitation)**:
        * 로봇 손목 카메라 관측 $w_t$, 고유수용성 데이터 $e_t$, 생성된 잠재 계획 $p_t$를 결합하여 원 스텝 토큰 임베딩 $s_t = [w_t, e_t, p_t]$를 생성합니다.
        * $T$ 시간 단계에 걸친 임베딩 시퀀스 $s_{[t:t+T]}$는 트랜스포머 아키텍처 $f_{trans}$를 통해 처리됩니다.
        * 트랜스포머는 자기회귀 방식으로 행동 예측 임베딩 $x_t$를 생성하며, 최종 로봇 제어 명령 $a_t$는 두 계층 완전 연결 네트워크와 MLP 기반 GMM을 통해 계산됩니다.

## 📊 Results

* **성능 우위**: 14가지 실제 장기 조작 작업에서 SOTA IL 방법을 능가하는 성능을 달성했습니다.
* **인간 플레이 데이터의 효과**: 장기 작업 설정에서 Ours는 Ours (0% human)보다 23% 이상 높은 성능을 보였으며(표 1, 2), 10분간의 저렴한 인간 플레이 데이터가 성공률과 샘플 효율성을 크게 향상시킴을 입증했습니다.
* **계층적 정책의 중요성**: Ours (0% human)는 기존의 종단 간(end-to-end) 학습 방법보다 장기 작업 설정에서 15% 이상 우수하며(표 1, 2), 사전 훈련된 잠재 계획을 기반으로 행동하는 것이 더 효과적임을 보여줍니다.
* **다중 작업 학습 이점**: MIMICPLAY는 다중 작업 훈련에서 가장 작은 성능 저하를 보였습니다(Ours-single vs. Ours, 표 3), 이는 사전 훈련된 잠재 계획 공간을 기반으로 하는 계획 안내 로봇 동작 학습의 장점을 강조합니다.
* **GMM의 중요성**: GMM을 사용한 MIMICPLAY 모델은 GMM 없이 학습한 모델(Ours (w/o GMM))보다 훨씬 우수한 성능을 보였으며(표 2), 인간 플레이 데이터의 다중 모드 궤적 분포를 처리하는 데 GMM이 필수적임을 나타냅니다.
* **KL 손실의 효과**: KL 손실을 사용하지 않은 모델(Ours (w/o KL))은 MIMICPLAY보다 17% 낮은 성공률을 보였으며(표 2), KL 손실이 인간과 로봇 데이터 간의 시각적 격차를 줄이는 데 효과적임을 보여줍니다.
* **인간 플레이 데이터 규모의 중요성**: 더 많은 인간 플레이 데이터(50% vs. 100%)를 사용할 때 새로운 하위 목표 구성에 대한 일반화 능력이 크게 향상되었습니다(표 2).
* **일반화 능력**: MIMICPLAY는 보지 못한 스터디 데스크 작업에서 모든 기준선보다 35% 이상 높은 성능을 보이며(표 2), 인간 플레이 데이터에서 새로운 잠재 계획을 추출하여 새로운 하위 목표 구성으로 일반화할 수 있음을 입증합니다.
* **직관적인 인터페이스 및 강건성**: 인간 비디오 프롬프트가 로봇 오라클 비디오 프롬프트만큼 경쟁력 있는 성능을 유지하며(그림 4), 실시간 재계획 능력으로 교란에 대해 강건함을 보여줍니다(그림 5).

## 🧠 Insights & Discussion

MIMICPLAY는 저렴한 인간 플레이 데이터로 고수준 계획을, 소량의 로봇 시연 데이터로 저수준 제어를 학습하는 상호 보완적인 강점을 효과적으로 활용합니다. 이 계층적 설계는 장기 작업에서 샘플 효율성, 일반화 능력, 강건성을 크게 향상시킵니다. GMM은 인간 동작의 다중 모드 궤적을 모델링하는 데, KL 손실은 인간과 로봇 도메인 간의 시각적 격차를 해소하는 데 핵심적인 역할을 합니다.

**제한 사항**:

1. 현재 고수준 잠재 계획은 장면 특정(scene-specific) 인간 플레이 데이터에서 학습됩니다. 인터넷 규모의 데이터셋으로 훈련할 경우 MIMICPLAY의 확장성이 크게 향상될 수 있습니다.
2. 현재 작업은 탁상 환경으로 제한되어 있습니다. 인간의 이동 행동에는 풍부한 고수준 계획 정보가 포함되어 있으므로, 모바일 조작(mobile manipulation)과 같은 더 도전적인 작업으로 확장될 수 있습니다.
3. 교차 신체 표현 학습(cross-embodiment representation learning)에 대한 개선의 여지가 많습니다 (예: 시간적 대조 학습(temporal contrastive learning), 주기 일관성 학습(cycle consistency learning)).

## 📌 TL;DR

**문제**: 로봇이 복잡하고 긴 작업(long-horizon tasks)을 모방 학습(imitation learning)하는 데 필요한 방대한 시연 데이터 수집의 어려움.
**방법**: MIMICPLAY는 저렴한 인간 플레이 데이터(human play data)에서 3D 잠재 계획(latent plans)을 학습하는 고수준 플래너와, 소량의 로봇 원격 조작 시연 데이터(robot teleoperation demonstrations)에서 저수준 비전-모터 제어를 학습하는 계층적 프레임워크를 제안합니다. GMM으로 다중 모드 궤적을 모델링하고, KL 손실로 인간-로봇 시각적 도메인 간의 격차를 줄입니다.
**핵심 결과**: MIMICPLAY는 14가지 실제 장기 조작 작업에서 기존 모방 학습 방법보다 성공률, 일반화 능력, 교란에 대한 강건성에서 50% 이상 우수한 성능을 보입니다. 인간 비디오를 프롬프트로 사용하여 로봇 동작을 지시할 수 있는 직관적인 인터페이스를 제공하며, 효율적인 데이터 사용을 통해 로봇 학습의 확장 가능성을 제시합니다.
