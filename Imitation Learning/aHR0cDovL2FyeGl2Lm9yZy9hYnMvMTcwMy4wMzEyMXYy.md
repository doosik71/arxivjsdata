# Coordinated Multi-Agent Imitation Learning

Hoang M. Le, Yisong Yue, Peter Carr, Patrick Lucey

## 🧩 Problem to Solve

이 논문은 여러 협력 에이전트의 시연(demonstration)으로부터 모방 학습(imitation learning)을 수행하는 문제를 다룹니다. 이 설정에서 핵심 과제는 **조정(coordination) 모델을 학습하기 어렵다**는 것입니다. 조정은 시연에서 종종 암묵적으로 이루어지며, 잠재 변수(latent variable)로 추론되어야 합니다. 특히, 에이전트의 역할이나 인덱스가 시연마다 바뀌거나 관찰되지 않는 "인덱스 프리(index-free)" 다중 에이전트 제어 환경(예: 팀 스포츠에서 플레이어의 역할)에서 일관된 상태 표현을 구성하는 것이 어렵습니다. 이는 다중 에이전트 시스템에서 높은 차원의 상태 및 액션 공간, 데이터 부족 등으로 인해 학습 성공이 제한적인 이유 중 하나입니다.

## ✨ Key Contributions

- **조정 모델과 개별 정책 동시 학습:** 잠재 조정 모델(latent coordination model)과 개별 정책(individual policies)을 동시에 학습하는 통합 접근 방식을 제안합니다.
- **비지도 구조 학습과 모방 학습 통합:** 기존 모방 학습에 비지도 구조 학습(unsupervised structure learning)을 통합합니다.
- **교대 최적화 기법:** 개별 정책과 잠재 구조 모델을 통합적이고 효율적으로 훈련하기 위한 교대 최적화(alternating optimization) 방법을 개발했습니다.
- **확장된 단일 에이전트 모방 학습:** 강력한 블랙박스 지도 학습 기술(예: 딥러닝)을 활용하여 단일 에이전트 모방 학습을 다중 에이전트 도메인으로 확장했습니다.
- **확률적 변분 추론 활용:** 잠재 구조 학습을 위해 확률적 변분 추론(stochastic variational inference) 접근 방식을 개발했습니다.
- **성능 입증:** 프레데터-프레이(predator-prey) 게임과 프로 축구 팀 수비 모델링이라는 두 가지 어려운 문제에서 제안된 방식이 기존 베이스라인 대비 모방 손실을 크게 개선함을 보였습니다.
- **대규모 적용:** 대규모 협력적 다중 에이전트 정책을 공동으로 학습하는 모방 학습 접근법을 적용한 첫 사례입니다.

## 📎 Related Works

- **다중 에이전트 학습:** 협력적 다중 에이전트 학습(Stone & Veloso, 2000; Panait & Luke, 2005) 및 다중 에이전트 강화 학습(Busoniu et al., 2008)의 비정상성(non-stationarity) 문제.
- **"인덱스 프리" 제어:** 로봇 포메이션 유지 등에서 에이전트 인덱싱이 불분명한 분산 제어 문제(Kingston & Egerstedt, 2010; Kloder & Hutchinson, 2006).
- **모방 학습:** 동적 오라클(dynamic oracle)을 활용하는 감소 기반(reduction-based) 단일 에이전트 모방 학습(Ross et al., 2011; Daum ́e III et al., 2009).
- **순환 신경망 및 샘플링:** 시퀀스 예측에서 입력 상태 분포 표류를 다루기 위한 스케줄드 샘플링(Scheduled sampling) (Bengio et al., 2015).
- **변분 추론:** 잠재 구조 모델 학습을 위한 확률적 변분 추론(Hoffman et al., 2013; Johnson & Willsky, 2014; Beal, 2003) 및 평균 필드 변분족(mean-field variational family) (Hoffman & Blei, 2014).
- **집합 기반 학습:** 입력 또는 출력이 집합일 때 순서의 중요성을 강조하는 연구(Vinyals et al., 2015).

## 🛠️ Methodology

논문은 다중 에이전트 모방 학습 문제를 다음과 같이 공식화하고 해결책을 제시합니다.

1. **문제 공식화:**

   - $K$개의 에이전트 정책 $\pi_1, \dots, \pi_K$을 학습하여 모방 손실 $L_{imitation} = \sum_{k=1}^K E_{s \sim d_{\pi_k}} [\ell(\pi_k(s_k))]$을 최소화하는 것이 목표입니다.
   - "인덱스 프리" 문제를 해결하기 위해 역할 학습(role learning)과 역할 기반 인덱스 할당(role-based index assignment)을 제안합니다.
   - 역할은 비지도 방식으로 학습되며, 조정 구조는 그래픽 모델로 표현됩니다.
   - 전체 목적 함수는 모방 손실과 역할 할당 엔트로피를 포함합니다:
     $$ \min*{\pi_1, \dots, \pi_K, A} \sum*{k=1}^K E*{s_k \sim d*{\pi_k}} [\ell(\pi_k(s_k)) | A, D] - \lambda H(A|D) $$
        여기서 $A$는 할당 함수, $H(A|D)$는 역할 할당의 엔트로피입니다.

2. **학습 접근 방식 (교대 최적화 - Algorithm 1):**
   - 정책 학습과 비지도 구조 학습을 통합하는 교대 최적화 방식을 사용합니다.
   - **역할 할당 (Line 2 - `Assign`):** 현재 잠재 구조 모델 $q$를 사용하여 훈련 데이터의 unstructured trajectories $U$를 ordered trajectories $A = [A_1, \dots, A_K]$로 매핑합니다 (Algorithm 4). 이는 비용 행렬 $M$을 구성하고 선형 할당 문제(Linear Assignment Problem)를 해결하여 최적의 역할-인덱스 매핑을 찾습니다.
   - **정책 학습 (Line 3-5 - `Learn`):** 할당된 (구조화된) 궤적 $A$를 사용하여 개별 정책 $\pi_k$를 업데이트합니다 (Algorithm 2).
     - **다중 에이전트 감소 기반 학습:** 각 에이전트의 정책은 LSTM과 같은 블랙박스 함수 클래스로 표현됩니다.
     - **교차 업데이트(Cross-update):** 각 에이전트의 상태 $s_{t,k}$는 다른 에이전트의 예측된 행동 $\hat{a}_{t,j}$에 의존하여 업데이트되어 다중 에이전트 환경의 비정상성을 다룹니다.
     - **점진적 롤아웃 지평(Gradually increasing roll-out horizon):** 훈련 초기 단계에서 학습 안정화를 위해 롤아웃 지평 $j$를 1부터 점진적으로 증가시킵니다.
   - **구조 학습 (Line 6 - `LearnStructure`):** 롤아웃된 궤적(rolled-out trajectories)을 기반으로 잠재 구조 모델 $q$의 매개변수 $\theta, z$를 재훈련합니다 (Algorithm 3).
     - **확률적 변분 추론 (SVI):** 추론 불가능한 사후 분포 $p(z|x, \theta)$를 더 단순한 분포 $q$로 근사하기 위해 SVI를 사용합니다.
     - **HMM 사용:** 역할 전환을 모델링하기 위해 은닉 마르코프 모델(Hidden Markov Models, HMM)을 사용하며, Forward-Backward 알고리즘으로 지역 업데이트 $q(z)$를 수행하고, 자연 경사 하강(natural gradient ascent)으로 전역 매개변수 $q(\theta)$를 업데이트합니다.

## 📊 Results

- **프레데터-프레이 도메인 (합성 실험):**

  - **조정 훈련 (제안 방법):** 평균 22 스텝, 8%의 실패율로 전문가 성능에 빠르게 도달했습니다.
  - **비구조화 훈련 (베이스라인):** 첫 번째 반복 이후 성능 향상이 미미했으며, 평균 42 스텝, 70%의 높은 실패율을 보였습니다.
  - 제안 방법은 실패율을 현저히 낮추고 성공까지 걸리는 평균 시간을 단축했습니다.

- **축구 다중 에이전트 모방 학습 (실제 데이터):**
  - 45개 프로 축구 경기 트래킹 데이터(7500개 궤적 세트, 130만 프레임)를 사용했습니다.
  - 정책은 LSTM으로 구현되었고, 구조 모델은 연속형 HMM(가우시안 혼합 방출 분포)을 사용했습니다.
  - **조정 학습 접근 방식 (제안 방법):** 구조화된 조정 없이 수행된 기존 모방 학습보다 모방 손실이 현저히 낮았습니다.
  - 분산 정책(decentralized policies)과 중앙 집중 정책(centralized policies) 간의 성능 차이는 미미했으며, 조정된 정책과 비구조화된 정책 간의 격차가 훨씬 커 구조화된 조정의 이점을 강조합니다.
  - 시각화 결과, 학습된 정책이 실제 시연과 질적으로 유사하며 반사실적 분석(counterfactual replay analysis)과 같은 응용에 유용함을 보여주었습니다.

## 🧠 Insights & Discussion

- **성과:** 잠재적 조정을 명시적으로 모델링하는 것이 다중 에이전트 모방 학습의 성능을 크게 향상시킬 수 있음을 입증했습니다. 정책 학습과 잠재 구조 학습 간의 순환 의존성(circular dependency)을 교대 최적화 방식이 효과적으로 해결했습니다. 또한, 점진적으로 증가하는 롤아웃 지평과 에이전트 간의 교차 업데이트는 순차 예측에서 발생하는 오차 누적(cascading errors)과 비정상성 문제를 해결하여 학습을 안정화하는 데 기여했습니다. 이 프레임워크는 다양한 블랙박스 지도 학습 모델과 그래픽 모델을 통합할 수 있는 유연성을 제공합니다.
- **한계:** 다양한 그래픽 모델에 따라 구조 학습 및 추론 절차가 달라질 수 있습니다. 미래 연구에서는 인덱스 프리 정책 학습을 지원하는 완전한 엔드투엔드(end-to-end) 미분 가능 학습 방법, 특히 딥러닝 기반 방법을 탐구하여 계산 속도를 높일 수 있을 것입니다.
- **의의:** 본 연구는 팀 스포츠와 같은 실제 세계의 복잡한 다중 에이전트 조정 문제에 대한 확장 가능한 해결책을 제시하며, 다중 에이전트 강화 학습에서 계산적으로 비현실적인 전통적인 탐색 기반 접근 방식의 유용한 구성 요소가 될 수 있습니다. 이는 생성 모델링(generative modeling) 및 효율적인 구조화된 탐색(structured exploration) 분야로의 확장을 위한 기반을 마련합니다.

## 📌 TL;DR

**문제:** 다중 에이전트 모방 학습은 암묵적인 조정과 알 수 없거나 동적인 에이전트 역할("인덱스 프리") 때문에 어려움을 겪습니다.

**방법:** 이 논문은 "조정된 다중 에이전트 모방 학습" 프레임워크를 제안합니다. 이는 개별 에이전트 정책(교차 업데이트 및 점진적 롤아웃 지평을 가진 감소 기반 접근 방식 사용)과 잠재 조정 구조 모델(HMM과 같은 그래픽 모델에 대한 확률적 변분 추론 및 역할 기반 인덱싱을 위한 선형 할당 문제 사용)을 교대로 학습합니다.

**핵심 발견:** 정책과 조정 구조를 동시에 학습하는 이 공동 학습 방식은 프레데터-프레이 및 프로 축구 수비와 같은 합성 및 실제 시나리오 모두에서 비구조화된 베이스라인보다 훨씬 우수한 성능을 보이며, 실패율을 줄이고 모방 손실을 낮춤으로써 명시적인 조정 모델링의 중요한 역할을 입증했습니다.
