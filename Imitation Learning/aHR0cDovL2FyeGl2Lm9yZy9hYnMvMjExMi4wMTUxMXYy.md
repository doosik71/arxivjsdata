# 시각 모방 학습을 위한 표현 학습의 놀라운 효과

Jyothish Pari, Nur Muhammad (Mahi) Shafiullah, Sridhar Pandian Arunachalam, Lerrel Pinto

## 🧩 Problem to Solve

시각 모방 학습은 다양한 시각 데이터에서 효과적인 표현을 학습하고, 동시에 이러한 표현과 행동을 연결하는 두 가지 문제를 동시에 해결하려고 합니다. 이 두 문제의 결합된 학습은 상호 의존성을 야기하며, 이는 종종 학습에 많은 양의 시연 데이터가 필요하게 만듭니다. 이로 인해 모델의 일반화 능력이 저해됩니다.

## ✨ Key Contributions

* **VINN(Visual Imitation through Nearest Neighbors) 프레임워크 제안:** 학습된 시각적 표현에서 비모수적 행동을 도출하는 새롭고 구현하기 쉬운 시각 모방 프레임워크를 제시합니다.
* **경쟁력 있는 성능 입증:** VINN이 표준 매개변수 기반 행동 복제(Behavioral Cloning, BC) 방식과 견줄 만하며, 일련의 조작 작업에서 이를 능가할 수 있음을 보여줍니다.
* **실제 로봇 적용 및 일반화 능력:** VINN이 실제 로봇의 문 열기 작업에 적용될 수 있으며, 새로운 문에 대해 높은 일반화 성능을 달성할 수 있음을 입증합니다.
* **광범위한 분석 및 견고성 입증:** 다양한 표현 방식, 훈련 데이터 양 및 기타 하이퍼파라미터에 대한 광범위한 실험 및 분석을 통해 VINN의 견고성을 보여줍니다.

## 📎 Related Works

* **행동 복제 (Behavioral Cloning, BC):** [43]과 같은 BC는 가장 일반적인 모방 학습 기법으로, 시연 데이터에서 행동을 학습합니다. 그러나 다양한 환경에 일반화하기 위해서는 많은 데이터나 도메인 적응 기술이 필요합니다. [49]는 확장 가능한 시연 데이터 수집 방법을 제시했습니다.
* **시각 표현 학습 (Visual Representation Learning):** [7, 8, 16, 5, 4]와 같은 자기 지도 학습(Self-supervised Learning, SSL) 기법은 레이블이 없는 대량의 데이터를 활용하여 유용한 시각적 특징을 추출합니다. 이는 [24, 50, 51] 등 로봇 공학 분야에서도 좋은 성과를 보였습니다. 본 연구에서는 BYOL [16] 스타일의 자기 지도 학습 프레임워크를 따릅니다.
* **비모수 제어 (Non-parametric Control):** [3]의 국소 가중 회귀(Locally Weighted Regression, LWL)와 같은 비모수 모델은 데이터 분포에 대한 매개변수를 모델링하는 대신, 이전에 관찰된 훈련 데이터를 기반으로 표현합니다. 이는 k-최근접 이웃(k-NN) 알고리즘 [1, 23] 등을 포함하며, 로봇 제어 문제 [3] 및 강화 학습 [22, 32, 33, 37]에서도 활용되었습니다.

## 🛠️ Methodology

본 연구는 시각 모방 학습을 표현 학습과 행동 학습으로 분리하는 VINN 프레임워크를 제안합니다.

* **시각 표현 학습:**
  * ImageNet으로 사전 훈련된 ResNet-50 인코더를 사용하여 초기화합니다.
  * 오프라인 시각 데이터(행동 정보가 없는 시연 프레임)를 사용하여 BYOL [16]과 같은 자기 지도 학습 방법으로 인코더를 미세 조정(fine-tune)합니다. BYOL은 온라인 네트워크와 타겟 네트워크를 사용하여 동일 이미지의 다른 증강 버전 간의 불일치를 줄이도록 훈련됩니다.
  * 훈련이 완료되면, 모든 시연 프레임을 인코더로 인코딩하여 임베딩 집합 $E$를 얻습니다.

* **k-최근접 이웃 기반 국소 가중 회귀 (k-Nearest Neighbors Based Locally Weighted Regression):**
  * 새로운 관측 $o_t$가 주어지면, 이를 인코딩하여 임베딩 $e$를 얻습니다.
  * 임베딩 공간에서 $e$와 가장 유사한 $k$개의 시연 임베딩 $(e^{(1)}, a^{(1)}), \ldots, (e^{(k)}, a^{(k)})$을 유클리드 거리 $||e - e^{(i)}||_2$를 기반으로 찾습니다.
  * 이 $k$개의 최근접 이웃에 해당하는 행동들을 유클리드 커널 가중 평균을 사용하여 예측 행동 $\hat{a}$를 계산합니다.
        $$\hat{a} = \frac{\sum_{i=1}^{k} \exp\left(-\|e - e^{(i)}\|_2\right) \cdot a^{(i)}}{\sum_{i=1}^{k} \exp\left(-\|e - e^{(i)}\|_2\right)}$$

* **실제 로봇 배포:** 로봇 제어는 폐쇄 루프 방식으로 진행됩니다. DemoAT [49] 도구를 사용하여 시연을 수집하고, Structure from Motion (SfM) [29]을 통해 카메라 포즈에서 변환 운동(translation motion)을 행동으로 추출합니다. 그리퍼 상태는 별도의 그리퍼 네트워크를 통해 예측합니다.

## 📊 Results

* **오프라인 MSE 손실:**
  * 소규모 데이터셋(시연 20개 미만)에서 VINN은 기존 종단 간(end-to-end) 행동 복제(BC) 방법보다 훨씬 우수하거나 경쟁력 있는 성능을 보였습니다.
  * 특히 스태킹(Stacking) 및 문 열기(Door-Opening) 작업에서 소규모 데이터셋일 때 VINN이 현저히 더 나은 성능을 달성했습니다.
  * 푸싱(Pushing) 작업에서는 소규모 데이터로 해결하기 어려웠으나, VICReg과 같은 다른 자기 지도 학습 방식을 사용할 경우 성능이 향상될 수 있음이 나타났습니다.
* **실제 로봇 평가 (문 열기):**
  * **동일 환경:** VINN은 핸들 잡기 및 문 열기 성공률에서 80%를 달성하여, BC (0%) 및 BC-rep (53.3%)를 크게 능가했습니다. ImageNet 특징 + NN 방식은 0%의 성공률을 보였습니다.
  * **새로운 환경으로의 일반화 (시각적 가림):** VINN은 시각적 가림이 있는 상황에서도 견고한 일반화 성능을 보였습니다. 핸들이 가려진 경우에도 70%, 더 많은 부분이 가려진 경우에도 50%의 성공률을 기록했으며, 이때 BC-rep는 완전히 실패했습니다 (0%). 모든 시각적 표식이 가려졌을 때만 VINN이 완전히 실패했습니다.
* **설계 선택의 중요성 (Ablation Studies):**
  * ImageNet 사전 훈련과 자기 지도 미세 조정(fine-tuning)은 성능에 결정적인 영향을 미칩니다. 이 중 하나라도 제거하면 성능이 크게 저하됩니다.
  * Implicit BC는 낮은 성능을 보였습니다.
  * VINN의 비모수적 정책은 매개변수 기반 BC-rep에 비해 분포 외(out-of-distribution) 샘플에 대해 더 견고한 것으로 나타났습니다.
  * k-NN에서 최적의 $k$ 값은 약 10-20 사이였습니다.

## 🧠 Insights & Discussion

* **표현 학습과 행동 학습의 분리 효과:** 시각적 표현 학습을 행동 학습과 분리하고, 자기 지도 학습된 표현에 대해 비모수적 행동 예측(국소 가중 회귀)을 적용하는 것이 놀랍도록 효과적임을 보여줍니다.
* **고품질 표현의 중요성:** ImageNet 사전 훈련과 도메인별 자기 지도 미세 조정이 결합될 때 최상의 표현 품질을 제공하며, 이는 실제 로봇 작업에서 높은 성능으로 이어집니다.
* **비모수적 방식의 견고성:** 매개변수 기반 모델이 로봇의 잘못된 움직임으로 인해 분포 외(out-of-distribution) 관측치가 발생했을 때 빠르게 실패하는 반면, VINN과 같은 비모수적 방식은 최근접 이웃을 통해 이러한 편차를 어느 정도 복구할 수 있어 실제 환경에서 더욱 견고합니다.
* **MSE와 실제 성능의 상관관계:** 낮은 MSE 손실은 좋은 실제 성능을 위해 필요조건이지만 충분조건은 아닙니다. 이는 모델이 훈련 데이터 분포를 벗어난 상황에서 오류를 수정하고 잘 작동하는 능력이 중요함을 시사합니다.
* **한계점 및 향후 연구:**
  * 장면의 급격한 변화(예: 모든 주요 시각적 표식 제거)에는 여전히 어려움을 겪습니다. 점진적인 표현 학습 알고리즘이 개선에 도움이 될 수 있습니다.
  * 현재 자기 지도 학습은 작업 관련 데이터로 이루어지지만, 이상적으로는 대규모 데이터셋을 활용한 작업에 무관한 사전 훈련(task-agnostic pre-training)이 더 나은 성능을 제공할 수 있습니다.
  * 현재는 단일 작업에 초점을 맞추고 있으며, 여러 작업을 위한 공동 표현 학습은 전체 훈련 오버헤드를 줄이면서 정확도를 유지할 수 있는 유망한 방향입니다.

## 📌 TL;DR

시각 모방 학습은 표현 학습과 행동 학습의 결합으로 인해 많은 데이터가 필요하며 일반화에 어려움을 겪습니다. 본 연구는 이 두 가지를 분리하는 VINN(Visual Imitation through Nearest Neighbors) 프레임워크를 제안합니다. VINN은 먼저 오프라인 데이터에서 자기 지도 학습(BYOL)을 통해 시각적 표현을 학습한 다음, 이 표현을 기반으로 비모수적 국소 가중 회귀(k-NN)를 사용하여 소수의 시연으로부터 행동을 예측합니다. 이 간단한 분리 접근 방식은 제한된 데이터 환경에서 종단 간 행동 복제보다 경쟁력 있거나 우수한 성능을 보여주며, 실제 로봇 문 열기 작업에서 새로운 시각적 가림에 대해서도 견고한 일반화 능력을 입증합니다.
