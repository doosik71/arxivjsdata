{
  "url": "http://arxiv.org/abs/2501.19406v2",
  "title": "Low-Rank Adapting Models for Sparse Autoencoders",
  "authors": "Matthew Chen, Joshua Engels, Max Tegmark",
  "year": 2025,
  "abstract": "Sparse autoencoders (SAEs) decompose language model representations into a\nsparse set of linear latent vectors. Recent works have improved SAEs using\nlanguage model gradients, but these techniques require many expensive backward\npasses during training and still cause a significant increase in cross entropy\nloss when SAE reconstructions are inserted into the model. In this work, we\nimprove on these limitations by taking a fundamentally different approach: we\nuse low-rank adaptation (LoRA) to finetune the \\textit{language model itself}\naround a previously trained SAE. We analyze our method across SAE sparsity, SAE\nwidth, language model size, LoRA rank, and model layer on the Gemma Scope\nfamily of SAEs. In these settings, our method reduces the cross entropy loss\ngap by 30\\% to 55\\% when SAEs are inserted during the forward pass. We also\nfind that compared to end-to-end (e2e) SAEs, our approach achieves the same\ndownstream cross entropy loss 3$\\times$ to 20$\\times$ faster on \\gemma and\n2$\\times$ to 10$\\times$ faster on \\llama. We further show that our technique\nimproves downstream metrics and can adapt multiple SAEs at once without harming\ngeneral language model capabilities. Our results demonstrate that improving\nmodel interpretability is not limited to post-hoc SAE training; Pareto\nimprovements can also be achieved by directly optimizing the model itself.",
  "citation": 0
}