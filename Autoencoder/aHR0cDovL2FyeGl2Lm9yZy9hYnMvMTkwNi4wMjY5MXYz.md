# An Introduction to Variational Autoencoders

Diederik P. Kingma, Max Welling

## 🧩 Problem to Solve

머신러닝에서 생성 모델링은 데이터의 공동 분포를 학습하여 데이터가 실제 세계에서 어떻게 생성되는지를 시뮬레이션하는 것을 목표로 합니다. 특히 심층 잠재 변수 모델(Deep Latent Variable Models, DLVMs)은 복잡한 실제 데이터 분포를 근사하는 데 매우 유용합니다. 그러나 DLVMs에서 최대 우도(Maximum Likelihood) 학습은 몇 가지 주요 어려움에 직면합니다.

* **주변 가능도의 계산 불가능성**: 관측된 데이터 $x$에 대한 주변 가능도 $p_{\theta}(x) = \int p_{\theta}(x,z)dz$는 잠재 변수 $z$에 대한 적분 때문에 일반적으로 해석적 해를 갖지 않거나 효율적인 추정량이 없어 계산이 불가능합니다.
* **사후 분포의 계산 불가능성**: 주변 가능도의 계산 불가능성은 곧 잠재 변수의 사후 분포 $p_{\theta}(z|x)$ 또한 계산하기 어렵다는 것을 의미합니다.
* **기존 추론 방법의 한계**: 전통적인 근사 추론 기법들은 데이터 포인트당 별도의 최적화 루프가 필요하여 비효율적이거나, 사후 분포에 대한 근사 품질이 좋지 않은 경향이 있습니다.

이러한 문제들을 해결하고 심층 잠재 변수 모델의 효율적이고 확장 가능한 학습 및 추론 방법을 제공하는 것이 이 연구의 주된 목표입니다.

## ✨ Key Contributions

이 논문은 변분 오토인코더(Variational Autoencoders, VAEs) 프레임워크와 주요 확장 기능들을 소개하며 다음과 같은 핵심 기여를 합니다.

* **원리적인 프레임워크 제시**: 심층 잠재 변수 모델과 이에 상응하는 추론 모델을 확률적 경사 하강법(SGD)을 사용하여 통합적으로 학습하는 원리적인 방법을 제공합니다.
* **상각 추론(Amortized Inference) 도입**: 데이터 포인트별로 개별적인 변분 분포를 최적화하는 대신, 입력 변수의 함수인 인코더($q_{\phi}(z|x)$, 인식 모델)를 도입하여 모든 데이터 포인트에 걸쳐 변분 파라미터를 공유하게 함으로써 추론의 효율성을 획기적으로 개선합니다.
* **재매개변수화 트릭(Reparameterization Trick)**: 연속형 잠재 변수에 대해 ELBO(Evidence Lower Bound)의 기울기를 낮은 분산으로 효율적으로 계산할 수 있는 재매개변수화 트릭을 제시하여, 확률적 노드에 대한 역전파를 가능하게 하고 심층 생성 모델의 종단 간 학습을 실현합니다. 이는 VAE 프레임워크의 가장 중요한 기여로 평가됩니다.
* **ELBO를 통한 공동 최적화**: 생성 모델($p_{\theta}$)과 추론 모델($q_{\phi}$)의 파라미터를 ELBO라는 단일 목표 함수를 통해 동시에 최적화하는 방법을 제시합니다. ELBO 최대화는 데이터의 주변 가능도를 근사적으로 최대화하고 근사 사후 분포와 실제 사후 분포 간의 KL 발산($D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))$)을 최소화합니다.
* **추론 모델의 유연성 향상 기법**: IAF(Inverse Autoregressive Flow)와 같은 정규화 흐름(Normalizing Flow)을 도입하여 근사 사후 분포의 유연성을 크게 향상시키는 방법을 제안하며, 이는 고차원 잠재 공간에서도 효과적으로 작동합니다.
* **심층 생성 모델 확장**: 여러 계층의 잠재 변수를 갖는 심층 생성 모델을 학습하는 방법과 모델의 표현력을 높이는 다양한 전략(예: autoregressive 모델, 가역 변환)을 탐구합니다.

## 📎 Related Works

* **Helmholtz Machine (Dayan et al., 1995)**: 인식 모델을 사용한 최초의 모델 중 하나. VAE는 하나의 목적 함수를 최적화하는 반면, Helmholtz Machine의 wake-sleep 알고리즘은 비효율적이고 단일 목적 함수를 최적화하지 않았습니다.
* **Variational Inference (VI)**: VAE는 VI의 원리를 따르지만, 각 데이터 케이스에 대해 별도의 변분 분포를 사용하는 전통적인 VI와 달리 인식 모델을 사용하여 파라미터를 공유하는 상각 추론(amortized inference) 방식을 도입합니다.
* **Stochastic Variational Inference (Hoffman et al., 2013)**: 확률적 경사 하강법을 사용하여 대규모 데이터셋에 대한 변분 추론을 가능하게 한 연구. VAE는 여기에 재매개변수화 트릭을 추가하여 기울기 분산을 크게 줄입니다.
* **Score Function Estimator (REINFORCE) (Williams, 1992)**: ELBO의 편향 없는 확률적 기울기 추정기로, 이산 잠재 변수에 적용 가능하나 연속형 잠재 변수의 경우 재매개변수화 트릭에 비해 훨씬 높은 분산을 가집니다.
* **선형 오토인코더와 PCA (Roweis, 1998; Bourlard & Kamp, 1988)**: 선형 오토인코더가 선형-가우시안 생성 모델의 특정 경우에 해당함을 보여주었으나, VAE는 더 광범위한 비선형 연속 잠재 변수 모델에 적용됩니다.
* **Generative Adversarial Networks (GANs) (Goodfellow et al., 2014)**: 최근 주목받는 또 다른 생성 모델 패러다임. VAE와 상호 보완적인 특성(GAN은 지각적 품질이 높으나 데이터 분포의 완전한 지지를 결여, VAE는 분산된 샘플 생성하지만 우도 기준 밀도 모델링에 우수)을 가지며, 많은 하이브리드 모델이 제안되었습니다.
* **Importance Weighted Autoencoders (IWAE) (Burda et al., 2015)**: ELBO보다 더 타이트한 하한을 최적화하는 방법으로, 중요도 샘플링을 사용합니다. 그러나 고차원 잠재 공간으로 확장하는 데 어려움이 있습니다.
* **Normalizing Flows (Rezende & Mohamed, 2015)**: 확률 분포의 유연성을 높이기 위한 일반적인 프레임워크. IAF는 이러한 흐름의 한 종류입니다. NICE (Dinh et al., 2014) 및 Real NVP (Dinh et al., 2016)와 같은 다른 흐름들도 소개되었습니다.
* **Auxiliary Latent Variables (Salimans et al., 2015; Ranganath et al., 2016; Maaløe et al., 2016)**: 추론 모델의 유연성을 높이는 또 다른 방법으로, 여러 계층의 잠재 변수를 사용하는 VAE와 유사하거나 결합될 수 있습니다.

## 🛠️ Methodology

VAEs는 심층 잠재 변수 모델($p_{\theta}(x,z)$)과 이에 상응하는 추론 모델($q_{\phi}(z|x)$)을 결합하여 학습합니다.

1. **생성 모델 (Decoder)**: $p_{\theta}(x,z) = p_{\theta}(z)p_{\theta}(x|z)$ 형태로 정의되며, $p_{\theta}(z)$는 잠재 변수에 대한 사전 분포(prior)이고, $p_{\theta}(x|z)$는 잠재 변수로부터 데이터를 생성하는 디코더(decoder)입니다. 이들은 신경망으로 파라미터화됩니다. 예를 들어, 이진 데이터 $x$에 대한 DLVM은 잠재 공간 $z$를 구형 가우시안으로, 관측 모델 $p(x|z)$를 신경망 출력에 의해 파라미터화된 베르누이 분포로 설정합니다.
2. **추론 모델 (Encoder)**: $p_{\theta}(z|x)$는 일반적으로 다루기 어렵기 때문에, 이를 근사하기 위한 파라미터화된 추론 모델 $q_{\phi}(z|x)$ (인코더 또는 인식 모델)를 도입합니다. 이 모델은 단일 신경망(`EncoderNeuralNet`)을 사용하여 잠재 변수의 분포($\mu, \sigma$)를 출력하며, 이는 `amortized inference` 방식입니다.
3. **ELBO (Evidence Lower Bound) 최적화**: VAE의 목표 함수는 ELBO를 최대화하는 것입니다.
    $$L_{\theta, \phi}(x) = E_{q_{\phi}(z|x)}[\log p_{\theta}(x,z) - \log q_{\phi}(z|x)]$$
    이 ELBO는 데이터의 주변 가능도 $\log p_{\theta}(x)$의 하한이며, $D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))$만큼 실제 주변 가능도보다 작습니다. ELBO를 최대화하면 생성 모델의 품질이 향상되고, $q_{\phi}(z|x)$가 $p_{\theta}(z|x)$에 더 가까워집니다.
4. **재매개변수화 트릭**: 연속형 잠재 변수의 경우, $z \sim q_{\phi}(z|x)$를 $z=g(\epsilon, \phi, x)$와 같이 재매개변수화합니다. 여기서 $\epsilon \sim p(\epsilon)$는 $\phi$나 $x$에 의존하지 않는 노이즈 변수입니다. 예를 들어, 가우시안 인코더의 경우 $z = \mu + \sigma \odot \epsilon$ ($ \epsilon \sim N(0,I) $) 입니다. 이 트릭을 통해 $\log p_{\theta}(x,z) - \log q_{\phi}(z|x)$의 기대값을 Monte Carlo 방식으로 추정하고, 이 추정량에 대한 기울기를 역전파를 통해 효율적으로 계산할 수 있습니다.
    * $\log q_{\phi}(z|x)$는 $\log p(\epsilon) - \log |\text{det}(\frac{\partial z}{\partial \epsilon})|$로 계산되며, Jacobian 행렬의 로그-결정식 계산이 간단합니다.
5. **ELBO의 확률적 최적화**: 개별 데이터 포인트의 ELBO 추정량의 기울기는 편향이 없으므로, 미니배치 SGD를 사용하여 $\theta$와 $\phi$를 공동으로 최적화할 수 있습니다. 이를 **AEVB (Auto-Encoding Variational Bayes)** 알고리즘이라고 합니다.
6. **추론 모델의 유연성 향상**:
    * **Auxiliary Latent Variables**: 추론 모델과 생성 모델에 보조 잠재 변수 $u$를 도입하여 $q_{\phi}(z|x)$의 표현력을 간접적으로 높입니다.
    * **Normalizing Flow (NF)**: 간단한 초기 분포($\epsilon_0 \sim p(\epsilon)$)에 일련의 가역 변환 $f_t$를 적용하여 $z_T$의 분포를 더욱 유연하게 만듭니다. 최종 분포의 로그 밀도 $\log q_{\phi}(z|x)$는 각 변환의 Jacobian 로그-결정식의 합으로 계산됩니다.
    * **Inverse Autoregressive Flow (IAF)**: 정규화 흐름의 한 유형으로, 각 변환이 역 자기회귀적(inverse autoregressive) 특성을 가집니다. 이는 병렬화가 가능하며 Jacobian 로그-결정식 계산이 간단하여 고차원 잠재 공간에 적합합니다.
7. **더 깊은 생성 모델**: 여러 계층의 잠재 변수($z_1, ..., z_K$)를 사용하여 생성 모델과 추론 모델을 구성할 수 있습니다. 추론 모델의 잠재 변수 순서와 생성 모델의 잠재 변수 순서가 다를 수 있지만, 순서를 공유하면 파라미터 공유를 통해 학습 효율성과 솔루션 품질을 높일 수 있습니다.

## 📊 Results

VAEs는 다양한 분야에서 인상적인 결과를 보여주었습니다.

* **반지도 학습(Semi-supervised learning)**: VAE는 분류 작업에서 데이터 효율성을 크게 향상시켰습니다. 예를 들어, MNIST 분류 문제에서 클래스당 10개의 레이블된 이미지만으로도 1% 미만의 분류 오류율을 달성하며, 이는 훈련 세트 레이블의 99.8% 이상을 제거한 경우에 해당합니다. ImageNet과 같은 대규모 데이터셋에서도 레이블된 데이터가 적을 때 순수 지도 학습 방식보다 훨씬 뛰어난 성능을 보였습니다.
* **데이터 이해 및 인공 창의성**:
  * **화학 물질 설계**: 수십만 개의 기존 화학 구조로 VAE를 훈련하여 연속적인 잠재 표현을 학습했습니다. 이를 통해 특정 원하는 속성을 최대화하는 새로운 분자를 그래디언트 기반으로 탐색할 수 있음을 입증했습니다. (Figure 4.2)
  * **자연어 생성**: VAE를 텍스트에 성공적으로 적용하여 문장 간의 보간(interpolation)과 누락된 단어의 대체(imputation)가 가능함을 보여주었습니다. 생성된 중간 문장은 문법적으로 올바르고 주제 및 문장 구조의 일관성을 유지했습니다. (Figure 4.3)
  * **천문학**: 먼 은하의 관측 시뮬레이션에 VAE를 적용하여 약한 중력 렌즈 효과로 인한 전단(shearing)을 탐지하는 시스템을 보정하는 데 기여했습니다.
  * **이미지 (재)합성**: VAE는 이미지의 잠재 공간에서 의미론적으로 의미 있는 변형을 가능하게 합니다. 예를 들어, 잠재 공간에서 "미소 벡터"를 따라 이미지를 변형하여 표정(행복, 슬픔)을 조작할 수 있음을 시연했습니다. (Figure 4.4)
* **IAF의 효과**: IAF는 장난감 데이터셋에서 근사 사후 분포의 유연성을 크게 개선하고, 실제 사후 분포와 사전 분포 간의 적합성을 높였습니다. `free bits` 기법과 결합될 때 벤치마크 데이터셋에서 주변 가능도를 크게 향상시켰습니다.

## 🧠 Insights & Discussion

* **ELBO의 이중 역할**: ELBO를 최대화하는 것은 단순히 주변 가능도의 하한을 높이는 것을 넘어, 생성 모델의 데이터 생성 능력($\log p_{\theta}(x)$)과 근사 추론 모델의 사후 분포 근사 능력($D_{KL}(q_{\phi}(z|x)||p_{\theta}(z|x))$)을 동시에 개선하는 역할을 합니다. 이는 VAE의 학습이 원리적으로 건전함을 보여줍니다.
* **상각 추론의 효율성**: 전통적인 변분 추론 방식과 달리, VAE의 상각 추론은 파라미터를 데이터 포인트 간에 공유함으로써 대규모 데이터셋에서도 효율적인 SGD 기반 학습을 가능하게 합니다.
* **재매개변수화 트릭의 중요성**: 이 트릭은 확률적 샘플링 과정을 미분 가능하게 만들어 심층 신경망을 통한 역전파를 가능하게 합니다. 이는 VAE가 심층 생성 모델을 학습하는 데 핵심적인 역할을 하며, 기존 Score function estimator가 가진 높은 분산 문제를 해결합니다.
* **최적화 문제와 해결책**: 수정되지 않은 ELBO는 훈련 초기에 $q(z|x) \approx p(z)$가 되는 바람직하지 않은 안정된 균형에 빠져 생성 모델이 흐릿한 샘플을 생성하는 문제(posterior collapse 또는 KL vanishing)가 발생할 수 있습니다. 이를 해결하기 위해 KL 항의 가중치를 점진적으로 증가시키는 `annealing`이나, 잠재 변수당 최소 정보량을 인코딩하도록 강제하는 `free bits`와 같은 기법이 제안되었습니다.
* **추론 모델의 유연성 향상 필요성**: ELBO 최적화는 $D_{KL}(q_{D,\phi}(x,z)||p_{\theta}(x,z))$를 최소화하는 것과 동등합니다. 완벽한 모델 적합이 불가능할 경우 $p_{\theta}$가 $q_{D,\phi}$보다 더 큰 분산을 가지는 경향이 있어 흐릿한 샘플을 생성할 수 있습니다. 이를 완화하기 위해서는 IAF와 같은 유연한 추론 모델이 필수적입니다.
* **모델 표현력의 시너지**: 잠재 변수, 자기회귀 모델, 가역 변환(정규화 흐름)의 세 가지 접근 방식을 결합하는 것이 테스트 데이터의 로그-가능도 측면에서 가장 우수한 모델을 구축하는 데 효과적입니다.
* **향후 연구 방향**: NICE 및 Real NVP와 같이 계산 비용이 저렴한 가역 변환을 VAE 프레임워크에 적용하여 강력한 사후 분포, 사전 분포 및 디코더를 가진 간단한 VAE를 구축하는 것이 유망한 연구 방향으로 제시됩니다. 이는 순수 자기회귀 모델과 경쟁하거나 능가하면서 훨씬 빠른 합성을 가능하게 할 수 있습니다.

## 📌 TL;DR

Variational Autoencoder (VAE)는 심층 잠재 변수 모델과 그에 상응하는 추론 모델을 효율적으로 학습하기 위한 원리적인 프레임워크입니다. 이 모델은 데이터의 주변 가능도를 직접적으로 최적화하기 어렵다는 문제를 해결하기 위해, 주변 가능도의 하한인 ELBO(Evidence Lower Bound)를 최대화합니다. 핵심 기여는 `재매개변수화 트릭(reparameterization trick)`을 통해 잠재 변수 샘플링을 통한 역전파를 가능하게 하여, 생성 모델($p_{\theta}$)과 추론 모델($q_{\phi}$)의 파라미터를 동시에 확률적 경사 하강법으로 학습하는 것입니다. 또한, IAF(Inverse Autoregressive Flow)와 같은 `정규화 흐름(Normalizing Flow)`을 도입하여 추론 모델의 유연성을 크게 향상시켰습니다. VAE는 반지도 학습, 데이터 시각화, 인공 창의성 등 다양한 응용 분야에서 강력한 성능을 보여줍니다.
