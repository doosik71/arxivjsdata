# An Introduction to Autoencoders

Umberto Michelucci

## 🧩 Problem to Solve

신경망은 일반적으로 레이블이 있는 지도 학습 환경에서 사용됩니다. 그러나 레이블이 없는 대량의 관측 데이터만 존재할 경우, 이러한 데이터로부터 유용하고 "정보성 있는" 표현을 어떻게 학습하고 추출할 수 있는지가 주요 문제입니다. 특히, 입력 데이터를 가능한 한 가장 낮은 오류로 재구성하여 데이터의 본질적인 특성을 파악하는 방법을 탐색하는 것이 이 논문의 핵심 목표입니다.

## ✨ Key Contributions

- **오토인코더의 기본 개념 및 아키텍처 정의**: 인코더, 잠재 특성 표현, 디코더로 구성된 오토인코더의 핵심 구조를 설명합니다.
- **"정보성 있는" 데이터 표현 학습 강조**: 단순히 입력 데이터를 재구성하는 것을 넘어, 데이터의 유용하고 의미 있는 잠재 표현을 학습하는 것이 중요함을 역설합니다.
- **재구성 오류 및 학습 메커니즘 설명**: 오토인코더가 입력과 출력 간의 차이를 최소화하는 방식으로 학습하며, 재구성 오류가 성능 지표로 사용됨을 명확히 합니다.
- **항등 함수 학습 방지 기법 제시**: 오토인코더가 의미 없이 항등 함수를 학습하는 것을 방지하기 위해 병목(bottleneck) 구조와 정규화(regularization) 기법의 필요성을 소개합니다.
- **출력 계층 활성화 함수 및 손실 함수 선택 가이드**: ReLU, Sigmoid 활성화 함수와 MSE, BCE 손실 함수의 적절한 사용 조건 및 수학적 근거를 제시합니다.
- **다양한 오토인코더 응용 분야 소개**: 차원 축소, 분류, 이상 감지, 노이즈 제거 등 오토인코더의 실용적인 활용 사례를 구체적인 예시와 함께 설명합니다.
- **PCA와의 관계 및 컨볼루션 오토인코더 언급**: 특정 조건 하에서 오토인코더가 PCA와 등가임을 밝히고, 피드포워드 네트워크 외에 컨볼루션 계층을 활용한 오토인코더의 효율성을 소개합니다.

## 📎 Related Works

- Rumelhart, D.E., Hinton, G.E. Williams, R.J.: Parallel distributed processing: Explorations in the microstructure of cognition, Vol. 1. Chap. Learning Internal Representations by Error Propagation, pp. 318-162, MIT Press, Cambridge, MA, USA (1986). (오토인코더의 최초 소개)
- Bank, D., Koenigstein, N., and Giryes, R., "Autoencoders", arXiv e-prints, 2020. (오토인코더 정의에 대한 참고 문헌)
- Baldi, P., Hornik, K.: Neural networks and principal component analysis: Learning from examples without local minima, Neural Netw. 2(1), 53-58 (1989). (선형 오토인코더가 PCA와 같음을 제시)
- Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental pca. In Advances in neural information processing systems, pages 3174–3182, 2013. (PCA 관련 참고 문헌)
- Vincent, P., Larochelle, H. Bengio, Y. Manzagol, P.A.: Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pp. 1096-1103. ACM, New York, NY USA (2008). (노이즈 제거 오토인코더 관련 참고 문헌)

## 🛠️ Methodology

오토인코더는 입력 데이터 $x_i$를 재구성하는 것을 목표로 하는 비지도 학습 신경망입니다. 이는 데이터의 "정보성 있는" 잠재 표현 $h_i$를 학습하는 데 중점을 둡니다.

1. **일반적인 아키텍처**:
   - **인코더(Encoder)**: 입력 $x_i \in \mathbb{R}^n$를 더 낮은 차원의 잠재 특성 $h_i \in \mathbb{R}^q$로 매핑하는 함수 $g(\cdot)$입니다. ($h_i = g(x_i)$)
   - **잠재 특성 표현(Latent Feature Representation)**: 인코더의 출력으로, 입력의 압축된 형태를 나타냅니다.
   - **디코더(Decoder)**: 잠재 특성 $h_i$를 사용하여 원래 입력과 유사한 재구성된 출력 $\tilde{x}_i \in \mathbb{R}^n$를 생성하는 함수 $f(\cdot)$입니다. ($\tilde{x}_i = f(h_i) = f(g(x_i))$)
2. **Feed-Forward Autoencoder (FFA) 구조**:
   - 가장 일반적인 형태로, 인코더와 디코더가 조밀 계층(dense layers)으로 구성됩니다.
   - 전형적으로 홀수 개의 계층을 가지며, 중간 계층(잠재 특성 계층)의 뉴런 수가 가장 적어 **병목(bottleneck)** 역할을 합니다 ($q < n$).
   - 중간 계층 이후의 계층은 중간 계층 이전의 계층을 미러링하는 대칭적인 구조를 가집니다.
3. **훈련 목표**:
   - 입력 $x_i$와 재구성된 출력 $\tilde{x}_i$ 간의 차이($\Delta$)를 최소화하는 인코더 $g(\cdot)$와 디코더 $f(\cdot)$의 파라미터를 찾는 것입니다.
   - $$ \argmin\_{f,g} \left< \left[ \Delta(x_i, f(g(x_i))) \right] \right> $$
   - 여기서 $\Delta$는 손실 함수이며, $\left<\cdot\right>$는 모든 관측값에 대한 평균을 나타냅니다.
4. **정규화 (Regularization)**:
   - 오토인코더가 단순히 항등 함수를 학습하는 것을 방지하고 잠재 표현에 유용한 희소성(sparsity)을 부여하는 데 사용됩니다.
   - **병목 구조**: 잠재 특성의 차원을 입력보다 훨씬 작게 하여 모델이 가장 중요한 정보만 인코딩하도록 강제합니다.
   - **$L_1$ 또는 $L_2$ 정규화**: 손실 함수에 가중치 파라미터 $\theta_i$에 대한 $L_1$ 또는 $L_2$ 페널티 항($\lambda \sum_i \theta_i^2$)을 추가합니다.
   - **가중치 묶기(Tying Weights)**: 인코더와 디코더의 가중치를 공유하여 파라미터 수를 줄이고 일반화를 개선합니다.
5. **출력 계층 활성화 함수**:
   - **ReLU**: 입력 $x_i$가 넓은 범위의 양수 값을 가질 때 적합합니다. ($ReLU(x) = \max(0, x)$)
   - **Sigmoid**: 입력 $x_i$가 $[0, 1]$ 범위로 정규화되었을 때 적합합니다. ($\sigma(x) = \frac{1}{1 + e^{-x}}$)
6. **손실 함수**:
   - **평균 제곱 오차 (Mean Squared Error, MSE)**: 대부분의 경우에 사용할 수 있는 일반적인 손실 함수로, 출력 레이어의 활성화 함수나 입력 데이터의 정규화 방식에 크게 구애받지 않습니다.
     $$ L*{MSE} = \frac{1}{M} \sum*{i=1}^M |x_i - \tilde{x}\_i|^2 $$
   - **이진 교차 엔트로피 (Binary Cross-Entropy, BCE)**: 출력 레이어 활성화 함수가 Sigmoid이고 입력 특성이 $[0, 1]$ 사이로 정규화된 경우에 사용됩니다.
     $$ L*{CE} = -\frac{1}{M} \sum*{i=1}^M \sum*{j=1}^n \left[ x*{j,i} \log \tilde{x}_{j,i} + (1 - x_{j,i}) \log(1 - \tilde{x}\_{j,i}) \right] $$

## 📊 Results

- **손글씨 숫자 재구성 (MNIST 데이터셋)**:
  - 784개 픽셀(특성)을 가진 MNIST 이미지를 오토인코더를 통해 재구성했습니다.
  - (784, 16, 784) 아키텍처는 원본 이미지를 알아보는 데 충분한 수준으로 재구성했습니다.
  - (784, 64, 784) 아키텍처는 훨씬 더 나은 재구성 품질을 보여주었습니다.
  - (784, 8, 784) 아키텍처는 너무 극단적인 병목으로 인해 재구성 오류가 커져 일부 숫자가 잘못 복원되는(예: '4'가 '9'로, '2'가 '3'으로) 현상을 보였습니다.
  - MSE와 BCE 손실 함수 모두 유사한 재구성 성능을 보여주었습니다.
- **잠재 특성을 활용한 분류 (MNIST 및 Fashion MNIST 데이터셋)**:
  - **MNIST**: 원본 784개 특성을 사용한 kNN 분류는 96.4% 정확도에 약 1000초가 소요되었지만, 8개 잠재 특성을 사용한 분류는 89% 정확도에 1.1초만 소요되어 실행 시간이 1000배 단축되었습니다.
  - **Fashion MNIST**: 원본 784개 특성으로 kNN 분류 시 85.4% 정확도에 약 1040초가 소요된 반면, 16개 잠재 특성을 사용한 분류는 83.6% 정확도에 3초만 소요되어 정확도는 1.8% 감소했지만 실행 시간은 330배 단축되었습니다.
  - 이는 차원 축소를 통해 분류 작업의 효율성을 크게 향상시킬 수 있음을 입증합니다.
- **이상 감지 (MNIST 및 Fashion MNIST 결합)**:
  - MNIST 숫자 이미지로 훈련된 오토인코더에 Fashion MNIST의 신발 이미지를 포함한 테스트 세트를 입력했을 때, 신발 이미지가 가장 높은 재구성 오류(RE)를 기록했습니다 (0.062).
  - 두 번째로 높은 RE는 0.022로, 신발 이미지의 RE보다 훨씬 낮았으며, 해당 이미지는 불완전한 숫자 이미지였습니다.
  - 오토인코더가 학습 데이터 분포에서 벗어나는 이질적인 데이터를 이상치로 효과적으로 식별할 수 있음을 보여주었습니다.
- **노이즈 제거 (Denoising Autoencoders)**:
  - 가우시안 노이즈가 추가된 MNIST 이미지를 입력으로, 원본 이미지를 출력으로 훈련된 오토인코더는 노이즈를 성공적으로 제거하고 깨끗한 이미지를 복원했습니다.
- **컨볼루션 오토인코더 (CA)의 성능**:
  - 동일한 파라미터로 훈련된 FFA와 비교했을 때, 컨볼루션 오토인코더(CA)는 이미지 재구성에서 더 나은(더 선명한) 결과를 보여주었습니다.

## 🧠 Insights & Discussion

- **데이터의 본질적 표현 학습**: 오토인코더는 단순히 입력을 복사하는 것이 아니라, 데이터를 효율적으로 재구성하는 데 필요한 핵심적인 "정보성 있는" 잠재 표현을 학습합니다. 이는 마치 인간이 숫자의 픽셀 값 하나하나가 아니라 쓰이는 방식(선, 각도 등)을 학습하는 것과 유사합니다.
- **차원 축소의 강력한 이점**:
  - **연산 효율성**: PCA와 달리 오토인코더는 미니배치 훈련이 가능하여 대규모 데이터셋에 대한 차원 축소를 매우 효율적으로 수행할 수 있습니다.
  - **비선형 변환**: 오토인코더는 PCA와 달리 비선형 특징 변환을 학습하여 복잡한 데이터 구조에서 더 유연하고 효과적인 차원 축소를 제공합니다.
  - **차원의 저주(Curse of Dimensionality) 완화**: 고차원 데이터에서 발생하는 데이터 희소성 문제를 해결하여, 분류나 클러스터링과 같은 후속 작업의 성능을 향상시키고 훈련에 필요한 데이터 양을 줄이는 데 기여합니다.
- **PCA와의 관계**: 특정 조건(선형 인코더/디코더, MSE 손실, 정규화된 입력) 하에서는 피드포워드 오토인코더가 PCA와 수학적으로 동등하다는 흥미로운 결과가 있습니다.
- **이상 감지의 실용성과 한계**:
  - 오토인코더는 정상 패턴을 학습하여 이상치에 대해 높은 재구성 오류를 보이는 원리를 통해 이상 감지에 효과적으로 활용될 수 있습니다. (예: 사기 거래 탐지, 산업 장비 고장 진단)
  - 하지만 이러한 방법은 과적합 및 결과의 불안정성(다른 아키텍처에서 다른 이상치를 탐지)에 취약할 수 있습니다. 이를 완화하기 위해 여러 모델 훈련 후 재구성 오류를 평균내거나 앙상블 기법을 사용하는 것이 중요합니다.
  - 훈련 데이터셋에 이상치가 미미하게 포함되어 있어도 오토인코더가 정상 패턴을 학습하는 데 큰 영향을 주지 않는다는 가정이 전제됩니다.
- **아키텍처 유연성**: 피드포워드 네트워크 외에도 컨볼루션 계층을 활용한 오토인코더(CA)는 이미지와 같은 공간적 구조를 가진 데이터에서 더욱 뛰어난 성능을 발휘할 수 있습니다. 잠재 표현이 텐서 형태 또는 1차원 배열 형태가 될 수 있는 등 문제에 따라 최적의 아키텍처를 찾아야 합니다.

## 📌 TL;DR

오토인코더는 **레이블 없는 데이터에서 유용한 잠재 표현을 학습하고 입력 데이터를 재구성**하는 비지도 학습 신경망입니다. **인코더-병목(latent representation)-디코더** 구조를 통해 입력의 차원을 효율적으로 줄여 핵심 특징을 추출하며, **MSE 또는 BCE 손실 함수와 적절한 활성화 함수**를 사용하여 훈련됩니다. 오토인코더는 대규모 데이터셋의 **차원 축소**를 통한 효율적인 분류 및 차원의 저주 극복, 재구성 오류를 활용한 **이상 감지**, 그리고 손상된 데이터의 **노이즈 제거** 등 다양한 분야에서 효과적으로 활용될 수 있습니다.
