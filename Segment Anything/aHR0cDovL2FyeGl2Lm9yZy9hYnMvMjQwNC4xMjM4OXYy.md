# Moving Object Segmentation: All You Need Is SAM (and Flow)

Junyu Xie, Charig Yang, Weidi Xie, and Andrew Zisserman

## 🧩 Problem to Solve

이 논문은 비디오에서 움직이는 객체를 탐지하고 분할하는, 즉 움직이는 객체 분할(Motion Segmentation) 문제를 다룹니다. 기존의 다양한 접근 방식에도 불구하고, 최근 이미지 분할 분야의 획기적인 발전인 Segment Anything Model (SAM)이 이 비디오 객체 분할 작업에 어떻게 기여할 수 있는지 탐구하는 것이 주된 질문입니다. SAM의 강력한 분할 능력과 광학 흐름(optical flow)의 움직이는 객체 탐지 및 그룹화 능력을 결합하는 방법을 모색합니다.

## ✨ Key Contributions

이 논문의 주요 기여는 다음과 같습니다.

* **FlowI-SAM 모델 개발:** 광학 흐름을 3채널 입력 이미지로 직접 사용하여 정확한 프레임별(per-frame) 분할 및 움직이는 객체 식별을 수행합니다.
* **FlowP-SAM 모델 제안:** RGB 이미지와 광학 흐름을 결합하여, 광학 흐름이 프롬프트(prompt)를 생성하여 SAM이 RGB 이미지에서 움직이는 객체를 식별하고 지역화하도록 안내하는 새로운 모델을 제시합니다.
* **최첨단 성능 달성:** DAVIS16, DAVIS17-m, YTVOS18-m, MoCA를 포함한 다양한 움직이는 객체 분할 벤치마크에서 기존의 모든 비지도 비디오 객체 분할(unsupervised video object segmentation) 방법론들을 상당한 차이로 능가하는 최첨단 성능을 달성합니다.

## 📎 Related Works

* **비디오 객체 분할 (VOS):** 비디오 시퀀스 내 주요 객체(들)를 분할하는 광범위하게 연구된 분야입니다. 단일 객체 및 다중 객체 시나리오를 위한 다양한 벤치마크(DAVIS, YouTubeVOS)가 있으며, 비지도(unsupervised) 및 반지도(semi-supervised) VOS 프로토콜로 나뉩니다. 본 논문은 비지도 VOS에 초점을 맞춥니다.
* **움직임 분할 (Motion Segmentation):** 객체의 움직임을 통해 객체를 탐지하고 해당 분할 마스크를 생성하는 데 중점을 둡니다. "흐름-전용(flow-only)" 접근법(예: [26, 27, 51])과 "RGB 기반(RGB-based)" 접근법(예: [2, 11, 25, 34, 48, 53])으로 나뉘며, 본 논문은 두 가지 설정을 모두 탐구합니다.
* **Segment Anything Model (SAM):** SA-1B 데이터셋으로 훈련된 대규모 이미지 분할 모델로, 제로샷(zero-shot) 일반화 능력이 뛰어납니다. SAM을 다양한 작업(추적, 변화 탐지, 3D 분할)에 적용하려는 시도가 많았지만, 본 논문은 광학 흐름을 프롬프트로 활용하는 새로운 방식을 탐구합니다.

## 🛠️ Methodology

이 논문은 SAM과 광학 흐름을 결합하는 두 가지 프레임 수준(frame-level) 방법과 한 가지 시퀀스 수준(sequence-level) 마스크 연결 방법을 제안합니다.

1. **프레임 수준 분할 I: 흐름을 입력으로 ($FlowI-SAM$)**
    * **입력:** 광학 흐름 $F_t \in \mathbb{R}^{H \times W \times 3}$를 직접 3채널 이미지 입력으로 사용합니다. 여러 프레임 간격(예: $(1,-1), (2,-2)$)의 흐름 입력을 독립적으로 처리한 후 평균하여 공간 특징(spatial features)을 얻습니다.
    * **SAM 적응:** 사전 훈련된 SAM 모델의 마스크 디코더만 미세 조정(finetune)하고, 이미지 인코더와 프롬프트 인코더는 고정합니다. 원래의 IoU 예측 헤드를 전경 객체 IoU (fIoU) 예측을 위해 용도 변경합니다.
    * **추론:** 균일한 그리드 상의 점 프롬프트를 사용하여 모든 움직이는 객체를 탐지하고, 예측된 fIoU 및 오버랩 비율을 기반으로 비최대 억제(NMS)를 통해 최종 분할을 선택합니다.
    * **손실 함수:** 마스크에 대한 이진 교차 엔트로피($L_{BCE}$)와 fIoU 예측에 대한 $L_2$ 손실을 사용합니다.
        $$ L_{FlowI-SAM} = \frac{1}{NT} \sum_{i,t}^{N,T} \left( L_{BCE}(M_t^i, \hat{M}_t^i) + \lambda_f \|s_{fIoU,t}^i - \hat{s}_{fIoU,t}^i \|_2^2 \right) $$

2. **프레임 수준 분할 II: 흐름을 프롬프트로 ($FlowP-SAM$)**
    * **입력:** RGB 프레임 $I_t$를 입력으로 받고, 광학 흐름 $F_t$는 프롬프트 역할을 합니다.
    * **구성:**
        * **흐름 프롬프트 생성기 (Flow Prompt Generator):** 광학 흐름을 입력으로 받아 흐름 프롬프트(flow prompts)와 움직이는 객체 점수(Moving Object Score, MOS)를 생성합니다. SAM의 이미지 인코더는 고정된 채 광학 흐름에서 특징을 추출하고, 흐름 트랜스포머(Flow Transformer)를 통해 점 프롬프트 및 학습 가능한 흐름 프롬프트와 상호작용하여 정제된 흐름 프롬프트와 MOS를 출력합니다.
        * **분할 모듈 (Segmentation Module):** 원래 SAM과 유사한 구조를 가지지만, IoU 예측 헤드를 fIoU 예측으로 용도 변경하고, 흐름 프롬프트 생성기에서 나온 토큰들을 추가 쿼리 입력으로 주입합니다.
    * **추론:** 균일한 그리드 상의 점 프롬프트를 사용하여 마스크, MOS, fIoU를 예측합니다. MOS와 fIoU의 평균값을 사용하여 NMS 및 마스크 오버레이를 통해 최종 분할을 수행합니다.
    * **손실 함수:** 마스크에 대한 $L_{BCE}$, MOS 예측에 대한 $L_{BCE}$, fIoU 예측에 대한 $L_2$ 손실을 포함합니다.
        $$ L_{FlowP-SAM} = \frac{1}{NT} \sum_{i,t}^{N,T} \left( L_{BCE}(M_t^i, \hat{M}_t^i) + \lambda_m L_{BCE}(s_{MOS,t}^i, \hat{s}_{MOS,t}^i) + \lambda_f \|s_{fIoU,t}^i - \hat{s}_{fIoU,t}^i \|_2^2 \right) $$

3. **시퀀스 수준 마스크 연결 (Sequence-level Mask Association)**
    * **목표:** 각 움직이는 객체의 프레임별 예측을 시퀀스 전체에 걸쳐 일관된 트랙으로 연결하여 객체 ID를 유지합니다.
    * **업데이트 메커니즘:** 이전 프레임의 시퀀스 수준 마스크 $M_{t-1}^i$를 광학 흐름을 사용하여 현재 프레임 $t$로 워프($M_{t \leftarrow t-1}^i = \text{warp}(M_{t-1}^i, F_{t-1})$)합니다.
    * 워프된 마스크, 현재 프레임의 프레임 수준 예측, 그리고 이웃 프레임의 정렬된 프레임 수준 예측 세 가지를 고려합니다. 각 마스크 세트 쌍에 대해 IoU 점수를 기반으로 헝가리안 매칭(Hungarian matching)을 수행하여 일관성 점수(consistency score) $\bar{c}^i$를 계산합니다.
    * 이 일관성 점수를 기준으로 현재 프레임의 시퀀스 수준 마스크 $M_t^i$를 업데이트할지 (현재 프레임의 예측 사용) 또는 이전 프레임의 워프된 마스크를 전파할지(propagation) 결정합니다.
        $$ M_t^i = \begin{cases} M_t^i & \text{if } \bar{c}^i \ge 0.5 \\ M_{t \leftarrow t-1}^i & \text{if } \bar{c}^i < 0.5 \end{cases} $$
    * 이 과정을 각 객체에 대해 독립적으로 수행한 후, 모든 객체를 원래 순서로 다시 쌓고 겹치는 부분을 제거하여 최종 시퀀스 수준 예측을 얻습니다.

## 📊 Results

* **프레임 수준 성능:**
  * **흐름-전용(Flow-only) 분할:** FlowI-SAM(ViT-B 및 ViT-H 사용)은 기존 방법들을 $10\%$ 이상 크게 능가하며 최첨단 성능을 달성했습니다.
  * **RGB 기반 분할:** FlowP-SAM 또한 특히 다중 객체 벤치마크에서 뛰어난 최첨단 성능을 보였습니다.
  * **결합 효과:** FlowI-SAM과 FlowP-SAM의 프레임 수준 예측을 결합($FlowP-SAM + FlowI-SAM$)했을 때 추가적인 성능 향상을 관찰했으며, 이는 흐름 및 RGB 모달리티의 보완적인 역할을 시사합니다.

* **시퀀스 수준 성능:**
  * **흐름 기반 분할:** FlowI-SAM(seq)는 OCLR-real보다 우수한 성능을 보였으며, 이는 사전 훈련된 SAM의 강력한 사전 지식 덕분입니다.
  * **RGB 기반 분할:** FlowI-SAM + FlowP-SAM(seq)는 단일 및 다중 객체 벤치마크 모두에서 뛰어난 성능을 달성하여 DAVIS16, DAVIS17-m, YTVOS18-m, MoCA에서 기존 방법들을 능가했습니다.

* **정성적 시각화:**
  * FlowI-SAM은 노이즈가 많은 광학 흐름 배경(예: 오리)에서 움직이는 객체를 정확하게 식별하고 분리하며, 섬세한 구조(예: 위장된 곤충)를 추출했습니다.
  * FlowI-SAM + FlowP-SAM(seq)은 이전 RGB 기반 방법론들이 객체를 식별하지 못하거나(예: 표범 누락) 여러 객체를 구분하지 못하는 문제(예: 얽힌 금붕어)를 해결하며 정확한 움직이는 객체 지역화 및 분할을 제공했습니다.

* **Ablation Study:**
  * FlowP-SAM의 경우, 흐름 프롬프트 주입 및 MOS와 fIoU를 추가적인 후처리 가이드로 사용하는 것이 성능 향상에 크게 기여했습니다.
  * SAM의 기본 IoU 예측 대신 제안된 MOS + fIoU 점수를 사용하면 마스크의 "객체성(objectness)"을 더 정확하게 평가하여 마스크 선택을 개선할 수 있음을 확인했습니다.
  * 시퀀스 수준 마스크 연결에서 제안된 시간적 일관성(temporal consistency) 방법이 객체 지속성을 유지하는 데 크게 도움이 됨을 입증했습니다.

## 🧠 Insights & Discussion

* **두 가지 접근 방식의 보완성:**
  * **FlowI-SAM:** 주로 움직임이 지배적이거나 RGB 정보가 혼란을 줄 수 있는 시나리오(예: 움직이는 객체 탐지, 위장 객체 탐지)에서 특히 효과적입니다. 광학 흐름의 단순한 텍스처와 낮은 도메인 간 격차 덕분에 일상 비디오를 넘어 다양한 도메인으로 일반화됩니다.
  * **FlowP-SAM:** RGB와 움직임 정보가 모두 유용하여 독립적인 객체에 대한 모호성을 해결하는 데 사용될 수 있는 일반적인 비디오에 중점을 둡니다. FlowI-SAM의 예측을 FlowP-SAM 뒤에 레이어링하여, FlowP-SAM이 놓쳤을 수 있는 예측(모션 블러, 낮은 조명, 작은 객체 등)을 보완할 수 있습니다.
* **객체성 평가의 중요성:** SAM의 기본 IoU 예측은 과분할 문제를 야기할 수 있지만, 제안된 MOS(움직이는 객체 점수)와 fIoU(전경 객체 IoU)는 마스크의 "객체성"을 효과적으로 평가하여 움직이는 전경 객체만 정확하게 필터링하는 데 기여합니다.
* **한계점:** 현재 작업의 주요 한계는 SAM의 계산적으로 무거운 이미지 인코더로 인한 긴 실행 시간입니다. 하지만 이 접근 방식은 다른 프롬프트 기반 분할 모델에도 적용 가능하며, 더 효율적인 SAM 버전의 출현으로 추론 시간이 크게 단축될 것으로 예상됩니다.
* **DAVIS17 데이터셋에 대한 논의:** DAVIS17 데이터셋은 객체 정의의 모호성과 클래스 기반 주석(class-based annotations)으로 인해 비지도 VOS 평가에 적합하지 않을 수 있습니다. 움직임 정보와의 불일치 또한 모델에 혼란을 줄 수 있으며, 이 때문에 본 논문에서는 움직임 기반 객체성을 명확하게 정의하는 DAVIS17-m 및 YTVOS18-m을 주로 사용합니다.

## 📌 TL;DR

이 논문은 비디오에서 움직이는 객체 분할을 위해 Segment Anything Model (SAM)과 광학 흐름(optical flow)을 효과적으로 결합하는 두 가지 단순하면서도 강력한 방법을 제안합니다. 첫째, **FlowI-SAM**은 광학 흐름을 SAM의 직접적인 입력으로 사용하여 움직이는 객체를 분할합니다. 둘째, **FlowP-SAM**은 RGB 이미지를 SAM에 입력하고, 광학 흐름 정보를 프롬프트로 활용하여 움직이는 객체 식별을 안내합니다. 이 두 프레임 수준 방법은 MOS(움직이는 객체 점수)와 fIoU(전경 객체 IoU)를 통해 "객체성"을 정량화하여 SAM의 과분할 문제를 해결합니다. 또한, 이전 프레임의 마스크를 워프하고 현재 프레임의 예측과 비교하여 시간적 일관성을 바탕으로 객체 ID를 유지하는 시퀀스 수준 마스크 연결 메커니즘을 개발했습니다. 이 단순한 방법들은 기존의 모든 움직이는 객체 분할 접근 방식을 크게 능가하며 최첨단 성능을 달성했으나, SAM의 무거운 이미지 인코더로 인해 실행 시간이 길다는 한계가 있습니다.
