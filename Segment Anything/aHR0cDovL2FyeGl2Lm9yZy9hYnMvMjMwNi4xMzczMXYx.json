{
  "url": "http://arxiv.org/abs/2306.13731v1",
  "title": "How to Efficiently Adapt Large Segmentation Model(SAM) to Medical Images",
  "authors": "Xinrong Hu, Xiaowei Xu, Yiyu Shi",
  "year": 2023,
  "abstract": "The emerging scale segmentation model, Segment Anything (SAM), exhibits\nimpressive capabilities in zero-shot segmentation for natural images. However,\nwhen applied to medical images, SAM suffers from noticeable performance drop.\nTo make SAM a real ``foundation model\" for the computer vision community, it is\ncritical to find an efficient way to customize SAM for medical image dataset.\nIn this work, we propose to freeze SAM encoder and finetune a lightweight\ntask-specific prediction head, as most of weights in SAM are contributed by the\nencoder. In addition, SAM is a promptable model, while prompt is not\nnecessarily available in all application cases, and precise prompts for\nmultiple class segmentation are also time-consuming. Therefore, we explore\nthree types of prompt-free prediction heads in this work, include ViT, CNN, and\nlinear layers. For ViT head, we remove the prompt tokens in the mask decoder of\nSAM, which is named AutoSAM. AutoSAM can also generate masks for different\nclasses with one single inference after modification. To evaluate the\nlabel-efficiency of our finetuning method, we compare the results of these\nthree prediction heads on a public medical image segmentation dataset with\nlimited labeled data. Experiments demonstrate that finetuning SAM significantly\nimproves its performance on medical image dataset, even with just one labeled\nvolume. Moreover, AutoSAM and CNN prediction head also has better segmentation\naccuracy than training from scratch and self-supervised learning approaches\nwhen there is a shortage of annotations.",
  "citation": 86
}