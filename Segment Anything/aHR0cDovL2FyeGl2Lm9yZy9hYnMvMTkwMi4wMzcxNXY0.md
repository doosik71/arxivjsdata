# Towards Segmenting Anything That Moves

Achal Dave, Pavel Tokmakov, Deva Ramanan

## 🧩 Problem to Solve

이 논문은 고정된 객체 카테고리에 갇히지 않고, 동영상 스트림 내의 **모든 움직이는 객체 인스턴스**를 카테고리에 상관없이, 심지어 이전에 학습되지 않은(never-before-seen) 객체까지도 강건하게 감지하고 분할하는 문제를 다룹니다. 기존의 폐쇄형(closed-world) 객체 인식 시스템은 학습 데이터셋에 없는 객체에는 취약하며, 고전적인 시공간 그룹핑(spatio-temporal grouping) 방법은 학습 기반 접근 방식의 발전 혜택을 받지 못했습니다.

## ✨ Key Contributions

* **최초의 딥러닝 기반 시공간 그룹핑 방법 제안**: 움직임(motion) 및 외형(appearance) 특징을 융합하는 2-스트림(two-stream) Mask R-CNN 아키텍처를 제시합니다.
* **새로운 평가 지표 및 대규모 벤치마크 도입**: 기존 FBMS 벤치마크의 한계(오탐지(false positive)를 패널티하지 않음)를 극복하기 위한 새로운 평가 지표와 DAVIS-Moving, YTVOS-Moving이라는 두 개의 새로운 벤치마크를 제안하여 연구 발전을 도모합니다.
* **최첨단 성능 달성**: FBMS 데이터셋 및 제안된 새로운 벤치마크에서 기존의 모든 방법론을 능가하는 최첨단 성능을 달성합니다. 특히 학습되지 않은 카테고리에서도 우수한 일반화 성능을 보여줍니다.

## 📎 Related Works

* **시공간 그룹핑 (Spatio-temporal grouping)**: 객체의 움직임을 기반으로 분할 및 추적하는 고전적인 접근 방식([35], [11, 6, 20, 27]). 픽셀 궤적을 추적하고 움직임 유사성에 따라 클러스터링합니다. [9]는 CNN을 사용하여 움직이는 객체를 감지하고 픽셀 궤적과 결합하며, [5], [45]는 학습 기반 접근 방식을 도입합니다.
* **전경/배경 비디오 분할 (Foreground/Background Video Segmentation)**: 움직이는 객체를 배경과 분리하는 이진 분할 작업에 초점을 맞춥니다. 초기 접근 방식은 옵티컬 플로우(optical flow) 필드의 휴리스틱스에 의존했으며([8, 29, 42, 22, 26, 4]), 최근에는 DAVIS 2016([31]) 벤치마크 이후 딥러닝 모델([18, 39, 40])이 등장했습니다. 하지만 이들은 개별 인스턴스 분할을 수행하지 않습니다.
* **객체 탐지 (Object Detection)**: COCO([23])와 같은 대규모 데이터셋 덕분에 정지 영상에서 객체 인스턴스를 분할하는 데 큰 성공을 거두었습니다. 하지만 이들은 고정된 카테고리 목록에만 집중하여 학습 세트에 없는 객체에는 맹점입니다. [15]는 새로운 카테고리로 일반화를 시도하지만 바운딩 박스 주석이 필요하며, [19]는 이미지 내의 모든 "객체 유사(object-like)" 영역을 이진 마스크로 출력하는 '객체성(objectness)' 개념을 다룹니다.

## 🛠️ Methodology

이 논문은 RGB 프레임과 해당 옵티컬 플로우를 입력으로 받아 모든 움직이는 객체를 분할하는 **2-스트림 시공간 그룹핑 방법**을 제안합니다.

1. **2-스트림 아키텍처 (Two-Stream Architecture)**:
    * **외형 스트림 (Appearance Stream)**: RGB 이미지를 처리하여 객체의 외형 특징을 추출합니다.
    * **움직임 스트림 (Motion Stream)**: 옵티컬 플로우를 처리하여 객체의 움직임 특징을 추출합니다.
    * 두 스트림의 특징은 **새롭게 설계된 영역 제안 네트워크 (Region Proposal Network, RPN)**에서 융합되어 카테고리에 상관없이 움직이는 객체를 감지하고 분할합니다. 기존 Mask R-CNN 아키텍처의 특징 추출, 영역 제안, 박스/마스크 회귀 단계를 활용합니다.

2. **단계별 학습 전략 (Stagewise Training Strategy)**: 적절한 학습 데이터 부족 문제를 해결하기 위해 다양한 데이터 소스를 활용하는 단계별 학습을 수행합니다.
    * **움직임 스트림 학습**: FlyingThings3D([25])와 같은 합성 데이터셋을 사용하여 움직이는 객체 인스턴스를 학습합니다. 이 스트림은 객체 카테고리에 구애받지 않고 유사하게 움직이는 부분들을 그룹화하는 데 특화됩니다. 추론 시 실제와 유사한 노이즈가 있는 추정 플로우(estimated flow)를 사용하기 위해, 처음에는 Groundtruth 플로우로 학습하고 이후 FlowNet2 플로우로 미세 조정하는 전략을 사용합니다.
    * **외형 스트림 학습**: MS COCO([23]) 데이터셋을 사용하여 이미지 기반 객체 분할 모델을 학습합니다. 80개 카테고리를 단일 "객체(object)" 카테고리로 묶어 학습하는 **"객체성(objectness)" Mask R-CNN** 방식을 채택하여 미학습 카테고리에 대한 일반화 성능을 향상시킵니다.
    * **합동 모델 학습 (Joint Model Training)**: DAVIS 2016 및 YTVOS-Moving 데이터셋의 서브셋으로 두 스트림을 결합한 모델을 미세 조정합니다. 이때, 외형 스트림과 움직임 스트림의 가중치를 고정하여 각 스트림의 일반화 능력을 유지하는 전략이 성능 향상에 중요함을 발견합니다.

3. **추적 (Tracking)**:
    * 간단한 **겹침 기반 추적기(overlap-based tracker)**를 사용하여 시간 경과에 따른 객체 정체성을 유지하고 객체가 움직임을 멈춘 후에도 계속 분할할 수 있도록 합니다.
    * 정지 객체(static objects) 추적을 위해, 2-스트림 모델과 병렬로 외형 스트림에서 학습된 '객체성' 모델을 활용하여 효율적으로 움직이는 객체와 정지 객체를 모두 예측합니다.

## 📊 Results

* **FBMS 벤치마크**: 제안된 방법은 기존의 모든 방법에 비해 월등히 우수한 성능을 보여주었습니다.
  * **공식 FBMS 지표(오탐지 미패널티)**: 외형 스트림(Ours-A)만으로도 기존 SOTA를 6.4% (TestSet F-measure) 능가했습니다. 전체 합동 모델(Ours-J)도 좋은 성능을 보였습니다.
  * **제안된 지표(오탐지 패널티)**: 합동 모델(Ours-J)이 TestSet F-measure에서 기존 SOTA 대비 11.3%의 큰 폭으로 성능을 향상시켰습니다. 외형 스트림만으로는 오탐지 패널티로 인해 성능이 크게 하락했습니다.
* **DAVIS-Moving 벤치마크**: 기존 FBMS SOTA 방법([20])이 42.3% F-measure를 달성한 반면, 제안된 방법은 77.9%로 훨씬 큰 성능 격차를 보였습니다. 이는 DAVIS 17의 더 빠르고 복잡한 움직임 및 고해상도 비디오에 대한 강점을 시사합니다.
* **YTVOS-Moving 벤치마크**: 학습되지 않은 다양한 객체(문어, 뱀 등)를 포함하는 YTVOS-Moving 데이터셋에서, Mask R-CNN(53.6%) 대비 외형 스트림(Ours-A, 69.0%)만으로도 크게 개선되었으며, 최종 합동 모델(Ours-J)은 전체적으로는 유사하지만, 특히 **미학습 객체(novel objects)**에 대해서는 움직임 스트림의 통합으로 완만한 성능 향상을 보였습니다.
* **정성적 결과**: 과분할(over-segmentation)이나 미분할(under-segmentation) 문제를 겪는 기존 방법과 달리, 움직임과 외형 정보를 융합하여 움직이는 객체의 전체 범위를 정확하게 분할함을 확인했습니다.

## 🧠 Insights & Discussion

* **미학습 객체에 대한 일반화**: 학습 기반 접근 방식이 이전에 보지 못한 객체에도 효과적으로 일반화될 수 있음을 보여줍니다. 특히 외형 스트림의 "객체성" 학습과 움직임 스트림의 제네릭 그룹핑 능력이 결합될 때 이러한 일반화 능력이 극대화됩니다.
* **합성 데이터의 유용성**: 합성 데이터(FlyingThings3D([25]))가 실제 세계 객체에 대한 사전 지식 없이도 진정으로 제네릭한 그룹핑 방법을 훈련하는 데 사용될 수 있다는 것을 입증했습니다. 이는 실제 주석 데이터의 부족 문제를 완화하는 데 중요한 통찰입니다.
* **움직임 및 외형 융합의 중요성**: 외형 스트림만으로는 정지 객체까지 탐지하여 오탐지 패널티가 있는 지표에서 성능이 저하될 수 있지만, 움직임 스트림과의 융합을 통해 오직 움직이는 객체만 정확하게 분할할 수 있음을 보여주었습니다.
* **제한 사항 및 실패 사례**: 가장 흔한 실패 사례는 추적 실패(예: 완전한 가림으로 인한 정체성 교환)와 카메라 근처의 정지 객체를 움직이는 객체로 오분류하는 경우입니다. 이는 움직임 신호의 노이즈나 학습 모델의 특정 편향 때문일 수 있습니다.

## 📌 TL;DR

이 논문은 고전적인 시공간 그룹핑 문제에 딥러닝을 적용하여 **모든 움직이는 객체를 카테고리에 상관없이 분할**하는 2-스트림 접근법을 제안합니다. 합성 데이터로 훈련된 움직임 스트림과 '객체성'으로 훈련된 외형 스트림을 Mask R-CNN 기반으로 융합하고 간단한 추적기를 사용합니다. 이 방법은 FBMS 벤치마크에서 SOTA를 달성했으며, DAVIS-Moving 및 YTVOS-Moving이라는 새로운 벤치마크에서도 우수한 성능을 보이며, 특히 **미학습 카테고리에 대한 강한 일반화 능력**을 입증했습니다. 또한 오탐지를 패널티하는 새로운 평가 지표를 제안하여 연구의 방향을 제시합니다.
