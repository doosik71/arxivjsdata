{
  "url": "http://arxiv.org/abs/2404.00650v2",
  "title": "Deep Instruction Tuning for Segment Anything Model",
  "authors": "Xiaorui Huang, Gen Luo, Chaoyang Zhu, Bo Tong, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji",
  "year": 2024,
  "abstract": "Recently, Segment Anything Model (SAM) has become a research hotspot in the\nfields of multimedia and computer vision, which exhibits powerful yet versatile\ncapabilities on various (un) conditional image segmentation tasks. Although SAM\ncan support different types of segmentation prompts, we note that, compared to\npoint- and box-guided segmentations, it performs much worse on text-instructed\ntasks, e.g., referring image segmentation (RIS). In this paper, we argue that\ndeep text instruction tuning is key to mitigate such shortcoming caused by the\nshallow fusion scheme in its default light-weight mask decoder. To address this\nissue, we propose two simple yet effective deep instruction tuning (DIT)\nmethods for SAM, one is end-to-end and the other is layer-wise. With minimal\nmodifications, DITs can directly transform the image encoder of SAM as a\nstand-alone vision-language learner in contrast to building another deep fusion\nbranch, maximizing the benefit of its superior segmentation capability.\nExtensive experiments on three highly competitive benchmark datasets of RIS\nshow that a simple end-to-end DIT can improve SAM by a large margin, while the\nlayer-wise DIT can further boost the performance to state-of-the-art with much\nless data and training expenditures. Our code is released at:\nhttps://github.com/wysnzzzz/DIT.",
  "citation": 5
}