# Segment Anything without Supervision

XuDong Wang, Jingfeng Yang, Trevor Darrell

## 🧩 Problem to Solve

최근 소개된 Segment Anything Model(SAM)은 다양한 이미지 분할 작업에서 뛰어난 능력을 보이지만, 방대한 양의 훈련 데이터에 대한 노동 집약적인 수동 어노테이션(이미지당 20분 이상 소요)이 필요하다는 한계를 가진다. 이는 SAM이 대규모 언어 모델(LLM)과 같은 스케일링 법칙을 따르기 어렵게 하며, 어노테이터의 인식에 따른 편향을 야기하여 이미지 내 작은 개체를 간과하게 만들 수 있다. 본 연구는 이러한 문제에 대응하여 "감독 없이 모든 것을 분할할 수 있는가?"라는 핵심 질문에 답하고자 한다.

## ✨ Key Contributions

* **UnSAM (Unsupervised SAM) 개발**: 사람의 어노테이션 없이 프롬프트 기반 및 자동 전체 이미지 분할을 수행할 수 있는 혁신적인 비지도 학습 방법론을 제시합니다.
* **Divide-and-Conquer 전략**: 시각적 장면의 계층적 구조를 "발견"하기 위한 새로운 의사 마스크(pseudo-mask) 생성 파이프라인을 도입합니다. 이는 상향식(top-down) 클러스터링으로 인스턴스/의미론적 수준 세그먼트를 추출하고, 하향식(bottom-up) 클러스터링으로 미세한 부분까지 계층적 마스크를 형성합니다.
* **경쟁력 있는 성능**: 7개의 인기 데이터셋에서 지도 학습 기반 SAM과 유사한 성능을 달성하며, 비지도 분할 분야의 이전 SOTA(State-of-the-Art) 모델을 AR(Average Recall) 면에서 11% 초과 달성합니다.
* **지도 학습 SAM 개선**: UnSAM이 생성한 비지도 의사 마스크를 SA-1B의 실제 마스크와 통합하여 학습시킨 UnSAM+는 SAM의 AR을 6.7% 이상, AP를 3.9% 초과 달성합니다. UnSAM 및 UnSAM+는 SAM이 놓치는 개체(예: 사람 귀, 동물 꼬리)를 자주 발견하는 능력을 보여줍니다.
* **적은 데이터 및 경량 모델**: SAM이 사용하는 데이터의 1%만으로, SAM보다 4배 적은 파라미터의 백본을 사용하여 경쟁력 있는 결과를 달성하며, 효율성과 경량성을 입증합니다.

## 📎 Related Works

* **자기 지도 이미지 분할**: `TokenCut` [44], `LOST` [32]는 자기 지도 Vision Transformer(ViT)의 클래스 어텐션 메커니즘을 활용하여 객체를 발견합니다. `CutLER` [39]는 MaskCut을 사용하여 고품질 의사 마스크를 생성하고 손실 드롭 전략으로 디텍터를 학습하며, `VideoCutLER` [40]로 확장되었습니다. `SOHES` [6]는 글로벌-로컬 자기 탐색(global-local self-exploration) 방법을 통해 이미지 피처를 클러스터링하여 계층적 의사 마스크를 얻습니다.
  * **UnSAM과의 차별점**: 기존 방법들이 상향식 클러스터링에만 의존하거나 제한된 계층 구조를 제공하는 반면, UnSAM은 `divide-and-conquer` 파이프라인으로 더 많은 의사 마스크를 생성하고, 더 풍부한 계층 구조와 미세한 디테일을 포착합니다.
* **프롬프트 기반 이미지 분할**: `SAM` [21]은 점, 텍스트, 바운딩 박스와 같은 사용자 입력에 기반하여 분할 마스크를 생성합니다. `Semantic-SAM` [23]은 클릭 포인트당 여러 수준의 마스크와 의미론적 레이블을 생성합니다.
  * **UnSAM과의 차별점**: 이 모델들은 대규모 지도 학습 데이터에 의존하는 반면, UnSAM은 비지도 학습 및 경량 준지도 학습(UnSAM+)으로 프롬프트 기반 분할 작업에서 우수한 성능을 보여, 완전히 지도 학습된 접근 방식에 대한 강력한 대안을 제공합니다.

## 🛠️ Methodology

UnSAM은 감독 없이 시각적 장면의 계층적 구조를 반영하는 의사 마스크를 생성하는 'divide-and-conquer' 파이프라인으로 시작합니다.

* **의사 마스크 생성 (Divide-and-Conquer 파이프라인)**:
    1. **Divide Stage (상향식 클러스터링)**:
        * Normalized Cuts(NCuts) 기반의 `CutLER` [39, 31] 방법을 사용하여 초기 의미론적 및 인스턴스 수준 마스크를 추출합니다.
        * 신뢰도 임계값 $\tau = 0.3$ 미만의 마스크는 노이즈를 줄이기 위해 필터링합니다.
    2. **Conquer Stage (하향식 클러스터링)**:
        * 'Divide Stage'에서 발견된 각 마스크에 대해 반복적 병합(iterative merging) [1,6]을 사용하여 거친 마스크를 더 단순한 부분으로 분해하여 계층적 구조를 형성합니다.
        * `DINO` 사전 학습된 `ViT-B/8` [8] 인코더 $f(\cdot)$를 사용하여 패치별 피처 $k_i = f(p_i)$를 추출합니다.
        * 미리 정의된 코사인 유사도 임계값 $\theta \in \{\theta_1, ..., \theta_l\}$ (예: $[0.6, 0.5, 0.4, 0.3, 0.2, 0.1]$)을 사용하여 유사한 패치들을 반복적으로 병합합니다.
        * 병합된 클러스터는 새로운 부분 수준 의사 마스크가 되며, 이 단계에서 생성된 마스크들은 계층적 구조 $H = \{S_0, S_1, ..., S_t, ..., S_l\}$를 이룹니다.
    3. **마스크 병합 및 정제**:
        * 'Conquer Stage'에서 발견된 새로운 부분 수준 의사 마스크를 'Divide Stage'의 마스크에 추가합니다.
        * `NMS (Non-Maximum Suppression)`를 사용하여 중복을 제거합니다.
        * `CRF (Conditional Random Fields)` [22] 및 `CascadePSP` [10]와 같은 마스크 정제 방법을 사용하여 마스크 가장자리를 정교화합니다.
        * 정제 전후 IoU(Intersection-over-Union)에 큰 차이가 있는 마스크는 필터링합니다.

* **모델 학습 및 자기 훈련 (Model Learning and Self-Training)**:
  * 생성된 의사 마스크는 노이즈가 있을 수 있으므로, 자기 훈련 전략을 사용하여 UnSAM의 모델 성능을 향상시킵니다.
  * **전체 이미지 분할**: `Masked Attention Mask Transformer (Mask2Former)` [9]를 기반 모델로 사용하며, 마스크 예측 손실 $L = \lambda_{\text{ce}}L_{\text{ce}} + \lambda_{\text{dice}}L_{\text{dice}}$로 훈련합니다.
    * 한 번의 자기 훈련 후, 높은 신뢰도($\tau_{\text{self-train}} = 0.7$)의 마스크 예측을 새로운 '실제' 어노테이션으로 병합하여 두 번째 자기 훈련을 수행합니다.
  * **프롬프트 기반 분할**: `Semantic-SAM` [23]을 기반 모델로 사용하여 단일 클릭에서 여러 세분화 수준의 마스크를 예측합니다. 마스크 내부 원에서 무작위로 점을 샘플링하여 사용자 클릭을 시뮬레이션합니다.

* **UnSAM+: 비지도 분할로 지도 학습 SAM 개선**:
  * `SA-1B`의 실제 마스크 $D_{\text{SA-1B}}$와 `UnSAM`의 비지도 분할 마스크 $D_{\text{UnSAM}}$를 `IoU` 기반으로 병합하여 훈련 데이터 $D_{\text{UnSAM+}}$를 구성합니다.
  * 병합 조건: $C_m \in D_{i,\text{UnSAM}}$의 최대 IoU가 $\tau_{\text{UnSAM+}} = 0.02$ 이하일 때만 $C_m$을 추가합니다. 이 전략은 사람 어노테이션이 놓치는 세부 개체들을 보완하여 학습 데이터의 다양성과 포괄성을 높입니다.

## 📊 Results

* **비지도 의사 마스크**: `divide-and-conquer` 파이프라인이 생성한 의사 마스크는 이전 SOTA 방법들(`CutLER`, `U2Seg`, `SOHES`) 대비 SA-1B 1000개 검증 이미지에서 AR 23.9%를 달성하여 45.7% 향상된 성능을 보입니다. 또한 `SA-1B`의 실제 레이블이 놓치는 미세한 부분까지 효과적으로 포착합니다 (Fig. 3 참조).
* **전체 이미지 분할**:
  * `UnSAM`은 7개 평가 데이터셋(`COCO`, `LVIS`, `ADE20K`, `EntitySeg`, `SA-1B`, `PartImageNet`, `PACO`) 전체에서 이전 SOTA를 능가합니다. ResNet-50 백본과 `SA-1B` 데이터의 1%만 사용하여 SOTA 대비 평균 AR 11.0% 향상되었으며, 특히 `PartImageNet` 및 `PACO`에서는 각각 16.6%, 12.6% 향상되었습니다 (Table 1 참조).
  * 지도 학습 `SAM`과 비교했을 때, `UnSAM`의 AR은 불과 1% 차이를 보였고, `PartImageNet` 및 `PACO`에서는 `SAM`을 각각 24.4%, 11.6% 능가했습니다. 이는 인간 어노테이터가 놓치기 쉬운 세부 사항을 `UnSAM`이 잘 발견함을 시사합니다.
  * `UnSAM+` (비지도 의사 마스크와 `SA-1B` 실제 마스크 통합)는 `SAM`의 AR을 6.7% 이상, AP를 3.9% 능가합니다. 특히 작은 개체에 대한 AR은 16.2% 더 높았습니다 (Table 2, 4 참조).
* **프롬프트 기반 분할**:
  * `UnSAM`은 `COCO`에서 40.3% MaxIoU 및 59.5% OracleIoU를 달성했습니다. 이는 `SAM`이 사용하는 데이터의 1%와 4배 적은 파라미터를 가진 백본으로 달성된 결과입니다.
  * `UnSAM+`는 `MaxIoU`와 `OracleIoU` 모두에서 `SAM`을 각각 0.3%, 1.3% 능가했습니다 (Table 5 참조).

## 🧠 Insights & Discussion

* **지도 학습의 한계 극복 및 확장성**: UnSAM은 대규모 수동 어노테이션의 필요성을 제거하여 SAM의 확장성 한계를 해결하고, 인간 어노테이터의 편향 및 세부 개체 누락 문제를 완화합니다.
* **계층적 분할의 중요성**: 인간 시각 시스템에서 영감을 받은 `divide-and-conquer` 전략은 다양한 세분화 수준에서 객체를 효과적으로 파싱하고 분할하는 데 핵심적 역할을 합니다. 이는 특히 미세한 디테일 감지에 강점을 보입니다.
* **의사 마스크의 가치**: 고품질 비지도 의사 마스크는 비지도 분할 모델의 성능을 크게 향상시킬 뿐만 아니라, 지도 학습 데이터셋(`SA-1B`)의 한계를 보완하여 `SAM`과 같은 강력한 지도 모델조차 개선할 수 있음을 입증합니다.
* **경량화 및 효율성**: UnSAM은 더 적은 훈련 데이터와 더 작은 백본 모델로도 SOTA 성능을 달성하여, 컴퓨팅 자원 및 데이터 효율성 측면에서 우수함을 보여줍니다.
* **한계점**:
  * 매우 조밀하고 반복적인 미세한 디테일이 있는 이미지(예: 잎사귀)에서는 일부를 놓치거나 과분할(over-segmentation)하는 경향이 있습니다.
  * 특히 의류의 주름이나 그림자 같은 시각적 정보를 잘못된 엔티티 구분 기준으로 오인할 수 있으며, 이는 인간 어노테이터가 가진 사전 지식의 부재로 인해 발생하는 문제로, 비지도 방법은 여전히 이 간극을 줄여야 합니다.
* **윤리적 고려사항**: `SA-1B` 데이터셋에 포함된 라이선스 이미지와 필터링된 콘텐츠를 사용하며, 지리적 다양성 및 경제적 집단의 불균형이 있을 수 있고, UnSAM/UnSAM+의 후속 사용에서 잠재적 편향이 발생할 수 있습니다.

## 📌 TL;DR

**문제**: 지도 학습 기반 SAM은 대규모 수동 어노테이션에 의존하여 확장성과 인간 편향 문제가 발생합니다. **제안 방법**: UnSAM은 감독 없이 프롬프트 기반 및 자동 전체 이미지 분할을 수행하는 비지도 학습 모델입니다. `divide-and-conquer` 전략(CutLER 기반 상향식 클러스터링 + 반복적 병합 기반 하향식 클러스터링)을 사용하여 시각적 장면의 계층적 구조를 발견하고 고품질의 다중 세분화 의사 마스크를 생성합니다. 이 의사 마스크는 Mask2Former와 Semantic-SAM 기반 모델 훈련에 사용되며, 자기 훈련으로 성능을 향상합니다. **핵심 결과**: UnSAM은 비지도 분할 SOTA 대비 평균 AR 11%를 초과하며, 지도 학습 SAM에 필적하는 성능을 보입니다. 특히 인간 어노테이터가 놓치기 쉬운 세부 개체(PartImageNet, PACO)에서 SAM을 크게 능가합니다. UnSAM+는 UnSAM의 비지도 의사 마스크와 SA-1B 실제 마스크를 통합하여 학습, SAM의 AR을 6.7% 이상, AP를 3.9% 초과 달성하며, 더 작은 백본과 1%의 데이터만으로 SAM보다 우수한 프롬프트 기반 분할 성능을 입증합니다.
