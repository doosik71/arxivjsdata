# Segment Anything Meets Point Tracking

Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu

## 🧩 Problem to Solve

Segment Anything Model (SAM)은 이미지 분할 분야에서 강력한 제로샷(zero-shot) 성능을 보여주며, 점(point) 기반 프롬프트 방식을 통해 효율적인 상호작용 및 주석(annotation)을 가능하게 했습니다. 그러나 기존 비디오 분할 방식은 주로 마스크 주석 및 전파에 초점을 맞추고 있으며, 학습되지 않은 데이터에 대한 제로샷 일반화 능력이 부족합니다. 특히 비디오 도메인에서 점 기반의 상호작용형 분할은 거의 연구되지 않았습니다. 본 논문은 비디오를 위한 상호작용형 분할에서 도메인 일반화 능력과 레이블링 효율성을 동시에 달성하는 것을 목표로 합니다.

## ✨ Key Contributions

* **SAM-PT 제안**: SAM과 장기 점 추적(long-term point tracking)을 결합한 새로운 점 중심(point-centric) 상호작용형 비디오 분할 방법인 SAM-PT를 제시합니다. 이는 비디오 분할 문제에 대한 새로운 관점을 제공합니다.
* **점 전파 전략 도입**: 객체 중심의 마스크 전파 대신 희소(sparse) 점 전파를 활용하여 객체 의미론에 구애받지 않는 지역 구조 정보를 활용합니다.
* **제로샷 일반화 달성**: 비디오 분할 데이터에 대한 별도의 훈련 없이도 뛰어난 제로샷 일반화 성능을 보여줍니다.
* **성능 우위 입증**: DAVIS, YouTube-VOS, BDD100K 등 인기 있는 비디오 객체 분할(VOS) 및 다중 객체 분할 추적(MOTS) 벤치마크에서 기존 제로샷 방법보다 우수한 성능을 달성합니다. 또한 UVO (Unidentified Video Objects) 벤치마크에서는 일부 완전 지도(fully-supervised) VIS(Video Instance Segmentation) 방법보다도 뛰어난 성능을 보입니다.
* **주석 노력 감소**: 새로운 대화형 점 기반 비디오 분할 벤치마크를 통해 SAM-PT가 수동 비디오 레이블링에 필요한 주석 노력을 크게 줄임을 입증합니다.
* **코드 공개**: 다양한 점 추적기 및 비디오 분할 벤치마크를 통합하는 코드를 공개합니다.

## 📎 Related Works

* **비디오 분할을 위한 점 추적**: Lucas-Kanade, SIFT, SURF와 같은 고전적인 방법부터 LIFT, SuperPoint, SuperGlue와 같은 최신 방법들이 희소 특징 추적에 사용되었습니다. PIPS [16], PIPS++ [53], OmniMotion [39], TAPIR [15], CoTracker [19] 등은 강건한 장기 궤적 및 폐색(occlusion) 처리에 최적화되어 있습니다. 본 연구는 이러한 방법을 비디오 분할에 적용하는 독특한 접근 방식을 취합니다.
* **무엇이든 분할 및 추적하는 모델**: SAM [21]은 이미지 분할을 위한 기초 모델이며, HQ-SAM [20]은 마스크 품질을 개선합니다. TAM [46] 및 SAM-Track [11]은 SAM을 비디오 분할로 확장하려 시도했지만, 제로샷 시나리오에서는 부족함을 보였습니다.
* **제로샷 VOS / VIS**: Painter [41]와 SegGPT [42]는 시각적 프롬프팅을 사용하여 비디오 분할을 수행하며, STC [18] 및 DINO [4]는 특징 매칭을 통해 VOS를 수행합니다. SAM-PT는 점 중심 접근 방식을 통해 제로샷 VOS 벤치마크에서 성능을 향상시킵니다.
* **상호작용형 VOS**: MiVOS [8]와 같은 방법들이 마스크 생성과 전파를 분리하여 사용자 상호작용을 통합합니다. SAM-PT는 마스크 전파 대신 점 전파를 사용하는 최초의 제로샷 상호작용형 VOS 방법입니다.

## 🛠️ Methodology

SAM-PT는 SAM의 강점과 점 추적기의 능력을 결합하여 비디오 분할을 위한 제로샷 접근 방식을 제공하며, 다음 네 가지 주요 단계로 구성됩니다.

1. **쿼리 점 선택 (Query Points Selection)**:
    * 첫 프레임에서 대상 객체(양성 점) 또는 배경/비대상 객체(음성 점)를 나타내는 쿼리 점을 정의합니다.
    * 사용자가 수동으로 제공하거나, 첫 프레임의 ground truth 마스크에서 파생될 수 있습니다.
    * 점 샘플링 기법: 무작위 샘플링(Random Sampling), K-Medoids 샘플링 (가장 좋은 결과), Shi-Tomasi 샘플링, 혼합 샘플링(Mixed Sampling)이 사용됩니다. K-Medoids 샘플링은 객체의 다양한 부분에 대한 좋은 커버리지를 보장합니다.

2. **점 추적 (Point Tracking)**:
    * 선택된 쿼리 점을 사용하여 PIPS [16] 및 CoTracker [19]와 같은 강건한 점 추적기를 이용해 모든 비디오 프레임에 걸쳐 점을 전파합니다.
    * 이 과정에서 점 궤적(trajectories)과 폐색 점수(occlusion scores)가 생성됩니다. 장기 점 추적기는 폐색 및 재등장과 같은 문제에 효과적입니다.

3. **분할 (Segmentation)**:
    * 예측된 궤적에서 폐색되지 않은 점들을 SAM의 프롬프트로 활용하여 프레임별 분할 마스크를 예측합니다.
    * **2단계 마스크 디코딩**:
        * **1단계**: 양성 점만으로 SAM을 프롬프트하여 객체의 초기 위치를 정의합니다.
        * **2단계**: 양성 및 음성 점, 그리고 이전 마스크 예측을 함께 SAM에 제공하여 마스크를 세밀하게 조정합니다. 음성 점은 배경과 객체를 더 명확하게 구분하고 잘못 분할된 영역을 제거하는 데 도움을 줍니다.
    * **반복적 정제(Iterative Refinement)**: 두 번째 단계를 여러 번 반복하여 마스크 품질을 더욱 향상시킵니다.

4. **점 추적 재초기화 (Point Tracking Reinitialization) (선택 사항)**:
    * $h=8$ 프레임의 예측 기간에 도달하면, 예측된 마스크를 사용하여 쿼리 점을 재초기화합니다.
    * 기존의 신뢰할 수 없는 점들은 폐기하고, 나중에 보이는 객체 부분에서 새로운 점들을 추가합니다.
    * 이 과정은 전체 비디오가 처리될 때까지 반복적으로 수행되어 추적 정확도를 높입니다.

## 📊 Results

* **비디오 객체 분할 (VOS)**:
  * **DAVIS2017**: HQ-SAM과 CoTracker를 사용하는 SAM-PT는 평균 $J\&F$ 점수 $79.4$를 기록하며 SegGPT ($75.6$) 및 DINO ($71.4$)와 같은 기존 제로샷 방법들을 능가합니다.
  * **DAVIS2016**: $84.3$점을 달성하여 SegGPT ($82.3$)를 넘어섭니다.
  * **YouTube-VOS2018**: 제로샷 방법 중 가장 높은 $G$ 점수 $76.2$를 달성합니다.
  * **BDD100K**: 비일시적 객체(non-transient objects)에 대해 SegGPT보다 우수하며, 대부분의 객체 가시성 지속 시간에서 XMem(완전 지도 방법)을 능가합니다.
  * **MOSE2023**: SegGPT와 경쟁력 있는 성능을 보입니다.
* **비디오 인스턴스 분할 (VIS)**: UVO v1.0 벤치마크에서 SAM-PT는 TAM [46]을 능가합니다. SAM-PT는 비디오 분할 데이터로 훈련되지 않았음에도 불구하고, XMem [7]에 사전 훈련된 TAM보다 우수한 성능을 보여줍니다.
* **대화형 점 기반 비디오 분할**: SAM-PT는 기본 SAM 단독 접근 방식보다 주석 노력을 크게 줄이며, 특히 오프라인 체크포인트 전략을 사용할 때 더욱 효율적입니다.
* **어블레이션 연구 (Ablation Study)**:
  * CoTracker [19]가 가장 우수한 점 추적 성능을 보였습니다.
  * K-Medoids 샘플링은 객체 부분의 좋은 커버리지로 인해 최적의 점 샘플링 방법으로 확인되었습니다.
  * 객체당 8개의 양성 점을 사용하는 것이 1개보다 $33.4$점 향상되었습니다.
  * 음성 점의 도입은 추적기 오류를 보정하여 $1.8$점의 성능 향상을 가져왔습니다.
  * 반복적 정제는 마스크 품질을 $2.2$점 향상시켰습니다.
  * 재초기화 전략은 MOSE 및 UVO와 같은 데이터셋에서 추적기 오류를 복구하는 데 유용했습니다.

## 🧠 Insights & Discussion

SAM-PT는 이미지 분할 파운데이션 모델인 SAM의 강력한 제로샷 일반화 능력을 장기 점 추적과 결합하여 비디오 영역으로 확장하는 데 성공했습니다. 이는 비디오 객체 분할 연구에 새로운 점 중심 관점을 제공하며, 비디오 분할 데이터에 대한 훈련 없이도 뛰어난 제로샷 성능과 효율적인 상호작용을 가능하게 합니다.

하지만 이 방법은 몇 가지 한계를 가집니다. 복잡한 시나리오, 예를 들어 폐색, 작은 객체, 빠른 움직임, 모션 블러, 마스크 예측의 불일치 등에서는 점 추적기의 신뢰성에 의존하기 때문에 어려움을 겪을 수 있습니다. 그럼에도 불구하고, 음성 점 전략과 점 재초기화 같은 기법들이 어느 정도 개선을 제공하며, 사용자 상호작용은 이러한 한계를 극복하고 완전 지도 학습 방법과의 성능 격차를 크게 줄일 수 있음을 입증했습니다. SAM-PT는 고품질 비디오 주석에 필요한 노력을 실질적으로 감소시키며, 실제 적용 가능성을 강조합니다.

## 📌 TL;DR

SAM-PT는 SAM의 제로샷 이미지 분할 능력과 장기 점 추적을 결합하여 비디오를 위한 점 중심의 대화형 분할 방법을 제시합니다. 이 방법은 희소 점 전파를 사용하여 객체 의미론에 구애받지 않는 지역 구조 정보를 활용하며, 비디오 분할 데이터 훈련 없이 뛰어난 제로샷 일반화 성능을 달성합니다. 실험 결과, SAM-PT는 DAVIS, YouTube-VOS, BDD100K와 같은 벤치마크에서 기존 제로샷 방법들을 능가하고, UVO 벤치마크에서는 일부 완전 지도 방법보다 우수한 성능을 보였습니다. 또한, 주석 노력을 크게 줄일 수 있음을 입증하여 실용적인 비디오 주석 도구로서의 잠재력을 보여줍니다.
