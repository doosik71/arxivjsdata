{
  "url": "http://arxiv.org/abs/2308.03726v1",
  "title": "AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene\n  Segmentation",
  "authors": "Jay N. Paranjape, Nithin Gopalakrishnan Nair, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel",
  "year": 2023,
  "abstract": "Segmentation is a fundamental problem in surgical scene analysis using\nartificial intelligence. However, the inherent data scarcity in this domain\nmakes it challenging to adapt traditional segmentation techniques for this\ntask. To tackle this issue, current research employs pretrained models and\nfinetunes them on the given data. Even so, these require training deep networks\nwith millions of parameters every time new data becomes available. A recently\npublished foundation model, Segment-Anything (SAM), generalizes well to a large\nvariety of natural images, hence tackling this challenge to a reasonable\nextent. However, SAM does not generalize well to the medical domain as is\nwithout utilizing a large amount of compute resources for fine-tuning and using\ntask-specific prompts. Moreover, these prompts are in the form of\nbounding-boxes or foreground/background points that need to be annotated\nexplicitly for every image, making this solution increasingly tedious with\nhigher data size. In this work, we propose AdaptiveSAM - an adaptive\nmodification of SAM that can adjust to new datasets quickly and efficiently,\nwhile enabling text-prompted segmentation. For finetuning AdaptiveSAM, we\npropose an approach called bias-tuning that requires a significantly smaller\nnumber of trainable parameters than SAM (less than 2\\%). At the same time,\nAdaptiveSAM requires negligible expert intervention since it uses free-form\ntext as prompt and can segment the object of interest with just the label name\nas prompt. Our experiments show that AdaptiveSAM outperforms current\nstate-of-the-art methods on various medical imaging datasets including surgery,\nultrasound and X-ray. Code is available at\nhttps://github.com/JayParanjape/biastuning",
  "citation": 58
}