{
  "title": "Learning to Prompt Segment Anything Models",
  "authors": "Jiaxing Huang, Kai Jiang, Jingyi Zhang, Han Qiu, Lewei Lu, Shijian Lu, Eric Xing",
  "year": 2024,
  "url": "http://arxiv.org/abs/2401.04651v1",
  "abstract": "Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great\npotential in learning to segment anything. The core design of SAMs lies with\nPromptable Segmentation, which takes a handcrafted prompt as input and returns\nthe expected segmentation mask. SAMs work with two types of prompts including\nspatial prompts (e.g., points) and semantic prompts (e.g., texts), which work\ntogether to prompt SAMs to segment anything on downstream datasets. Despite the\nimportant role of prompts, how to acquire suitable prompts for SAMs is largely\nunder-explored. In this work, we examine the architecture of SAMs and identify\ntwo challenges for learning effective prompts for SAMs. To this end, we propose\nspatial-semantic prompt learning (SSPrompt) that learns effective semantic and\nspatial prompts for better SAMs. Specifically, SSPrompt introduces spatial\nprompt learning and semantic prompt learning, which optimize spatial prompts\nand semantic prompts directly over the embedding space and selectively leverage\nthe knowledge encoded in pre-trained prompt encoders. Extensive experiments\nshow that SSPrompt achieves superior image segmentation performance\nconsistently across multiple widely adopted datasets.",
  "citation": 19
}