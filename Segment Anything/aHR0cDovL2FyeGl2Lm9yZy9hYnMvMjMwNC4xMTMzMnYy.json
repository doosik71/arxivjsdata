{
  "url": "http://arxiv.org/abs/2304.11332v2",
  "title": "Input Augmentation with SAM: Boosting Medical Image Segmentation with\n  Segmentation Foundation Model",
  "authors": "Yizhe Zhang, Tao Zhou, Shuo Wang, Peixian Liang, Danny Z. Chen",
  "year": 2023,
  "abstract": "The Segment Anything Model (SAM) is a recently developed large model for\ngeneral-purpose segmentation for computer vision tasks. SAM was trained using\n11 million images with over 1 billion masks and can produce segmentation\nresults for a wide range of objects in natural scene images. SAM can be viewed\nas a general perception model for segmentation (partitioning images into\nsemantically meaningful regions). Thus, how to utilize such a large foundation\nmodel for medical image segmentation is an emerging research target. This paper\nshows that although SAM does not immediately give high-quality segmentation for\nmedical image data, its generated masks, features, and stability scores are\nuseful for building and training better medical image segmentation models. In\nparticular, we demonstrate how to use SAM to augment image input for\ncommonly-used medical image segmentation models (e.g., U-Net). Experiments on\nthree segmentation tasks show the effectiveness of our proposed SAMAug method.\nThe code is available at \\url{https://github.com/yizhezhang2000/SAMAug}.",
  "citation": 103
}