# Tracking Anything with Decoupled Video Segmentation

Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, Joon-Young Lee

## 🧩 Problem to Solve

기존 비디오 분할(video segmentation) 접근법들은 주석이 달린 비디오 데이터셋에 대해 종단 간(end-to-end) 모델을 훈련합니다. 그러나 이러한 데이터셋은 어노테이션 비용이 비싸고 어휘(object category) 수가 적어, 대규모 어휘 또는 개방형(open-world) 비디오 분할 작업으로 확장하기 어렵습니다. 특정 작업마다 비디오 데이터로 훈련할 필요 없이 "모든 것을 추적"하기 위해, 데이터 부족 환경에서 종단 간 접근법의 한계를 극복하고 외부 데이터를 활용하여 일반화 성능을 높이는 것이 주된 연구 문제입니다.

## ✨ Key Contributions

* **분리형 비디오 분할(DEVA) 제안:** 특정 작업에 대한 이미지 수준 분할과 클래스/작업에 구애받지 않는 양방향 시간 전파를 분리하여 비디오 분할을 수행하는 DEVA(Decoupled Video Segmentation Approach)를 개발했습니다. 이는 이미지 수준 모델만 훈련하면 되므로 비용 효율적이며, 시간 전파 모델은 한 번 훈련하면 여러 작업에 일반화될 수 있습니다.
* **양방향 전파 알고리즘 개발:** 이미지 수준 분할 결과의 노이즈를 제거하고(클립 내 합의, in-clip consensus), 시간 전파 결과와 효과적으로 결합하는 (반)온라인 양방향 전파 알고리즘을 개발했습니다.
* **다양한 데이터 부족 작업에서 우수한 성능 입증:** 대규모 비디오 파노라마 분할(VIPSeg), 개방형 비디오 분할(BURST), 참조 비디오 분할(Ref-YouTubeVOS, Ref-DAVIS), 비지도 비디오 객체 분할(DAVIS-16/17) 등 여러 데이터 부족 환경에서 종단 간 접근법보다 유리한 성능을 달성했습니다.
* **범용 이미지 분할 모델 통합 용이성:** Segment Anything (SAM)과 같은 범용 프롬프터블 이미지 분할 모델을 비디오 분할 파이프라인에 원활하게 통합할 수 있도록 했습니다.

## 📎 Related Works

* **종단 간 비디오 분할:** YouTube-VIS, Cityscape-VPS와 같은 소규모 어휘 데이터셋에서 상당한 발전을 이루었지만, 대규모/개방형 데이터셋에서는 확장성에 한계를 보입니다 (예: MaskProp, Video-K-Net).
* **개방형 비디오 분할:** BURST, OWTB와 같은 접근법은 종종 프레임별 객체 감지를 추적하는 '추적-by-감지(tracking-by-detection)' 패러다임을 사용하며, 이미지 분할 오류에 민감합니다.
* **분리형 비디오 분할 / 추적-by-감지:** 이전 연구들(예: [26, 58, 3])은 이미지 수준 감지를 불변으로 간주하고 시간 모델이 감지된 객체를 연결하는 데만 집중하여 이미지 수준 오류에 취약했습니다.
* **비디오 객체 분할(VOS):** 초기 분할을 비디오 전체에 전파하는 것을 목표로 하며 (예: XMem), 초기 분할의 오류를 처리하거나 나중에 나타나는 새로운 객체를 통합하지 못합니다.
* **통합 비디오 분할:** Video-K-Net, Unicorn, TarViS, UNINEXT 등은 여러 비디오 작업을 위한 통합 프레임워크를 목표로 하지만, 대부분 각 작업에 대한 별도의 종단 간 훈련이 필요하거나 클래스 특정 기능 학습으로 인해 클래스에 구애받지 않는 추적 정확도가 낮습니다.
* **Segmenting/Tracking Anything:** SAM [30]은 범용 이미지 분할의 효과를 보여주며, 후속 연구(예: [12, 68])는 SAM 마스크를 비디오로 전파하지만, 본 연구의 클립 내 합의(in-clip consensus)와 같은 노이즈 제거 기능은 부족합니다.

## 🛠️ Methodology

DEVA는 작업별 이미지 분할 모델과 범용 시간 전파 모델을 분리하여 (반)온라인 양방향 전파 방식으로 비디오를 분할합니다.

1. **분리형 비디오 분할 구조:**
    * **이미지 분할 모델 ($Seg(I)$):** 특정 작업에 훈련된 이미지 수준 모델로, 작업별 이미지 분할 가설을 제공합니다. (예: Mask2Former, EntitySeg, ReferFormer, DIS)
    * **시간 전파 모델 ($Prop(H, I)$):** 클래스에 구애받지 않는 마스크 전파 데이터셋(예: DAVIS, YouTubeVOS, OVIS)으로 훈련된 범용 모델로, 세그멘테이션 가설을 연결하고 전파합니다. XMem [9] 기반 모델을 수정하여 사용했습니다.

2. **양방향 전파 파이프라인 (주로 반-온라인 설정):**
    * **초기화:** 첫 번째 프레임에서 이미지 분할 모델을 사용하여 초기 세그멘테이션을 생성합니다.
    * **클립 내 합의 (In-clip Consensus):**
        * 현재 프레임 $I_t$와 가까운 미래의 $n-1$개 프레임 ($I_{t+1}, ..., I_{t+n-1}$)에 대한 이미지 분할 결과들을 활용하여 현재 프레임의 노이즈 없는 합의 결과 $C_t$를 생성합니다.
        * **공간 정렬 (Spatial Alignment):** 시간 전파 모델 $Prop(\{I_{t+i}, Seg_{t+i}\}, I_t)$을 사용하여 미래 프레임의 세그멘테이션 $Seg_{t+i}$를 현재 프레임 $I_t$에 정렬합니다.
        * **표현 (Representation):** 모든 정렬된 세그멘테이션을 현재 프레임 $I_t$의 '객체 제안(object proposal)' 집합 $P$로 통합합니다.
        * **정수 계획법 (Integer Programming):** 다음 두 가지 기준에 따라 객체 제안을 선택하는 지표 변수 $v$를 최적화합니다.
            * 선택된 제안은 다른 제안들의 지지를 받아야 합니다 (IoU > 0.5일 때 지지). ($Supp_i$)
            * 선택된 제안들끼리 크게 겹치지 않아야 합니다. ($Overlap_{ij} = 0$)
            * 노이즈 제거를 위한 페널티($Penal_i = -\alpha v_i, \alpha=0.5$)를 적용합니다.
    * **시간 전파:** $C_t$는 시간 전파 모델을 통해 이후 프레임으로 전파되어 전파된 세그멘테이션 $R_t = Prop(H, I_t)$를 형성합니다. 여기서 $H$는 과거 프레임으로부터 구성된 기억(memory)입니다.
    * **전파 및 합의 병합 (Merging Propagation and Consensus):** 주기적으로 (예: 5프레임마다), 새로 발견된 이미지 분할 결과($C_t$)를 전파된 결과($R_t$)와 병합하여 최종 세그멘테이션 $M_t$를 생성합니다.
        * **연결 IoU 최대화:** $R_t$와 $C_t$ 간의 쌍별 IoU를 최대화하여 연결 $a_{ij}$를 찾습니다 (최소 IoU 0.5). 연결된 세그먼트는 합쳐지고, 연결되지 않은 세그먼트는 그대로 출력됩니다.
        * **세그먼트 삭제:** $L$회 연속으로 합의 세그먼트와 연결되지 않은 비활성 세그먼트는 메모리에서 삭제됩니다 (실험에서 $L=5$).

## 📊 Results

* **대규모 비디오 파노라마 분할 (VIPSeg):** 동일한 백본을 사용하는 종단 간 접근법(Clip-PanoFCN, Video-K-Net)보다 우수한 VPQ(Video Panoptic Quality)를 달성했으며, 특히 장기 연결이 중요한 큰 $k$ 값에서 성능 향상이 두드러졌습니다. 이미지 모델이 개선될수록 성능이 더욱 향상되었습니다. 훈련 데이터가 적은 경우(특히 희귀 클래스), 종단 간 모델 대비 VPQ가 60% 이상 크게 개선되었습니다.
* **개방형 비디오 분할 (BURST):** 추적-by-감지 방식의 베이스라인(Mask2Former w/ Box tracker/STCN tracker, OWTB)을 능가했습니다. EntitySeg와 같은 이미지 모델을 사용했을 때 새로운 객체에 대한 추적 성능이 뛰어났습니다.
* **참조 비디오 분할 (Ref-DAVIS, Ref-YouTubeVOS):** 기존 접근법(URVOS, ReferFormer, VLT)보다 높은 J&F 점수를 기록했습니다.
* **비지도 비디오 객체 분할 (DAVIS-16/17):** 기존 접근법(RTNet, PMN, UnOVOST, Propose-Reduce)보다 높은 J&F 점수를 달성했습니다.
* **어블레이션 연구:**
  * **훈련 데이터 변화:** 분리형 접근 방식은 희귀 클래스에서 훈련 데이터가 적을 때 Video-K-Net에 비해 훨씬 높은 상대적 VPQ 개선을 보였습니다.
  * **클립 내 합의:** 클립 크기($n=3$), 병합 빈도(5프레임마다), 공간 정렬이 성능과 속도 균형에 중요함이 입증되었습니다.
  * **시간 전파 방식:** 마스크 IoU, 광학 흐름(optical flow) 이용, 쿼리 연결, 'ShortTrack', 'TrustImageSeg' 등 다른 전파 방식보다 제안된 양방향 전파 방식이 가장 뛰어난 성능을 보였습니다.

## 🧠 Insights & Discussion

* 이 연구는 비디오 분할에서 데이터 부족 문제를 해결하기 위한 효과적인 전략으로 '분리형 접근 방식'의 강력함을 입증했습니다. 작업별 이미지 분할과 범용 시간 전파의 분리를 통해, 각 모듈을 독립적으로 최적화하고 외부 데이터를 활용할 수 있게 하여 일반화 성능을 크게 향상시켰습니다.
* 특히, '클립 내 합의'와 '양방향 병합' 메커니즘은 단순한 추적-by-감지 방식의 한계를 넘어 이미지 분할 오류를 효과적으로 제거하고 시간적 일관성을 확보하는 데 핵심적인 역할을 했습니다. 이는 장기적인 객체 추적에서 기존 종단 간 모델보다 뛰어난 성능을 보여주는 원동력이 됩니다.
* DEVA 프레임워크는 SAM과 같은 최신 이미지 분할 모델을 손쉽게 통합할 수 있어, 이미지 분할 기술의 발전 혜택을 직접적으로 받을 수 있는 유연한 구조를 제공합니다.
* 제한 사항으로는 새로운 객체의 지연된 감지(시간 전파 모델이 스스로 새로운 객체를 감지하지 못하므로, 이미지 모델이 이를 발견하고 병합될 때까지 기다려야 함)와, 데이터가 충분한 소규모 어휘 환경에서는 여전히 종단 간 모델이 더 나은 성능을 보인다는 점이 있습니다.
* 그럼에도 불구하고, DEVA는 대규모 어휘 및 개방형 비디오 분할을 위한 강력한 초기 단계이자 미래 연구를 위한 유망한 기준점(baseline)을 제시합니다.

## 📌 TL;DR

**문제:** 비싼 비디오 어노테이션 데이터로 인해 종단 간 비디오 분할 모델은 대규모 어휘 및 개방형 환경으로 확장하기 어렵습니다.

**해결책:** DEVA(Decoupled Video Segmentation Approach)는 작업별 이미지 분할 모델(예: SAM)과 범용, 클래스에 구애받지 않는 시간 전파 모델(수정된 XMem)을 분리합니다. '클립 내 합의'를 통해 이미지 마스크의 노이즈를 제거하고, 전파된 마스크와 '양방향 병합'을 통해 효과적으로 통합하는 파이프라인을 제안합니다.

**결과:** DEVA는 대규모 비디오 파노라마, 개방형, 참조, 비지도 비디오 객체 분할과 같은 데이터 부족 환경에서 최첨단 또는 경쟁력 있는 성능을 달성하여, 제한된 데이터로도 우수한 일반화 능력을 입증했습니다.
