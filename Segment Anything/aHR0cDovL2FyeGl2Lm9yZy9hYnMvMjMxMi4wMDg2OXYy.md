# Segment and Caption Anything

Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu

## 🧩 Problem to Solve

Segment Anything Model (SAM)은 이미지 내 모든 것을 분할하는 뛰어난 일반화 능력을 보여주지만, 학습 데이터에 의미론적 레이블이 없어 이미지의 의미를 이해하는 데 한계가 있습니다. 특히, 특정 이미지 **영역에 대한 캡션(regional caption)**을 생성하는 능력은 부족합니다. 또한, 지역 캡셔닝을 위한 고품질의 대규모 학습 데이터(예: Visual Genome은 10만 개 이미지)가 매우 제한적이라, SAM과 같은 대규모 분할 모델(1100만 개 이미지, 10억 개 마스크)의 능력을 지역 캡셔닝으로 확장하기 어렵습니다. 따라서 본 연구는 대규모 데이터와 컴퓨팅 자원 없이도 SAM에 효율적으로 지역 캡셔닝 능력을 부여하는 방법을 찾는 것을 목표로 합니다.

## ✨ Key Contributions

* **효율적인 SAM 확장:** SAM의 이미지 인코더와 사전 학습된 언어 모델을 고정하고, 경량의 쿼리 기반 특징 믹서(hybrid feature mixture)만을 학습하여 SAM에 지역 캡셔닝 기능을 효율적으로 부여했습니다. 이 믹서는 수천만 개의 파라미터만을 가지므로 빠르고 확장 가능한 학습이 가능합니다.
* **약한 감독 사전 학습 전략:** 지역 캡셔닝 데이터 부족 문제를 해결하기 위해, 객체 탐지 및 분할 데이터셋(Objects365, COCO-Panoptic)을 활용한 '약한 감독 사전 학습(weak supervision pretraining)' 방식을 제안했습니다. 이 방식은 범주 이름만 있는 데이터를 사용하여 모델의 일반화 능력을 향상시킵니다.
* **최고 성능 달성:** Visual Genome 벤치마크에서 CIDEr-D 149.8, METEOR 17.5, SPICE 31.4 등 기존 SOTA(State-of-the-Art) 모델을 능가하는 성능을 달성했습니다.
* **SAM의 암묵적 의미론적 이해 활용:** SAM이 명시적인 의미 레이블 없이도 "인식하는 모든 것"에 대한 마스크를 생성하도록 학습되었기 때문에, 낮은 수준의 분할을 넘어선 고수준의 의미론적 정보를 암묵적으로 내포하고 있음을 발견하고 이를 지역 캡셔닝에 활용했습니다.

## 📎 Related Works

* **객체 탐지, 분할 및 상호작용 분할:** CNN 기반에서 Transformer 기반 모델로 발전했으며, SAM($\text{arXiv:2304.02643}$)과 같은 대규모 상호작용 분할 시스템이 등장했습니다.
* **이미지 캡셔닝 및 Dense 캡셔닝:** 전체 이미지 설명을 생성하는 이미지 캡셔닝($\text{arXiv:2305.06500}$)과 이미지 영역 설명을 생성하는 Dense 캡셔닝($\text{CVPR, 2016}$)으로 나뉩니다. 본 연구는 후자에 중점을 둡니다.
* **영역 이해 시스템 스케일링:** 대규모 데이터셋과 약한 감독, 자기 훈련($\text{arXiv:2306.09683}$) 등을 통해 모델의 일반화 능력을 향상시키는 연구가 진행되고 있습니다.
* **최근 동시 연구:** Large Language Model (LLM)과 SAM을 결합하여 영역 이해를 수행하는 Caption Anything($\text{arXiv:2305.02677}$), GPT4ROI($\text{arXiv:2307.03601}$), Region-BLIP($\text{arXiv:2308.02299}$) 등의 연구가 있습니다.

## 🛠️ Methodology

1. **모델 아키텍처 (SCA):**
    * **이미지 인코더:** 사전 학습된 SAM의 ViT 기반 이미지 인코더를 사용하며, 학습 중 **고정(frozen)**됩니다.
    * **지역 특징 믹서:** SAM의 특징 믹서 위에 경량의 양방향 트랜스포머 기반 '텍스트 특징 믹서'($E_{\text{Cap}}^{\text{R}}$)를 추가합니다. 이 믹서는 SAM의 토큰을 재사용하며, 사용자가 제공한 시각적 프롬프트(점, 박스, 마스크)와 이미지 특징을 융합하여 지역별 특징을 추출합니다. 이 믹서만이 학습 가능한 주된 모듈입니다.
    * **텍스트 디코더:** 사전 학습된 인과적 언어 모델(예: GPT2-large, OpenLLAMA-3B)을 사용하며, 이 역시 학습 중 **고정**됩니다.
2. **지역 특징 추출 및 캡션 생성:**
    * 이미지 인코더가 전역 이미지 특징 $I$를 추출하고, 프롬프트 인코더가 사용자 프롬프트 $P_{\left\{b,p,m\right\}}$를 토큰화합니다.
    * SAM의 마스크 쿼리 토큰 $M$과 새로운 텍스트 쿼리 토큰 $Q$, 프롬프트 토큰 $P$를 텍스트 특징 믹서 $E_{\text{Cap}}^{\text{R}}$에 입력하여 지역별 특징 $\hat{Q}$를 얻습니다.
    * 이 지역 특징 $\hat{Q}$와 학습 가능한 태스크 토큰 $T$를 고정된 텍스트 디코더 $D_{\text{Cap}}$의 입력으로 활용하여 지역 캡션을 생성합니다.
3. **학습 목표:** 다음 토큰 예측에 대한 교차 엔트로피 손실($\text{Cross-Entropy Loss}$)을 최소화하는 방식으로 텍스트 특징 믹서, 캡션 쿼리 토큰, 태스크 토큰을 최적화합니다.
    $$ \mathcal{L} = \frac{1}{N_{\text{T}} + 1} \sum_{k=1}^{N_{\text{T}}+1} \text{CE}(T_k, p(T_k | T, Q, T_{\text{0:k-1}})) $$
4. **약한 감독 사전 학습:**
    * 초기에는 Objects365와 COCO-Panoptic 데이터셋(범주 이름만 포함)을 사용하여 텍스트 특징 믹서를 사전 학습합니다.
    * 이후 Visual Genome 지역 캡셔닝 데이터로 모델을 미세 조정합니다.
    * 과적합 방지 및 성능 향상을 위해 large-scale jittering과 같은 강력한 데이터 증강 기법을 사용합니다.

## 📊 Results

* **벤치마크 성능:** Visual Genome 데이터셋에서 기존 SOTA 모델인 GRiT의 성능을 뛰어넘는 CIDEr-D 149.8, METEOR 17.5, SPICE 31.4를 달성했습니다. 특히 LLAMA-3B를 텍스트 디코더로 사용했을 때 가장 좋은 성능을 보였습니다.
* **효율성 비교:** GRiT는 전체 모델을 학습하여 막대한 자원이 필요했지만, SCA는 경량 특징 믹서만 학습하여 적은 자원으로도 우수한 성능을 달성했습니다. 또한, SAM+Image Captioner 조합은 맥락 정보 손실 및 해상도 불일치 등으로 낮은 성능을 보였습니다.
* **약한 감독 사전 학습 효과:** Objects365와 COCO-Panoptic 데이터셋으로 약한 감독 사전 학습을 수행했을 때, 사전 학습을 하지 않은 경우보다 성능이 크게 향상되었습니다. 특히 사전 학습 데이터셋의 이미지 규모가 클수록 성능 향상 폭이 컸습니다.
* **하이퍼파라미터 영향:** 특징 믹서는 상대적으로 높은 학습률에서 잘 수렴했으며, 대규모 언어 모델(GPT2-large)을 텍스트 디코더로 사용할 경우 디코더를 고정하는 것이 미세 조정하는 것보다 더 좋은 성능을 보였습니다.
* **특징 믹서 크기:** 텍스트 특징 믹서의 레이어 수를 2개에서 12개로 늘렸을 때 성능이 지속적으로 향상되었고, 12개 레이어(19.1M 파라미터)에서 정점을 찍었습니다.
* **특징 믹서 아키텍처:** ROI-Align 기반 믹서보다 쿼리 기반 믹서가 월등히 우수했으며, SAM의 쿼리 토큰을 직접 사용하는 것보다 캡셔닝을 위한 별도 쿼리 토큰을 도입하고 SAM 특징을 재사용하는 방식이 가장 효과적이었습니다.
* **SAM 이미지 인코더 크기:** SAM의 ViT-base, -large, -huge 인코더는 최종 캡셔닝 성능에 큰 차이를 보이지 않았지만, ViT-huge가 약간 더 좋은 성능을 보여 기본으로 사용했습니다.
* **데이터 증강 효과:** Large-scale jittering (LSJ)과 같은 강력한 데이터 증강 기법은 과적합 문제를 완화하고 모델 성능을 향상시키는 데 기여했습니다.

## 🧠 Insights & Discussion

* **SAM의 숨겨진 의미론적 잠재력:** SAM이 비록 낮은 수준의 분할 작업을 위해 학습되었지만, 주석자가 '인식하는 모든 것'을 마스킹하도록 지시받았기 때문에 고수준의 의미론적 개념을 암묵적으로 학습했음을 보여줍니다. 이 연구는 이러한 암묵적 지식을 자연어와 정렬하여 효과적인 지역 캡셔닝이 가능함을 입증했습니다.
* **효율적인 지식 전이:** 대규모 기반 모델(SAM, LLM)의 사전 학습된 지식을 활용하면서도 경량 모듈만 학습하여 계산 효율성을 극대화한 것은 대규모 지역 캡셔닝 데이터셋 구축의 현실적인 어려움을 극복하는 중요한 단계입니다.
* **데이터 스케일의 중요성:** 약한 감독 사전 학습 결과는 클래스 라벨의 다양성보다는 사전 학습에 사용되는 이미지 데이터의 규모가 모델의 일반화 능력에 더 큰 영향을 미친다는 것을 시사하며, 이는 향후 더 큰 데이터셋으로의 사전 학습 가능성을 열어줍니다.
* **한계점:** 현재 모델은 잘못된 속성 예측(예: 색상), 유사한 시각적 개념의 혼동(예: 레몬과 오렌지), 그리고 마스크 예측과의 정렬 부족(전경/배경 캡션 분리)과 같은 한계점을 보입니다. 이러한 문제점은 향후 약한 감독 또는 자기 훈련 기법을 통해 개선될 수 있을 것으로 예상됩니다.

## 📌 TL;DR

이 논문은 SAM(Segment Anything Model)에 지역 캡셔닝 능력을 효율적으로 부여하는 **SCA(Segment and Caption Anything)**를 제안합니다. SAM의 이미지 인코더와 대규모 언어 모델을 고정하고, **경량의 쿼리 기반 특징 믹서**만을 학습시켜 적은 파라미터로 빠르고 확장 가능한 훈련을 가능하게 합니다. 또한, 지역 캡셔닝 데이터 부족 문제를 해결하기 위해 객체 탐지 및 분할 데이터셋을 활용하는 **약한 감독 사전 학습**을 도입했습니다. 그 결과, SCA는 Visual Genome 벤치마크에서 SOTA 성능을 달성하며, SAM이 암묵적으로 가진 고수준의 의미론적 지식을 활용해 효율적으로 지역 캡션을 생성할 수 있음을 입증했습니다.
