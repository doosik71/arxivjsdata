# Track Anything: Segment Anything Meets Videos

Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng

## 🧩 Problem to Solve

최근 이미지 분할에서 뛰어난 성능을 보여준 Segment Anything Model (SAM)은 비디오에서는 시간적 일관성(temporal correspondence) 부족으로 인해 지속적인 객체 분할(consistent segmentation)에 어려움을 겪습니다. 기존 비디오 객체 추적/분할(VOT/VOS) 모델은 대규모 수동 주석 데이터셋으로 학습되며, 초기화를 위해 바운딩 박스나 정확한 마스크를 필요로 하여 많은 인적 노동과 시간 소모를 야기합니다. 따라서 이 연구는 비디오에서 사용자 상호작용을 통해 효율적이고 고성능의 객체 추적 및 분할을 달성하고, 대규모 수동 주석 및 초기화의 부담을 줄이는 것을 목표로 합니다.

## ✨ Key Contributions

* SAM의 적용 범위를 비디오 수준으로 확장하여 대화형 비디오 객체 추적 및 분할을 가능하게 했습니다. 단순히 프레임별로 SAM을 사용하는 것이 아니라, 시간적 일관성을 구축하는 과정에 SAM을 통합했습니다.
* 효율적인 주석 및 사용자 친화적인 추적 인터페이스를 위해 단일 추론(one-pass inference) 방식의 대화형 추적 및 분할 방법을 제안했습니다. 이를 통해 최소한의 인간 참여로 비디오 객체 인식의 극심한 난이도를 해결합니다.
* 제안된 방법은 복잡한 장면에서 뛰어난 성능과 높은 유용성을 보이며, 다양한 잠재적 응용 분야를 가집니다.

## 📎 Related Works

* **Segment Anything Model (SAM)**: 이미지 분할을 위한 대규모 파운데이션 모델로, 다양한 프롬프트에 유연하게 반응하여 실시간으로 마스크를 계산합니다. 뛰어난 이미지 분할 능력과 높은 상호작용성을 가지고 있지만, 비디오의 시간적 일관성 처리에는 한계가 있습니다.
* **XMem**: 첫 프레임의 객체 마스크 설명을 기반으로 후속 프레임에서 객체를 추적하고 마스크를 생성하는 VOS 모델입니다. 긴 비디오의 문제점을 해결하기 위해 통합된 특징 메모리 저장소를 사용하지만, 정확한 마스크 초기화가 필요하고 추적/분할 실패 시 복구가 어렵다는 단점이 있습니다.
* **Interactive Video Object Segmentation (Interactive VOS)**: 스크리블과 같은 사용자 상호작용을 입력으로 받아 반복적으로 분할 결과를 개선합니다. 기존 방법들은 결과 개선을 위해 여러 라운드가 필요하여 효율성이 떨어집니다.

## 🛠️ Methodology

제안된 Track Anything Model (TAM)은 SAM과 XMem을 대화형 방식으로 통합하여 비디오 객체 추적 및 분할을 수행합니다. 다음 네 단계로 구성됩니다:

1. **Step 1: Initialization with SAM**:
    * 사용자는 클릭이나 바운딩 박스와 같은 약한 프롬프트를 통해 SAM을 사용하여 관심 객체의 초기 마스크를 생성합니다. 여러 번의 클릭으로 마스크를 수정하여 만족스러운 초기화를 얻을 수 있습니다.
2. **Step 2: Tracking with XMem**:
    * 초기화된 마스크를 기반으로 XMem이 후속 프레임에서 반지도 학습 VOS를 수행합니다. XMem은 대부분의 간단한 시나리오에서 만족스러운 결과를 제공합니다. 마스크 품질이 좋지 않을 경우, XMem의 예측과 중간 파라미터(예: probes 및 affinities)를 저장하고 3단계로 넘어갑니다.
3. **Step 3: Refinement with SAM**:
    * VOS 모델이 시간이 지남에 따라 마스크가 점차 거칠게 분할되는 경향이 있으므로, XMem의 예측 마스크 품질이 만족스럽지 않을 때 SAM을 사용하여 마스크를 정제합니다.
    * XMem의 probes와 affinities를 SAM의 포인트 프롬프트로 투영하고, 2단계에서 예측된 마스크를 SAM의 마스크 프롬프트로 사용합니다. 이 프롬프트들을 통해 SAM은 정제된 분할 마스크를 생성합니다.
    * 이렇게 정제된 마스크는 XMem의 시간적 일관성에 추가되어 후속 객체 구별을 개선합니다.
4. **Step 4: Correction with human participation**:
    * 극도로 어려운 시나리오, 특히 긴 비디오에서 객체를 정확하게 구별하기 어려울 때 인간의 개입을 통해 마스크를 수정합니다.
    * 사용자는 TAM 프로세스를 강제로 중지하고 긍정/부정 클릭(positive and negative clicks)을 통해 현재 프레임의 마스크를 수정할 수 있습니다.

## 📊 Results

* **정량적 결과**: DAVIS-2016-val 및 DAVIS-2017-test-dev 데이터셋에서 TAM은 각각 88.4와 73.1의 J&F 점수를 달성했습니다. TAM은 클릭으로 초기화되고 단일 추론으로 평가되었습니다. 이는 어려운 복잡한 시나리오에서도 잘 작동함을 보여줍니다.
* **정성적 결과**: 다중 객체 분리, 타겟 변형, 스케일 변화, 카메라 움직임 등을 성공적으로 처리하는 것을 시각적으로 입증했습니다 (Figure 2 참고).
* **실패 사례 분석**:
  * **긴 비디오에서의 장기 메모리 부족**: 기존 VOS 모델은 단기 메모리에 초점을 맞춰 긴 비디오에서 마스크 축소 또는 정제 부족 현상이 발생합니다. SAM의 정제 능력이 기대에 미치지 못할 때 발생하며, 장기 메모리 보존 및 단기 메모리 업데이트 메커니즘 개선의 중요성을 시사합니다.
  * **복잡한 객체 구조**: 자전거 바퀴처럼 복잡하고 정밀한 구조를 가진 객체의 경우, 클릭으로 미세한 초기 마스크를 얻기 어렵습니다. 이는 SAM이 복잡하고 정밀한 구조에서 여전히 어려움을 겪고 있음을 나타냅니다.

## 🧠 Insights & Discussion

* **의미**: TAM은 SAM의 강력한 이미지 분할 능력과 XMem의 효율적인 비디오 추적 능력을 결합하여, 최소한의 사용자 상호작용으로 비디오 객체 추적 및 분할의 효율성과 정확성을 크게 향상시켰습니다. 이는 대규모 수동 주석의 필요성을 줄여 실용적인 응용 분야에 큰 기여를 합니다.
* **한계**: 긴 비디오에서의 장기적인 일관성 유지와 자전거 바퀴와 같이 복잡하고 정밀한 객체 구조에 대한 초기 마스크 생성에서 여전히 개선의 여지가 있습니다. SAM의 다중 프롬프트 기반 정제 능력과 장기 메모리 처리 메커니즘을 더욱 발전시킬 필요가 있습니다.
* **응용 분야**:
  * **효율적인 비디오 주석**: 클릭 기반 상호작용으로 VOS 및 VOT 작업을 위한 비디오 주석 프로세스를 매우 효율적으로 만듭니다.
  * **장기 객체 추적**: 긴 비디오의 샷 변경을 처리하여 실세계 응용에 더 적합한 장기 객체 추적을 지원합니다.
  * **사용자 친화적인 비디오 편집**: TAM이 제공하는 객체 분할 마스크를 사용하여 비디오 내 객체를 제거하거나 변경할 수 있습니다 (예: E$_{2}$FGVI [11]와 결합하여 비디오 인페인팅).
  * **비디오 작업을 위한 시각화된 개발 툴킷**: VOS, VOT, 비디오 인페인팅 등 다양한 비디오 작업을 위한 시각화된 인터페이스를 제공하여 사용자가 모델을 실제 비디오에 적용하고 결과를 즉시 시각화할 수 있도록 돕습니다.

## 📌 TL;DR

본 논문은 SAM의 이미지 분할 능력과 XMem의 비디오 추적 능력을 결합하여 대화형 비디오 객체 추적 및 분할 모델인 Track Anything Model (TAM)을 제안합니다. SAM의 시간적 일관성 부족 문제를 해결하기 위해, 사용자의 최소한의 클릭으로 초기 마스크를 얻고, XMem으로 추적한 후, SAM으로 마스크를 정제하며, 필요한 경우 인간 개입으로 오류를 수정하는 4단계 프로세스를 사용합니다. TAM은 복잡한 비디오 시나리오에서 뛰어난 성능을 보이며, 효율적인 비디오 주석, 장기 객체 추적, 사용자 친화적인 비디오 편집 등 다양한 응용 가능성을 제시합니다.
