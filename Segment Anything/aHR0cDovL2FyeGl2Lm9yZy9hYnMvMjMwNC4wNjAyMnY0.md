# SAM Struggles in Concealed Scenes – Empirical Study on “Segment Anything”

Ge-Peng Ji, Deng-Ping Fan, Peng Xu, Ming-Ming Cheng, Bowen Zhou, Luc Van Gool

## 🧩 Problem to Solve

SAM(Segment Anything Model)은 컴퓨터 비전 분야의 파운데이션 모델로서 다양한 시나리오에서 유망한 분할 능력을 보여주었습니다. 하지만 이 논문은 SAM이 잘 작동하지 않는 특정 상황, 특히 "은폐된 장면(concealed scenes)"에서의 성능 특성을 탐구하고자 합니다. 은폐된 장면은 위장 동물, 산업 결함, 의료 병변과 같이 객체가 배경에 시각적으로 섞여 있어 구별하기 어려운 시나리오를 의미하며, 논문은 SAM이 이러한 장면에서 능숙하지 못함을 밝히는 것을 목표로 합니다.

## ✨ Key Contributions

* **SAM의 은폐 장면 한계점 식별:** SAM이 위장 동물, 산업 결함, 의료 병변과 같은 세 가지 은폐된 장면에서 객체 분할에 심각한 어려움을 겪음을 실증적으로 보여주었습니다.
* **정량적 및 정성적 분석:** 최첨단 위장 객체 분할(COS) 모델과 SAM의 성능을 COD10K, CAMO, NC4K 벤치마크에서 정량적으로 비교하여 SAM의 성능 격차를 수치적으로 입증했습니다. 또한, 다양한 실패 사례를 시각화하여 정성적 분석을 제공했습니다.
* **향후 연구 방향 제시:** SAM의 의미론적 능력 개선, 더 도전적인 훈련 작업의 시도, 선험적 지식(prior knowledge) 주입을 통한 정확도 및 불확실성 문제 해결 등 SAM의 한계를 극복하기 위한 잠재적 해결책과 연구 방향을 제시했습니다.

## 📎 Related Works

* **Segment Anything Model (SAM) [1]:** 논문의 주된 평가 대상인 대규모 ViT 기반 분할 모델.
* **최첨단 위장 객체 분할(COS) 모델:**
  * CamoFormer-P/-S [7]
  * HitNet [8]
* **은폐된 장면 이해 (Concealed Scene Understanding) 관련 연구:**
  * 딥 은폐 장면 이해에 대한 최신 서베이 [2]
  * 은폐 객체 탐지 [3]
* **사용된 데이터셋:**
  * **위장 동물:** CAMO [4], COD10K [5], NC4K [6]
  * **산업 결함:** KolektorSDD [17], MagneticTile [18], MVTecAD [19]
  * **의료 병변:** CVC-300 [20], BraTS2021 [21], COVID-SemiSeg [22], MSD [23]

## 🛠️ Methodology

* **평가 설정:** SAM을 '프롬프트 없는(unprompted)' 설정에서 평가했습니다.
* **마스크 선택 전략:**
  * SAM이 하나의 입력 이미지에 대해 여러 개의 이진 마스크 $\\{P_n\\}_{n=1}^{N}$를 생성할 경우, 해당 이미지의 Ground-Truth(GT) 마스크 $G$와 가장 높은 IoU(Intersection over Union) 점수를 보이는 마스크를 최종 예측으로 선택했습니다.
  * $IoU_n = \frac{P_n \cap G}{P_n \cup G}$
* **데이터셋:** 세 가지 위장 객체 분할 벤치마크 (CAMO, COD10K, NC4K)를 사용했습니다. 또한, 산업 결함 및 의료 영상 데이터셋을 정성적 평가에 활용했습니다.
* **비교 모델:** 공정한 비교를 위해 Transformer 아키텍처를 사용하는 최상위 COS 모델인 CamoFormer-P/-S [7] 및 HitNet [8]을 선택했습니다.
* **평가 지표:**
  * 구조 측정 ($S_{\alpha}$) [9]
  * 향상된 정렬 측정 ($E_{\phi}$) [10] (적응형 $E_{ad}^{\phi}$, 평균 $E_{mn}^{\phi}$, 최대 $E_{mx}^{\phi}$ 값 보고)
  * F-측정 ($F_{\beta}$) [11], [12] (적응형 $F_{ad}^{\beta}$, 평균 $F_{mn}^{\beta}$, 최대 $F_{mx}^{\beta}$ 값 보고)
  * 가중 F-측정 ($F_{w}^{\beta}$) [13]
  * 평균 절대 오차 ($M$)

## 📊 Results

* **정량적 평가:**
  * SAM은 ViT-B에서 ViT-L, ViT-H로 모델 크기가 커질수록 성능이 향상되었으나, 최첨단 COS 모델들과 비교했을 때 여전히 상당한 성능 격차를 보였습니다.
  * 예를 들어, COD10K 데이터셋에서 SAM (ViT-H)의 $E_{mx}^{\phi}$ 점수는 CamoFormer-S [7] 대비 13.8% 낮았고, CAMO 데이터셋에서는 25.6%, NC4K 데이터셋에서는 16.9% 낮았습니다.
  * $F_{w}^{\beta}$ 점수 역시 CAMO에서 ViT-B가 0.238, ViT-L이 0.534, ViT-H가 0.594로 개선되지만, CamoFormer-P의 0.831에는 미치지 못했습니다.
* **정성적 평가 (실패 사례):**
  * **위장 동물:** 나뭇잎에 웅크린 사마귀, 산호 속에 있는 해마 등 배경과 유사한 모양과 색상을 가진 은폐된 동물을 감지하고 분할하는 데 어려움을 겪었습니다.
  * **산업 결함:** 제품의 표면 균열이나 가죽의 찔린 자국 등 텍스처 배경과 결함을 구별하기 어려워했습니다. 이는 SAM이 표준 크기와 높은 대비를 가진 자연물에 훈련되었기 때문으로 분석됩니다.
  * **의료 병변:** 주변 조직과 색상이 유사한 양성 대장 용종과 같은 은폐된 패턴을 잘 처리하지 못했습니다. 또한, MRI 및 CT 스캔의 무정형 병변(암, 혈관, 종양)을 인식하는 데 어려움을 겪었으며, 이는 SAM에 의료 도메인 지식이 부족함을 시사합니다.

## 🧠 Insights & Discussion

* **부분적인 마스크 생성:** SAM은 종종 가려진 객체를 여러 개의 분리된 마스크로 분할하는 경향을 보여, 은폐된 장면에서의 의미론적 기능 개선이 필요함을 나타냅니다.
* **출현(Emergent) 및 추론 능력의 부재:** 자기 지도 학습 기반의 대규모 언어 모델과 달리, 지도 학습으로 훈련된 SAM에서는 실험에서 출현 및 추론 능력이 관찰되지 않았습니다. 더 도전적인 훈련 과제를 통해 성능 향상 가능성을 탐색할 필요가 있습니다.
* **세분화 및 불확실성 병목:** SAM의 세분화(granularity) 및 불확실성(uncertainty)은 자율 주행 및 임상 진단과 같이 높은 정확도를 요구하는 응용 분야에서 한계로 작용합니다. 이러한 문제를 완화하기 위해 선험적 지식(prior knowledge)을 모델에 주입하는 것이 잠재적인 해결책이 될 수 있습니다 (예: 간 종양은 간 내부에 있어야 한다는 구조적 지식).
* **데이터 중심 AI의 힘:** SAM의 성공은 대규모 모델 시대에 데이터 중심 AI의 중요성을 보여주며, 인간 피드백 기반 학습과 대규모 파운데이션 모델이 비전 커뮤니티에 새로운 기회를 제공할 것이라는 시사점을 던집니다.

## 📌 TL;DR

이 논문은 "Segment Anything Model (SAM)"이 위장 동물, 산업 결함, 의료 병변과 같은 **은폐된 장면(concealed scenes)**에서 객체 분할에 **취약함**을 밝히는 실증 연구입니다. 저자들은 SAM을 최첨단 위장 객체 분할 모델과 정량적으로 비교하고, 각 은폐된 시나리오에서 SAM의 실패 사례를 정성적으로 분석하여 **SAM이 은폐된 객체를 식별하는 데 큰 성능 격차와 한계**를 가지고 있음을 입증했습니다. 이는 SAM이 자연 객체 중심의 훈련으로 인해 특수한 도메인 지식이나 미묘한 질감 차이 인식 능력이 부족하기 때문으로 분석되며, 향후 SAM의 정교함과 도메인 특화 능력 개선을 위한 방향을 제시합니다.
