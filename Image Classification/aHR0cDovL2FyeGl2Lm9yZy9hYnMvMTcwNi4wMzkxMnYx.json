{
  "title": "SEP-Nets: Small and Effective Pattern Networks",
  "authors": "Zhe Li, Xiaoyu Wang, Xutao Lv, Tianbao Yang",
  "year": 2017,
  "url": "http://arxiv.org/abs/1706.03912v1",
  "abstract": "While going deeper has been witnessed to improve the performance of\nconvolutional neural networks (CNN), going smaller for CNN has received\nincreasing attention recently due to its attractiveness for mobile/embedded\napplications. It remains an active and important topic how to design a small\nnetwork while retaining the performance of large and deep CNNs (e.g., Inception\nNets, ResNets). Albeit there are already intensive studies on compressing the\nsize of CNNs, the considerable drop of performance is still a key concern in\nmany designs. This paper addresses this concern with several new contributions.\nFirst, we propose a simple yet powerful method for compressing the size of deep\nCNNs based on parameter binarization. The striking difference from most\nprevious work on parameter binarization/quantization lies at different\ntreatments of $1\\times 1$ convolutions and $k\\times k$ convolutions ($k>1$),\nwhere we only binarize $k\\times k$ convolutions into binary patterns. The\nresulting networks are referred to as pattern networks. By doing this, we show\nthat previous deep CNNs such as GoogLeNet and Inception-type Nets can be\ncompressed dramatically with marginal drop in performance. Second, in light of\nthe different functionalities of $1\\times 1$ (data projection/transformation)\nand $k\\times k$ convolutions (pattern extraction), we propose a new block\nstructure codenamed the pattern residual block that adds transformed feature\nmaps generated by $1\\times 1$ convolutions to the pattern feature maps\ngenerated by $k\\times k$ convolutions, based on which we design a small network\nwith $\\sim 1$ million parameters. Combining with our parameter binarization, we\nachieve better performance on ImageNet than using similar sized networks\nincluding recently released Google MobileNets."
}