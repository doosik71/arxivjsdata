# Self-Normalizing Neural Networks

Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter

## 🧩 Problem to Solve

딥러닝은 합성곱 신경망(CNN) 및 순환 신경망(RNN)을 통해 비전 및 자연어 처리 분야에서 혁명적인 성공을 거두었습니다. 그러나 표준 피드포워드 신경망(FNN)의 경우, 다층의 추상 표현을 활용할 수 있는 깊은 구조에서 성공 사례가 드뭅니다. 기존의 배치 정규화(Batch Normalization)와 같은 정규화 기법들은 확률적 경사 하강법(SGD)이나 드롭아웃(dropout)과 같은 교란(perturbations)에 민감하여 훈련 오류의 분산이 높아지고 학습이 불안정해지는 문제가 있었습니다. 이러한 불안정성은 깊은 FNN을 훈련하기 어렵게 만들고, 기울기 소실(vanishing gradients) 및 폭주(exploding gradients) 문제를 야기하여 FNN의 성능 잠재력을 제한했습니다.

## ✨ Key Contributions

- **자체 정규화 신경망(Self-Normalizing Neural Networks, SNNs) 도입:** 뉴런 활성화가 명시적인 정규화 없이도 자동으로 0 평균 및 단위 분산(unit variance)에 수렴하는 새로운 FNN 아키텍처를 제안했습니다.
- **Scaled Exponential Linear Units (SELUs) 활성화 함수 제안:** SNN의 핵심 활성화 함수로, 자체 정규화 특성을 유도하는 매개변수 $\alpha$와 $\lambda$를 포함합니다.
- **수학적 증명:**
  - Banach 고정점 정리(Banach Fixed-point Theorem)를 사용하여 뉴런 활성화가 네트워크의 여러 층을 거치면서 0 평균 및 단위 분산에 수렴함을 증명했습니다. 이 수렴성은 노이즈 및 교란이 존재하더라도 유지됩니다.
  - 활성화 분산이 단위 분산에서 벗어나더라도 분산의 상한과 하한이 존재함을 증명하여 기울기 소실 및 폭주 문제를 원천적으로 방지합니다.
- **깊은 네트워크 훈련 및 강력한 정규화 가능성:** SNNs의 수렴 특성 덕분에 많은 층을 가진 깊은 네트워크를 안정적으로 훈련할 수 있으며, '알파 드롭아웃(alpha dropout)'이라는 새로운 정규화 기법을 도입하여 학습 강건성(robustness)을 높였습니다.
- **뛰어난 실험 성능:**
  - 121개 UCI 머신러닝 데이터셋에서 기존 FNN 방법들(Batch Norm, Layer Norm, Weight Norm, Highway Networks, Residual Networks 포함)을 유의미하게 능가했습니다.
  - 약물 발견(Tox21) 및 천문학(HTRU2) 벤치마크에서 모든 경쟁 방법을 능가하며 새로운 기록을 수립했습니다. 최적의 SNN 아키텍처는 다른 FNN들보다 훨씬 깊은 경향이 있었습니다.

## 📎 Related Works

- **Convolutional Neural Networks (CNNs) & Recurrent Neural Networks (RNNs):** 각자의 도메인(비전, 시퀀스)에서 딥러닝 성공의 주역.
- **Batch Normalization (BatchNorm)** [20]: 깊은 CNN 훈련의 안정화에 기여한 표준 정규화 기법.
- **Layer Normalization (LayerNorm)** [2]: 배치 정규화와 유사하게 층별 활성화 평균과 분산을 제어하는 기법.
- **Weight Normalization (WeightNorm)** [32]: 가중치를 재매개변수화하여 학습 속도를 가속화하는 기법.
- **Highway Networks** [35] & **Residual Networks (ResNet)** [16]: 매우 깊은 네트워크 훈련을 가능하게 한 특수 아키텍처.
- **ReLU Networks with MSRA Initialization** [17]: 정규화 기법 없이 ReLU 활성화 함수를 사용하는 FNN.
- **Exponential Linear Units (ELUs)** [7]: SNNs의 SELU 활성화 함수의 기반이 된 활성화 함수.

## 🛠️ Methodology

SNNs는 'Scaled Exponential Linear Units (SELUs)' 활성화 함수와 특정 가중치 초기화 전략을 통해 자체 정규화 특성을 유도합니다.

1. **활성화 함수: Scaled Exponential Linear Units (SELUs)**

   - SELU 함수는 다음과 같이 정의됩니다:
     $$ \text{selu}(x) = \lambda \begin{cases} x & \text{if } x > 0 \\ \alpha e^x - \alpha & \text{if } x \le 0 \end{cases} $$
        여기서 매개변수 $\alpha \approx 1.6733$ 및 $\lambda \approx 1.0507$는 활성화가 0 평균 및 단위 분산에 수렴하도록 보장하는 고정점 방정식을 풀어 얻은 값입니다.
   - SELU는 음수 및 양수 값을 모두 가져 평균을 제어하고, 포화 영역을 가져 분산을 감쇠시키며, 1보다 큰 기울기를 가져 분산을 증가시키는 등 자체 정규화에 필수적인 특성을 가집니다.

2. **자체 정규화(Self-Normalizing) 특성 증명**

   - 이전 층의 활성화($x_i$)의 평균($\mu$)과 분산($\nu$)이 주어졌을 때, 다음 층의 활성화($y = \text{selu}(W^T x)$)의 평균($\tilde{\mu}$)과 분산($\tilde{\nu}$)을 매핑하는 함수 $g: (\mu, \nu) \to (\tilde{\mu}, \tilde{\nu})$를 분석적으로 유도합니다.
   - **Banach 고정점 정리**를 사용하여, 이 매핑 $g$가 특정 조건 하에서 안정적이고 매력적인 고정점 $(0, 1)$을 가짐을 증명합니다. 이는 네트워크가 깊어지더라도 활성화가 자동으로 0 평균 및 단위 분산에 수렴함을 의미합니다.
   - 또한, 활성화 분산이 고정점에서 벗어나더라도 상한과 하한 내에 머무름을 증명하여 기울기 소실 및 폭주 문제가 발생하지 않음을 보입니다.

3. **가중치 초기화(Initialization)**

   - SNNs는 정규화된 가중치($\sum w_i = 0$, $\sum w_i^2 = 1$)에 대해 고정점 $(0, 1)$을 가지므로, 가중치를 평균 $E(w_i) = 0$ 및 분산 $Var(w_i) = 1/n$인 가우시안 분포에서 샘플링하여 기대치적으로 이러한 조건을 만족하도록 합니다.

4. **알파 드롭아웃(Alpha Dropout)**

   - 표준 드롭아웃이 활성화를 0으로 설정하는 것과 달리, SELU의 음수 포화 값인 $\alpha' = -\lambda\alpha$로 활성화를 설정하는 새로운 드롭아웃 기법을 제안합니다. 이 기법은 드롭아웃 후에도 활성화의 평균과 분산을 보존하도록 스케일링 파라미터를 사용함으로써 SNN의 자체 정규화 특성을 유지합니다.

5. **중앙 극한 정리(Central Limit Theorem) 적용**
   - 각 층의 네트워크 입력이 많은 수의 독립적인 이전 층 활성화의 가중치 합이므로, 중앙 극한 정리(CLT)에 의해 이 입력이 정규 분포에 가깝다는 가정을 바탕으로 이론을 전개합니다.

## 📊 Results

- **121개 UCI 머신러닝 데이터셋:**

  - SNNs는 기존의 배치 정규화, 레이어 정규화, 가중치 정규화, 하이웨이 네트워크, 잔차 네트워크 등을 포함한 모든 경쟁 FNN 방법들을 평균 순위 및 쌍별 비교(paired Wilcoxon test)에서 _유의미하게_ 능가했습니다.
  - 1,000개 이상의 데이터 포인트를 가진 대규모 데이터셋(46개)에서는 SNNs가 SVM, Random Forests와 같은 전통적인 머신러닝 방법들을 포함한 모든 경쟁 방법들 중 최고의 성능을 보였습니다.
  - 최적의 SNN 아키텍처는 평균 10.8개 레이어를 가지는 등 다른 FNN(평균 3.8~7.1개 레이어)보다 훨씬 깊은 경향이 있었습니다.

- **약물 발견: Tox21 챌린지 데이터셋:**

  - SNNs는 평균 AUC(Area Under the Curve) 0.845 $\pm$ 0.003를 달성하여 챌린지 우승 앙상블(0.846)에 근접하는 단일 모델 최고의 성능을 기록했습니다.
  - 다른 FNN들이 2~4개 레이어에서 최적의 성능을 보인 반면, SNNs는 8개 레이어에서 가장 뛰어난 결과를 얻어 깊은 네트워크에서의 우수성을 다시 한번 입증했습니다. 네트워크가 깊어질수록 SNNs의 성능 이점이 더욱 명확했습니다.

- **천문학: HTRU2 데이터셋의 펄서 예측:**
  - SNNs는 AUC 0.9803 $\pm$ 0.010을 기록하여 기존의 Naive Bayes (0.976) 등 다른 최첨단 방법들을 능가하고 새로운 기록을 수립했습니다.

## 🧠 Insights & Discussion

- **깊은 FNN의 잠재력 해방:** SNNs는 활성화의 자체 정규화 특성을 통해 기존 FNN이 겪었던 깊은 아키텍처에서의 학습 불안정성을 해결했습니다. 이는 FNN도 CNN이나 RNN처럼 다단계 추상 표현 학습의 이점을 충분히 활용할 수 있음을 보여줍니다.
- **학습의 강건성 및 효율성 증대:** SELU 활성화 함수는 네트워크가 교란에도 불구하고 활성화의 평균과 분산을 안정적으로 유지하게 하여, 훈련 오류의 분산을 낮추고 더 빠르고 안정적인 학습을 가능하게 합니다. 이는 강력한 정규화 기법의 적용을 용이하게 합니다.
- **활성화 함수 설계의 중요성:** SELU는 활성화 값의 평균과 분산을 제어하는 데 필수적인 특성(음수/양수 값, 포화 영역, 기울기 > 1 영역)을 명시적으로 갖도록 설계되었습니다. 이는 활성화 함수의 미세한 특성이 네트워크의 전반적인 학습 역학에 지대한 영향을 미칠 수 있음을 시사합니다.
- **새로운 드롭아웃 기법의 필요성:** '알파 드롭아웃'의 도입은 특정 활성화 함수에 최적화된 정규화 전략이 필요하며, 이러한 맞춤형 접근 방식이 자체 정규화 특성 유지를 돕는 데 중요하다는 것을 보여줍니다.
- **일반적인 분류/회귀 문제에서의 경쟁력:** SNNs의 성공은 비전 및 시퀀스 데이터 외의 다양한 일반적인 분류/회귀 문제에서도 깊은 FNN이 그래디언트 부스팅, 랜덤 포레스트, SVM과 같은 전통적인 강자들과 효과적으로 경쟁할 수 있음을 보여줍니다.
- **제한 사항:** 중앙 극한 정리(CLT)의 적용은 뉴런 활성화의 독립성을 가정하는데, 실제로는 약한 의존성이 존재할 수 있습니다. 논문은 이에 대한 설명을 제공했으나, 이 가정의 엄격한 유효성에 대한 추가 분석이 필요할 수 있습니다.

## 📌 TL;DR

기존 FNN이 깊은 네트워크에서 불안정하다는 문제에 대응하여, 본 논문은 **자체 정규화 신경망** (**Self-Normalizing Neural Networks, SNNs**)과 **Scaled Exponential Linear Units (SELUs)** 활성화 함수를 제안합니다. SNNs는 활성화가 자동으로 0 평균 및 단위 분산에 수렴하도록 유도하며, 기울기 소실 및 폭주를 방지한다는 수학적 증명을 제공합니다. SNNs는 UCI, Tox21, HTRU2 벤치마크에서 기존의 모든 FNN 및 다른 머신러닝 방법들을 능가하는 뛰어난 성능을 보였으며, 최적의 아키텍처는 대부분 깊은 구조를 가졌습니다.
