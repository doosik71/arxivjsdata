# Activation Ensembles for Deep Neural Networks

Mark Harmon, Diego Klabjan

## 🧩 해결할 문제

기존 딥 신경망에서 활성화 함수(activation function)를 선택하는 것은 시행착오를 수반하며, 특정 계층이나 뉴런에 가장 적합한 함수를 찾는 것이 어렵습니다. 대부분의 연구는 새로운 활성화 함수를 제안하거나 단일 활성화 함수에 집중하지만, 이는 각 계층 또는 뉴런의 동적인 요구 사항을 충족시키지 못할 수 있습니다. 본 논문은 이러한 비효율성을 극복하고, 네트워크가 각 뉴런에서 최적의 활성화 함수 조합을 스스로 선택하도록 하는 새로운 방법론을 제안합니다.

## ✨ 주요 기여

- **활성화 앙상블(Activation Ensemble) 개념 도입**: 각 뉴런에서 여러 활성화 함수를 조합하여 사용하는 "활성화 앙상블"이라는 새로운 방법론을 제안합니다.
- **동적 활성화 함수 선택 메커니즘**: 네트워크가 학습을 통해 각 뉴런에서 최적의 활성화 함수 조합을 동적으로 결정할 수 있도록 $\alpha$, $\eta$, $\delta$와 같은 추가 파라미터를 도입합니다.
- **성능 향상**: FFN, CNN, 잔차 신경망, 합성곱 오토인코더 등 다양한 네트워크 아키텍처와 데이터셋(MNIST, ISOLET, CIFAR-100, STL-10)에서 기존 단일 활성화 함수 사용 대비 우수한 성능을 달성했습니다.
- **새로운 볼록 조합 활성화 함수**: 알려진 함수들의 볼록 조합으로 이루어진 새로운 형태의 활성화 함수를 생성하고, 관련 최적화 문제를 해결하는 알고리즘을 제시합니다.
- **활성화 함수에 대한 심층적 이해**: 각 계층, 모델, 데이터셋에 따라 네트워크가 선호하는 활성화 함수가 다름을 보여주며, 활성화 함수와 네트워크 학습 간의 관계에 대한 통찰을 제공합니다.

## 📎 관련 연구

- **활성화 함수 개선**:
  - **ReLU**: 딥 신경망의 인기를 다시 불러온 핵심 활성화 함수로, 계산 용이성 및 학습 효율성이 장점입니다. (Glorot et al., 2011; Nair & Hinton, 2010)
  - **Sigmoid 및 Hyperbolic Tangent**: 포화 영역 문제로 딥 네트워크 학습에 어려움이 있었으나, Gulcehre et al. (2016a)는 확률 변수를 도입하여 개선을 시도했습니다.
  - **ELU (Exponential Linear Unit)**: Clevert et al. (2016)은 음수 값을 활용하는 더 부드러운 Leaky ReLU 형태로 ELU를 제안했습니다.
  - **PELU (Parametric Exponential Linear Unit)**: Trottier et al. (2016)은 ELU를 추가 파라미터 $a, b$를 포함한 $\begin{cases} \frac{a}{b}x & x \ge 0 \\ a(e^{\frac{x}{b}}-1) & x \le 0 \end{cases}$ 형태로 일반화하여 정확도를 크게 높였습니다.
- **활성화 함수 학습/조합**:
  - **Maxout Networks**: Goodfellow et al. (2013)은 여러 선형 함수의 최댓값을 취하여 활성화 함수를 선택하도록 하지만, 학습해야 할 가중치가 많고 모든 특징을 보존하지 못할 수 있습니다.
  - **Agostinelli et al. (2015)**: 훈련 중에 활성화 함수를 구성하는 접근 방식을 사용하며, $\max(0,x) + \sum_{s=1}^{S} a_s^{i} \max(0,-s+b_s^{i})$ 형태로 ReLU에 학습 가능한 변수를 추가합니다. 본 연구는 다양한 활성화 함수를 조합하는 방식에서 차이를 보입니다.
  - **Chen (2016)**: 확률적 제어 분야에서 각 뉴런에 여러 활성화 함수를 결합하지만, 단순히 더하는 방식이며 최적의 조합을 네트워크가 선택하게 하지는 않습니다.
  - **Scardapane et al. (2016)**: 큐빅 스플라인 보간법을 사용하여 훈련 중에 활성화 함수를 생성합니다.
  - **Jin et al. (2016)**: ReLU 기반의 선형 함수 조합에 초점을 맞춥니다.
- **앙상블 기법**: 여러 예측 모델을 앙상블하여 정확도를 높이는 것은 보편적인 기술입니다. Dropout (Srivastava et al., 2014) 또한 여러 네트워크를 함께 훈련하는 앙상블의 한 형태로 볼 수 있습니다.

## 🛠️ 방법론

본 연구는 "활성화 앙상블"이라는 새로운 계층 디자인을 제안하며, 각 뉴런 또는 필터(CNN의 경우)에서 여러 활성화 함수의 조합을 사용합니다.

1. **초기 아이디어**: 단순히 여러 활성화 함수의 출력을 더하는 방식은, 범위가 다른 함수들로 인해 절대값이 큰 함수가 다른 함수들을 압도할 수 있다는 문제가 있습니다.
2. **활성화 함수 정규화**: 이 문제를 해결하기 위해 각 활성화 함수 $f_j(z)$의 출력을 트레이닝 미니배치 내의 최소값과 최대값을 사용하여 $[0,1]$ 범위로 정규화합니다.
   $$h_j^{i}(z) = \frac{f_j(z) - \min_k(f_j(z_{ki}))}{\max_k f_j(z_{ki}) - \min_k f_j(z_{ki}) + \epsilon}$$
   여기서 $\epsilon$은 분모가 0이 되는 것을 방지하는 작은 값이며, $k$는 미니배치 내의 샘플들을 나타냅니다.
3. **가중치 파라미터 ($\alpha$) 도입**: 각 뉴런의 각 활성화 함수에 가중치 $\alpha_j^{i}$를 부여하여 네트워크가 최적의 조합을 학습할 수 있도록 합니다. 각 뉴런 $i$의 최종 활성화 출력 $y_i(z)$는 다음과 같습니다.
   $$y_i(z) = \sum_{j=1}^{m} \alpha_j^{i} h_j^{i}(z)$$
   여기서 $m$은 활성화 함수의 개수입니다.
4. **$\alpha$ 파라미터 제약 조건 및 최적화**: $\alpha_j^{i}$ 값들이 활성화 함수의 기여도를 나타내도록, 각 뉴런에 대해 다음과 같은 제약 조건을 가합니다.
   - $\sum_{j=1}^{m} \alpha_j = 1$
   - $\alpha_j \ge 0$
     이 제약 조건을 만족하면서 $\alpha$를 업데이트하기 위해 다음과 같은 볼록 최적화 문제(투영 부분 문제)를 해결합니다.
     $$ \text{minimize}_{\alpha} \sum_{j=1}^{m} \frac{1}{2}(\hat{\alpha}_j - \alpha_j)^2 $$
    $$ \text{subject to } \sum_{j=1}^{m} \alpha_j = 1 $$
    $$ \alpha_j \ge 0, j=1,2,...m. $$
    이 문제는 $O(m)$ 시간에 해결 가능한 "물 채우기 문제(water-filling problem)"와 유사하며, 각 뉴런마다 해결됩니다.
5. **오프셋 파라미터 ($\eta, \delta$) 도입**: Batch Normalization에서 영감을 받아, 네트워크가 활성화 함수를 원래 상태로 유지할 수 있도록 $\eta$와 $\delta$ 파라미터를 추가합니다. 최종 활성화 함수는 다음과 같습니다.
   $$y_i = \sum_{j=1}^{m} \alpha_j(\eta_j h_j^{i} + \delta_j)$$
6. **학습**: 새로운 파라미터들($\alpha, \eta, \delta$)은 표준 역전파(backpropagation)를 통해 훈련됩니다.
   $$ \frac{\partial \mathcal{L}}{\partial \alpha_j} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot (\eta_j h_j^{i} + \delta_j) $$
    $$ \frac{\partial \mathcal{L}}{\partial \eta_j} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \alpha_j h_j^{i} $$
    $$ \frac{\partial \mathcal{L}}{\partial \delta_j} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \alpha_j $$
   CNN의 경우, 뉴런 대신 필터 단위로 활성화 앙상블을 적용합니다.
7. **활성화 함수 세트**: 세 가지 활성화 함수 세트를 탐색합니다.
   - **세트 1 (다양한 함수)**: Sigmoid($\frac{1}{1+e^{-z}}$), Hyperbolic Tangent($\tanh(z)$), Soft Linear Rectifier($\ln(1+e^{-z})$), Linear Rectifier($\max(0,z)$), Inverse Absolute Value($\frac{z}{1+|z|}$), Exponential Linear Function($\begin{cases} z & z \ge 0 \\ e^z-1 & z \le 0 \end{cases}$)
   - **세트 2 (다양한 절편의 ReLU)**: $f_b(z) = \max(0, z+b)$ 형태로, $b \in \{-1.0, -0.5, 0, 0.5, 1.0\}$ 값을 사용합니다.
   - **세트 3 (절대값 함수 변형)**: $f_1(z) = \max(0,-z)$와 $f_2(z) = \max(0,z)$ 두 개의 미러링된 ReLU 함수로 구성됩니다.

## 📊 결과

활성화 앙상블은 다양한 데이터셋과 네트워크 모델에서 단일 활성화 함수(ReLU)를 사용하는 기존 방식보다 우수한 성능을 보였습니다.

- **성능 향상 요약 (평균 5회 실행 결과):**

  - **MNIST FFN**: 97.73% → 98.37%
  - **MNIST CNN**: 99.34% → 99.40%
  - **ISOLET FFN**: 95.16% → 96.28%
  - **CIFAR-100 잔차 신경망**: 73.64% → 74.20%
  - **STL-10 CAE**: 0.03524 (재구성 손실) → 0.03502 (재구성 손실)
    특히 ISOLET 데이터셋에서 가장 큰 성능 향상을 보였습니다.

- **활성화 함수 비교 분석**:
  - **MNIST FFN**: 상위 계층에서는 ReLU가 빠르게 지배적인 함수로 선택되지만, 하위 계층으로 갈수록 ReLU의 지배력이 약해지며 Inverse Absolute Value 함수가 중요해집니다.
  - **MNIST CNN**: 상위 합성곱 계층에서는 ReLU가 지배적이지만, 피드포워드 계층으로 전환되는 하위 계층에서는 Hyperbolic Tangent 및 Inverse Absolute Value 함수가 ReLU보다 중요해지는 경향을 보였습니다. 이는 계층 유형에 따라 선호되는 활성화 함수가 변할 수 있음을 시사합니다.
  - **ISOLET FFN**: MNIST와는 다르게, ReLU의 지배력이 강하지 않으며, 많은 활성화 함수들의 $\alpha$ 값이 서로 비슷하게 나타납니다. 하위 계층에서는 Hyperbolic Tangent가 선호됩니다.
  - **CIFAR-100 잔차 신경망**: ReLU에 대한 잔차 신경망의 의존성 때문에, 다양한 절편을 가진 ReLU들로 구성된 활성화 세트 2가 가장 좋은 성능을 보였습니다.
  - **STL-10 CAE**: 절대값 함수와 유사한 활성화 세트 3이 가장 좋은 성능을 보였습니다.

## 🧠 통찰 및 논의

- **유연한 활성화 함수 선택**: 본 방법론은 네트워크가 각 뉴런/필터 수준에서 데이터와 태스크에 가장 적합한 활성화 함수를 동적으로 "선택"할 수 있는 유연성을 제공합니다. 이는 단일 고정된 활성화 함수를 사용하는 것보다 강력합니다.
- **계층, 모델, 데이터셋 의존성**: 최적의 활성화 함수는 네트워크의 계층, 사용된 모델 아키텍처(FFN vs. CNN), 그리고 입력 데이터셋의 특성에 따라 크게 달라진다는 중요한 통찰을 얻었습니다. 예를 들어, MNIST FFN의 상위 계층은 ReLU를 선호하지만, 동일한 데이터셋의 CNN 하위 계층은 Hyperbolic Tangent나 Inverse Absolute Value를 더 선호했습니다. ISOLET 데이터셋에서는 ReLU의 지배력이 MNIST보다 약했습니다.
- **$\alpha$ 파라미터를 통한 통찰**: 학습된 $\alpha$ 파라미터는 네트워크가 어떤 활성화 함수에 더 큰 가중치를 부여하는지 정량적으로 보여줌으로써, 각 활성화 함수가 어떤 특징을 포착하는지에 대한 심층적인 이해를 돕습니다.
- **잠재적 확장성**: 본 아키텍처는 이론적으로 개방형 파라미터를 가진 활성화 함수에도 적용 가능하며, 사용자 컴퓨팅 능력에 따라 더 많은 활성화 함수를 앙상블에 포함할 수 있습니다.
- **한계 및 미래 방향**: 본 연구는 주로 기존 활성화 함수들의 조합에 초점을 맞췄습니다. 향후에는 더욱 복잡하거나 맞춤형 활성화 함수를 앙상블에 통합하는 연구가 가능할 것입니다. 또한, 앙상블 계층의 오버헤드(추가 파라미터 및 계산)에 대한 면밀한 분석도 필요합니다.

## 📌 TL;DR

본 논문은 딥 신경망에서 활성화 함수를 수동으로 선택하는 문제를 해결하기 위해, 각 뉴런에서 여러 활성화 함수를 동적으로 조합하는 "활성화 앙상블" 기법을 제안합니다. 이 방법은 $\alpha, \eta, \delta$ 파라미터와 볼록 최적화 문제를 사용하여 활성화 함수들을 $[0,1]$ 범위로 정규화하고 가중 볼록 조합을 형성합니다. MNIST, ISOLET, CIFAR-100 등 다양한 데이터셋과 FFN, CNN, 잔차 신경망 등 여러 모델에서 기존 단일 활성화 함수 대비 성능 향상을 달성했으며, 최적의 활성화 함수가 계층, 모델, 데이터셋에 따라 달라진다는 중요한 통찰을 제공합니다.
