{
  "title": "A Method on Searching Better Activation Functions",
  "authors": "Haoyuan Sun, Zihao Wu, Bo Xia, Pu Chang, Zibin Dong, Yifu Yuan, Yongzhe Chang, Xueqian Wang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2405.12954v2",
  "abstract": "The success of artificial neural networks (ANNs) hinges greatly on the\njudicious selection of an activation function, introducing non-linearity into\nnetwork and enabling them to model sophisticated relationships in data.\nHowever, the search of activation functions has largely relied on empirical\nknowledge in the past, lacking theoretical guidance, which has hindered the\nidentification of more effective activation functions. In this work, we offer a\nproper solution to such issue. Firstly, we theoretically demonstrate the\nexistence of the worst activation function with boundary conditions (WAFBC)\nfrom the perspective of information entropy. Furthermore, inspired by the\nTaylor expansion form of information entropy functional, we propose the\nEntropy-based Activation Function Optimization (EAFO) methodology. EAFO\nmethodology presents a novel perspective for designing static activation\nfunctions in deep neural networks and the potential of dynamically optimizing\nactivation during iterative training. Utilizing EAFO methodology, we derive a\nnovel activation function from ReLU, known as Correction Regularized ReLU\n(CRReLU). Experiments conducted with vision transformer and its variants on\nCIFAR-10, CIFAR-100 and ImageNet-1K datasets demonstrate the superiority of\nCRReLU over existing corrections of ReLU. Extensive empirical studies on task\nof large language model (LLM) fine-tuning, CRReLU exhibits superior performance\ncompared to GELU, suggesting its broader potential for practical applications.",
  "citation": 5
}