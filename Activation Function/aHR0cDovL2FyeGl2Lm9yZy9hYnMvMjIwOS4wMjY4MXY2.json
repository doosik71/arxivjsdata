{
  "title": "How important are activation functions in regression and classification?\n  A survey, performance comparison, and future directions",
  "authors": "Ameya D. Jagtap, George Em Karniadakis",
  "year": 2022,
  "url": "http://arxiv.org/abs/2209.02681v6",
  "abstract": "Inspired by biological neurons, the activation functions play an essential\npart in the learning process of any artificial neural network commonly used in\nmany real-world problems. Various activation functions have been proposed in\nthe literature for classification as well as regression tasks. In this work, we\nsurvey the activation functions that have been employed in the past as well as\nthe current state-of-the-art. In particular, we present various developments in\nactivation functions over the years and the advantages as well as disadvantages\nor limitations of these activation functions. We also discuss classical (fixed)\nactivation functions, including rectifier units, and adaptive activation\nfunctions. In addition to discussing the taxonomy of activation functions based\non characterization, a taxonomy of activation functions based on applications\nis presented. To this end, the systematic comparison of various fixed and\nadaptive activation functions is performed for classification data sets such as\nthe MNIST, CIFAR-10, and CIFAR- 100. In recent years, a physics-informed\nmachine learning framework has emerged for solving problems related to\nscientific computations. For this purpose, we also discuss various requirements\nfor activation functions that have been used in the physics-informed machine\nlearning framework. Furthermore, various comparisons are made among different\nfixed and adaptive activation functions using various machine learning\nlibraries such as TensorFlow, Pytorch, and JAX.",
  "citation": 155
}