# SEARCHING FOR ACTIVATION FUNCTIONS

Prajit Ramachandran, Barret Zoph, Quoc V. Le

## 🧩 Problem to Solve

심층 신경망에서 활성화 함수의 선택은 훈련 역학 및 태스크 성능에 지대한 영향을 미칩니다. 현재 가장 성공적이고 널리 사용되는 활성화 함수는 ReLU(Rectified Linear Unit)입니다. ReLU를 대체하기 위해 다양한 수동 설계 활성화 함수들이 제안되었지만, 대부분의 경우 일관되지 않은 성능 향상으로 인해 ReLU만큼 널리 채택되지 못했습니다. 본 연구는 이러한 수동 설계의 한계를 극복하고, ReLU를 능가하는 일관된 성능을 보이는 새로운 활성화 함수를 자동화된 방식으로 발견하는 문제를 해결하고자 합니다.

## ✨ Key Contributions

- **자동화된 탐색 기법 적용**: 새로운 스칼라 활성화 함수를 발견하기 위해 전수 탐색(exhaustive search)과 강화 학습 기반 탐색(RNN controller)을 결합한 자동화된 탐색 기법을 활용했습니다.
- **Swish 활성화 함수 발견**: 탐색을 통해 $f(x) = x \cdot \text{sigmoid}(\beta x)$로 정의되는 Swish라는 비단조적이고 부드러운 활성화 함수를 발견했습니다. 여기서 $\beta$는 상수 또는 학습 가능한 파라미터입니다.
- **ReLU 대비 뛰어난 성능 입증**: Swish가 ImageNet 이미지 분류, CIFAR-10/100 이미지 분류, WMT English$\to$German 기계 번역 등 다양한 도전적 데이터셋과 심층 모델(Mobile NASNet-A, Inception-ResNet-v2 등)에서 ReLU 및 다른 최신 활성화 함수보다 일관되게 우수한 성능을 보이거나 동등한 성능을 달성함을 광범위한 실험을 통해 입증했습니다. 예를 들어, ImageNet에서 Mobile NASNet-A의 Top-1 정확도를 0.9%p, Inception-ResNet-v2에서 0.6%p 향상시켰습니다.
- **Swish의 특성 분석**: Swish가 위쪽으로는 비유계, 아래쪽으로는 유계이며, ReLU와 달리 부드럽고 비단조적이라는 특성을 분석했습니다. 특히 $x<0$ 영역에서 나타나는 비단조적인 "범프(bump)"가 성능에 중요함을 강조했습니다.

## 📎 Related Works

- **기존 활성화 함수**: ReLU (Hahnloser et al., 2000; Jarrett et al., 2009; Nair & Hinton, 2010)의 지배적인 위치와 함께, Leaky ReLU (Maas et al., 2013), PReLU (He et al., 2015), Softplus (Nair & Hinton, 2010), ELU (Clevert et al., 2015), SELU (Klambauer et al., 2017), GELU (Hendrycks & Gimpel, 2016) 등 ReLU의 대안으로 제안된 수동 설계 활성화 함수들이 본 연구의 비교 대상으로 언급되었습니다.
- **자동화된 탐색 및 메타 학습**: 신경망 아키텍처 (Zoph & Le, 2016; Bello et al., 2017; Zoph et al., 2017) 및 옵티마이저 (Bello et al., 2017) 발견에 활용된 자동화된 탐색 기법과 메타 학습 (Schmidhuber, 1987; Naik & Mammone, 1992; Thrun & Pratt, 2012) 연구가 본 연구의 방법론적 배경을 이룹니다.
- **활성화 함수 비교 연구**: Xu et al. (2015)와 같이 다양한 활성화 함수를 체계적으로 비교한 이전 연구들이 존재하지만, 여러 도전적인 데이터셋에 걸쳐 스칼라 활성화 함수를 비교한 본 연구가 최초라고 언급됩니다.

## 🛠️ Methodology

1. **탐색 공간 설계**:
   - 활성화 함수는 코어 유닛(core unit) $b(u_1(x_1), u_2(x_2))$의 반복적인 조합으로 구성됩니다.
   - 각 코어 유닛은 두 개의 스칼라 입력을 받아 독립적으로 두 개의 단항 함수($u_1, u_2$)를 통과시킨 후, 그 출력을 이항 함수($b$)로 결합하여 스칼라 출력을 생성합니다.
   - 단항 함수의 입력은 레이어의 사전 활성화 값 $x$와 이전 이항 함수의 출력으로 제한됩니다.
   - 다양한 단항 함수($x, -x, |x|, x^2, \beta x, \text{log}(|x|+\epsilon), \text{exp}(x), \text{sin}(x), \text{sigmoid}(x), \text{max}(x,0)$ 등)와 이항 함수($x_1+x_2, x_1 \cdot x_2, \text{max}(x_1,x_2), \text{exp}(-\beta(x_1-x_2)^2)$ 등) 목록이 정의되어 탐색 공간을 형성합니다.
2. **탐색 알고리즘**:
   - **전수 탐색**: 탐색 공간이 작은 경우(예: 단일 코어 유닛 사용), 모든 가능한 활성화 함수를 열거하여 성능을 평가합니다.
   - **강화 학습 기반 탐색**: 탐색 공간이 큰 경우(예: 여러 코어 유닛 사용), RNN 컨트롤러를 사용하여 활성화 함수의 구성 요소를 순차적으로 예측합니다. 컨트롤러는 자식 네트워크(child network)의 검증 정확도를 보상으로 사용하여 Policy Proximal Optimization (PPO) 알고리즘으로 훈련됩니다.
3. **성능 평가 및 분산 훈련**:
   - 탐색 알고리즘이 생성한 각 후보 활성화 함수에 대해 ResNet-20 아키텍처의 자식 네트워크를 CIFAR-10 데이터셋에서 훈련시킵니다.
   - 훈련 후 자식 네트워크의 검증 정확도를 기록하고, 이를 탐색 알고리즘 업데이트에 사용합니다.
   - 탐색 과정의 계산 효율성을 높이기 위해 분산 훈련 방식을 사용하여 여러 자식 네트워크의 훈련을 병렬화합니다.

## 📊 Results

- **발견된 활성화 함수의 특성**: 탐색 결과 복잡한 활성화 함수보다 1~2개의 코어 유닛으로 구성된 간단한 함수가 더 좋은 성능을 보였습니다. 최상위 함수들은 $b(x, g(x))$ 형태를 자주 사용했으며, 주기 함수($\sin$, $\cos$)를 활용하는 함수들도 발견되었습니다.
- **Swish의 CIFAR-10/100 성능**: ResNet-164, Wide ResNet 28-10, DenseNet 100-12 모델에서 Swish 및 Swish-1($\beta=1$ 고정)은 ReLU를 일관되게 능가하거나 동등한 성능을 보였고, 대부분의 모델에서 다른 모든 기준선 활성화 함수와 비교하여 가장 좋은 성능을 달성하거나 그에 버금가는 성능을 보였습니다.
- **Swish의 ImageNet 성능**:
  - **Mobile NASNet-A**: Swish는 ReLU 대비 Top-1 정확도에서 1.4%p 향상(73.8% $\to$ 75.2%)을 달성했습니다.
  - **Inception-ResNet-v2**: Swish는 ReLU 대비 Top-1 정확도에서 0.5%p 향상(79.8% $\to$ 80.3%)을 달성했습니다.
  - **MobileNet**: Swish는 ReLU 대비 Top-1 정확도에서 2.2%p 향상(72.0% $\to$ 74.2%)을 달성했습니다.
  - Inception-v3 및 Inception-v4에서도 Swish는 ReLU보다 우수하거나 동등한 성능을 보이며, 대부분의 경우 다른 기준선 함수보다 뛰어났습니다.
- **Swish의 기계 번역 성능**: WMT 2014 English$\to$German 데이터셋의 Transformer 모델에서 Swish는 여러 newstest 세트에서 ReLU를 포함한 다른 기준선 함수들을 능가하거나 동등한 BLEU 점수를 달성했습니다. 특히 Swish-1은 newstest2016에서 0.6 BLEU 포인트 향상을 보였습니다.

## 🧠 Insights & Discussion

- **Swish의 독특한 특성**: Swish ($f(x) = x \cdot \text{sigmoid}(\beta x)$)는 ReLU와 달리 부드럽고 비단조적인 함수이며, 위쪽으로는 비유계, 아래쪽으로는 유계입니다. 특히 $x < 0$ 영역에서 나타나는 비단조적인 "범프(bump)"는 훈련된 네트워크의 사전 활성화 값 분포에서 중요한 영역에 해당하여 성능에 긍정적인 영향을 미치는 것으로 분석되었습니다.
- **$\beta$ 파라미터의 유연성**: $\beta$ 파라미터는 Swish 함수가 선형 함수 ($f(x) = x/2$ when $\beta=0$)와 ReLU 함수 ($\beta \to \infty$) 사이를 비선형적으로 보간하도록 조절합니다. $\beta$를 학습 가능한 파라미터로 설정했을 때, 모델이 이 유연성을 활용하여 성능을 더욱 개선할 수 있음을 실험적으로 보여주었습니다 (훈련된 $\beta$ 값 분포가 0에서 1.5 사이에 분포하며 약 1에서 피크를 가짐).
- **ReLU에 대한 도전과 새로운 통찰**: Swish의 강력한 성능은 ReLU의 핵심적인 이점으로 여겨졌던 "기울기 보존(gradient preserving)" 특성(즉, $x>0$일 때 기울기가 1)이 현대 신경망 아키텍처(예: 잔차 연결)에서는 더 이상 결정적인 이점이 아닐 수 있다는 새로운 통찰을 제공합니다. 이는 아키텍처 개선이 개별 구성 요소가 기울기를 보존해야 할 필요성을 줄인다는 것을 시사합니다.
- **자동화된 탐색의 잠재력**: 본 연구는 활성화 함수와 같이 전통적으로 사람이 설계하던 신경망 구성 요소를 자동화된 탐색 기법을 통해 발견하는 것이 효과적임을 입증하며, 메타 학습의 잠재력을 강조합니다.
- **실용적 적용 용이성**: Swish는 구현이 매우 간단하여 대부분의 딥러닝 라이브러리에서 한 줄의 코드 변경만으로 ReLU를 대체할 수 있습니다. 다만, 배치 정규화(BatchNorm) 사용 시 스케일 파라미터 설정에 유의해야 하며, ReLU 네트워크보다 약간 낮은 학습률이 권장됩니다.

## 📌 TL;DR

본 연구는 자동화된 탐색(전수 탐색 및 강화 학습 기반 RNN 컨트롤러)을 통해 새로운 활성화 함수 Swish ($f(x) = x \cdot \text{sigmoid}(\beta x)$)를 발견했습니다. Swish는 부드럽고 비단조적인 특성을 가지며, ImageNet, CIFAR, 기계 번역 등 다양한 심층 모델과 데이터셋에서 ReLU 및 다른 활성화 함수들보다 일관되게 우수하거나 동등한 성능을 달성했습니다. 이는 자동화된 탐색의 유효성을 입증하고, ReLU의 핵심 장점으로 여겨지던 기울기 보존 특성에 대한 기존 통념에 도전하는 새로운 활성화 함수를 제시합니다.
