{
  "title": "Trainable Highly-expressive Activation Functions",
  "authors": "Irit Chelly, Shahaf E. Finder, Shira Ifergane, Oren Freifeld",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.07564v2",
  "abstract": "Nonlinear activation functions are pivotal to the success of deep neural\nnets, and choosing the appropriate activation function can significantly affect\ntheir performance. Most networks use fixed activation functions (e.g., ReLU,\nGELU, etc.), and this choice might limit their expressiveness. Furthermore,\ndifferent layers may benefit from diverse activation functions. Consequently,\nthere has been a growing interest in trainable activation functions. In this\npaper, we introduce DiTAC, a trainable highly-expressive activation function\nbased on an efficient diffeomorphic transformation (called CPAB). Despite\nintroducing only a negligible number of trainable parameters, DiTAC enhances\nmodel expressiveness and performance, often yielding substantial improvements.\nIt also outperforms existing activation functions (regardless whether the\nlatter are fixed or trainable) in tasks such as semantic segmentation, image\ngeneration, regression problems, and image classification. Our code is\navailable at https://github.com/BGU-CS-VIL/DiTAC.",
  "citation": 9
}