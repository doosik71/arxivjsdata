{
  "title": "Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs)",
  "authors": "Brad Carlile, Guy Delamarter, Paul Kinney, Akiko Marti, Brian Whitney",
  "year": 2017,
  "url": "http://arxiv.org/abs/1710.09967v2",
  "abstract": "We introduce the \"inverse square root linear unit\" (ISRLU) to speed up\nlearning in deep neural networks. ISRLU has better performance than ELU but has\nmany of the same benefits. ISRLU and ELU have similar curves and\ncharacteristics. Both have negative values, allowing them to push mean unit\nactivation closer to zero, and bring the normal gradient closer to the unit\nnatural gradient, ensuring a noise-robust deactivation state, lessening the\nover fitting risk. The significant performance advantage of ISRLU on\ntraditional CPUs also carry over to more efficient HW implementations on HW/SW\ncodesign for CNNs/RNNs. In experiments with TensorFlow, ISRLU leads to faster\nlearning and better generalization than ReLU on CNNs. This work also suggests a\ncomputationally efficient variant called the \"inverse square root unit\" (ISRU)\nwhich can be used for RNNs. Many RNNs use either long short-term memory (LSTM)\nand gated recurrent units (GRU) which are implemented with tanh and sigmoid\nactivation functions. ISRU has less com- putational complexity but still has a\nsimilar curve to tanh and sigmoid.",
  "citation": 67
}