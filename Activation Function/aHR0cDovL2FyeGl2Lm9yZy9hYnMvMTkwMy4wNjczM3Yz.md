# Dying ReLU and Initialization: Theory and Numerical Examples

Lu Lu, Yeonjong Shin, Yanhui Su, and George Em Karniadakis

## 🧩 Problem to Solve

본 논문은 ReLU(Rectified Linear Unit) 활성화 함수를 사용하는 신경망에서 발생하는 "Dying ReLU" 문제를 다룹니다. Dying ReLU는 ReLU 뉴런이 특정 입력에 대해 계속 0만 출력하여 비활성화되는 현상으로, 이는 특히 깊은(deep) 신경망의 훈련을 어렵게 만드는 주요 장애물 중 하나입니다. 기존에는 경험적, 휴리스틱한 설명이 주를 이루었으나, 이 문제에 대한 엄밀한 이론적 분석은 부족했습니다.

## ✨ Key Contributions

- **Dying ReLU에 대한 최초의 이론적 분석:** 깊은 ReLU 네트워크가 깊이($L$)가 무한대로 갈수록 초기화 시 "사망(dead)"할 확률이 1로 수렴함을 엄밀히 증명했습니다 (Theorem 3.1).
- **"Born Dead" 확률 분석:** 일반적인 대칭 확률 분포로 가중치와 편향을 초기화할 경우, 네트워크가 훈련 전에 죽는 "Born Dead Probability (BDP)"의 상한과 하한을 제시했습니다 (Theorem 3.2, 3.3). 폭($N$)이 증가하면 BDP가 0으로 수렴하고, 깊이가 증가하면 1로 수렴함을 보였습니다.
- **사망 네트워크의 훈련 결과:** 네트워크가 초기화 시에 사망한 경우, 어떤 경사 기반 최적화 방법을 사용하더라도 해당 네트워크는 손실을 최소화하는 상수 함수로 수렴함을 증명했습니다 (Theorem 3.4).
- **무작위 비대칭 초기화(Randomized Asymmetric Initialization, RAI) 제안:** Dying ReLU 문제를 효과적으로 방지하기 위한 새로운 초기화 기법인 RAI를 제안했습니다.
- **RAI의 이론적 우월성:** RAI가 표준 대칭 초기화(예: He 초기화)보다 "Born Dead" 확률의 상한이 더 작음을 이론적으로 증명했습니다 (Theorem 4.1).
- **매개변수 이론적 설계:** 폭주 기울기(exploding gradient) 문제를 피하기 위해 2차 모멘트 분석을 통해 RAI에 필요한 모든 초기화 매개변수($\sigma^2_{\ell}$, $\mathcal{P}_{\ell}$)를 이론적으로 설계했습니다.
- **RAI의 성능 입증:** 다양한 수치적 예시(함수 근사, MNIST 분류)를 통해 RAI가 깊고 좁은(deep and narrow) 네트워크의 Dying ReLU 문제를 해결하고, 훈련 및 일반화 성능을 현저히 향상시킴을 보여주었습니다.

## 📎 Related Works

- **ReLU 활성화 함수의 성공:** ReLU는 sigmoid나 tanh와 같은 다른 활성화 함수에 비해 뛰어난 훈련 성능을 바탕으로 딥러닝에서 널리 사용되어 왔습니다 [30, 35, 40].
- **깊은 신경망 훈련의 어려움:** 깊은 신경망을 훈련할 때 발생하는 근본적인 어려움은 기울기 소실/폭주(vanishing/exploding gradient) 문제로 잘 알려져 있습니다 [6, 17, 39]. Dying ReLU는 기울기 소실의 한 형태로 간주됩니다 [1, 50].
- **Dying ReLU 완화를 위한 기존 접근법:**
  - **네트워크 아키텍처 수정:** 레이어 수, 뉴런 수, 연결 방식 변경 또는 새로운 활성화 함수(예: ELU [7], Leaky ReLU [33], SELU [28]) 제안.
  - **추가 훈련 단계 도입:** Batch Normalization [24], Layer Normalization [4], Dropout [45]과 같은 정규화 기법.
  - **가중치 및 편향 초기화 절차 변경:** Glorot [15], He [20] 초기화 등. 본 논문은 이 마지막 접근 방식에 초점을 맞춥니다.
- **비정상적인 국소 최솟값:** 초기화가 잘못되면 Dying ReLU와 같은 문제를 야기하는 좋지 않은 국소 최솟값(bad local minima)에 빠질 수 있습니다 [12, 36].

## 🛠️ Methodology

1. **"Dying ReLU 신경망" 정의:**
   - 네트워크의 모든 출력 뉴런이 어떤 입력에 대해서도 상수 함수(특히 0)를 출력하는 경우를 "dying ReLU neural network"로 정의합니다.
   - 이 현상을 두 가지 단계로 분류합니다: (1) 훈련 전에 네트워크가 사망하는 경우("born dead", BD), (2) 훈련 후에 네트워크가 사망하는 경우. BD는 후자를 함축하지만 역은 성립하지 않습니다.
2. **BD 확률의 이론적 분석:**
   - 임의의 가중치($W^{\ell}$) 및 편향($b^{\ell}$)이 $P(W^{\ell}_{j} \in \mathbb{R}^{N_{\ell-1}}_{-}, b^{\ell}_{j} < 0) \ge p > 0$ 조건을 만족하는 확률 분포에서 초기화될 때, 네트워크의 깊이 $L \to \infty$이면 BD 확률 $P(J_{\tilde{N}_{L}}) \to 1$임을 증명합니다 (Theorem 3.1).
   - 가중치와 편향이 0을 중심으로 대칭인 확률 분포에서 독립적으로 초기화될 경우, BD 확률의 상한을 $P(J_{\tilde{N}_{L}}) \le 1 - \prod_{\ell=1}^{L-1} (1 - (1/2)^{N_{\ell}})$으로 제시합니다 (Theorem 3.2).
   - 깊이와 폭에 따른 네트워크의 "안전 영역(safe operating region)"을 제안하며, 특히 깊이가 $L$, 폭이 $N$일 때 $N = \log_2(L/\delta)$를 만족하면 $P(J^{c}_{\tilde{N}_{L}}) > 1-\delta$ 임을 보입니다 (Corollary 3.5).
3. **무작위 비대칭 초기화 (Randomized Asymmetric Initialization, RAI) 제안:**
   - **목표:** He 초기화와 유사한 일반화 성능을 유지하면서 Dying ReLU 문제를 완화합니다.
   - **절차:**
     - **첫 번째 레이어($\ell=1$):** 표준 He 초기화($W^1_{ij} \sim N(0, 2/d_{in})$, $b^1=0$)를 사용합니다. 입력이 음수 값을 가질 수 있기 때문입니다.
     - **두 번째 레이어($\ell \ge 2$)부터:** 각 레이어의 가중치 행렬 $W^\ell$과 편향 벡터 $b^\ell$을 결합한 벡터 $V^\ell_j = [W^\ell_j, b^\ell_j]$ ($j$-번째 행)에 대해:
       1. `{1, ..., N_{\ell-1}+1}` 범위에서 인덱스 $k^\ell_j$를 무작위로 선택합니다.
       2. $k^\ell_j$를 제외한 $V^\ell_j$의 나머지 성분들 $((V^\ell_j)^{-k^\ell_j})$은 $N(0, \sigma^2_\ell I)$에서 샘플링합니다.
       3. $k^\ell_j$에 해당하는 성분 $((V^\ell_j)_{k^\ell_j})$은 0 주변으로 비대칭인 확률 분포 $\mathcal{P}_\ell$ (예: $\text{Beta}(2,1)$)에서 샘플링합니다.
   - **이론적 특성:** RAI가 대칭 초기화보다 더 작은 BD 확률 상한을 가짐을 증명합니다 (Theorem 4.1).
   - **매개변수 설계:** 폭주 기울기를 방지하기 위해 각 레이어의 정규화된 입력 벡터 제곱 길이의 기댓값($E[q^\ell(x)]$)을 추적하는 2차 모멘트 분석을 수행합니다 (Theorem 4.3). 이를 통해 $\sigma^2_\ell$ 및 $\mathcal{P}_\ell$의 매개변수를 결정하며, 실험에서는 $\sigma_w = \sqrt{2} \left( -\frac{\mu'_1}{\sqrt{\pi}} + \sqrt{\frac{\mu'^2_1}{\pi} + 1 - \mu'_2} \right)$를 사용합니다.

## 📊 Results

- **Dying Probability (BDP) 감소:**
  - RAI는 모든 너비($N=2,3,4,5$)에서 기존 He 초기화, 직교 초기화, 심지어 큰 양수 편향을 가진 He 초기화보다 네트워크의 BDP를 **극적으로 낮추는** 것으로 나타났습니다 (Figure 4). 예를 들어, 너비 2의 10층 네트워크에서 He 초기화는 90% 이상의 사망 확률을 보였으나, RAI는 40% 미만으로 감소시켰습니다.
- **"안전 영역"의 확장:**
  - RAI는 He 초기화에 비해 1% 사망 확률을 유지하는 최대 레이어 수를 크게 확장하여, 더 깊은 네트워크에서도 안정적인 초기화가 가능함을 보여주었습니다 (Figure 5).
- **함수 근사 성능 향상:**
  - 1D 및 2D 테스트 함수 근사(예: $f_1(x)=|x|$, $f_2(x)=x\sin(5x)$) 실험에서, 10층 또는 20층의 깊고 좁은 ReLU 네트워크를 사용했습니다. He 초기화는 70~90% 이상의 확률로 네트워크가 상수 함수로 붕괴했지만, RAI는 붕괴 확률을 10~40% 수준으로 대폭 줄였으며, 목표 함수를 성공적으로 근사하는 비율을 크게 높였습니다 (Figures 6-9).
- **MNIST 분류 성능 향상:**
  - 얕고 넓은 네트워크(깊이 2, 너비 1024)에서는 RAI와 He 초기화가 유사한 테스트 정확도를 보였습니다.
  - 그러나 깊고 좁은 네트워크(깊이 50, 너비 10)에서는 RAI가 He 초기화보다 **더 높은 테스트 정확도를 달성**하며 뛰어난 일반화 성능을 보여주었습니다 (Figure 10).

## 🧠 Insights & Discussion

- 본 연구의 이론적 분석은 왜 깊고 좁은 네트워크가 실용적으로 널리 사용되지 못했는지에 대한 명확한 설명을 제공합니다. 기존의 대칭 초기화 방식에서는 네트워크의 깊이가 깊어질수록 Dying ReLU 문제로 인해 훈련 전에 사망할 확률이 기하급수적으로 증가하기 때문입니다.
- RAI는 이러한 근본적인 한계를 극복하는 효과적인 초기화 전략을 제시합니다. 이는 단순히 "born dead" 확률을 낮추는 것을 넘어, 실제로 훈련 과정을 안정화하고 최종 모델의 근사 품질 및 일반화 성능을 향상시키는 것으로 확인되었습니다.
- 특히 깊고 좁은 네트워크는 이론적 연구(예: 표현력, PDEs 솔루션)에서 중요한 역할을 하지만, 실용적인 적용이 어려웠습니다. RAI는 이러한 깊고 좁은 네트워크의 잠재력을 실현할 수 있는 새로운 길을 열어줄 수 있습니다.
- 일부 수치적 예시에서 관찰된 "부분 붕괴(partial collapse)"와 같이 네트워크가 완전히 사망하지는 않지만 학습이 제대로 이루어지지 않는 현상에 대해서는 향후 추가 연구가 필요함을 시사합니다.

## 📌 TL;DR

이 논문은 ReLU 신경망의 'Dying ReLU' 문제에 대한 최초의 이론적 분석을 제공하며, 깊은 ReLU 네트워크가 깊이가 무한대로 갈수록 초기화 시 높은 확률로 비활성화(사망)됨을 엄밀히 증명합니다. 이 문제를 해결하기 위해, 저자들은 가중치와 편향의 일부를 0 주변에서 비대칭적으로 초기화하는 '무작위 비대칭 초기화(RAI)' 기법을 제안합니다. RAI는 이론적으로 기존 대칭 초기화보다 사망 확률의 상한을 낮추도록 설계되었으며, 수치적 실험 결과 깊고 좁은 네트워크에서 Dying ReLU 발생 확률을 크게 줄이고, 함수 근사 및 MNIST 분류 작업에서 훈련 및 일반화 성능을 현저히 향상시키는 것을 보여줍니다.
