# Deep Network Approximation: Beyond ReLU to Diverse Activation Functions

Shijun Zhang, Jianfeng Lu, Hongkai Zhao

## 🧩 Problem to Solve

이 논문은 다양한 활성화 함수를 사용하는 딥 뉴럴 네트워크의 표현력을 탐구합니다. 기존 연구 대부분은 ReLU(Rectified Linear Unit) 활성화 함수에 초점을 맞추었으나, LeakyReLU, ELU, GELU 등 다양한 비-ReLU 활성화 함수들이 경험적으로 우수한 성능을 보여왔습니다. 그러나 이러한 비-ReLU 활성화 함수의 이론적 기반은 아직 충분히 확립되지 않았습니다. 이 연구의 주요 목표는 이러한 다양한 활성화 함수를 사용하는 딥 뉴럴 네트워크의 표현 능력을 조사하고, 이들 함수와 ReLU 사이의 관계를 정립하여 기존의 ReLU 네트워크 근사 결과를 다른 활성화 함수로 확장하는 것입니다.

## ✨ Key Contributions

- **다양한 활성화 함수 집합 A 정의:** ReLU, LeakyReLU, ReLU$^2$, ELU, CELU, SELU, Softplus, GELU, SiLU, Swish, Mish, Sigmoid, Tanh, Arctan, Softsign, dSiLU, SRS 등 널리 사용되는 대부분의 활성화 함수를 포괄하는 집합 A를 정의했습니다.
- **ReLU 네트워크 근사의 일반화 (Theorem 1):** 집합 A에 속하는 임의의 활성화 함수 $\rho$에 대해, 너비 $N$과 깊이 $L$의 ReLU 네트워크를 너비 $3N$과 깊이 $2L$의 $\rho$-활성화 네트워크로 임의의 정밀도로 근사할 수 있음을 증명했습니다. 이는 대부분의 기존 ReLU 네트워크 근사 결과들을 더 넓은 범위의 활성화 함수로 확장할 수 있게 합니다.
- **최적의 스케일링 인자 도출:**
  - $\rho \in A_{1,k}$ (예: ReLU$^2$)의 경우, 스케일링 인자는 너비 $(k+2)$, 깊이 $1$로 나타납니다 (Theorem 7).
  - $\rho \in A_2$ (예: $x \cdot \text{Softsign}(x)$)의 경우, 스케일링 인자를 너비 $2$, 깊이 $1$로 줄일 수 있음을 보였습니다 (Theorem 8).
  - **핵심 기여 (Theorem 9):** $A_2$의 특정 부분집합 $\tilde{A}_2$ (예: ELU, CELU, SELU, Softplus, GELU, SiLU, Swish, Mish)에 속하는 활성화 함수의 경우, 너비와 깊이 스케일링 인자를 **$(1,1)$**로 줄일 수 있음을 증명했습니다. 이는 이러한 활성화 함수들이 근사 능력 면에서 ReLU와 동등하게 최적의 성능을 발휘할 수 있음을 의미합니다.
- **ReLU 네트워크 근사 결과 확장:** 논문은 이 결과를 바탕으로 연속 함수, 평활 함수, 구분적 선형 함수에 대한 기존 ReLU 근사 이론을 A에 속하는 다른 활성화 함수들로 확장하는 여러 따름정리들을 제시합니다.
- **근사 오차 정량화:** 단일 활성 뉴런을 사용하여 $\rho \in \tilde{A}_2$가 ReLU를 근사할 때의 근사 오차를 정량적으로 비교 분석했습니다.

## 📎 Related Works

- **초기 보편적 근사 정리:** 단일 은닉 계층 네트워크가 특정 유형의 함수를 임의의 정밀도로 근사할 수 있음을 증명한 Cybenko (1989), Hornik et al. (1989) 등의 연구.
- **근사 오차 분석:** 단일 은닉 계층 네트워크의 너비에 따른 $O(n^{-1/2})$ 근사 오차를 분석한 Barron (1993), Barron and Klusowski (2018) 등의 연구.
- **ReLU 네트워크에 대한 집중 연구:** ReLU의 효과를 입증한 Krizhevsky et al. (2012) 이후, 깊은 ReLU 네트워크의 표현 능력에 대한 광범위한 연구 (Lu et al., 2021; Shen et al., 2019, 2020, 2022; Yarotsky, 2017, 2018; Zhang, 2020; Zhang et al., 2023a).
- **대체 활성화 함수 제안:** LeakyReLU (Maas et al., 2013), ELU (Clevert et al., 2016), GELU (Hendrycks and Gimpel, 2016) 등 다양한 대체 활성화 함수들이 경험적 성능 개선을 보였으며, BERT, GPT-3 등 주요 모델에서 성공적으로 활용되었습니다. 본 연구는 이러한 함수의 이론적 기반을 마련합니다.

## 🛠️ Methodology

이 논문의 핵심 방법론은 임의의 활성화 함수 $\rho \in A$를 사용하는 네트워크가 ReLU 네트워크를 근사할 수 있음을 증명하는 것입니다. 이는 주로 다음의 주요 명제들을 통해 단계적으로 이루어집니다.

1. **명제 10 (일반 근사 정리):** 만약 어떤 활성화 함수 $\tilde{\rho}$가 $\rho$-활성화 네트워크로 임의의 정밀도로 근사될 수 있다면, $\tilde{\rho}$-활성화 네트워크도 $\rho$-활성화 네트워크로 근사될 수 있음을 증명합니다. 이는 네트워크의 활성화 함수를 대체할 때 너비와 깊이에 비례하여 스케일링 인자가 증가함을 보여줍니다.
2. **명제 11 (유한 차분 근사):** $n$차 미분 가능한 함수 $f \in C^n((a_0,b_0))$에 대해, 유한 차분 $\sum_{l=0}^{n} (-1)^{l} \binom{n}{l} f(x+lt) / (-t)^n$이 $t \to 0$일 때 $f^{(n)}(x)$로 균등 수렴함을 증명합니다. 이는 활성화 함수의 미분 또는 그에 준하는 특성을 활용하여 다른 함수를 근사하는 데 중요한 도구입니다. 이 증명은 코시 평균값 정리와 램마 14($\sum_{l=0}^{n} (-1)^{l} \binom{n}{l} l^i$의 값)를 사용합니다.
3. **명제 12 (활성화 함수 $\rho \in A_{1,k}$를 이용한 ReLU 근사):** $\rho \in A_{1,k}$에 대해 너비 $k+2$, 깊이 $1$의 $\rho$-활성화 네트워크로 ReLU를 유계 집합에서 근사할 수 있음을 보입니다. 이는 $\rho^{(k)}$의 특성(kink 존재)과 명제 11을 활용하여 ReLU와 유사한 형태를 구성하는 방식입니다.
4. **명제 13 (활성화 함수 $\rho \in A_2 \cup A_3$를 이용한 ReLU 근사):**
   - **$\rho \in \tilde{A}_2$의 경우:** $\rho(x) = x \cdot h(x)$ 형태의 특성을 활용하여 단일 활성 뉴런(너비 $1$, 깊이 $1$)으로 ReLU를 근사합니다. 여기서 $h(x)$는 $\lim_{x \to -\infty} h(x)=0$ 또는 $\lim_{x \to \infty} h(x)=0$을 만족하는 S-형 함수입니다.
   - **$\rho \in A_2$의 경우:** 너비 $2$, 깊이 $1$의 네트워크로 ReLU를 근사합니다.
   - **$\rho \in A_3$의 경우:** $\rho''(x_0) \neq 0$인 특성을 활용하여 램마 16을 통해 $xy$ 곱셈을 근사하고, 램마 17을 통해 ReLU를 구성하는 방식으로 너비 $3$, 깊이 $2$의 네트워크로 ReLU를 근사합니다.
5. **증명 도구:** 페아노 형태의 나머지 항을 포함하는 테일러 정리 (램마 15, 램마 16), 코시 평균값 정리, 균등 연속성, 그리고 네트워크 구성을 위한 수학적 귀납법이 광범위하게 사용됩니다.

## 📊 Results

- **일반적인 확장:** $\rho \in A$인 경우, 너비 $N$, 깊이 $L$의 ReLU 네트워크는 너비 $3N$, 깊이 $2L$의 $\rho$-활성화 네트워크로 임의의 유계 집합에서 임의의 정밀도로 근사될 수 있습니다. (Theorem 1)
- **$A_{1,k}$ 활성화 함수 ($k$차 '꺾임'을 가진 구분적 평활 함수)에 대한 확장:** 너비 $N$, 깊이 $L$의 ReLU 네트워크는 너비 $(k+2)N$, 깊이 $L$의 $\rho$-활성화 네트워크로 근사될 수 있습니다. (Theorem 7)
- **$A_2$ 활성화 함수 (ReLU의 평활 변형)에 대한 확장:** 너비 $N$, 깊이 $L$의 ReLU 네트워크는 너비 $2N$, 깊이 $L$의 $\rho$-활성화 네트워크로 근사될 수 있습니다. (Theorem 8)
- **$\tilde{A}_2$ 활성화 함수 (특정 평활 변형)에 대한 최적 확장:** ELU, CELU, SELU, Softplus, GELU, SiLU, Swish, Mish와 같이 $\tilde{A}_2$에 속하는 활성화 함수의 경우, 너비 $N$, 깊이 $L$의 ReLU 네트워크는 **너비 $N$, 깊이 $L$**의 $\rho$-활성화 네트워크로 근사될 수 있습니다. 이는 너비와 깊이의 증가 없이 ReLU와 동일한 표현력을 가짐을 의미합니다. (Theorem 9)
- **따름정리:** 위 정리들을 통해 기존 ReLU 네트워크의 근사율(연속 함수, 평활 함수, 구분적 선형 함수 근사)이 A에 속하는 활성화 함수를 사용하는 네트워크로 확장될 수 있음을 보였습니다.
- **단일 뉴런 근사 오차:** $\rho \in \tilde{A}_2$ 활성화 함수를 사용하는 단일 뉴런이 ReLU를 근사할 때의 오차 상수를 제시했습니다. 예를 들어, Softplus는 약 $0.693/K$, GELU는 약 $0.170\sigma/K$, SiLU는 약 $0.278/K$의 오차를 가집니다 (여기서 $K$는 스케일링 인자).

## 🧠 Insights & Discussion

- **이론적 정당성 부여:** 이 연구는 ReLU가 아닌 다양한 최신 활성화 함수들이 딥 뉴럴 네트워크에서 성공적으로 사용되는 이유에 대한 강력한 이론적 기반을 제공합니다. 이들 함수의 표현력이 ReLU와 비슷하거나 심지어 동등하다는 것을 수학적으로 증명했습니다.
- **ReLU 연구 결과의 광범위한 적용:** ReLU 네트워크에 대해 확립된 대부분의 근사 이론 및 결과들을 이제 폭넓은 종류의 다른 활성화 함수에도 적용할 수 있게 되어 딥러닝 이론의 일반성을 크게 확장했습니다.
- **$\tilde{A}_2$ 활성화 함수의 중요성:** ELU, GELU, SiLU, Mish와 같은 활성화 함수들이 너비나 깊이의 증가 없이 ReLU 네트워크와 동일한 표현력을 가진다는 발견(즉, $(1,1)$ 스케일링)은 매우 중요합니다. 이는 이들 함수가 근사 능력 면에서 ReLU에 비해 **전혀 열등하지 않음**을 의미하며, 향후 활성화 함수 설계 시 표현력보다는 학습 및 수치적 안정성 등 다른 속성에 초점을 맞출 수 있는 이론적 지침을 제공합니다.
- **한계점:** 본 연구에서는 너비 및 깊이 스케일링 인자에 대한 하한을 설정하지 못했으며, 따라서 현재 발견된 스케일링 인자가 일반적인 맥락에서 최적인지는 미해결 과제로 남아 있습니다.
- **근사 불가능한 활성화 함수:** 다항 함수나 유리 함수는 ReLU를 임의로 잘 근사할 수 없음을 명확히 했습니다. 이는 고정된 크기의 네트워크가 이러한 활성화 함수를 사용할 경우 결국 유리 함수 형태를 가지게 되며, 유리 함수는 절댓값 함수와 같은 비-유리 함수를 특정 오차 이하로 근사할 수 없다는 이론적 하한이 존재하기 때문입니다.

## 📌 TL;DR

- **문제:** ReLU 외 다양한 활성화 함수의 경험적 성공에도 불구하고, 이들의 이론적 표현력은 불분명했습니다.
- **방법:** 널리 사용되는 활성화 함수들을 포괄하는 집합 A를 정의하고, $\rho$-활성화 네트워크가 미분, 항등 함수, 곱셈 함수를 근사할 수 있음을 단계적으로 증명하여, 궁극적으로 ReLU 네트워크를 근사할 수 있음을 보였습니다.
- **발견:**
  1. 일반적으로 ReLU 네트워크 (너비 $N$, 깊이 $L$)는 집합 A의 $\rho$-활성화 네트워크 (너비 $3N$, 깊이 $2L$)로 근사 가능합니다.
  2. **핵심적으로**, ELU, GELU, SiLU, Mish와 같은 특정 활성화 함수($\tilde{A}_2$ 부분집합)의 경우, ReLU 네트워크를 **동일한 너비 $N$과 깊이 $L$**의 네트워크로 근사할 수 있어, 표현력 면에서 ReLU와 동등한 최적의 능력을 가짐을 입증했습니다.
- **시사점:** 이 연구는 다양한 현대 활성화 함수의 표현력에 대한 견고한 이론적 토대를 제공하여, 기존 ReLU 네트워크 근사 이론의 적용 범위를 확장하고 새로운 활성화 함수 설계 방향을 제시합니다.
