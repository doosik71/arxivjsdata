{
  "title": "Dynamic ReLU",
  "authors": "Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, Zicheng Liu",
  "year": 2020,
  "url": "http://arxiv.org/abs/2003.10027v2",
  "abstract": "Rectified linear units (ReLU) are commonly used in deep neural networks. So\nfar ReLU and its generalizations (non-parametric or parametric) are static,\nperforming identically for all input samples. In this paper, we propose dynamic\nReLU (DY-ReLU), a dynamic rectifier of which parameters are generated by a\nhyper function over all in-put elements. The key insight is that DY-ReLU\nencodes the global context into the hyper function, and adapts the piecewise\nlinear activation function accordingly. Compared to its static counterpart,\nDY-ReLU has negligible extra computational cost, but significantly more\nrepresentation capability, especially for light-weight neural networks. By\nsimply using DY-ReLU for MobileNetV2, the top-1 accuracy on ImageNet\nclassification is boosted from 72.0% to 76.2% with only 5% additional FLOPs.",
  "citation": 292
}