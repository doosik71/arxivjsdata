# LEARNING ACTIVATION FUNCTIONS TO IMPROVE DEEP NEURAL NETWORKS

Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi

## π§© Problem to Solve

μΈκ³µ μ‹ κ²½λ§μ€ μΌλ°μ μΌλ΅ κ° λ‰΄λ°μ— κ³ μ •λ λΉ„μ„ ν• ν™μ„±ν™” ν•¨μλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤ (μ: ReLU, Sigmoid, Tanh). μ΄λ΅ μ μΌλ΅ μ¶©λ¶„ν ν° λ„¤νΈμ›ν¬λ” μ–΄λ–¤ ν•¨μλ“  κ·Όμ‚¬ν•  μ μμ§€λ§, μ ν•ν• λ„¤νΈμ›ν¬μ—μ„ ν™μ„±ν™” ν•¨μμ μ„ νƒμ€ ν•™μµ μ—­ν•™(νΉν κΉμ€ λ„¤νΈμ›ν¬μ—μ„)κ³Ό λ„¤νΈμ›ν¬μ ν‘ν„λ ¥μ— ν° μν–¥μ„ λ―ΈμΉ©λ‹λ‹¤. λΉ λ¥΄κ³  μ •ν™•ν• λ”¥ λ‰΄λ΄ λ„¤νΈμ›ν¬ ν›λ ¨μ„ κ°€λ¥ν•κ² ν•λ” ν™μ„±ν™” ν•¨μλ¥Ό μ„¤κ³„ν•λ” κ²ƒμ€ ν™λ°ν• μ—°κµ¬ λ¶„μ•Όμ΄μ§€λ§, κ°€λ¥ν• ν•¨μμ κ³µκ°„μ€ κ±°μ νƒμƒ‰λμ§€ μ•μ•μµλ‹λ‹¤. κΈ°μ΅΄μ ν™μ„±ν™” ν•¨μ ν•™μµ λ°©λ²•(μ: μ μ „ μ•κ³ λ¦¬μ¦)μ€ κΈ°λ¥μ΄ μ ν•μ μ΄μ—μµλ‹λ‹¤.

## β¨ Key Contributions

- **μƒλ΅μ΄ μ μ‘ν• μ΅°κ°λ³„ μ„ ν•(Adaptive Piecewise Linear, APL) ν™μ„±ν™” ν•¨μ μ μ•:** κ° λ‰΄λ°μ΄ κ·Έλλ””μ–ΈνΈ ν•κ°•μ„ μ‚¬μ©ν•μ—¬ λ…λ¦½μ μΌλ΅ ν•™μµν•λ” νλΌλ―Έν„°ν™”λ μ΅°κ°λ³„ μ„ ν• ν™μ„±ν™” ν•¨μλ¥Ό λ„μ…ν–μµλ‹λ‹¤.
- **λ†’μ€ ν‘ν„λ ¥:** APL μ λ‹›μ€ λ³Όλ΅(convex) λ° λΉ„λ³Όλ΅(non-convex) ν•¨μλ¥Ό λ¨λ‘ ν‘ν„ν•  μ μμµλ‹λ‹¤.
- **μµμ²¨λ‹¨ μ„±λ¥ λ‹¬μ„±:** CIFAR-10 (7.51%), CIFAR-100 (30.83%) λ° Higgs λ³΄μ† λ¶•κ΄΄ λ¨λ“ λ°μ΄ν„°μ…‹μ—μ„ μ •μ  ReLU(Rectified Linear Unit) κΈ°λ° λ„¤νΈμ›ν¬λ³΄λ‹¤ μ°μν• μµμ²¨λ‹¨ μ„±λ¥μ„ λ‹¬μ„±ν–μµλ‹λ‹¤.
- **ν¨μ¨μ μΈ νλΌλ―Έν„° μ‚¬μ©:** ν™μ„±ν™” ν•¨μ ν•™μµμ— ν•„μ”ν• μ¶”κ°€ νλΌλ―Έν„° μκ°€ λ„¤νΈμ›ν¬μ μ „μ²΄ κ°€μ¤‘μΉ μμ— λΉ„ν•΄ μ‘μµλ‹λ‹¤.
- **λ‹¤μ–‘ν• ν™μ„±ν™” ν•¨μ ν•™μµ:** λ‰΄λ°λ“¤μ΄ μ„λ΅ λ‹¤λ¥Έ ν™μ„±ν™” ν•¨μλ¥Ό ν•™μµν•μ—¬, 'ν•λ‚μ ν™μ„±ν™” ν•¨μκ°€ λ¨λ“  κ²ƒμ— μ ν•©ν•λ‹¤'λ” μΌλ°μ μΈ μ ‘κ·Ό λ°©μ‹μ΄ μµμ ν™”λμ§€ μ•μ„ μ μμμ„ μ‹μ‚¬ν•©λ‹λ‹¤.

## π“ Related Works

- **κ³ μ •λ λΉ„μ„ ν• ν™μ„±ν™” ν•¨μ:**
  - **Logistic, Tanh:** ν¬ν™”(saturation) λ¬Έμ λ΅ μΈν•΄ κΈ°μΈκΈ° μ†μ‹¤(vanishing gradients) λ¬Έμ κ°€ λ°μƒν•  μ μμµλ‹λ‹¤.
  - **ReLU (Rectified Linear Unit):** (Jarrett et al., 2009; Glorot et al., 2011) κΈ°μΈκΈ° μ†μ‹¤ λ¬Έμ λ¥Ό μ™„ν™”ν•κ³  ν›λ ¨ μ†λ„λ¥Ό ν–¥μƒμ‹μΌ°μµλ‹λ‹¤.
  - **Maxout:** (Goodfellow et al., 2013) μ—¬λ¬ μ„ ν• ν•¨μμ μµλ“κ°’μ„ μ·¨ν•λ©°, μ–΄λ–¤ λ³Όλ΅ ν•¨μλ„ κ·Όμ‚¬ν•  μ μμµλ‹λ‹¤.
  - **Probout:** (Springenberg & Riedmiller, 2013) ν™•λ¥ μ  Max ν•¨μλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤.
  - **$L_P$ norm activation:** (Gulcehre et al., 2014) Max ν•¨μ λ€μ‹  $L_P$ λ…Έλ¦„μ„ μ‚¬μ©ν•©λ‹λ‹¤.
  - **Leaky ReLU:** (Maas et al., 2013) μμ μ…λ ¥μ— λ€ν•΄ μ‘μ€ κΈ°μΈκΈ°($k$)λ¥Ό κ°€μ§‘λ‹λ‹¤.
- **ν™μ„±ν™” ν•¨μ ν•™μµ μ—°κµ¬:**
  - μ£Όλ΅ μ μ „ λ° μ§„ν™” μ•κ³ λ¦¬μ¦(Yao, 1999)μ— μ¤‘μ μ„ λ‘μ–΄ λ―Έλ¦¬ μ •μλ ν™μ„±ν™” ν•¨μ μ„ΈνΈμ—μ„ κ° λ‰΄λ°μ— λ€ν• ν•¨μλ¥Ό μ„ νƒν•©λ‹λ‹¤.
  - λ‹¨μΌ μ¤μΌ€μΌλ§ νλΌλ―Έν„°λ¥Ό ν•™μµν•λ” λ°©μ‹(Turner & Miller, 2014)λ„ μ μ•λμ—μµλ‹λ‹¤.
- **λ„¤νΈμ›ν¬ κµ¬μ΅° νμ‹ :**
  - **Network-in-Network (NIN):** (Lin et al., 2013) κ°„λ‹¨ν• ReLUλ¥Ό λ§¤κ°λ³€μλ¥Ό ν•™μµν•λ” μ™„μ „ μ—°κ²° λ„¤νΈμ›ν¬λ΅ λ€μ²΄ν•μ—¬ λ³΄λ‹¤ λ³µμ΅ν• λ³€ν™μ„ κ°€λ¥ν•κ² ν•©λ‹λ‹¤.

## π› οΈ Methodology

1. **μ μ‘ν• μ΅°κ°λ³„ μ„ ν• (APL) μ λ‹› μ •μ:**
   - APL μ λ‹› $i$μ ν™μ„±ν™” ν•¨μ $h_i(x)$λ” νμ§€(hinge) ν•νƒμ ν•¨μλ“¤μ ν•©μΌλ΅ μ •μλ©λ‹λ‹¤.
     $$ h*i(x) = \max(0, x) + \sum*{s=1}^{S} a*{s}^{i} \max(0, -x + b*{s}^{i}) $$
   - $S$λ” νμ§€(piece)μ μλ¥Ό κ²°μ •ν•λ” ν•μ΄νΌνλΌλ―Έν„°μ…λ‹λ‹¤.
   - λ³€μ $a_{s}^{i}$λ” μ„ ν• μ„Έκ·Έλ¨ΌνΈμ κΈ°μΈκΈ°λ¥Ό μ μ–΄ν•κ³ , $b_{s}^{i}$λ” νμ§€μ μ„μΉλ¥Ό κ²°μ •ν•©λ‹λ‹¤.
   - μ΄ νλΌλ―Έν„°λ“¤($a_{s}^{i}$, $b_{s}^{i}$)μ€ ν›λ ¨ μ¤‘ ν‘μ¤€ κ·Έλλ””μ–ΈνΈ ν•κ°•μ„ μ‚¬μ©ν•μ—¬ κ° λ‰΄λ°μ— λ€ν•΄ λ…λ¦½μ μΌλ΅ ν•™μµλ©λ‹λ‹¤.
2. **μ¶”κ°€ νλΌλ―Έν„° μ:** APL μ λ‹› μ‚¬μ© μ‹ μ¶”κ°€λλ” νλΌλ―Έν„° μλ” $2SM$μ΄λ©°, $M$μ€ λ„¤νΈμ›ν¬μ μ΄ μ€λ‹‰ μ λ‹› μμ…λ‹λ‹¤. μ΄λ” μΌλ°μ μΈ λ„¤νΈμ›ν¬μ μ΄ κ°€μ¤‘μΉ μμ— λΉ„ν•΄ μ μ€ μ–‘μ…λ‹λ‹¤.
3. **μ΄λ΅ μ  ν‘ν„λ ¥:**
   - APL μ λ‹›μ€ λ‹¨μΌ μ λ‹›μΌλ΅ λΉ„λ³Όλ΅ ν•¨μλ¥Ό ν•™μµν•  μ μμµλ‹λ‹¤ (Maxoutμ€ λ³Όλ΅ ν•¨μλ§ κ°€λ¥).
   - **μ •λ¦¬ 1:** μ–΄λ–¤ μ—°μ†μ μΈ μ΅°κ°λ³„ μ„ ν• ν•¨μ $g(x)$λΌλ„ μ¶©λ¶„ν ν° $S$μ— λ€ν•΄ μ£Όμ–΄μ§„ λ°©μ •μ‹μΌλ΅ ν‘ν„λ  μ μμµλ‹λ‹¤ (λ‹¨, $x \geq u$μΌ λ• $g(x)=x$μ΄κ³  $x < v$μΌ λ• $\nabla_{x} g(x)=\alpha$λΌλ” λ‘ κ°€μ§€ μ΅°κ±΄ ν•μ—).
4. **λ‹¤λ¥Έ ν™μ„±ν™” ν•¨μμ™€μ λΉ„κµ:**
   - **Maxout:** APL μ λ‹›μ λ™μ‘μ„ μ¬ν„ν•κΈ° μ„ν•΄ ν›¨μ”¬ λ” λ§μ€ νλΌλ―Έν„°($O(SK)$)κ°€ ν•„μ”ν•©λ‹λ‹¤. APLμ€ νλΌλ―Έν„° ν¨μ¨μ„±μ΄ λ” λ†’μµλ‹λ‹¤.
   - **Network-in-Network (NIN):** APL μ λ‹›μ λ™μ‘μ„ μ¬ν„ν•λ ¤λ©΄ λ§¤μ° κ³µκ²©μ μΈ κ°€μ¤‘μΉ κ³µμ (weight-tying) λ°©μ‹μ΄ ν•„μ”ν•©λ‹λ‹¤. APLμ€ NINμ λ‚΄λ¶€ ReLU μ λ‹›μ„ λ€μ²΄ν•μ—¬ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¬ μ μμµλ‹λ‹¤.
5. **ν›λ ¨ μ„¤μ •:**
   - CAFFE μ†ν”„νΈμ›¨μ–΄ ν¨ν‚¤μ§€λ¥Ό μ‚¬μ©ν•μ—¬ μ‹¤ν—μ„ μν–‰ν–μµλ‹λ‹¤.
   - ν•μ΄νΌνλΌλ―Έν„° $S$λ” κ° λ°μ΄ν„°μ…‹μ— λ€ν• κ²€μ¦ μ„ΈνΈλ¥Ό μ‚¬μ©ν•μ—¬ κ²°μ •λμ—μµλ‹λ‹¤.
   - $a_{s}^{i}$ λ° $b_{s}^{i}$ νλΌλ―Έν„°λ” μμΉμ  λ¶μ•μ •μ„±μ„ λ°©μ§€ν•κ³  κ²°κ³Όλ¥Ό κ°μ„ ν•κΈ° μ„ν•΄ $0.001$λ΅ μ¤μΌ€μΌλ§λ L2 μ •κ·ν™” νλ„ν‹°λ¥Ό μ μ©ν–μµλ‹λ‹¤.

## π“ Results

- **CIFAR-10 λ° CIFAR-100 λ°μ΄ν„°μ…‹:**
  - APL μ λ‹›μ€ ν‘μ¤€ CNNκ³Ό Network-in-Network (NIN) μ•„ν‚¤ν…μ² λ¨λ‘μ—μ„ ReLU λ° Leaky ReLUλ¥Ό μΌκ΄€λκ² λ¥κ°€ν–μµλ‹λ‹¤.
  - λ°μ΄ν„° μ¦κ°•(Data Augmentation)μ„ μ‚¬μ©ν• NIN + APL μ λ‹›μ€ CIFAR-10μ—μ„ **7.51%**μ μ¤λ¥μ¨, CIFAR-100μ—μ„ **30.83%**μ μ¤λ¥μ¨μ„ κΈ°λ΅ν•λ©°, λ‹Ήμ‹ λ³΄κ³ λ μµκ³  μ„±λ¥μ„ λ‹¬μ„±ν–μµλ‹λ‹¤.
  - APL μ λ‹›μ€ Leaky ReLUλ³΄λ‹¤ μ§€μ†μ μΌλ΅ λ” λ‚μ€ μ„±λ¥μ„ λ³΄μ—¬, λΉ„μ„ ν•μ„±μ„ λ―Έμ„Έ μ΅°μ •ν•λ” κ²ƒμ΄ μ¤‘μ”ν•¨μ„ μ…μ¦ν–μµλ‹λ‹¤.
- **Higgs λ³΄μ† λ¶•κ΄΄ λ°μ΄ν„°μ…‹:**
  - APL μ λ‹›μ„ μ‚¬μ©ν• DNNμ€ AUC **0.804**μ™€ Discovery Significance **3.41$\sigma$**λ¥Ό λ‹¬μ„±ν•μ—¬, ReLU κΈ°λ° λ‹¨μΌ λ„¤νΈμ›ν¬ λ° 5κ° λ„¤νΈμ›ν¬ μ•™μƒλΈ”λ³΄λ‹¤ λ›°μ–΄λ‚ μµμ²¨λ‹¨ μ„±λ¥μ„ κΈ°λ΅ν–μµλ‹λ‹¤.
- **APL μ λ‹› ν•μ΄νΌνλΌλ―Έν„° $S$μ ν¨κ³Ό:**
  - ν™μ„±ν™” ν•¨μλ¥Ό ν•™μµν•λ” κ²ƒμ΄ μ¤‘μ”ν•©λ‹λ‹¤: $S=1$λ΅ μ„¤μ •ν•κ³  ν•™μµν•μ§€ μ•μ€ κ²½μ°(κ³ μ •λ ν™μ„±ν™” ν•¨μ)λ” κΈ°λ³Έ ReLUμ™€ λΉ„μ·ν• μ¤λ¥μ¨μ„ λ³΄μ€μ§€λ§, $S=1$λ΅ μ„¤μ •ν•κ³  ν•™μµν• κ²½μ°λ” μ„±λ¥μ΄ ν¬κ² ν–¥μƒλμ—μµλ‹λ‹¤.
  - $S$ κ°’μ„ μ¦κ°€μ‹ν‚¬μλ΅ μ„±λ¥μ΄ ν–¥μƒλλ‹¤κ°€ (μ: $S=1$μ—μ„ $S=5$), μΌμ • μμ¤€ μ΄μƒμ—μ„λ” κ°μ†ν•κ±°λ‚ μ •μ²΄λλ” κ²½ν–¥μ„ λ³΄μ€μµλ‹λ‹¤ (μ: $S=10$).
- **ν™μ„±ν™” ν•¨μ μ‹κ°ν™” λ° λ¶„μ„:**
  - ν•™μµλ APL ν•¨μλ“¤μ€ λ‰΄λ°λ§λ‹¤ λ§¤μ° λ‹¤μ–‘ν•κ² λ‚νƒ€λ‚¬μΌλ©°, μ΄λ” "ν•λ‚μ ν™μ„±ν™” ν•¨μκ°€ λ¨λ“  κ²ƒμ— μ ν•©ν•λ‹¤"λ” μ ‘κ·Ό λ°©μ‹μ΄ μµμ μ΄ μ•„λ‹ μ μμμ„ λ³΄μ—¬μ¤λ‹λ‹¤.
  - CIFAR-100 λ° Higgs λ°μ΄ν„°μ…‹μ κ²½μ° CIFAR-10λ³΄λ‹¤ ν•™μµλ ν™μ„±ν™” ν•¨μμ λ¶„μ‚°(variance)μ΄ λ” μ»Έμµλ‹λ‹¤. Higgs λ°μ΄ν„°μ…‹μ—μ„λ” μƒμ„ κ³„μΈµμΌλ΅ κ°μλ΅ λ¶„μ‚°μ΄ κ°μ†ν•λ” κ²½ν–¥μ΄ μμ—μµλ‹λ‹¤.

## π§  Insights & Discussion

- μ΄ μ—°κµ¬μ ν•µμ‹¬ ν†µμ°°μ€ λ¨λ“  λ‰΄λ°μ— λ‹¨μΌν•κ³  κ³ μ •λ ν™μ„±ν™” ν•¨μλ¥Ό μ‚¬μ©ν•λ” λ€μ‹ , κ° λ‰΄λ°μ΄ μμ²΄μ μΈ ν™μ„±ν™” ν•¨μλ¥Ό ν•™μµν•κ² ν•¨μΌλ΅μ¨ λ”¥ λ‰΄λ΄ λ„¤νΈμ›ν¬μ μ„±λ¥μ„ ν¬κ² ν–¥μƒμ‹ν‚¬ μ μλ‹¤λ” κ²ƒμ…λ‹λ‹¤.
- APL μ λ‹›μ€ μ΄λ¬ν• ν•™μµμ„ μ„ν• μ μ—°ν•κ³  ν‘ν„λ ¥μ΄ ν’λ¶€ν• λ°©λ²•μ„ μ κ³µν•λ©°, μƒλ€μ μΌλ΅ μ μ€ νλΌλ―Έν„° μ¦κ°€λ΅ λ³µμ΅ν• (λΉ„λ³Όλ΅ ν¬ν•¨) μ΅°κ°λ³„ μ„ ν• ν•¨μλ¥Ό κ·Όμ‚¬ν•  μ μμµλ‹λ‹¤.
- ReLUλ‚ Leaky ReLUμ™€ κ°™μ€ κ³ μ • ν•¨μλ³΄λ‹¤ APL μ λ‹›μ΄ μΌκ΄€λκ² μ°μν• μ„±λ¥μ„ λ³΄μ΄λ” κ²ƒμ€ νΉμ • μ‘μ—…κ³Ό λ°μ΄ν„°μ— λ§κ² λΉ„μ„ ν•μ„±μ„ μ μ‘μ μΌλ΅ μ΅°μ •ν•λ” κ²ƒμ μ¤‘μ”μ„±μ„ κ°•μ΅°ν•©λ‹λ‹¤.
- λ‹¤μ–‘ν•κ² ν•™μµλ ν™μ„±ν™” ν•¨μμ μ‹κ°ν™”λ” λ‹¤λ¥Έ λ‰΄λ°λ“¤μ΄ κ°κΈ° λ‹¤λ¥Έ λΉ„μ„ ν• λ³€ν™μΌλ΅λ¶€ν„° μ΄μ μ„ μ–»μ„ μ μμμ„ μ‹μ‚¬ν•λ©°, κ· μΌν• ν™μ„±ν™” ν•¨μμ μ „ν†µμ μΈ κ°€μ •μ—μ„ λ²—μ–΄λ‚  ν•„μ”μ„±μ„ μ κΈ°ν•©λ‹λ‹¤.
- $S$μ™€ κ°™μ€ ν•μ΄νΌνλΌλ―Έν„°μ μµμ  κ°’μ„ μ°Ύλ” κ²ƒμ€ μ—¬μ „ν κ²€μ¦ κ³Όμ •μ„ ν•„μ”λ΅ ν•μ§€λ§, $a_{s}^{i}, b_{s}^{i}$μ— λ€ν• L2 μ •κ·ν™”λ” μμΉμ  μ•μ •μ„±κ³Ό μ„±λ¥ ν–¥μƒμ— ν•„μμ μ΄μ—μµλ‹λ‹¤.
- Maxout λ° Network-in-Networkμ™€μ λΉ„κµλ¥Ό ν†µν•΄ APLμ νλΌλ―Έν„° ν¨μ¨μ„±μ„ μ…μ¦ν•μ€μΌλ©°, μ΄λ” ν•μ΄λΈλ¦¬λ“ μ•„ν‚¤ν…μ²(μ: MLPConv κ³„μΈµ λ‚΄ APL μ λ‹›)μ κ°€λ¥μ„±μ„ μ—΄μ–΄μ¤λ‹λ‹¤.

## π“ TL;DR

κΈ°μ΅΄ λ”¥λ¬λ‹μ—μ„ κ³ μ •λ ν™μ„±ν™” ν•¨μλ¥Ό μ‚¬μ©ν•λ” λ€μ‹ , κ° λ‰΄λ°μ΄ κ·Έλλ””μ–ΈνΈ ν•κ°•μ„ ν†µν•΄ μμ²΄μ μΈ μ΅°κ°λ³„ μ„ ν•(piecewise linear) ν™μ„±ν™” ν•¨μλ¥Ό ν•™μµν•λ” Adaptive Piecewise Linear (APL) μ λ‹›μ„ μ μ•ν•©λ‹λ‹¤. μ΄ λ°©μ‹μ€ Maxoutκ³Ό κ°™μ€ κΈ°μ΅΄ λ°©λ²•λ³΄λ‹¤ μ μ€ νλΌλ―Έν„°λ΅ λΉ„λ³Όλ΅(non-convex) ν•¨μκΉμ§€ ν‘ν„ν•  μ μμΌλ©°, CIFAR-10, CIFAR-100, Higgs λ³΄μ† λ¶•κ΄΄μ™€ κ°™μ€ λ²¤μΉλ§ν¬μ—μ„ μµμ²¨λ‹¨ μ„±λ¥μ„ λ‹¬μ„±ν•μ—¬, λ‰΄λ°λ³„ ν™μ„±ν™” ν•¨μ ν•™μµμ΄ λ”¥λ¬λ‹ λ¨λΈμ μ„±λ¥μ„ ν¬κ² ν–¥μƒμ‹ν‚΄μ„ λ³΄μ—¬μ¤λ‹λ‹¤.
