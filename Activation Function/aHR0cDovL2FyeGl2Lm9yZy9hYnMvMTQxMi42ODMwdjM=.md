# LEARNING ACTIVATION FUNCTIONS TO IMPROVE DEEP NEURAL NETWORKS

Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi

## 🧩 Problem to Solve

인공 신경망은 일반적으로 각 뉴런에 고정된 비선형 활성화 함수를 사용합니다 (예: ReLU, Sigmoid, Tanh). 이론적으로 충분히 큰 네트워크는 어떤 함수든 근사할 수 있지만, 유한한 네트워크에서 활성화 함수의 선택은 학습 역학(특히 깊은 네트워크에서)과 네트워크의 표현력에 큰 영향을 미칩니다. 빠르고 정확한 딥 뉴럴 네트워크 훈련을 가능하게 하는 활성화 함수를 설계하는 것은 활발한 연구 분야이지만, 가능한 함수의 공간은 거의 탐색되지 않았습니다. 기존의 활성화 함수 학습 방법(예: 유전 알고리즘)은 기능이 제한적이었습니다.

## ✨ Key Contributions

- **새로운 적응형 조각별 선형(Adaptive Piecewise Linear, APL) 활성화 함수 제안:** 각 뉴런이 그래디언트 하강을 사용하여 독립적으로 학습하는 파라미터화된 조각별 선형 활성화 함수를 도입했습니다.
- **높은 표현력:** APL 유닛은 볼록(convex) 및 비볼록(non-convex) 함수를 모두 표현할 수 있습니다.
- **최첨단 성능 달성:** CIFAR-10 (7.51%), CIFAR-100 (30.83%) 및 Higgs 보손 붕괴 모드 데이터셋에서 정적 ReLU(Rectified Linear Unit) 기반 네트워크보다 우수한 최첨단 성능을 달성했습니다.
- **효율적인 파라미터 사용:** 활성화 함수 학습에 필요한 추가 파라미터 수가 네트워크의 전체 가중치 수에 비해 작습니다.
- **다양한 활성화 함수 학습:** 뉴런들이 서로 다른 활성화 함수를 학습하여, '하나의 활성화 함수가 모든 것에 적합하다'는 일반적인 접근 방식이 최적화되지 않을 수 있음을 시사합니다.

## 📎 Related Works

- **고정된 비선형 활성화 함수:**
  - **Logistic, Tanh:** 포화(saturation) 문제로 인해 기울기 소실(vanishing gradients) 문제가 발생할 수 있습니다.
  - **ReLU (Rectified Linear Unit):** (Jarrett et al., 2009; Glorot et al., 2011) 기울기 소실 문제를 완화하고 훈련 속도를 향상시켰습니다.
  - **Maxout:** (Goodfellow et al., 2013) 여러 선형 함수의 최댓값을 취하며, 어떤 볼록 함수도 근사할 수 있습니다.
  - **Probout:** (Springenberg & Riedmiller, 2013) 확률적 Max 함수를 사용합니다.
  - **$L_P$ norm activation:** (Gulcehre et al., 2014) Max 함수 대신 $L_P$ 노름을 사용합니다.
  - **Leaky ReLU:** (Maas et al., 2013) 음수 입력에 대해 작은 기울기($k$)를 가집니다.
- **활성화 함수 학습 연구:**
  - 주로 유전 및 진화 알고리즘(Yao, 1999)에 중점을 두어 미리 정의된 활성화 함수 세트에서 각 뉴런에 대한 함수를 선택합니다.
  - 단일 스케일링 파라미터를 학습하는 방식(Turner & Miller, 2014)도 제안되었습니다.
- **네트워크 구조 혁신:**
  - **Network-in-Network (NIN):** (Lin et al., 2013) 간단한 ReLU를 매개변수를 학습하는 완전 연결 네트워크로 대체하여 보다 복잡한 변환을 가능하게 합니다.

## 🛠️ Methodology

1. **적응형 조각별 선형 (APL) 유닛 정의:**
   - APL 유닛 $i$의 활성화 함수 $h_i(x)$는 힌지(hinge) 형태의 함수들의 합으로 정의됩니다.
     $$ h*i(x) = \max(0, x) + \sum*{s=1}^{S} a*{s}^{i} \max(0, -x + b*{s}^{i}) $$
   - $S$는 힌지(piece)의 수를 결정하는 하이퍼파라미터입니다.
   - 변수 $a_{s}^{i}$는 선형 세그먼트의 기울기를 제어하고, $b_{s}^{i}$는 힌지의 위치를 결정합니다.
   - 이 파라미터들($a_{s}^{i}$, $b_{s}^{i}$)은 훈련 중 표준 그래디언트 하강을 사용하여 각 뉴런에 대해 독립적으로 학습됩니다.
2. **추가 파라미터 수:** APL 유닛 사용 시 추가되는 파라미터 수는 $2SM$이며, $M$은 네트워크의 총 은닉 유닛 수입니다. 이는 일반적인 네트워크의 총 가중치 수에 비해 적은 양입니다.
3. **이론적 표현력:**
   - APL 유닛은 단일 유닛으로 비볼록 함수를 학습할 수 있습니다 (Maxout은 볼록 함수만 가능).
   - **정리 1:** 어떤 연속적인 조각별 선형 함수 $g(x)$라도 충분히 큰 $S$에 대해 주어진 방정식으로 표현될 수 있습니다 (단, $x \geq u$일 때 $g(x)=x$이고 $x < v$일 때 $\nabla_{x} g(x)=\alpha$라는 두 가지 조건 하에).
4. **다른 활성화 함수와의 비교:**
   - **Maxout:** APL 유닛의 동작을 재현하기 위해 훨씬 더 많은 파라미터($O(SK)$)가 필요합니다. APL은 파라미터 효율성이 더 높습니다.
   - **Network-in-Network (NIN):** APL 유닛의 동작을 재현하려면 매우 공격적인 가중치 공유(weight-tying) 방식이 필요합니다. APL은 NIN의 내부 ReLU 유닛을 대체하여 성능을 향상시킬 수 있습니다.
5. **훈련 설정:**
   - CAFFE 소프트웨어 패키지를 사용하여 실험을 수행했습니다.
   - 하이퍼파라미터 $S$는 각 데이터셋에 대한 검증 세트를 사용하여 결정되었습니다.
   - $a_{s}^{i}$ 및 $b_{s}^{i}$ 파라미터는 수치적 불안정성을 방지하고 결과를 개선하기 위해 $0.001$로 스케일링된 L2 정규화 페널티를 적용했습니다.

## 📊 Results

- **CIFAR-10 및 CIFAR-100 데이터셋:**
  - APL 유닛은 표준 CNN과 Network-in-Network (NIN) 아키텍처 모두에서 ReLU 및 Leaky ReLU를 일관되게 능가했습니다.
  - 데이터 증강(Data Augmentation)을 사용한 NIN + APL 유닛은 CIFAR-10에서 **7.51%**의 오류율, CIFAR-100에서 **30.83%**의 오류율을 기록하며, 당시 보고된 최고 성능을 달성했습니다.
  - APL 유닛은 Leaky ReLU보다 지속적으로 더 나은 성능을 보여, 비선형성을 미세 조정하는 것이 중요함을 입증했습니다.
- **Higgs 보손 붕괴 데이터셋:**
  - APL 유닛을 사용한 DNN은 AUC **0.804**와 Discovery Significance **3.41$\sigma$**를 달성하여, ReLU 기반 단일 네트워크 및 5개 네트워크 앙상블보다 뛰어난 최첨단 성능을 기록했습니다.
- **APL 유닛 하이퍼파라미터 $S$의 효과:**
  - 활성화 함수를 학습하는 것이 중요합니다: $S=1$로 설정하고 학습하지 않은 경우(고정된 활성화 함수)는 기본 ReLU와 비슷한 오류율을 보였지만, $S=1$로 설정하고 학습한 경우는 성능이 크게 향상되었습니다.
  - $S$ 값을 증가시킬수록 성능이 향상되다가 (예: $S=1$에서 $S=5$), 일정 수준 이상에서는 감소하거나 정체되는 경향을 보였습니다 (예: $S=10$).
- **활성화 함수 시각화 및 분석:**
  - 학습된 APL 함수들은 뉴런마다 매우 다양하게 나타났으며, 이는 "하나의 활성화 함수가 모든 것에 적합하다"는 접근 방식이 최적이 아닐 수 있음을 보여줍니다.
  - CIFAR-100 및 Higgs 데이터셋의 경우 CIFAR-10보다 학습된 활성화 함수의 분산(variance)이 더 컸습니다. Higgs 데이터셋에서는 상위 계층으로 갈수록 분산이 감소하는 경향이 있었습니다.

## 🧠 Insights & Discussion

- 이 연구의 핵심 통찰은 모든 뉴런에 단일하고 고정된 활성화 함수를 사용하는 대신, 각 뉴런이 자체적인 활성화 함수를 학습하게 함으로써 딥 뉴럴 네트워크의 성능을 크게 향상시킬 수 있다는 것입니다.
- APL 유닛은 이러한 학습을 위한 유연하고 표현력이 풍부한 방법을 제공하며, 상대적으로 적은 파라미터 증가로 복잡한 (비볼록 포함) 조각별 선형 함수를 근사할 수 있습니다.
- ReLU나 Leaky ReLU와 같은 고정 함수보다 APL 유닛이 일관되게 우수한 성능을 보이는 것은 특정 작업과 데이터에 맞게 비선형성을 적응적으로 조정하는 것의 중요성을 강조합니다.
- 다양하게 학습된 활성화 함수의 시각화는 다른 뉴런들이 각기 다른 비선형 변환으로부터 이점을 얻을 수 있음을 시사하며, 균일한 활성화 함수의 전통적인 가정에서 벗어날 필요성을 제기합니다.
- $S$와 같은 하이퍼파라미터의 최적 값을 찾는 것은 여전히 검증 과정을 필요로 하지만, $a_{s}^{i}, b_{s}^{i}$에 대한 L2 정규화는 수치적 안정성과 성능 향상에 필수적이었습니다.
- Maxout 및 Network-in-Network와의 비교를 통해 APL의 파라미터 효율성을 입증하였으며, 이는 하이브리드 아키텍처(예: MLPConv 계층 내 APL 유닛)의 가능성을 열어줍니다.

## 📌 TL;DR

기존 딥러닝에서 고정된 활성화 함수를 사용하는 대신, 각 뉴런이 그래디언트 하강을 통해 자체적인 조각별 선형(piecewise linear) 활성화 함수를 학습하는 Adaptive Piecewise Linear (APL) 유닛을 제안합니다. 이 방식은 Maxout과 같은 기존 방법보다 적은 파라미터로 비볼록(non-convex) 함수까지 표현할 수 있으며, CIFAR-10, CIFAR-100, Higgs 보손 붕괴와 같은 벤치마크에서 최첨단 성능을 달성하여, 뉴런별 활성화 함수 학습이 딥러닝 모델의 성능을 크게 향상시킴을 보여줍니다.
