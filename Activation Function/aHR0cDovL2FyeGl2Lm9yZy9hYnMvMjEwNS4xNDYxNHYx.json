{
  "title": "Evolution of Activation Functions: An Empirical Investigation",
  "authors": "Andrew Nader, Danielle Azar",
  "year": 2021,
  "url": "http://arxiv.org/abs/2105.14614v1",
  "abstract": "The hyper-parameters of a neural network are traditionally designed through a\ntime consuming process of trial and error that requires substantial expert\nknowledge. Neural Architecture Search (NAS) algorithms aim to take the human\nout of the loop by automatically finding a good set of hyper-parameters for the\nproblem at hand. These algorithms have mostly focused on hyper-parameters such\nas the architectural configurations of the hidden layers and the connectivity\nof the hidden neurons, but there has been relatively little work on automating\nthe search for completely new activation functions, which are one of the most\ncrucial hyper-parameters to choose. There are some widely used activation\nfunctions nowadays which are simple and work well, but nonetheless, there has\nbeen some interest in finding better activation functions. The work in the\nliterature has mostly focused on designing new activation functions by hand, or\nchoosing from a set of predefined functions while this work presents an\nevolutionary algorithm to automate the search for completely new activation\nfunctions. We compare these new evolved activation functions to other existing\nand commonly used activation functions. The results are favorable and are\nobtained from averaging the performance of the activation functions found over\n30 runs, with experiments being conducted on 10 different datasets and\narchitectures to ensure the statistical robustness of the study.",
  "citation": 22
}