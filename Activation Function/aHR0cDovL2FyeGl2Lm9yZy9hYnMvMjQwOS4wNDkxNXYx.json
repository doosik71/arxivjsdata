{
  "title": "Activation Function Optimization Scheme for Image Classification",
  "authors": "Abdur Rahman, Lu He, Haifeng Wang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.04915v1",
  "abstract": "Activation function has a significant impact on the dynamics, convergence,\nand performance of deep neural networks. The search for a consistent and\nhigh-performing activation function has always been a pursuit during deep\nlearning model development. Existing state-of-the-art activation functions are\nmanually designed with human expertise except for Swish. Swish was developed\nusing a reinforcement learning-based search strategy. In this study, we propose\nan evolutionary approach for optimizing activation functions specifically for\nimage classification tasks, aiming to discover functions that outperform\ncurrent state-of-the-art options. Through this optimization framework, we\nobtain a series of high-performing activation functions denoted as Exponential\nError Linear Unit (EELU). The developed activation functions are evaluated for\nimage classification tasks from two perspectives: (1) five state-of-the-art\nneural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and\nCompact Convolutional Transformer which cover computationally heavy to light\nneural networks, and (2) eight standard datasets, including CIFAR10,\nImagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15,\nand TinyImageNet which cover from typical machine vision benchmark,\nagricultural image applications to medical image applications. Finally, we\nstatistically investigate the generalization of the resultant activation\nfunctions developed through the optimization scheme. With a Friedman test, we\nconclude that the optimization scheme is able to generate activation functions\nthat outperform the existing standard ones in 92.8% cases among 28 different\ncases studied, and $-x\\cdot erf(e^{-x})$ is found to be the best activation\nfunction for image classification generated by the optimization scheme.",
  "citation": 4
}