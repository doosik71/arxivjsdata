# FReLU: Flexible Rectified Linear Units for Improving Convolutional Neural Networks

Suo Qiu, Xiangmin Xu and Bolun Cai

## π§© Problem to Solve

μ‹¬μΈµ ν•©μ„±κ³± μ‹ κ²½λ§(CNN)μ—μ„ λ„λ¦¬ μ‚¬μ©λλ” ν™μ„±ν™” ν•¨μμΈ ReLU(Rectified Linear Unit)λ” μμ κ°’μ„ 0μΌλ΅ κ³ μ •ν•μ—¬ "μμ μ •λ³΄ μ†μ‹¤(negative missing)"μ„ λ°μƒμ‹ν‚¤κ³ , ν‘ν„λ ¥μ„ μ ν•ν•©λ‹λ‹¤. PReLUλ‚ LReLUμ™€ κ°™μ€ ReLU λ³€ν•λ“¤μ€ μμ λ¶€λ¶„μ„ ν—μ©ν•μ—¬ μ΄ λ¬Έμ λ¥Ό μ™„ν™”ν•μ§€λ§, ν¬μ†μ„±(sparsity)μ„ μƒμ„ μ μμµλ‹λ‹¤. ELUλ” ν™μ„±ν™” ν‰κ· μ„ 0μ— κ°€κΉκ² μ μ§€ν•λ” "μ λ΅-μ μ‚¬(zero-like)" μ†μ„±μ„ μ κ³µν•μ§€λ§, Batch Normalization(BN)κ³Όμ νΈν™μ„± λ¬Έμ κ°€ μκ³  μ§€μ ν•¨μλ΅ μΈν•΄ κ³„μ‚° λΉ„μ©μ΄ λ†’μµλ‹λ‹¤. μ΄ μ—°κµ¬λ” μμ κ°’μ ν¨κ³Όλ¥Ό νƒμƒ‰ν•μ—¬ ReLUμ ν•κ³„λ¥Ό κ·Ήλ³µν•κ³ , κ³„μ‚° ν¨μ¨μ μ΄λ©° BNκ³Ό νΈν™λλ” μƒλ΅μ΄ ν™μ„±ν™” ν•¨μλ¥Ό μ μ•ν•λ” κ²ƒμ„ λ©ν‘λ΅ ν•©λ‹λ‹¤.

## β¨ Key Contributions

- **FReLU(Flexible Rectified Linear Unit) μ μ•:** ReLUμ μ •λ¥ μ§€μ (rectified point)μ„ ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°λ΅ μ¬μ„¤κ³„ν•μ—¬ μƒλ΅μ΄ ν™μ„±ν™” ν•¨μ FReLUλ¥Ό μ μ•ν•©λ‹λ‹¤.
- **ν‘ν„λ ¥ ν–¥μƒ:** ν•™μµ κ°€λ¥ν• νΈν–¥ $b$λ¥Ό ν†µν•΄ ν™μ„±ν™” μ¶λ ¥μ μƒνƒλ¥Ό ν™•μ¥ν•μ—¬ μ‹ κ²½λ§μ ν‘ν„λ ¥μ„ ν–¥μƒμ‹ν‚µλ‹λ‹¤. ν•™μµμ΄ μ„±κ³µμ μΌλ΅ μ΄λ£¨μ–΄μ§€λ©΄ $b$λ” μμ κ°’μΌλ΅ μλ ΄ν•λ” κ²½ν–¥μ„ λ³΄μ…λ‹λ‹¤.
- **λΉ λ¥Έ μλ ΄ λ° λ†’μ€ μ„±λ¥:** λ‹¤μ–‘ν• CNN μ•„ν‚¤ν…μ²μ—μ„ κΈ°μ΅΄ ReLU λ° κ·Έ λ³€ν•λ“¤λ³΄λ‹¤ λ” λΉ λ¥΄κ³  μ•μ •μ μΈ μλ ΄κ³Ό λ” λ†’μ€ λ¶„λ¥ μ„±λ¥μ„ λ‹¬μ„±ν•©λ‹λ‹¤.
- **λ‚®μ€ κ³„μ‚° λΉ„μ©:** μ§€μ ν•¨μλ¥Ό μ‚¬μ©ν•μ§€ μ•μ•„ κ³„μ‚° λΉ„μ©μ΄ λ‚®μΌλ©°, ReLUμ™€ μ μ‚¬ν• λΉ„μ„ ν•μ„± λ° ν¬μ†μ„± νΉμ„±μ„ μ μ§€ν•©λ‹λ‹¤.
- **Batch Normalization νΈν™μ„±:** Batch Normalization(BN)κ³Ό λ†’μ€ νΈν™μ„±μ„ λ³΄μ—¬, ν° ν•™μµλ¥ μ—μ„λ„ μ•μ •μ μΈ ν•™μµκ³Ό μ„±λ¥ ν–¥μƒμ„ κ°€λ¥ν•κ² ν•©λ‹λ‹¤.
- **μκΈ° μ μ‘μ„±:** νΉμ • κ°€μ •μ— μμ΅΄ν•μ§€ μ•κ³  μμ²΄ μ μ‘μ μΌλ΅ μµμ μ ν™μ„±ν™” ν•¨μ ν•νƒλ¥Ό ν•™μµν•©λ‹λ‹¤.

## π“ Related Works

- **ReLU [2, 5]:** μ–‘μ μ…λ ¥μ€ κ·Έλ€λ΅ μ „λ‹¬ν•κ³  μμ μ…λ ¥μ€ 0μΌλ΅ λ§λ“λ” ν™μ„±ν™” ν•¨μμ…λ‹λ‹¤. κΈ°μΈκΈ° μ†μ‹¤ λ¬Έμ λ¥Ό μ™„ν™”ν•κ³  κ³„μ‚° ν¨μ¨μ μ΄μ§€λ§, μμ κ°’ μ •λ³΄λ¥Ό μ†μ‹¤ν•κ³  λΉ„-μ λ΅ μ¤‘μ‹¬(non-zero-like) νΉμ„±μ„ κ°€μ§‘λ‹λ‹¤.
- **LReLU [7], PReLU [8], RReLU [9]:** μμ λ¶€λ¶„μ— μ‘μ€ κ³ μ • λλ” ν•™μµ κ°€λ¥ν• κΈ°μΈκΈ°λ¥Ό λ¶€μ—¬ν•μ—¬ μμ κ°’ μ •λ³΄λ¥Ό μΌλ¶€ μ μ§€ν•λ” ReLUμ λ³€ν•λ“¤μ…λ‹λ‹¤. ν•μ§€λ§ ν¬μ†μ„±μ„ νκ΄΄ν•  μ μμµλ‹λ‹¤.
- **ELU [10]:** μμ κ°’μ„ μ μ§€ν•κ³  μμ λ¶€λ¶„μ„ ν¬ν™”μ‹μΌ ν™μ„±ν™” ν‰κ· μ„ 0μ— κ°€κΉκ² λ§λ“λ” "μ λ΅-μ μ‚¬" μ†μ„±μ„ μ¶”κµ¬ν•©λ‹λ‹¤. κ·Έλ¬λ‚ BNκ³Όμ νΈν™μ„±μ΄ λ‚®κ³  μ§€μ ν•¨μλ΅ μΈν•΄ κ³„μ‚° λΉ„μ©μ΄ λ†’μµλ‹λ‹¤.
- **SReLU:** $max(x, Ξ”)$λ΅ μ •μλλ©°, $Ξ”$λ” ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°μ…λ‹λ‹¤. FReLUμ™€ μ μ‚¬ν•κ² μν‰ λ° μμ§ μ΄λ™μ μ μ—°μ„±μ„ κ°€μ§‘λ‹λ‹¤.

## π› οΈ Methodology

1. **FReLU μ •μ:**
   - κΈ°μ΅΄ ReLUλ” $relu(x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases}$ λ΅ μ •μλ©λ‹λ‹¤.
   - FReLUλ” ReLUμ μ •λ¥ μ§€μ μ„ ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°λ΅ μ¬μ„¤κ³„ν•μ—¬ λ‹¤μκ³Ό κ°™μ΄ μ •μλ©λ‹λ‹¤:
     $$frelu(x) = relu(x) + b_l$$
     μ—¬κΈ°μ„ $b_l$μ€ $l$-λ²μ§Έ λ μ΄μ–΄μ ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°μ…λ‹λ‹¤.
   - **μμ „ν(Forward Pass):**
     $$frelu(x) = \begin{cases} x + b_l & \text{if } x > 0 \\ b_l & \text{if } x \le 0 \end{cases}$$
   - **μ—­μ „ν(Backward Pass):**
     $$\frac{\partial frelu(x)}{\partial x} = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases}$$
     $$\frac{\partial frelu(x)}{\partial b_l} = 1$$
2. **νλΌλ―Έν„° μ΄κΈ°ν™”:**
   - μ•μ •μ μΈ μ—­μ „νλ¥Ό μ„ν•΄ MSRA [8]μ™€ λ™μΌν• μ΄κΈ°ν™” μ΅°κ±΄ $(1/2) \hat{n_l} Var[w_l] = 1, \forall l$λ¥Ό λ”°λ¦…λ‹λ‹¤.
   - Batch Normalization(BN)μ„ μ‚¬μ©ν•λ©΄ μ΄κΈ°ν™” λ°©μ‹μ— λ λ―Όκ°ν•΄μ§€λ―€λ΅, λ¨λ“  μ‹¤ν—μ—μ„ MSRA λ°©λ²•μ„ μ‚¬μ©ν•©λ‹λ‹¤.
3. **FReLU λ¶„μ„ λ° λ…Όμ:**
   - **μƒνƒ ν™•μ¥(State Extension):** $b < 0$μΌ λ•, FReLUλ” μ„Έ κ°€μ§€ μ¶λ ¥ μƒνƒ(μ–‘μ, μμ, λΉ„ν™μ„±ν™”)λ¥Ό κ°€μ§ μ μμ–΄, ReLUμ λ‘ κ°€μ§€ μƒνƒλ³΄λ‹¤ $n$κ°μ μ λ‹›μ— λ€ν•΄ $2^n$μ—μ„ $3^n$μΌλ΅ λ” λ§μ€ μ¶λ ¥ μƒνƒλ¥Ό μƒμ„±ν•μ—¬ ν‘ν„λ ¥μ„ λ†’μ…λ‹λ‹¤.
   - **Batch Normalizationκ³Όμ νΈν™μ„±:** FReLUμ $max(x,0)$ κµ¬μ΅°λ” BNμ μ¤μΌ€μΌ($\gamma$) λ° νΈν–¥($\beta$) νλΌλ―Έν„°μ™€ FReLUμ $b$ νλΌλ―Έν„° μ‚¬μ΄μ ν•™μµ μ¶©λμ„ λ¶„λ¦¬ν•μ—¬ νΈν™μ„±μ„ λ†’μ…λ‹λ‹¤.
   - **λ‹¤λ¥Έ ν™μ„±ν™” ν•¨μμ™€μ λΉ„κµ:**
     - ReLUμ™€ λΉ„κµν•μ—¬ ν•™μµ κ°€λ¥ν• νΈν–¥ $b$λ¥Ό μ¶”κ°€ν•μ—¬ μ¶λ ¥ λ²”μ„λ¥Ό ν™•μ¥ν•κ³  ν‘ν„λ ¥μ„ λ†’μ…λ‹λ‹¤.
     - PReLU/LReLUμ™€ λ‹¬λ¦¬ μμ μ…λ ¥μ— λ€ν• κΈ°μΈκΈ°λ¥Ό 0μΌλ΅ μ μ§€ν•μ—¬ ν¬μ†μ„±μ„ λ³΄μ΅΄ν•©λ‹λ‹¤.
     - ELUμ™€ λ‹¬λ¦¬ μ§€μ μ—°μ‚° μ—†μ΄ νΈν–¥ $b$λ¥Ό μ‚¬μ©ν•λ―€λ΅ κ³„μ‚° λ³µμ΅μ„±μ΄ λ‚®κ³  BN νΈν™μ„±μ΄ λ›°μ–΄λ‚©λ‹λ‹¤.

## π“ Results

- **SmallNet (CIFAR-100):** FReLUλ” λ‹¤λ¥Έ ν™μ„±ν™” ν•¨μ(ReLU, PReLU, ELU, SReLU) λ€λΉ„ κ°€μ¥ λΉ λ¥Έ μλ ΄ μ†λ„μ™€ κ°€μ¥ λ‚®μ€ ν…μ¤νΈ μ¤λ¥μ¨(36.87%)μ„ λ‹¬μ„±ν–μµλ‹λ‹¤. Batch Normalizationμ„ μ‚¬μ©ν•  λ•λ„ FReLUμ μ„±λ¥ ν–¥μƒμ΄ λ‘λ“λ¬μ΅μΌλ©°, νΉν λ†’μ€ ν•™μµλ¥ (0.1)μ—μ„ BNκ³Όμ νΈν™μ„±μ΄ λ›°μ–΄λ‚¨μ„ μ…μ¦ν–μµλ‹λ‹¤.
- **FReLU μ΄κΈ°ν™” κ°’μ μν–¥:** $b$μ μ΄κΈ°ν™” κ°’(μ–‘μ λλ” μμ)κ³Ό κ΄€κ³„μ—†μ΄, FReLUμ ν•™μµ κ°€λ¥ν• $b$ νλΌλ―Έν„°λ” μ μ‚¬ν• μμ κ°’(μ•½ -0.3 ~ -0.4)μΌλ΅ μλ ΄ν•λ” κ²½ν–¥μ„ λ³΄μ€μµλ‹λ‹¤. μ΄λ” FReLUκ°€ μκΈ° μ μ‘μ μΌλ΅ "μ λ΅-μ μ‚¬" μ†μ„±μ„ ν•™μµν•¨μ„ μ‹μ‚¬ν•©λ‹λ‹¤.
- **FReLUμ ν‘ν„λ ¥ μ‹κ°ν™” (MNIST, LeNets++):** FReLU λ μ΄μ–΄μ νΉμ§• μ„λ² λ”©μ΄ ReLUλ³΄λ‹¤ λ” νλ³„μ μ΄λ©°, λ” λ„“μ€ νΉμ§• ν‘ν„ κ³µκ°„μ„ μ κ³µν•¨μ„ λ³΄μ—¬μ£Όμ—μµλ‹λ‹¤. FReLU λ„¤νΈμ›ν¬μ μ •ν™•λ„λ” 97.8%λ΅ ReLU λ„¤νΈμ›ν¬μ 97.05%λ³΄λ‹¤ λ†’μ•μµλ‹λ‹¤.
- **NIN (CIFAR-10/100, ImageNet):** NIN λ¨λΈμ—μ„ FReLUλ” CIFAR-10 (7.30%), CIFAR-100 (28.47%), ImageNet (Top-1 34.82%, Top-5 14.00%) λ“± λ¨λ“  λ°μ΄ν„°μ…‹μ—μ„ κ°€μ¥ λ‚®μ€ μ¤λ¥μ¨μ„ κΈ°λ΅ν–μµλ‹λ‹¤.
- **Residual Networks (CIFAR-10/100):** ResNet-20λ¶€ν„° ResNet-110κΉμ§€μ λ‹¤μ–‘ν• κΉμ΄μ—μ„, FReLUλ” ReLUλ¥Ό λ€μ²΄ν–μ„ λ• μ›λ³Έ ResNet λ° ELUλ³΄λ‹¤ λ” λ‚®μ€ μ¤λ¥μ¨μ„ λ‹¬μ„±ν–μµλ‹λ‹¤. μ΄λ” FReLUκ°€ ResNet μ•„ν‚¤ν…μ² λ° BNκ³Ό λ†’μ€ νΈν™μ„±μ„ κ°€μ§€κ³  μμμ„ λ³΄μ—¬μ¤λ‹λ‹¤.

## π§  Insights & Discussion

- **μμ κ°’μ μ¤‘μ”μ„±:** FReLUμ ν•™μµ κ°€λ¥ν• νΈν–¥ $b$κ°€ μμ κ°’μΌλ΅ μλ ΄ν•λ” κ²½ν–¥μ€ μμ κ°’μ΄ μ‹ κ²½λ§μ ν‘ν„λ ¥κ³Ό ν•™μµ μ„±λ¥μ— κΈμ •μ μΈ μν–¥μ„ λ―ΈμΉλ‹¤λ” κ²ƒμ„ μ‹μ‚¬ν•©λ‹λ‹¤.
- **μ μ‘ν• μ •λ¥ μ§€μ μ μ΄μ :** κ³ μ •λ μ •λ¥ μ§€μ μ„ μ‚¬μ©ν•λ” ReLUμ™€ λ‹¬λ¦¬, FReLUλ” λ„¤νΈμ›ν¬κ°€ λ°μ΄ν„° λ¶„ν¬μ— λ”°λΌ μµμ μ μ •λ¥ μ§€μ μ„ μ μ—°ν•κ² ν•™μµν•  μ μλ„λ΅ ν•μ—¬ μ„±λ¥ ν–¥μƒμ— κΈ°μ—¬ν•©λ‹λ‹¤.
- **ν¨μ¨μ„±κ³Ό νΈν™μ„±μ κ· ν•:** FReLUλ” ELUμ "μ λ΅-μ μ‚¬" νΉμ„±κ³Ό ReLUμ κ³„μ‚° ν¨μ¨μ„± λ° ν¬μ†μ„±μ„ λ¨λ‘ μ μ§€ν•λ©΄μ„, BNκ³Όμ νΈν™μ„± λ¬Έμ κΉμ§€ ν•΄κ²°ν•μ—¬ μ‹¤μ©μ μΈ ν™μ„±ν™” ν•¨μλ΅μ κ°€μΉλ¥Ό λ†’μ…λ‹λ‹¤.
- **μ ν• μ‚¬ν•­ λ° ν–¥ν›„ μ—°κµ¬:** "μ£½μ€ λ‰΄λ°(dead neuron)" λ¬Έμ μ— λ€ν• λ” λ‚μ€ ν•΄κ²°μ±…κ³Ό μμ κ°’μ„ λ” ν¨κ³Όμ μΌλ΅ ν™μ©ν•κ³  ν•™μµ νΉμ„±μ„ κ°μ„ ν•  μ μλ” ν™μ„±ν™” ν•¨μ μ„¤κ³„μ— λ€ν• μ¶”κ°€ μ—°κµ¬κ°€ ν•„μ”ν•©λ‹λ‹¤.

## π“ TL;DR

FReLUλ” ReLUμ μ •λ¥ μ§€μ μ„ ν•™μµ κ°€λ¥ν• νΈν–¥ $b$λ΅ λ€μ²΄ν•μ—¬ μ‹ κ²½λ§μ ν‘ν„λ ¥μ„ λ†’μ΄κ³  μμ μ •λ³΄λ¥Ό ν™μ©ν•λ” μƒλ΅μ΄ ν™μ„±ν™” ν•¨μμ…λ‹λ‹¤. FReLUλ” κ³„μ‚° λΉ„μ©μ΄ λ‚®κ³ , Batch Normalizationκ³Ό λ†’μ€ νΈν™μ„±μ„ κ°€μ§€λ©°, $b$κ°€ μμ κ°’μΌλ΅ μλ ΄ν•λ©΄μ„ "μ λ΅-μ μ‚¬" μ†μ„±μ„ ν•™μµν•©λ‹λ‹¤. CIFAR-10/100 λ° ImageNet λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν• μ‹¤ν—μ—μ„ FReLUλ” ReLU λ° κ·Έ λ³€ν•λ“¤λ³΄λ‹¤ λΉ λ¥΄κ³  μ•μ •μ μΈ μλ ΄κ³Ό μ°μν• μ„±λ¥μ„ λ³΄μ—¬μ£Όμ—μµλ‹λ‹¤.
