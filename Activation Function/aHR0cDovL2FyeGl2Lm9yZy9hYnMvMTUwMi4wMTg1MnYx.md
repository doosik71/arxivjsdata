# Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification

Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun

## 🧩 Problem to Solve

이 연구는 최신 딥 뉴럴 네트워크의 핵심 요소인 정류 활성화 함수(rectifiers)를 심층적으로 탐구하고, 이를 통해 이미지 분류 성능을 향상시키는 것을 목표로 합니다. 주요 해결 과제는 다음과 같습니다:

- 기존 ReLU(Rectified Linear Unit)의 단점을 보완하고, 모델 피팅 능력을 개선하는 새로운 활성화 함수를 개발하는 것.
- 매우 깊은 정류 네트워크를 처음부터(from scratch) 안정적으로 학습시킬 수 있는 강건한 초기화 방법을 개발하는 것.
- 이러한 개선을 통해 ImageNet 분류 태스크에서 최첨단 성능을 달성하고, 나아가 인간 수준의 인식 성능을 뛰어넘는 것.

## ✨ Key Contributions

- **Parametric Rectified Linear Unit (PReLU) 제안**: 기존 ReLU를 일반화한 PReLU를 제안했습니다. 이 활성화 함수는 음수 부분의 기울기($a_i$)를 학습 가능한 파라미터로 도입하여, 거의 제로에 가까운 추가 계산 비용과 적은 과적합 위험으로 모델 피팅을 개선했습니다.
- **정류 네트워크를 위한 강건한 초기화 방법 도출**: ReLU/PReLU의 비선형성을 명시적으로 고려하여, 매우 깊은 네트워크(예: 30개 가중치 레이어)를 처음부터 성공적으로 학습시킬 수 있는 이론적으로 건전한 필터 가중치 초기화 방법을 개발했습니다.
- **ImageNet 분류에서 인간 수준 성능 초과 달성**: PReLU 네트워크(PReLU-net)를 사용하여 ImageNet 2012 분류 데이터셋에서 4.94%의 Top-5 테스트 에러율을 달성했습니다. 이는 ILSVRC 2014 우승 모델(GoogLeNet의 6.66%) 대비 26% 상대적 개선이며, 당시 보고된 인간 수준 성능(5.1%)을 최초로 뛰어넘은 결과입니다.

## 📎 Related Works

- **Rectified Linear Unit (ReLU) 및 변형**: Nair와 Hinton [21], Glorot 등 [8, 20, 34]이 제안한 ReLU는 딥 네트워크의 성공에 핵심적인 역할을 했습니다. Leaky ReLU (LReLU) [20]는 0이 아닌 작은 고정 값으로 음수 기울기를 설정했지만, 성능 개선은 미미했습니다.
- **초기화 방법**: Glorot와 Bengio [7]의 "Xavier" 초기화는 선형 활성화 함수를 가정하여 파생되었으며, ReLU와 같은 비선형 활성화 함수에는 적합하지 않았습니다. VGG 팀 [25]은 깊은 모델 학습을 위해 얕은 모델 사전 학습 방식을 사용했습니다.
- **네트워크 아키텍처**: GoogLeNet [29], VGG [25], OverFeat [24] 등은 깊이, 너비, 스트라이드, 계층 설계 등 다양한 측면에서 모델 복잡성을 증가시켰습니다.
- **정규화 및 데이터 증강**: 드롭아웃 [12, 26], 드롭커넥트 [31]와 같은 정규화 기법과 공격적인 데이터 증강 [16, 13, 25, 29]은 일반화 성능 향상에 기여했습니다.
- **Spatial Pyramid Pooling (SPP-net)**: He 등 [11]의 SPP는 다양한 크기의 입력 이미지에서 고정 크기 특징 맵을 추출하여 네트워크에 유연성을 제공했습니다.

## 🛠️ Methodology

### Parametric Rectified Linear Unit (PReLU)

- **정의**: PReLU는 다음과 같이 정의되는 활성화 함수입니다:
  $$f(y_i) = \begin{cases} y_i, & \text{if } y_i > 0 \\ a_i y_i, & \text{if } y_i \le 0 \end{cases}$$
  여기서 $y_i$는 $i$번째 채널의 비선형 활성화 함수 입력이고, $a_i$는 음수 부분의 기울기를 제어하는 계수입니다. $a_i=0$이면 ReLU가 되고, $a_i$가 학습 가능한 파라미터일 경우 PReLU가 됩니다.
- **최적화**: $a_i$는 역전파(backpropagation)를 통해 다른 레이어들과 동시에 최적화됩니다. $a_i$의 기울기는 다음으로 주어집니다:
  $$\frac{\partial E}{\partial a_i} = \sum_{y_i} \frac{\partial E}{\partial f(y_i)} \frac{\partial f(y_i)}{\partial a_i}$$
  여기서 $\frac{\partial f(y_i)}{\partial a_i} = y_i$ (만약 $y_i \le 0$) 또는 $0$ (만약 $y_i > 0$)입니다. $a_i$ 업데이트 시 가중치 감소(weight decay)를 사용하지 않아 ReLU로 편향되는 것을 방지합니다. $a_i$는 0.25로 초기화됩니다. 채널별 학습 ($a_i$) 또는 레이어 내 채널 공유 학습 ($a$) 변형이 가능합니다.

### 정류기를 위한 필터 가중치 초기화

- **전방 전파 (Forward Propagation) 분석**: 각 레이어의 응답 분산을 조사하여 신호의 크기가 지수적으로 감소하거나 증폭되는 것을 방지합니다. $l$번째 합성곱 레이어의 출력을 $y_l = W_l x_l + b_l$로 정의하고, $w_l$이 영평균 대칭 분포를 따른다고 가정하면, ReLU 활성화 이후 $x_l = \max(0, y_{l-1})$이므로 $E[x_l^2] = \frac{1}{2}\text{Var}[y_{l-1}]$이 됩니다.
  안정적인 전방 전파를 위한 충분 조건은 $\frac{1}{2}n_l \text{Var}[w_l] = 1$이며, 이는 가중치를 평균이 0이고 표준 편차가 $\sqrt{2/n_l}$인 가우시안 분포에서 샘플링하는 것으로 이어집니다. ($n_l$은 연결 수)
- **후방 전파 (Backward Propagation) 분석**: 유사하게, 기울기(gradient)의 분산을 분석하여 안정적인 후방 전파를 보장합니다. ReLU의 경우 $f'(y_l)$이 0 또는 1이 될 확률이 같으므로 $E[(\Delta y_l)^2] = \frac{1}{2}\text{Var}[\Delta x_{l+1}]$입니다.
  안정적인 후방 전파를 위한 충분 조건은 $\frac{1}{2}\hat{n}_l \text{Var}[w_l] = 1$이며, 이는 가중치를 평균이 0이고 표준 편차가 $\sqrt{2/\hat{n}_l}$인 가우시안 분포에서 샘플링하는 것으로 이어집니다. ($\hat{n}_l$은 역전파 시 연결 수)
- **PReLU 초기화**: PReLU의 경우, $(1+a^2)$ 항을 포함하여 일반화된 초기화 조건은 $\frac{1}{2}(1+a^2)n_l \text{Var}[w_l] = 1$ 및 $\frac{1}{2}(1+a^2)\hat{n}_l \text{Var}[w_l] = 1$가 됩니다.

### 아키텍처 및 학습/테스트 상세

- **네트워크 아키텍처**: VGG-19 [25]를 기반으로 수정된 A, B, C 모델을 사용합니다. Model A는 첫 레이어에서 7x7 필터 및 스트라이드 2를 사용하고, SPP (Spatial Pyramid Pooling) [11]를 첫 FC 레이어 전에 적용합니다. Model B는 A보다 깊고, Model C는 B보다 필터 수를 늘려 너비를 확장했습니다. 깊이를 지나치게 늘리는 대신 너비를 확장하는 데 중점을 두었습니다.
- **학습**: 스케일 지터링([256, 512] 범위에서 무작위 스케일), 수평 뒤집기, 무작위 색상 변경 등 공격적인 데이터 증강 기법을 적용합니다. 기존과 달리 얕은 모델의 사전 학습 없이 초기화 방법으로 매우 깊은 모델을 직접 학습시킵니다.
- **테스트**: SPP-net [11]에서 사용된 "특징 맵 상의 다중 뷰 테스트" 전략을 채택하며, [24, 25]의 dense sliding window 방법을 사용하여 이를 개선했습니다. 여러 스케일에서의 결과를 결합합니다.
- **Multi-GPU 구현**: Krizhevsky [15]의 데이터 병렬화 방식을 사용하여 합성곱 레이어를 병렬화하고, FC 레이어는 단일 GPU에서 처리하여 단순성과 효율성을 추구했습니다.

## 📊 Results

- **PReLU vs. ReLU 성능**: PReLU는 작은 모델(14 레이어)에서 Top-1 에러율을 33.82%에서 32.64%로 1.18%p 감소시켰고, 큰 모델(A)에서는 멀티스케일 조합에서 Top-1 에러율을 24.02%에서 22.97%로 1.05%p 감소시켰습니다. PReLU는 추가 계산 비용 없이 일관되게 성능을 향상시켰습니다.
- **초기화 방법의 효과**: 제안된 초기화 방법은 30개 레이어의 극도로 깊은 모델을 안정적으로 학습시킬 수 있었던 반면, "Xavier" 초기화는 학습이 정체되는 현상을 보였습니다.
- **단일 모델 결과**: PReLU가 적용된 모델 C는 ImageNet 2012 검증 세트에서 5.71%의 Top-5 에러율을 달성하여, 기존의 모든 멀티 모델 결과보다 우수했습니다. 너비 확장(C vs. B)이 깊이 증가(B vs. A)보다 정확도 향상에 더 효과적임을 보여주었습니다.
- **멀티 모델 결과**: 6개의 모델을 결합한 결과, ImageNet 2012 테스트 세트에서 4.94%의 Top-5 에러율을 달성했습니다. 이는 ILSVRC 2014 우승 모델(GoogLeNet, 6.66%)보다 26% 상대적으로 개선된 수치입니다.
- **인간 수준 성능 초과**: ImageNet 데이터셋에서 보고된 인간 수준 성능(5.1%)을 처음으로 초과 달성했습니다.

## 🧠 Insights & Discussion

- **활성화 함수의 중요성**: PReLU는 학습 가능한 파라미터를 통해 활성화 함수의 형태를 데이터에 맞게 적응적으로 조정함으로써, 미미한 오버헤드만으로도 모델의 피팅 능력을 크게 향상시킬 수 있음을 입증했습니다. 초기 합성곱 레이어에서는 큰 계수를 학습하여 저수준 정보를 폭넓게 활용하고, 깊은 레이어에서는 작은 계수를 학습하여 점진적으로 더 비선형적이고 판별적인 특징을 추출하는 경향을 보였습니다.
- **깊은 네트워크 학습의 기반**: 제안된 초기화 방법은 비선형 활성화 함수의 특성을 반영하여 매우 깊은 네트워크의 학습을 가능하게 하는 중요한 토대를 마련했습니다. 이를 통해 향후 더 복잡한 태스크에서 심층 네트워크 아키텍처 탐색의 문을 열었습니다.
- **깊이와 너비의 균형**: ImageNet과 같은 대규모 데이터셋에서는 모델이 충분히 깊어지면, 깊이를 무작정 늘리는 것보다 너비를 확장하는 것이 정확도 향상에 더 효과적일 수 있음을 시사합니다. 너무 깊은 모델은 오히려 정확도가 포화되거나 저하될 수 있다는 기존 연구 결과와 맥락을 같이 합니다.
- **인간-기계 인식 비교**: ImageNet에서 인간 수준 성능을 초과 달성한 것은 기계 시각 알고리즘의 엄청난 잠재력을 보여줍니다. 특히, 미세한 식별(fine-grained recognition) 능력에서 기계가 인간보다 뛰어날 수 있음을 시사합니다. 그러나 여전히 인간에게는 쉽지만 컨텍스트 이해나 고수준 지식이 필요한 경우에는 기계가 오류를 범하는 한계를 드러냅니다. 이는 일반적인 객체 인식에서 기계가 인간 시각을 능가한다는 의미는 아닙니다.

## 📌 TL;DR

이 논문은 이미지 분류를 위한 딥 정류 신경망의 두 가지 핵심 측면을 다룹니다. 첫째, 음수 부분의 기울기($a_i$)를 학습 가능한 파라미터로 도입한 **PReLU(Parametric Rectified Linear Unit)**를 제안하여 모델 피팅을 개선했습니다. 둘째, ReLU/PReLU 비선형성을 고려한 **강건한 가중치 초기화 방법**을 도출하여 매우 깊은(최대 30개 레이어) 네트워크를 처음부터 안정적으로 학습시킬 수 있게 했습니다. 이 PReLU-net과 새로운 초기화 방법을 통해 ImageNet 2012 데이터셋에서 4.94%의 Top-5 에러율을 달성하여, 당시 **인간 수준 성능(5.1%)을 최초로 뛰어넘는** 놀라운 결과를 보여주었습니다. 이는 활성화 함수와 초기화 전략의 중요성을 강조하고, 딥러닝 모델 설계에 대한 새로운 통찰을 제공합니다.
