{
  "title": "Improving Deep Neural Network with Multiple Parametric Exponential\n  Linear Units",
  "authors": "Yang Li, Chunxiao Fan, Yong Li, Qiong Wu, Yue Ming",
  "year": 2016,
  "url": "http://arxiv.org/abs/1606.00305v3",
  "abstract": "Activation function is crucial to the recent successes of deep neural\nnetworks. In this paper, we first propose a new activation function, Multiple\nParametric Exponential Linear Units (MPELU), aiming to generalize and unify the\nrectified and exponential linear units. As the generalized form, MPELU shares\nthe advantages of Parametric Rectified Linear Unit (PReLU) and Exponential\nLinear Unit (ELU), leading to better classification performance and convergence\nproperty. In addition, weight initialization is very important to train very\ndeep networks. The existing methods laid a solid foundation for networks using\nrectified linear units but not for exponential linear units. This paper\ncomplements the current theory and extends it to the wider range. Specifically,\nwe put forward a way of initialization, enabling training of very deep networks\nusing exponential linear units. Experiments demonstrate that the proposed\ninitialization not only helps the training process but leads to better\ngeneralization performance. Finally, utilizing the proposed activation function\nand initialization, we present a deep MPELU residual architecture that achieves\nstate-of-the-art performance on the CIFAR-10/100 datasets. The code is\navailable at https://github.com/Coldmooon/Code-for-MPELU.",
  "citation": 106
}