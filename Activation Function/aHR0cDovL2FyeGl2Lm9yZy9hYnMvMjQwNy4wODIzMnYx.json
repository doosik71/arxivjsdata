{
  "title": "SwishReLU: A Unified Approach to Activation Functions for Enhanced Deep\n  Neural Networks Performance",
  "authors": "Jamshaid Ul Rahman, Rubiqa Zulfiqar, Asad Khan,  Nimra",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.08232v1",
  "abstract": "ReLU, a commonly used activation function in deep neural networks, is prone\nto the issue of \"Dying ReLU\". Several enhanced versions, such as ELU, SeLU, and\nSwish, have been introduced and are considered to be less commonly utilized.\nHowever, replacing ReLU can be somewhat challenging due to its inconsistent\nadvantages. While Swish offers a smoother transition similar to ReLU, its\nutilization generally incurs a greater computational burden compared to ReLU.\nThis paper proposes SwishReLU, a novel activation function combining elements\nof ReLU and Swish. Our findings reveal that SwishReLU outperforms ReLU in\nperformance with a lower computational cost than Swish. This paper undertakes\nan examination and comparison of different types of ReLU variants with\nSwishReLU. Specifically, we compare ELU and SeLU along with Tanh on three\ndatasets: CIFAR-10, CIFAR-100 and MNIST. Notably, applying SwishReLU in the\nVGG16 model described in Algorithm 2 yields a 6% accuracy improvement on the\nCIFAR-10 dataset.",
  "citation": 10
}