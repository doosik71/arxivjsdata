# Maxout Networks

Ian J. Goodfellow, David Warde-Farley, Mehdi Mirzam, Aaron Courville, Yoshua Bengio

---

## 🧩 Problem to Solve

이 논문은 최근 소개된 근사 모델 평균화 기법인 드롭아웃(dropout)을 효과적으로 활용하고, 드롭아웃의 빠른 근사 모델 평균화 기법의 정확도를 개선할 수 있는 모델을 설계하는 문제를 다룹니다. 기존 드롭아웃은 다양한 모델에 적용되어 성능 향상을 가져왔지만, 심층 신경망에서 실제로 모델 평균화를 정확히 수행하는지, 그리고 특정 모델 설계를 통해 드롭아웃의 능력을 더욱 향상시킬 수 있는지에 대한 탐구가 필요했습니다.

## ✨ Key Contributions

- **Maxout 네트워크 제안**: 드롭아웃 최적화를 촉진하고 드롭아웃의 근사 모델 평균화 기법의 정확도를 향상시키기 위해 Maxout이라는 새로운 활성화 함수를 사용하는 단순한 모델을 정의했습니다.
- **범용 근사자 증명**: 두 개의 Maxout 은닉 유닛을 가진 Maxout 네트워크가 모든 연속 함수를 임의로 잘 근사할 수 있음을 이론적으로 증명했습니다.
- **최신 성능 달성**: MNIST, CIFAR-10, CIFAR-100, SVHN의 네 가지 벤치마크 데이터셋에서 드롭아웃과 Maxout을 사용하여 분류 성능의 최신 기록을 경신했습니다.
- **모델 평균화 개선**: 드롭아웃의 근사 모델 평균화가 심층 신경망에서도 작동하며, 특히 Maxout 유닛이 tanh 유닛보다 이 근사를 더 정확하게 수행함을 보였습니다.
- **최적화 능력 향상**: Maxout이 정류 선형 유닛(rectifier linear units)의 포화(saturation) 문제와 달리, 모든 유닛에서 항상 경사(gradient)가 흐르도록 하여 드롭아웃 훈련 시 최적화 성능을 개선함을 입증했습니다.

## 📎 Related Works

- **Dropout (Hinton et al., 2012)**: 이 논문의 핵심 동기가 되는 기술로, 파라미터를 공유하는 대규모 모델 앙상블을 훈련하고 예측을 근사적으로 평균화하는 방법을 제공합니다.
- **Bagging (Breiman, 1994)**: 드롭아웃 훈련과 유사하게 여러 모델을 다른 데이터 하위 집합에서 훈련하는 앙상블 기법입니다.
- **Rectified Linear Units (ReLU) (Glorot et al., 2011)**: Maxout과 비교되는 활성화 함수로, Maxout은 ReLU의 0 포화 문제를 개선합니다.
- **Deep Boltzmann Machines (Salakhutdinov & Hinton, 2009)**: MNIST 벤치마크에서 비교 대상이 된 모델 중 하나입니다.
- **Convolutional Neural Networks (Krizhevsky et al., 2012)**: Maxout을 적용한 CNN 모델이 벤치마크 테스트에 사용되었습니다.

## 🛠️ Methodology

1. **Maxout 유닛 정의**:
   - Maxout 은닉 계층은 입력 $x \in R^d$에 대해 다음 함수를 구현합니다:
     $$h_i(x) = \max_{j \in [1,k]} z_{ij}$$
     여기서 $z_{ij} = x^T W_{\cdot ij} + b_{ij}$ 이고, $W \in R^{d \times m \times k}$ 및 $b \in R^{m \times k}$는 학습 가능한 파라미터입니다.
   - 이는 각 Maxout 유닛이 $k$개의 어파인(affine) 함수의 최댓값을 취함을 의미합니다.
   - Maxout 유닛은 임의의 볼록 함수(convex function)에 대한 구간 선형(piecewise linear) 근사를 수행할 수 있습니다.
2. **드롭아웃과 통합**:
   - 드롭아웃 훈련 시, 드롭아웃 마스크와의 요소별 곱셈은 항상 가중치 곱셈 *직전*에 수행됩니다. Max 연산의 입력은 드롭아웃되지 않습니다.
3. **네트워크 아키텍처**:
   - **MLP**: 조밀하게 연결된 Maxout 계층과 소프트맥스 계층으로 구성됩니다.
   - **CNN**: 컨볼루션 Maxout 은닉 계층과 공간 Max 풀링(spatial max pooling), 그리고 조밀하게 연결된 Maxout/소프트맥스 계층으로 구성됩니다. 컨볼루션 네트워크에서 Maxout 피처 맵은 $k$개의 어파인 피처 맵 중에서 최댓값을 취하여 구성됩니다(채널 간 풀링).
4. **훈련 절차**:
   - 검증 세트에서 오류를 최소화하여 하이퍼파라미터를 선택합니다.
   - 전체 훈련 세트를 사용하여 모델을 재훈련합니다. (MNIST의 경우, 최소 검증 오류 시점의 로그 가능도를 기록하고, 전체 세트로 훈련하여 이 값에 도달할 때까지 계속합니다. CIFAR-10의 경우, 처음부터 모델을 재훈련하고 새 가능도가 이전 값과 일치할 때 중단합니다.)
5. **보정**:
   - 드롭아웃과 함께 각 가중치 벡터의 노름(norm)에 대한 제약(Srebro & Shraibman, 2005)을 적용하여 모델을 보정합니다.

## 📊 Results

- **MNIST**: 순열 불변(permutation invariant) MNIST에서 0.94% 오류율을 달성하여 지도 학습 방식 중 최신 기록을 세웠습니다 (DBM + dropout 0.79%보다는 높음, 그러나 비지도 사전 훈련 없는 방식 중에서는 최고). 일반 MNIST에서는 0.45% 오류율로 새로운 최신 기록을 세웠습니다.
- **CIFAR-10**: 데이터 증강 없이 11.68%, 데이터 증강과 함께 9.38% 오류율을 달성하여 최신 기록을 경신했습니다.
- **CIFAR-100**: 38.57% 오류율로 최신 기록을 세웠습니다.
- **SVHN**: 2.47% 오류율로 최신 기록을 세웠습니다.
- **정류 선형 유닛(Rectifiers)과의 비교**: 동일한 전처리 및 모델 크기로 정류 선형 유닛과 비교했을 때 Maxout이 일관되게 더 나은 성능을 보였습니다. Maxout은 더 적은 파라미터로도 더 나은 일반화 성능을 달성할 수 있었습니다.
- **모델 평균화 정확도**: Maxout 네트워크에서 드롭아웃의 가중치 $W/2$ 방법이 샘플링된 서브 모델들의 기하 평균과 더 잘 일치하여, Maxout이 tanh 유닛보다 드롭아웃의 모델 평균화를 더 정확하게 근사함을 입증했습니다.

## 🧠 Insights & Discussion

- **드롭아웃 모델 평균화와의 높은 호환성**: Maxout 유닛은 본질적으로 구간 선형이기 때문에, 드롭아웃 마스크가 변경되어도 효과적인 입력이 선형 영역 내에 머무를 가능성이 높습니다. 이는 깊은 아키텍처에서 드롭아웃의 $W/2$ 예측 평균화가 더욱 정확한 근사치로 작동하게 합니다. 이는 tanh와 같이 곡률이 있는 활성화 함수보다 이점이 있습니다.
- **최적화 이점**:
  - **포화 문제 회피**: 정류 선형 유닛(ReLU)은 0으로 포화될 수 있어 경사 흐름을 막고 "죽은 뉴런" 문제로 이어질 수 있습니다. Maxout은 $z_{ij}$들 중에서 0이 아닌 값이 선택될 수 있으므로 (또는 0이 선택되더라도 이 0이 파라미터의 함수이므로), 모든 유닛에서 항상 경사가 흐르게 됩니다. 이는 드롭아웃 훈련 시 최적화 과정을 훨씬 견고하게 만듭니다.
  - **깊은 네트워크 훈련**: Maxout은 깊이가 증가해도 최적화 성능이 안정적으로 유지되어, 기존 정류 선형 유닛보다 더 깊은 네트워크를 훈련할 수 있게 합니다.
  - **하위 계층으로의 경사 전파 개선**: Maxout은 드롭아웃 마스크 선택에 따른 경사의 변동을 네트워크의 최하위 계층까지 더 잘 전파하여, 드롭아웃 훈련이 모든 파라미터에 대해 배깅(bagging)과 유사한 효과를 더욱 충실히 모방하도록 돕습니다.

## 📌 TL;DR

**문제**: 드롭아웃의 모델 평균화 능력을 극대화하고, 심층 신경망에서의 드롭아웃 훈련 효율성을 개선할 새로운 모델이 필요했습니다.
**방법**: 이 논문은 여러 어파인 함수의 최댓값을 취하는 새로운 활성화 함수인 **Maxout 유닛**을 제안하고, 이를 활용한 Maxout 네트워크를 드롭아웃과 함께 훈련했습니다. Maxout은 범용 근사자임이 증명되었습니다.
**발견**: Maxout 네트워크는 MNIST, CIFAR-10, CIFAR-100, SVHN 등 네 가지 벤치마크 데이터셋에서 최신 성능을 달성했습니다. Maxout은 정류 선형 유닛(ReLU)의 포화 문제를 피하여 드롭아웃 훈련 시 최적화를 크게 개선하고, 드롭아웃의 모델 평균화 근사를 더욱 정확하게 만듭니다.
