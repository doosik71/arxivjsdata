{
  "title": "Improving Quaternion Neural Networks with Quaternionic Activation\n  Functions",
  "authors": "Johannes PÃ¶ppelbaum, Andreas Schwung",
  "year": 2024,
  "url": "http://arxiv.org/abs/2406.16481v1",
  "abstract": "In this paper, we propose novel quaternion activation functions where we\nmodify either the quaternion magnitude or the phase, as an alternative to the\ncommonly used split activation functions. We define criteria that are relevant\nfor quaternion activation functions, and subsequently we propose our novel\nactivation functions based on this analysis. Instead of applying a known\nactivation function like the ReLU or Tanh on the quaternion elements\nseparately, these activation functions consider the quaternion properties and\nrespect the quaternion space $\\mathbb{H}$. In particular, all quaternion\ncomponents are utilized to calculate all output components, carrying out the\nbenefit of the Hamilton product in e.g. the quaternion convolution to the\nactivation functions. The proposed activation functions can be incorporated in\narbitrary quaternion valued neural networks trained with gradient descent\ntechniques. We further discuss the derivatives of the proposed activation\nfunctions where we observe beneficial properties for the activation functions\naffecting the phase. Specifically, they prove to be sensitive on basically the\nwhole input range, thus improved gradient flow can be expected. We provide an\nelaborate experimental evaluation of our proposed quaternion activation\nfunctions including comparison with the split ReLU and split Tanh on two image\nclassification tasks using the CIFAR-10 and SVHN dataset. There, especially the\nquaternion activation functions affecting the phase consistently prove to\nprovide better performance.",
  "citation": 2
}