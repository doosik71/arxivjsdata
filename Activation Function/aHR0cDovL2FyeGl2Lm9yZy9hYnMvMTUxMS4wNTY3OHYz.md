# Expressiveness of Rectifier Networks

Xingyuan Pan, Vivek Srikumar

## 🧩 Problem to Solve

이 논문은 Rectified Linear Units (ReLU)를 사용하는 신경망의 표현력(expressiveness)에 대한 이해 부족을 다룹니다. 특히, 임계값(threshold) 및 시그모이드(sigmoid) 네트워크와 비교하여 ReLU 네트워크의 이론적 표현력, 즉 어떤 함수를 표현할 수 있는지에 대한 연구가 덜 이루어져 있습니다. ReLU 네트워크의 경험적 성공에도 불구하고, 그 이면에 있는 형식적인 이유와 결정 경계 특성에 대한 분석이 필요합니다.

## ✨ Key Contributions

- 2계층 ReLU 네트워크의 결정 경계가 기능적으로 동등한 임계값 네트워크로 표현될 수 있음을 건설적으로 증명했습니다. 하지만 이때 임계값 네트워크는 기하급수적으로 더 많은 은닉 유닛을 필요로 하며, 일부 ReLU 네트워크는 더 작은 임계값 네트워크로 표현될 수 없음을 보였습니다.
- 임의의 임계값 네트워크를 로그적으로 더 작은 ReLU 네트워크로 압축하기 위한 충분 조건을 제시했습니다.
- 은닉 계층 예측을 다중 클래스 분류로 취급하여, 출력 상태 대신 은닉 계층 상태의 등가성을 요구하는 완화된 조건을 식별했습니다. 이를 통해 기하급수적으로 큰 임계값 네트워크에 은닉 계층 등가인 ReLU 네트워크를 근사적으로 복구할 수 있음을 보였습니다.
- 합성 데이터에 대한 실험을 통해 표현력이 학습 가능성(learnability)을 보장하지 않음을 보여주며, 임계값 네트워크와 ReLU 네트워크의 학습 능력을 비교 분석했습니다.

## 📎 Related Works

- **네트워크 표현력:**
  - 단일 은닉 계층 시그모이드 네트워크는 모든 연속 함수를 임의의 정확도로 근사할 수 있는 "범용 근사자(universal approximators)"입니다 (Cybenko, 1989).
  - 2계층 임계값 네트워크는 모든 부울 함수를 표현할 수 있지만, 필요한 은닉 유닛 수는 기하급수적일 수 있습니다.
  - Maass et al. (1991, 1994)은 시그모이드 네트워크가 유사 크기의 임계값 네트워크보다 더 표현력이 강할 수 있다고 비교했습니다.
  - ReLU 네트워크는 조각별 선형(piece-wise linear) 함수만을 표현할 수 있으며, 표현력은 입력 공간의 선형 분할 영역 수로 측정될 수 있습니다. Pascanu et al. (2014) 및 Montufar et al. (2014)은 깊은 ReLU 아키텍처가 얕은 아키텍처보다 기하급수적으로 더 많은 선형 영역을 표현할 수 있음을 보였습니다.
- **학습 복잡도:**
  - 부울 하이퍼큐브 입력에 대해, 임계값 활성화 함수를 가진 2계층 네트워크는 효율적으로 학습 불가능합니다 (Blum & Rivest, 1992).
  - 가중치 제한이 없는 경우, 시그모이드 또는 ReLU 활성화를 가진 2계층 네트워크도 일반적으로 효율적으로 학습 불가능합니다. (Livni et al., 2014)

## 🛠️ Methodology

이 논문은 주로 하나의 은닉 계층($n$개 유닛)과 단일 이진 출력을 가진 얕은 네트워크에 초점을 맞춥니다. 출력 활성화 함수는 항상 임계값 함수($sgn(x)$)입니다.

1. **ReLU 네트워크 정의:**
   입력 $x$에 대해, ReLU 네트워크의 출력 $y$는 다음과 같이 정의됩니다:
   $$y = \text{sgn}\left(w_0 + \sum_{k=1}^{n} w_k R(u_k \cdot x + b_k)\right)$$
   여기서 $R(x) = \text{max}(0, x)$ 입니다.

2. **일반 형태 변환:**
   $w_k$의 부호를 $R(\cdot)$ 함수 안으로 흡수하고, 은닉 유닛을 $w_k > 0$인 양수 세트 $P$와 $w_k < 0$인 음수 세트 $N$으로 분할하여 일반 형태를 얻습니다:
   $$y = \text{sgn}\left(w_0 + \sum_{k \in P} R(a_k(x)) - \sum_{k \in N} R(a_k(x))\right)$$
   여기서 $a_k(x) = u_k \cdot x + b_k$ 입니다.

3. **주요 정리 (Theorem 1):**
   2계층 ReLU 네트워크가 입력 $x$를 양성으로 분류하는 조건과 $P$ 및 $N$의 부분집합 $S_1, S_2$를 사용한 일련의 부등식 조건이 동등함을 증명합니다. 이는 ReLU 네트워크의 결정 경계를 부울 논리식 (DNF 또는 CNF)으로 표현할 수 있음을 의미합니다. 각 부등식 항은 입력 공간에서 하나의 초평면(hyperplane)에 해당합니다.

4. **ReLU를 임계값 네트워크로 변환:**
   Theorem 1에 따라, $n_1 = |P|$, $n_2 = |N|$일 때, $2^{n_1+n_2}$개의 임계값 유닛을 가진 3계층 임계값 네트워크로 2계층 ReLU 네트워크를 건설적으로 표현할 수 있음을 보입니다. 이는 필요한 임계값 유닛의 수가 기하급수적으로 많다는 것을 의미합니다.

5. **임계값 네트워크를 ReLU 네트워크로 변환:**

   - **근사(Lemma 1):** 임의의 $n$개 유닛 임계값 네트워크는 $2n$개 유닛 ReLU 네트워크로 임의의 정확도로 근사될 수 있습니다: $\text{sgn}(v \cdot x + d) \approx \frac{1}{\epsilon} [R(v \cdot x + d + \epsilon) - R(v \cdot x + d - \epsilon)] - 1$.
   - **불가능성(Lemma 2):** $n$개 임계값 유닛을 가진 특정 임계값 네트워크는 더 적은 수의 은닉 ReLU 유닛으로 표현될 수 없음을 반례를 통해 보입니다.
   - **로그적 압축의 충분 조건(Lemma 3):** 특정 임계값 네트워크(출력이 은닉 유닛들의 `OR` 연산인 경우)는 가중치 파라미터가 특정 행렬 인수분해 형태($V=UT$)를 만족하면 $n$개의 ReLU 유닛으로 표현될 수 있음을 제시합니다.

6. **은닉 계층 등가성 (Theorem 3):**
   두 네트워크의 출력 값이 아닌, 은닉 계층 예측(최고 점수 유닛)이 동일할 경우를 "은닉 계층 등가"로 정의합니다. 특정 조건($\left\|(V-UT)^T\right\|_{\infty} \le \frac{\gamma(x)}{2\|x\|_{\infty}}$) 하에서 $2^n$ 클래스 분류기의 2계층 임계값 네트워크를 $n$개의 ReLU 유닛을 가진 네트워크로 은닉 계층 등가하게 학습할 수 있음을 보입니다. 여기서 $\gamma(x)$는 다중 클래스 마진입니다.

7. **실험:**
   - 무작위로 생성된 2계층 ReLU 네트워크를 사용하여 합성 데이터를 생성합니다.
   - 이 데이터를 사용하여 세 가지 다른 설정으로 네트워크를 학습합니다:
     1. 데이터 생성에 사용된 수($n$)의 ReLU 유닛을 가진 네트워크.
     2. $n$개의 '압축된 tanh' 유닛을 가진 네트워크 (임계값 함수 시뮬레이션).
     3. $2^n$개의 '압축된 tanh' 유닛을 가진 네트워크.
   - 역전파를 사용하여 학습하고 테스트 오차를 비교합니다.

## 📊 Results

- **$n$개 ReLU 네트워크 학습:** 데이터를 생성한 ReLU 네트워크와 동일한 수의 은닉 유닛을 가진 ReLU 네트워크로 학습했을 때, 항상 낮은 테스트 오차를 달성했습니다. 이는 모델이 참 개념을 표현하고 역전파가 성공적으로 이를 찾았음을 보여줍니다.
- **$n$개 압축된 tanh 네트워크 학습:** 동일한 수의 은닉 유닛을 가진 '압축된 tanh'(임계값 네트워크)로 학습했을 때는 참 개념을 복구하지 못하고 높은 오차를 보였습니다. 이는 ReLU 네트워크가 동일한 수의 임계값 유닛으로는 표현할 수 없는 더 복잡한 결정 경계를 나타낼 수 있다는 이론적 예측과 일치합니다.
- **$2^n$개 압축된 tanh 네트워크 학습:**
  - 데이터 생성에 3개의 ReLU 유닛($n=3$)이 사용된 경우, 기하급수적으로 많은 $2^3 = 8$개의 압축된 tanh 유닛으로 학습했을 때도 테스트 오차가 상당히 높았습니다. 이는 가설 클래스가 참 개념을 표현할 수 있음에도 불구하고 역전파가 최적해를 찾지 못했음을 시사합니다.
  - 데이터 생성에 10개의 ReLU 유닛($n=10$)이 사용된 경우, $2^{10} = 1024$개의 압축된 tanh 유닛으로 학습했을 때는 더 나은 성능을 달성했습니다.
- **결과 분석:** 이러한 실험 결과는 표현력만으로는 학습 가능성을 보장하지 않음을 보여줍니다. 목적 함수의 비볼록성(non-convexity)과 임계값 함수가 표현하는 함수의 집합이 더 넓다는 사실(Lemma 2의 결과) 간의 상호작용이 학습 실패의 원인으로 추정됩니다.

## 🧠 Insights & Discussion

- 이 논문은 2계층 ReLU 네트워크가 임계값 유닛 네트워크로 표현될 수 있지만, 이때 필요한 임계값 유닛의 수가 기하급수적으로 많다는 중요한 표현력 특성을 밝혀냈습니다. 이는 ReLU 네트워크가 상대적으로 적은 수의 유닛으로 복잡한 함수를 표현할 수 있음을 이론적으로 뒷받침합니다.
- 일반적인 임계값 네트워크를 로그적으로 더 작은 ReLU 네트워크로 축소하는 것은 불가능하지만, 특정 구조의 네트워크에 대해서는 이러한 축소가 가능한 충분 조건을 제시했습니다. 이는 특정 문제에서 ReLU 네트워크의 효율성을 높일 수 있는 잠재력을 시사합니다.
- "은닉 계층 등가성"이라는 개념을 도입하여, 출력값이 아닌 은닉 계층의 응답 패턴이 동일하다는 완화된 조건에서 임계값 네트워크를 ReLU 네트워크로 근사할 수 있음을 보였습니다.
- 실험을 통해 표현력이 반드시 학습 가능성으로 이어지지 않는다는 중요한 통찰을 얻었습니다. ReLU 네트워크가 표현할 수 있는 함수 집합이 넓더라도, 역전파와 같은 최적화 알고리즘이 해당 함수를 효과적으로 찾아내지 못할 수 있습니다.
- **제한 및 향후 연구:**
  - 이 논문의 표현력 결과를 심층(deep) 네트워크로 일반화하는 것이 과제입니다.
  - 표현력 결과가 샘플 복잡도(sample complexity) 연구에 어떻게 적용될 수 있는지 탐구해야 합니다.
  - 표현력, 샘플 복잡도, 그리고 훈련 목적 함수의 볼록성(convexity) 간의 복잡한 상호작용을 더 깊이 연구하는 것이 흥미로운 미래 연구 방향입니다.

## 📌 TL;DR

2계층 ReLU 네트워크의 결정 경계는 기능적으로 동등한 임계값 네트워크로 표현될 수 있지만, **기하급수적으로 더 많은 임계값 유닛**이 필요합니다. 이는 ReLU가 적은 유닛으로도 복잡한 함수를 표현할 수 있음을 시사합니다. 특정 조건 하에서는 임계값 네트워크를 로그적으로 더 작은 ReLU 네트워크로 압축할 수 있습니다. 하지만 합성 데이터 실험 결과, **높은 표현력이 항상 효과적인 학습(backpropagation)을 보장하지는 않으며**, 최적화 문제와 상호작용할 수 있음을 발견했습니다.
