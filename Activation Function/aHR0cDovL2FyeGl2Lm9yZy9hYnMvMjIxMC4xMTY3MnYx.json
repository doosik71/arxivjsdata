{
  "title": "Stochastic Adaptive Activation Function",
  "authors": "Kyungsu Lee, Jaeseung Yang, Haeyun Lee, Jae Youn Hwang",
  "year": 2022,
  "url": "http://arxiv.org/abs/2210.11672v1",
  "abstract": "The simulation of human neurons and neurotransmission mechanisms has been\nrealized in deep neural networks based on the theoretical implementations of\nactivation functions. However, recent studies have reported that the threshold\npotential of neurons exhibits different values according to the locations and\ntypes of individual neurons, and that the activation functions have limitations\nin terms of representing this variability. Therefore, this study proposes a\nsimple yet effective activation function that facilitates different thresholds\nand adaptive activations according to the positions of units and the contexts\nof inputs. Furthermore, the proposed activation function mathematically\nexhibits a more generalized form of Swish activation function, and thus we\ndenoted it as Adaptive SwisH (ASH). ASH highlights informative features that\nexhibit large values in the top percentiles in an input, whereas it rectifies\nlow values. Most importantly, ASH exhibits trainable, adaptive, and\ncontext-aware properties compared to other activation functions. Furthermore,\nASH represents general formula of the previously studied activation function\nand provides a reasonable mathematical background for the superior performance.\nTo validate the effectiveness and robustness of ASH, we implemented ASH into\nmany deep learning models for various tasks, including classification,\ndetection, segmentation, and image generation. Experimental analysis\ndemonstrates that our activation function can provide the benefits of more\naccurate prediction and earlier convergence in many deep learning applications.",
  "citation": 7
}