# A Simple Way to Initialize Recurrent Networks of Rectified Linear Units

Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton

## 🧩 Problem to Solve

순환 신경망(RNN)은 시퀀스 데이터 처리에 강력하지만, 역전파(back-propagation) 중 기울기 소실(vanishing gradients) 또는 폭발(exploding gradients) 문제로 인해 장기 의존성(long-term dependencies)을 학습하기 어렵다는 고질적인 문제가 있습니다. 이를 해결하기 위해 LSTM(Long Short-Term Memory)과 같은 복잡한 아키텍처나 정교한 최적화 기법이 개발되었으나, 본 논문에서는 ReLU(Rectified Linear Unit) 기반의 RNN에서 간단한 가중치 초기화만으로 이러한 어려움을 극복할 수 있는지 탐구합니다.

## ✨ Key Contributions

- **간단한 초기화 방안 제시:** ReLU 활성화 함수를 사용하는 RNN의 순환 가중치 행렬을 항등 행렬($I$) 또는 스케일된 항등 행렬($cI$)로 초기화하는 방법을 제안합니다.
- **장기 의존성 학습 능력 개선:** 이 초기화 방법을 통해 ReLU 기반 RNN(IRNN)이 기울기 소실/폭발 문제를 효과적으로 완화하여 LSTM과 유사하게 장기 의존성을 학습할 수 있음을 입증합니다.
- **LSTM에 필적하는 성능:** 다양한 벤치마크(Adding Problem, MNIST 시퀀스 분류, 언어 모델링, 음성 인식)에서 IRNN이 훨씬 간단한 구조임에도 불구하고 표준 LSTM 구현과 필적하거나 능가하는 성능을 보임을 보여줍니다.
- **모델 복잡도 감소:** LSTM에 비해 훨씬 적은 수의 매개변수와 단순한 아키텍처로 유사한 성능을 달성하여, RNN 학습의 복잡성을 줄일 수 있는 가능성을 제시합니다.

## 📎 Related Works

- **기울기 문제 해결 노력:** Hessian-Free 최적화(Martens, Sutskever), 기울기 클리핑(Pascanu et al.), 적응형 학습률(Duchi et al., Hinton) 등 RNN의 기울기 문제를 다루기 위한 다양한 기법들이 연구되었습니다.
- **LSTM:** Hochreiter와 Schmidhuber가 제안한 LSTM은 게이팅 메커니즘을 통해 장기 의존성 학습에 가장 성공적인 기법으로 꼽힙니다. 망각 게이트(forget gate)의 도입으로 성능이 더욱 향상되었습니다.
- **ReLU의 적용:** 딥 피드포워드 네트워크에서 ReLU가 tanh나 시그모이드(logistic) 함수보다 학습이 용이하다는 것이 입증되었습니다(Nair, Hinton).
- **초기화 기법:** Mikolov et al.은 가중치 행렬의 일부를 항등 행렬로 고정하는 아이디어를 제안했고, Socher et al.은 트리 구조 네트워크에서 스케일된 항등 행렬 초기화를 사용했습니다. Saxe et al.은 딥 네트워크에서 직교 행렬을 초기화로 사용하는 것을 연구했습니다. 본 연구는 이러한 아이디어들과 ReLU를 결합하여 RNN에 적용합니다.

## 🛠️ Methodology

1. **IRNN 아키텍처:** 순환 신경망의 활성화 함수로 ReLU를 사용합니다.
2. **가중치 초기화:**
   - **순환 가중치 행렬($W_{hh}$):** 항등 행렬($I$)로 초기화하거나, 특정 태스크(예: 단기 기억이 중요한 음성 인식)에서는 작은 스칼라 값($c$)을 곱한 스케일된 항등 행렬($cI$)로 초기화합니다.
   - **비순환 가중치 행렬($W_{ih}, W_{ho}$):** 평균이 0이고 표준 편차가 0.001인 가우시안 분포에서 샘플링된 무작위 행렬로 초기화합니다.
   - **바이어스 벡터($b$):** 모두 0으로 초기화합니다.
3. **훈련:**
   - 시간에 따른 역전파(Backpropagation Through Time, BPTT)를 사용하여 기울기를 계산합니다.
   - 확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 사용하며, 고정된 학습률(learning rate)과 기울기 클리핑(gradient clipping)을 적용합니다.
   - 최적의 하이퍼파라미터를 찾기 위해 학습률($\alpha$) 및 기울기 클리핑 값($gc$)에 대한 그리드 탐색을 수행합니다.
   - 모든 실험에서 동일한 배치 크기(16)를 사용합니다.

## 📊 Results

- **덧셈 문제 (Adding Problem):** 긴 시퀀스($T=150, 200, 300, 400$)에서 IRNN은 LSTM과 유사하게 수렴하며, tanh 및 무작위 ReLU RNN보다 훨씬 우수한 성능을 보였습니다. 특히 $T=400$과 같이 매우 긴 시퀀스에서도 효과적이었습니다.
- **픽셀 단위 MNIST 분류:** 784개의 픽셀을 순차적으로 읽는 매우 긴 장기 의존성 문제에서, IRNN은 3%의 테스트 오류율을 달성하여 대부분의 선형 분류기보다 우수했으며, LSTM보다도 나은 성능을 보였습니다. 픽셀을 무작위로 섞은 Permuted MNIST에서도 IRNN은 비섞인 MNIST의 LSTM보다 더 나은 성능을 보여주었습니다.
- **언어 모델링:** 10억 단어 벤치마크 데이터셋에서, 유사한 매개변수 수를 가진 LSTM과 IRNN(4개 레이어, 512유닛 또는 1개 레이어, 1024유닛)이 비슷한 perplexity(68.8 vs 69.4/70.2)를 기록하며 IRNN이 LSTM에 필적하는 성능을 보여주었습니다.
- **음성 인식 (TIMIT):** 단기 기억이 주로 필요한 이 태스크에서는 항등 행렬 초기화 대신 $0.01I$와 같이 스케일된 항등 행렬 초기화(iRNNs)가 더 좋은 결과를 가져왔습니다. iRNN은 tanh 유닛을 사용하는 RNN보다 훨씬 우수했으며, LSTM과 비교할 만한 프레임 오류율(FER)을 달성했습니다 (예: 5개 레이어 양방향 LSTM 28.5/29.1, iRNN 28.9/29.7).

## 🧠 Insights & Discussion

- **단순함의 효율성:** 복잡한 게이팅 메커니즘을 가진 LSTM과 달리, ReLU 기반 RNN에 항등 행렬 초기화라는 단순한 기법을 적용하는 것만으로 장기 의존성 학습 능력을 크게 향상시킬 수 있음을 보여줍니다.
- **기울기 안정성:** 항등 행렬 초기화는 역전파 시 기울기가 거의 일정하게 유지되도록 하여, LSTM의 망각 게이트가 감쇠 없이 값을 유지하는 것과 유사한 방식으로 장기 의존성 문제를 해결합니다.
- **스케일링의 중요성:** 모든 태스크에서 항등 행렬이 최적인 것은 아니며, 음성 인식처럼 단기 의존성이 더 중요한 경우 작은 값으로 스케일된 항등 행렬($0.01I$)이 더 빠르게 수렴하고 더 좋은 성능을 낼 수 있습니다. 이는 망각 게이트가 빠르게 감쇠하도록 설정된 LSTM의 동작과 유사합니다.
- **미래 연구 방향:** LSTM의 성공에 기여하는 핵심 요소가 무엇인지에 대한 이해를 높이며, 더 간단하고 효율적인 RNN 모델 설계의 가능성을 열었습니다. 모델이 더 단순하기 때문에 구현 및 튜닝이 용이합니다.
- **제한 사항:** LSTM의 하이퍼파라미터 튜닝을 더 세밀하게 했다면 결과가 달라질 수 있다는 점을 인정합니다.

## 📌 TL;DR

본 논문은 ReLU를 활성화 함수로 사용하는 RNN(IRNN)에서 순환 가중치 행렬을 항등 행렬($I$) 또는 스케일된 항등 행렬($cI$)로 초기화하는 간단한 방법을 제안합니다. 이 "초기화 트릭"은 IRNN이 기울기 소실 및 폭발 문제를 효과적으로 완화하여 장기 의존성을 성공적으로 학습하게 합니다. 다양한 벤치마크 실험에서 IRNN은 복잡한 LSTM에 필적하거나 때로는 더 나은 성능을 보여주며, 훨씬 단순한 구조와 적은 매개변수로도 강력한 RNN 모델을 구축할 수 있음을 입증합니다.
