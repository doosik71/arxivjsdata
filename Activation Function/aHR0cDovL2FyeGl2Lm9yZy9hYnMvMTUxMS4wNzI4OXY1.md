# FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUs)

Djork-Arn ́e Clevert, Thomas Unterthiner & Sepp Hochreiter

## 🧩 Problem to Solve

기존 딥 신경망의 활성화 함수(예: ReLU)는 여러 문제점을 안고 있습니다.

1. **평균 활성화 값의 0 편향 문제**: ReLU는 양수 값에 대해서만 활성화되므로, 유닛의 평균 활성화 값이 0보다 크게 되어 다음 레이어에 바이어스(bias)로 작용합니다. 이러한 "바이어스 시프트(bias shift)"는 학습을 방해하고 표준 경사 하강법이 자연 경사(natural gradient)에 가까워지는 것을 저해하여 학습 속도를 늦춥니다.
2. **그래디언트 소실 문제**: 시그모이드(sigmoid)나 tanh와 같은 전통적인 활성화 함수는 그래디언트 소실(vanishing gradient) 문제에 취약하여 깊은 네트워크 학습을 어렵게 합니다. ReLU는 이 문제를 완화했지만, 위에서 언급한 평균 활성화 값 편향 문제를 해결하지 못합니다.
3. **잡음 강건성(Noise Robustness) 부족**: Leaky ReLU(LReLU)나 Parametric ReLU(PReLU)와 같이 음수 값을 허용하는 활성화 함수도 있지만, 이들은 "잡음에 강건한 비활성화 상태(noise-robust deactivation state)"를 보장하지 않아 입력의 작은 변화에도 크게 반응할 수 있습니다.

## ✨ Key Contributions

이 논문은 Exponential Linear Unit (ELU)라는 새로운 활성화 함수를 제안하며 다음과 같은 핵심 기여를 합니다.

- **학습 속도 향상 및 정확도 증진**: ELU는 기존의 ReLU, LReLU, PReLU보다 딥 신경망의 학습 속도를 크게 향상시키고 더 높은 분류 정확도를 달성합니다.
- **평균 활성화 값을 0에 가깝게 유지**: ELU는 음수 값을 가짐으로써 유닛의 평균 활성화 값을 0에 가깝게 만듭니다. 이는 바이어스 시프트 효과를 줄여 표준 그래디언트를 자연 그래디언트에 더 가깝게 만들고 학습 속도를 높입니다. 배치 정규화(Batch Normalization)와 유사한 효과를 더 낮은 계산 복잡도로 달성합니다.
- **잡음에 강건한 비활성화 상태 제공**: ELU는 음수 입력에 대해 음수 값으로 포화(saturate)되어 잡음에 강건한 비활성화 상태를 제공합니다. 이는 전달되는 분산 및 정보를 감소시켜 보다 강건하고 저복잡도(low-complex)의 표현을 가능하게 합니다.
- **다양한 벤치마크 데이터셋에서 우수한 성능 입증**: CIFAR-10, CIFAR-100, ImageNet 등 다양한 이미지 데이터셋에서 ReLU 및 LReLU 네트워크보다 더 나은 일반화 성능을 보였습니다. 특히, 배치 정규화를 적용한 ReLU 네트워크보다 우수하며, CIFAR-100에서 새로운 최고 성능을 달성했습니다.

## 📎 Related Works

- **ReLU (Rectified Linear Unit)**: 깊은 신경망에서 그래디언트 소실 문제를 완화하고 희소 코드(sparse codes)를 생성하여 널리 사용됩니다. (Nair & Hinton, 2010; Glorot et al., 2011) 하지만 비음수 값만을 가지므로 평균 활성화 값이 0보다 크다는 단점이 있습니다.
- **LReLU (Leaky ReLU) 및 PReLU (Parametric ReLU)**: ReLU의 음수 부분을 선형 함수로 대체하여 음수 값을 허용합니다. (Maas et al., 2013; He et al., 2015) PReLU는 음수 부분의 기울기를 학습하여 성능을 개선합니다. 하지만 잡음에 강건한 비활성화 상태를 보장하지 못합니다.
- **RReLU (Randomized Leaky Rectified Linear Unit)**: 음수 부분의 기울기를 무작위로 샘플링하여 성능을 높입니다. (Xu et al., 2015)
- **Batch Normalization**: 활성화 값의 평균을 0, 분산을 1로 정규화하여 내부 공변량 시프트(internal covariate shift)를 줄이고 학습 속도를 가속화합니다. (Ioffe & Szegedy, 2015)
- **Natural Gradient**: 피셔 정보 행렬(Fisher information matrix)의 역수를 사용하여 그래디언트 방향을 수정함으로써 리만 매개변수 다양체(Riemannian parameter manifold)에서 가장 가파른 하강을 보장하는 최적의 학습 방법입니다. (Amari, 1998) 논문에서는 유닛 자연 경사(unit natural gradient)를 사용하여 바이어스 시프트 효과를 분석합니다.
- **Centering Activations**: 활성화 값의 중심을 0으로 맞추는 것은 학습 속도를 높이는 것으로 알려져 있습니다. (LeCun et al., 1991; 1998; Schraudolph, 1998)

## 🛠️ Methodology

논문은 ELU(Exponential Linear Unit) 활성화 함수를 제안하고, 그 효과를 바이어스 시프트 보정(bias shift correction) 이론을 통해 설명합니다.

1. **ELU 활성화 함수 정의**:
   ELU는 다음과 같이 정의됩니다 ($0 < \alpha$):
   $$f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(\exp(x) - 1) & \text{if } x \le 0 \end{cases}$$
   그 미분은 다음과 같습니다:
   $$f'(x) = \begin{cases} 1 & \text{if } x > 0 \\ f(x) + \alpha & \text{if } x \le 0 \end{cases}$$
   여기서 $\alpha$는 음수 입력에 대해 ELU가 포화되는 값을 제어하는 하이퍼파라미터입니다.

2. **바이어스 시프트 보정 이론**:
   - **문제점**: ReLU와 같이 비음수 활성화 값을 갖는 유닛은 평균 활성화 값이 0보다 커서 다음 레이어에 바이어스 역할을 합니다. 이러한 비정규화된(unnormalized) 평균은 "바이어스 시프트"를 유발하여 학습을 방해합니다.
   - **자연 경사(Natural Gradient)**: 자연 경사는 역 피셔 정보 행렬 $\tilde{F}^{-1}$을 그래디언트 $\nabla_{w}R_{emp}$에 곱하여 그래디언트 방향을 보정합니다: $\nabla_{nat}{w}R_{emp} = \tilde{F}^{-1}\nabla_{w}R_{emp}$. 이는 바이어스 시프트를 완화하거나 방지하여 효율적인 학습을 가능하게 합니다.
   - **ELU의 역할**:
     - **평균 활성화 값을 0에 가깝게**: ELU는 음수 값을 가지므로 유닛의 평균 활성화 값을 0에 가깝게 만듭니다. 논문은 유닛 자연 경사 이론을 통해 평균 활성화 값이 0에 가까워질수록 표준 그래디언트가 자연 경사에 더 가까워져 학습이 빨라짐을 보입니다 (Theorem 2).
     - **잡음에 강건한 비활성화 상태**: $x \le 0$인 경우 ELU는 음수 값으로 포화됩니다. 이는 입력값이 작아질수록 미분값이 작아져 다음 레이어로 전달되는 변동성과 정보량을 감소시킵니다. 따라서 비활성화된 유닛은 잡음에 덜 민감해지고, 활성화된 유닛만이 관련 정보를 전달하며 상호작용할 수 있도록 합니다. 이는 표현의 잡음 강건성 및 저복잡도에 기여합니다.

## 📊 Results

ELU는 MNIST, CIFAR-10, CIFAR-100, ImageNet 등 다양한 벤치마크 데이터셋에서 뛰어난 성능을 입증했습니다.

- **MNIST 데이터셋 (8개 은닉층 fully-connected 네트워크)**:

  - **평균 활성화 값**: ELU 네트워크는 훈련 과정 내내 다른 활성화 함수(ReLU, LReLU)보다 유닛의 평균 활성화 값을 0에 더 가깝게 유지했습니다.
  - **훈련 손실**: ELU 네트워크의 훈련 손실은 다른 네트워크보다 훨씬 빠르게 감소했습니다.
  - **오토인코더 학습**: 다양한 학습률(learning rates)에서 ELU는 훈련/테스트 세트 재구성 오차 측면에서 경쟁 활성화 함수를 능가했습니다.

- **CIFAR-100 데이터셋 (11개 Convolutional 레이어 CNN)**:

  - **성능 비교**: ELU는 평균 28.75% (±0.24)%의 테스트 에러를 달성하여 SReLU(29.35%), ReLU(31.56%), LReLU(30.59%)보다 우수한 성능을 보였습니다. 훈련 손실과 테스트 에러 모두 다른 활성화 함수보다 유의미하게 낮았습니다.
  - **배치 정규화(Batch Normalization)와의 비교**: ELU 네트워크는 배치 정규화를 적용한 ReLU 네트워크보다 훨씬 우수한 성능을 보였습니다. 반면, ELU 네트워크에 배치 정규화를 적용해도 성능 향상은 거의 없었습니다.

- **CIFAR-10 및 CIFAR-100 데이터셋 (18개 Convolutional 레이어 CNN)**:

  - **CIFAR-10**: ELU 네트워크는 6.55%의 테스트 에러로, 발표된 결과 중 상위 10위 안에 들었습니다.
  - **CIFAR-100**: ELU 네트워크는 24.28%의 테스트 에러를 달성하여 멀티-뷰 평가나 모델 앙상블 없이도 CIFAR-100에서 새로운 최고 성능(state-of-the-art)을 수립했습니다.

- **ImageNet 데이터셋 (15개 레이어 CNN)**:
  - **학습 속도**: ELU 네트워크는 ReLU 네트워크보다 훨씬 일찍 오류 감소를 시작했습니다. ELU 네트워크는 160k 반복 후 20% top-5 에러에 도달했지만, ReLU 네트워크는 동일한 에러율에 도달하는 데 200k 반복이 필요했습니다.
  - **최고 성능**: 단일 모델, 단일 크롭(single crop)에서 10% 미만의 top-5 검증 에러를 달성했습니다.
  - **계산 시간**: 현재 구현에서 ELU 네트워크는 ReLU 네트워크보다 ImageNet에서 약 5% 느렸으나, 구현 최적화를 통해 개선될 수 있을 것으로 예상됩니다.

## 🧠 Insights & Discussion

ELU의 성공은 평균 활성화 값을 0에 가깝게 유지하는 능력에 있습니다. 이는 바이어스 시프트 효과를 줄여 표준 그래디언트가 자연 그래디언트에 더 근접하게 만들고, 결과적으로 학습 속도를 가속화합니다. 이는 LReLU, PReLU, 그리고 배치 정규화의 성공 이유와도 연결됩니다. 그러나 ELU는 LReLU나 PReLU와 달리 음수 영역에서 명확한 포화(saturation) 구간을 가지므로, 더욱 강건하고 안정적인 표현을 학습할 수 있습니다. 이러한 포화 특성은 비활성화된 유닛이 입력 잡음에 덜 민감하게 반응하도록 하여, 네트워크가 입력의 특정 현상 "존재 여부"를 효율적으로 인코딩하고 "부재"를 정량적으로 모델링하지 않도록 합니다.

실험 결과는 ELU가 단순히 학습을 가속화하는 것을 넘어, 기존 ReLU 및 배치 정규화가 적용된 ReLU 네트워크보다 더 나은 일반화 성능과 더 높은 정확도를 달성한다는 것을 명확히 보여줍니다. 특히 CIFAR-100에서 새로운 최고 성능을 달성한 것은 ELU의 강력한 잠재력을 시사합니다. 이러한 뛰어난 성능을 고려할 때, ELU는 딥 신경망, 특히 학습 시간이 긴 컨볼루션 네트워크에서 상당한 시간 절약을 가져올 것으로 기대됩니다.

## 📌 TL;DR

ELU(Exponential Linear Unit)는 평균 활성화 값을 0에 가깝게 유지하고 음수 입력에 대해 포화되는 새로운 활성화 함수입니다. 이는 바이어스 시프트 효과를 줄여 학습 속도를 가속화하고 잡음에 강건한 표현을 학습하게 합니다. 실험 결과, ELU는 MNIST, CIFAR-10/100, ImageNet에서 기존 ReLU 및 배치 정규화가 적용된 ReLU 네트워크보다 빠르고 정확한 학습 성능을 보였으며, CIFAR-100에서 새로운 최고 성능을 달성했습니다.
