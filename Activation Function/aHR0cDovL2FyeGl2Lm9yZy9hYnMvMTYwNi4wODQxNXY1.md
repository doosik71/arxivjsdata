# GAUSSIAN ERROR LINEAR UNITS (GELUS)

Dan Hendrycks, Kevin Gimpel

## 🧩 Problem to Solve

신경망 활성화 함수는 네트워크의 학습 및 성능에 중요한 영향을 미칩니다. 기존의 활성화 함수들은 다음과 같은 한계를 가집니다:

- **시그모이드(Sigmoid)**: 깊은 네트워크에서 역전파 학습이 비효율적입니다.
- **ReLU (Rectified Linear Unit)**: 입력의 부호에 따라 0 또는 1을 곱하는 "하드 게이팅" 방식을 사용하여 결정론적입니다. 통계적 동기가 부족하지만 공학적으로는 효과적입니다.
- **ELU (Exponential Linear Unit)**: ReLU의 성공을 바탕으로 음수 값을 허용하여 훈련 속도를 향상시키지만, 여전히 입력의 부호에 기반한 결정을 내립니다.
- **활성화 함수와 정규화의 분리**: 드롭아웃(Dropout)과 같은 확률적 정규화 기법은 활성화 함수와 독립적으로 작동하며, 뉴런의 최종 출력에 영향을 미치지만 두 혁신은 분리되어 있습니다.

본 논문은 이러한 한계를 극복하고, 입력의 부호에 따라 게이팅하는 대신 입력 값 자체에 따라 입력에 가중치를 부여하는 새로운 활성화 함수를 제안합니다. 이는 확률적 정규화의 특성을 활성화 함수에 통합하여 뉴런 출력에 대한 보다 확률론적인 관점을 제시하는 것을 목표로 합니다.

## ✨ Key Contributions

- **GELU (Gaussian Error Linear Unit) 활성화 함수 제안**: $x \Phi(x)$ 형태로 정의되는 새로운 고성능 활성화 함수를 도입했습니다. 여기서 $\Phi(x)$는 표준 가우시안 누적 분포 함수(Standard Gaussian Cumulative Distribution Function)입니다.
- **확률론적 가중치 부여 메커니즘**: ReLU가 입력의 부호에 따라 게이팅(0 또는 1)하는 것과 달리, GELU는 입력 값에 따라 확률적으로 입력에 가중치를 부여합니다.
- **확률적 정규화와의 연관성**: GELU는 Adaptive Dropout의 변형에 대한 기댓값으로 해석될 수 있으며, 이는 뉴런 출력에 대한 확률론적인 관점을 제공합니다.
- **다양한 태스크에서 성능 향상**: 컴퓨터 비전, 자연어 처리, 음성 인식 등 다양한 태스크에서 ReLU 및 ELU보다 일관되게 우수하거나 동등한 성능을 달성했습니다.
- **새로운 수학적 특성**: 비볼록(non-convex), 비단조(non-monotonic) 함수이며 모든 지점에서 곡률(curvature)을 보여, ReLU나 ELU보다 복잡한 함수를 더 쉽게 근사할 수 있는 잠재력을 가집니다.

## 📎 Related Works

- **이진 임계 단위 (Binary Threshold Units)**: 초기 인공 뉴런 모델 (Hopfield, McCulloch & Pitts).
- **시그모이드 활성화 함수 (Sigmoid Activations)**: 이진 결정을 매끄럽게 하여 역전파 학습을 가능하게 함.
- **ReLU (Rectified Linear Units)**: 비선형성 및 빠른 수렴으로 깊은 네트워크에서 시그모이드보다 효과적 (Nair & Hinton, 2010).
- **ELU (Exponential Linear Units)**: ReLU를 개선하여 음수 출력을 허용, 학습 속도 향상 (Clevert et al., 2016).
- **드롭아웃 (Dropout)**: 뉴런 출력을 무작위로 0으로 만들어 과적합을 방지하는 확률적 정규화 기법 (Srivastava et al., 2014).
- **존아웃 (Zoneout)**: RNN에서 은닉 활성화를 무작위로 보존하여 정규화하는 기법 (Krueger et al., 2016).
- **적응형 드롭아웃 (Adaptive Dropout)**: 활성화 함수와 함께 사용되는 확률적 정규화 기법 (Ba & Frey, 2013).
- **SiLU (Sigmoid Linear Unit) / Swish**: 본 논문에서 $x\sigma(x)$ 형태로 제안되었으며, 이후 다른 연구에서 독립적으로 발견되어 'Swish'라는 이름으로 불리기도 했으나, 이 논문에서 먼저 'Sigmoid Linear Unit (SiLU)'로 명명되었음을 부록에서 상세히 밝히고 있습니다.

## 🛠️ Methodology

GELU는 드롭아웃, 존아웃, ReLU의 특성을 결합하여 설계되었습니다.

1. **확률적 마스킹**: 뉴런 입력 $x$에 대해, 표준 정규 분포 $X \sim N(0,1)$의 누적 분포 함수 $\Phi(x) = P(X \le x)$에 따라 베르누이 분포 $m \sim \text{Bernoulli}(\Phi(x))$를 따르는 마스크 $m$을 생성합니다. 입력 $x$는 이 마스크 $m$과 곱해집니다.
   - 이 방식은 $x$ 값이 작아질수록 입력이 "드롭(drop)"될 확률이 높아지도록 하여, 입력 값에 따라 확률적으로 변환을 적용합니다. 이는 Adaptive Dropout과 유사합니다.
2. **GELU 공식화**: 신경망에서 결정론적인 결정을 위해, 이 확률적 정규화가 입력 $x$에 가하는 *기대되는 변환*을 활성화 함수로 사용합니다.
   - 기대되는 변환은 $x \Phi(x) + 0 \cdot (1-\Phi(x)) = x\Phi(x)$ 입니다.
   - 따라서 GELU는 다음과 같이 정의됩니다:
     $$ \text{GELU}(x) = x P(X \le x) = x \Phi(x) = x \cdot \frac{1}{2} \left[1 + \text{erf}\left(x/\sqrt{2}\right)\right] $$
   - 여기서 $\text{erf}(\cdot)$는 오차 함수(Error Function)입니다.
3. **근사 (Approximations)**: 정확성을 다소 희생하고 빠른 순방향 계산 속도가 필요할 경우, 다음과 같은 근사식을 사용할 수 있습니다.
   - $0.5x(1 + \tanh[\sqrt{2/\pi}(x+ 0.044715x^3)])$ (본 논문 실험에서 사용)
   - $x\sigma(1.702x)$
4. **하이퍼파라미터**: 표준 정규 분포의 $\mu=0, \sigma=1$을 사용하므로 새로운 하이퍼파라미터를 도입하지 않습니다.

## 📊 Results

GELU는 다양한 컴퓨터 비전, 자연어 처리, 음성 인식 태스크에서 ReLU 및 ELU보다 일관되게 우수한 성능을 보였습니다.

- **MNIST 분류**: 드롭아웃 적용 유무에 관계없이 가장 낮은 중앙값 로그 손실을 기록했으며, 노이즈가 추가된 입력에 대해 더 강건했습니다.
- **MNIST 오토인코딩**: 다른 활성화 함수에 비해 다양한 학습률에서 훨씬 더 나은 재구성 오류를 달성했습니다.
- **Twitter POS 태깅**: 상대적으로 작은 데이터셋에서 GELU가 12.57%의 테스트 오류율로 ReLU(12.67%) 및 ELU(12.91%)보다 우수했습니다.
- **TIMIT 음소 인식**: GELU가 29.3%의 오류율로 ReLU(29.5%) 및 ELU(29.6%)보다 근소하게 우수했습니다.
- **CIFAR-10/100 분류**:
  - CIFAR-10 (얕은 CNN): GELU 7.89%, ReLU 8.16%, ELU 8.41% 오류율.
  - CIFAR-100 (깊은 Wide Residual Network): GELU 20.74%, ReLU 21.77%, ELU 22.98% 오류율.
- **일관된 우위**: 모든 실험에서 GELU는 다른 활성화 함수들보다 더 빠르게 수렴하거나 더 낮은 오류율을 달성했습니다.

## 🧠 Insights & Discussion

- **ReLU와의 관계**: GELU는 ReLU의 "매끄러운 버전"으로 볼 수 있습니다. $\sigma \to 0, \mu=0$일 때 GELU는 ReLU와 동일해지며, 점근적으로도 동일합니다. 가우시안 CDF가 ReLU의 이진 함수를 부드럽게 근사하는 것처럼 작동합니다.
- **주요 차이점**:
  - **곡률 및 비단조성**: GELU는 모든 지점에서 곡률을 가지는 비볼록, 비단조 함수입니다. 반면 ReLU와 ELU는 양수 도메인에서 선형이므로 곡률이 부족할 수 있습니다. GELU의 이러한 특성은 복잡한 함수를 더 쉽게 근사할 수 있도록 합니다.
  - **가중치 부여 방식**: ReLU는 입력의 부호에 따라 게이팅하지만, GELU는 입력이 다른 입력보다 얼마나 큰지에 따라 가중치를 부여합니다.
  - **확률론적 해석**: GELU는 확률적 정규화의 기댓값으로 해석될 수 있다는 중요한 특징을 가집니다.
- **실용적인 팁**:
  - GELU로 훈련할 때는 모멘텀을 사용하는 옵티마이저를 사용하는 것이 좋습니다.
  - 가우시안 누적 분포 함수에 대한 정확한 근사 사용이 중요합니다. 본 논문에서는 $\tanh$ 기반 근사식을 사용했으며, 이는 SiLU($x\sigma(x)$)보다 우수했습니다.
- **SiLU (Sigmoid Linear Unit) vs. Swish 논란**: 본 논문은 $x\sigma(x)$를 SiLU로 명명하여 2016년에 처음 제안했습니다. 이후 다른 연구에서 독립적으로 동일한 함수를 'Swish'로 발표하며 저작권 문제와 관련된 논의가 있었으나, 최종적으로 'SiLU'가 원본 명칭으로 인정받아 TensorFlow, PyTorch와 같은 라이브러리에서 해당 함수를 'SiLU'로 변경했습니다.

## 📌 TL;DR

GELU는 기존 활성화 함수(ReLU, ELU)의 한계를 극복하기 위해 제안된 새로운 활성화 함수 $x \Phi(x)$입니다. 이는 입력 값에 따라 확률적으로 가중치를 부여하며, 확률적 정규화 기법의 기댓값으로 해석될 수 있습니다. 컴퓨터 비전, NLP, 음성 인식 등 다양한 딥러닝 태스크에서 ReLU 및 ELU보다 일관되게 우수한 성능을 보였으며, 비볼록 및 비단조 특성으로 복잡한 함수 근사에 유리합니다.
