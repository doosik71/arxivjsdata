{
  "title": "Growing Cosine Unit: A Novel Oscillatory Activation Function That Can\n  Speedup Training and Reduce Parameters in Convolutional Neural Networks",
  "authors": "Mathew Mithra Noel, Arunkumar L, Advait Trivedi, Praneet Dutta",
  "year": 2021,
  "url": "http://arxiv.org/abs/2108.12943v3",
  "abstract": "Convolutional neural networks have been successful in solving many socially\nimportant and economically significant problems. This ability to learn complex\nhigh-dimensional functions hierarchically can be attributed to the use of\nnonlinear activation functions. A key discovery that made training deep\nnetworks feasible was the adoption of the Rectified Linear Unit (ReLU)\nactivation function to alleviate the vanishing gradient problem caused by using\nsaturating activation functions. Since then, many improved variants of the ReLU\nactivation have been proposed. However, a majority of activation functions used\ntoday are non-oscillatory and monotonically increasing due to their biological\nplausibility. This paper demonstrates that oscillatory activation functions can\nimprove gradient flow and reduce network size. Two theorems on limits of\nnon-oscillatory activation functions are presented. A new oscillatory\nactivation function called Growing Cosine Unit(GCU) defined as $C(z) = z\\cos z$\nthat outperforms Sigmoids, Swish, Mish and ReLU on a variety of architectures\nand benchmarks is presented. The GCU activation has multiple zeros enabling\nsingle GCU neurons to have multiple hyperplanes in the decision boundary. This\nallows single GCU neurons to learn the XOR function without feature\nengineering. Experimental results indicate that replacing the activation\nfunction in the convolution layers with the GCU activation function\nsignificantly improves performance on CIFAR-10, CIFAR-100 and Imagenette.",
  "citation": 45
}