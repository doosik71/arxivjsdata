# Convergence of Deep ReLU Networks

Yuesheng Xu, Haizhang Zhang

## 🧩 Problem to Solve

본 논문은 심층 ReLU 신경망(Deep ReLU Networks)의 깊이(depth)가 무한대로 증가할 때, 해당 신경망이 의미 있는 함수로 수렴하는지 여부를 탐구합니다. 특히, 신경망의 가중치 행렬($W_n$)과 편향 벡터($b_n$)에 어떤 조건을 부여해야 네트워크가 수렴하는지 밝히는 것을 목표로 합니다. 이는 신경망이 특정 대상 함수에 근사하는 것과는 다른, 네트워크 자체의 기능적 수렴에 대한 근본적인 질문입니다.

## ✨ Key Contributions

- **활성화 도메인 및 활성화 행렬 도입**: ReLU 네트워크의 선형 구성 요소를 나타내는 '활성화 도메인(activation domains)'과 대각 성분이 0 또는 1인 '활성화 행렬(activation matrices)'의 개념을 새롭게 정의합니다.
- **ReLU 네트워크의 명시적 표현**: ReLU 활성화 함수 적용을 활성화 행렬 곱셈으로 대체하여, 심층 ReLU 네트워크의 출력을 입력 변수에 대한 명시적인 다층 선형 함수 형태로 표현하는 공식을 유도합니다.
- **무한 행렬 곱으로의 수렴 문제 전환**: 심층 ReLU 네트워크의 수렴 문제를 무한 행렬 곱($\prod_{i=1}^{n} I_i W_i$) 및 관련 합($\sum_{i=1}^{n} (\prod_{j=i+1}^{n} I_j W_j) I_i b_i$)의 존재 조건으로 공식화합니다.
- **무한 행렬 곱 수렴 조건 분석**: 이러한 무한 행렬 곱이 수렴하기 위한 충분조건 및 필요조건을 수학적으로 분석합니다.
- **ReLU 네트워크 수렴의 필요조건**: 네트워크가 수렴하기 위한 필요조건으로, 가중치 행렬 시퀀스($W_n$)는 단위 행렬 $I$로, 편향 벡터 시퀀스($b_n$)는 0으로 수렴해야 함을 증명합니다.
- **ReLU 네트워크 점별(Pointwise) 수렴 충분조건**: 가중치 행렬과 편향 벡터에 대한 특정 조건을 통해 심층 ReLU 네트워크의 점별 수렴에 대한 충분조건을 제시합니다.
- **Residual Networks(ResNets) 설계에 대한 수학적 통찰 제공**: 제시된 수렴 조건이 널리 사용되는 ResNets의 설계 전략에 대한 수학적 설명을 제공함을 보입니다.

## 📎 Related Works

본 논문은 주로 심층 신경망의 근사 및 표현력(approximation and expressive powers)에 대한 기존 연구들과는 다른 관점에서 수렴 문제를 다룹니다. 관련 연구로는 Poggio 등(2017)의 심층 네트워크가 얕은 네트워크보다 지수적으로 더 나은 함수 근사 능력을 가짐을 보인 연구, Montanelli & Du(2017), Yarotsky(2017), Montanelli & Yang(2020)의 차원의 저주를 완화하는 심층 네트워크의 능력을 다룬 연구들이 언급됩니다. 또한 Shen 등(2020, 2021)의 ReLU 네트워크의 최적 근사 속도에 대한 연구, Daubechies 등(2019)의 DNN이 전통적인 비선형 근사 방법보다 뛰어난 근사력을 가짐을 보인 연구도 참조됩니다.

특히, 본 연구는 컴퓨터 과학 커뮤니티에서 무한 깊이 또는 무한 너비 네트워크의 Conjugate Kernels 및 Neural Tangent Kernels 수렴을 다루는 연구들(예: Jacot 등(2018), Hu & Huang(2021))과 차이점을 명확히 합니다. 후자의 연구들이 무작위 초기화된 DNN의 확률적 수렴에 초점을 맞추는 반면, 본 논문은 네트워크의 모든 파라미터에 대한 분석적 방법을 통해 DNN이 결정론적 함수로 수렴하는 조건을 연구합니다.

## 🛠️ Methodology

1. **ReLU 네트워크의 대수적 공식화**:
   - 단일 계층 ReLU 네트워크 $N_1(x) = \sigma(W_1 x + b_1)$의 활성화 패턴을 식별하기 위해 '활성화 행렬' $J \in D_m$ (대각 성분이 0 또는 1인 $m \times m$ 대각 행렬)을 정의합니다.
   - 각 $J$에 대해 '활성화 도메인' $D_{J,W,b}$를 정의하여 입력 $x$가 통과할 때 활성화되는 노드를 표시합니다.
   - 이 활성화 도메인 위에서 ReLU 활성화 함수를 활성화 행렬 곱셈으로 대체하여 $N_1(x) = I_1(W_1 x + b_1)$로 표현합니다.
2. **다층 ReLU 네트워크의 명시적 표현**:
   - $n$개 층의 ReLU 네트워크 $N_n(x)$에 대해, 각 층에 대한 활성화 행렬 시퀀스 $\bar{I}_n = (I_1, \ldots, I_n)$를 도입합니다.
   - $n$개 층 네트워크의 활성화 도메인 $D_{\bar{I}_n, \bar{W}_n, \bar{b}_n}$을 재귀적으로 정의합니다.
   - 재귀적 정의를 통해, 임의의 입력 $x$가 특정 활성화 도메인 $D_{\bar{I}_n, \bar{W}_n, \bar{b}_n}$에 속할 때, $N_n(x)$를 다음과 같이 명시적으로 표현합니다 (Theorem 3.4):
     $$ N*n(x) = \left(\prod*{i=1}^{n} I*i W_i\right) x + \sum*{i=1}^{n} \left(\prod\_{j=i+1}^{n} I_j W_j\right) I_i b_i $$
3. **수렴 조건 도출**:
   - $N_n(x)$가 $L_p$ 공간에서 수렴하려면 위의 명시적 표현의 두 항(무한 행렬 곱과 무한 급수)이 수렴해야 함을 보입니다 (Theorem 3.6).
   - **필요 조건**: 무한 행렬 곱 $\prod_{n=2}^{\infty} I_n W_n$이 수렴하기 위한 충분조건으로 $\sum_{n=2}^{\infty} \|P_n\| < \infty$ (여기서 $W_n = I + P_n$)를 제시하고 (Theorem 4.3), 이를 통해 $W_n \to I$ 및 $b_n \to 0$이 필요함을 보입니다 (Theorem 4.4).
   - **충분 조건**: 무한 급수 항이 수렴하기 위한 충분조건으로 $\sum_{n=1}^{\infty} \|b_n\| < \infty$ 및 행렬 곱 $\prod_{j=i}^{\infty} I_j W_j$의 수렴과 boundedness를 요구합니다 (Theorem 4.5).
   - 이러한 개별 조건들을 종합하여, 심층 ReLU 네트워크의 점별 수렴에 대한 전반적인 충분조건을 제시합니다 (Theorem 5.1).

## 📊 Results

- **네트워크 수렴의 필요조건 (Theorem 4.4)**:
  - $L_p$ 노름에서 심층 ReLU 네트워크 $N_n$이 수렴한다면, 모든 활성화 행렬 시퀀스 $I=(I_n \in D_m : n \in \mathbb{N})$에 대해 다음이 성립합니다:
    - 가중치 행렬 시퀀스 $\{W_n\}$은 단위 행렬 $I$로 수렴합니다: $\lim_{n \to \infty} W_n = I$.
    - 편향 벡터 시퀀스 $\{b_n\}$은 0으로 수렴합니다: $\lim_{n \to \infty} b_n = 0$.
  - 이 조건은 $W_n = I + P_n$일 때 $\sum_{n=2}^{\infty} \|P_n\| < \infty$ 및 $\sum_{i=n+1}^{\infty} \|P_i\| = o(\frac{1}{n})$을 만족하고, $b_n$이 유계일 때 도출됩니다.
- **네트워크 점별 수렴의 충분조건 (Theorem 5.1)**:
  - 벡터 노름이 (4.3)을 만족하고 그에 의해 유도된 행렬 노름을 사용할 때, 다음 두 조건이 만족되면 ReLU 네트워크 $N_n$은 $[0,1]^d$에서 점별(pointwise) 수렴합니다:
    - 가중치 행렬 $W_n$ ($n \ge 2$)은 $W_n = I + P_n$ 형태로 표현되며, $\sum_{n=2}^{\infty} \|P_n\| < \infty$를 만족합니다.
    - 편향 벡터 $b_i$ ($i \in \mathbb{N}$)는 $\sum_{n=1}^{\infty} \|b_n\| < \infty$를 만족합니다.

## 🧠 Insights & Discussion

- **깊은 층에서의 점진적 변화**: 네트워크가 수렴하기 위한 필요조건($W_n \to I$ 및 $b_n \to 0$)은 깊은 층($n$이 커질수록)에서 선형 함수 $W_n x + b_n$이 거의 항등 함수($x$)에 가까워져야 함을 의미합니다. 이는 네트워크의 깊은 가중치 층이 궁극적인 입출력 관계에 점진적인 변화를 적용해야 한다는 것을 시사합니다.
- **ResNets 설계 원리에 대한 수학적 정당화**: 이러한 통찰은 이미지 분류에서 큰 성공을 거둔 ResNets(Residual Networks)의 설계 전략에 대한 강력한 수학적 해석을 제공합니다. ResNets는 $x_{l+1} = \sigma(x_l + F(x_l, W_l, b_l))$ 형태의 잔여 블록으로 구성되며, 여기서 잔여 함수 $F$는 0에 가까워질 것으로 기대됩니다. 본 연구의 결과는 이러한 기대를 수학적으로 뒷받침하며, $W_n x + b_n$이 항등 함수에 가까워져야 한다는 조건이 ResNets의 핵심 아이디어와 일치함을 보여줍니다.
- **한계점**: 본 연구는 무작위 초기화나 훈련 과정의 수렴보다는 고정된 매개변수 시퀀스에 대한 네트워크 함수 자체의 수렴에 초점을 맞춥니다. 실제 훈련된 네트워크의 수렴 행동을 완전히 설명하기 위해서는 추가적인 분석이 필요할 수 있습니다.

## 📌 TL;DR

본 논문은 심층 ReLU 네트워크의 깊이가 무한대로 증가할 때의 수렴 문제를 해결합니다. ReLU 비선형성을 '활성화 도메인'과 '활성화 행렬'을 통해 대수적으로 표현함으로써, 네트워크의 출력을 무한 행렬 곱과 무한 급수의 형태로 공식화합니다. 분석 결과, 네트워크가 수렴하려면 가중치 행렬 $W_n$이 단위 행렬로, 편향 벡터 $b_n$이 0으로 수렴해야 함을 밝히고, $\sum \|P_n\| < \infty$ ($W_n = I+P_n$) 및 $\sum \|b_n\| < \infty$와 같은 구체적인 충분조건을 제시합니다. 이는 깊은 층에서 네트워크가 거의 항등 함수처럼 작동해야 함을 시사하며, ResNets 설계의 '잔여 함수가 0에 가까워야 한다'는 직관에 대한 강력한 수학적 근거를 제공합니다.
