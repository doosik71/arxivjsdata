{
  "title": "ErfReLU: Adaptive Activation Function for Deep Neural Network",
  "authors": "Ashish Rajanand, Pradeep Singh",
  "year": 2023,
  "url": "http://arxiv.org/abs/2306.01822v1",
  "abstract": "Recent research has found that the activation function (AF) selected for\nadding non-linearity into the output can have a big impact on how effectively\ndeep learning networks perform. Developing activation functions that can adapt\nsimultaneously with learning is a need of time. Researchers recently started\ndeveloping activation functions that can be trained throughout the learning\nprocess, known as trainable, or adaptive activation functions (AAF). Research\non AAF that enhance the outcomes is still in its early stages. In this paper, a\nnovel activation function 'ErfReLU' has been developed based on the erf\nfunction and ReLU. This function exploits the ReLU and the error function (erf)\nto its advantage. State of art activation functions like Sigmoid, ReLU, Tanh,\nand their properties have been briefly explained. Adaptive activation functions\nlike Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and\nSerf have also been described. Lastly, performance analysis of 9 trainable\nactivation functions along with the proposed one namely Tanhsoft1, Tanhsoft2,\nTanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf has been shown by\napplying these activation functions in MobileNet, VGG16, and ResNet models on\nCIFAR-10, MNIST, and FMNIST benchmark datasets.",
  "citation": 25
}