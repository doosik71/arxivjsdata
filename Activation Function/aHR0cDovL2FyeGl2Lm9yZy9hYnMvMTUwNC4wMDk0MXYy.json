{
  "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units",
  "authors": "Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton",
  "year": 2015,
  "url": "http://arxiv.org/abs/1504.00941v2",
  "abstract": "Learning long term dependencies in recurrent networks is difficult due to\nvanishing and exploding gradients. To overcome this difficulty, researchers\nhave developed sophisticated optimization techniques and network architectures.\nIn this paper, we propose a simpler solution that use recurrent neural networks\ncomposed of rectified linear units. Key to our solution is the use of the\nidentity matrix or its scaled version to initialize the recurrent weight\nmatrix. We find that our solution is comparable to LSTM on our four benchmarks:\ntwo toy problems involving long-range temporal structures, a large language\nmodeling problem and a benchmark speech recognition problem.",
  "citation": 977
}