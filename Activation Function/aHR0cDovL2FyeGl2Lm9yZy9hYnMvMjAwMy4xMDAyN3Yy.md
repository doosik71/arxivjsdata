# Dynamic ReLU

Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu

## 🧩 Problem to Solve

심층 신경망에서 널리 사용되는 ReLU(Rectified Linear Unit) 및 그 일반화된 버전(LeakyReLU, PReLU 등)은 **정적(static)**이어서 모든 입력 샘플에 대해 동일하게 작동합니다. 이는 활성화 함수가 입력에 고정되어야 하는지 아니면 적응형이어야 하는지에 대한 의문을 제기합니다. 특히 경량 신경망의 경우, 고정된 활성화 함수는 모델의 표현 능력을 제한하여 충분한 성능을 발휘하지 못하게 합니다. 본 연구는 이러한 정적인 활성화 함수의 한계를 극복하고 입력에 따라 동적으로 활성화 함수를 조절함으로써 모델의 표현 능력을 효율적으로 향상시키는 방법을 모색합니다.

## ✨ Key Contributions

- **동적 ReLU(DY-ReLU) 제안:** 입력의 전역적인 맥락을 인코딩하여 활성화 함수의 매개변수를 생성하는 하이퍼 함수(hyper function)를 통해 동적으로 조정되는 조각 선형 활성화 함수 DY-ReLU를 제안합니다.
- **뛰어난 표현 능력과 효율성:** DY-ReLU는 기존의 정적 활성화 함수에 비해 계산 비용이 미미하게 증가(예: FLOPs 5% 증가)하지만, 특히 경량 신경망에서 표현 능력을 크게 향상시킵니다.
- **성능 향상 입증:**
  - ImageNet 분류에서 MobileNetV2에 DY-ReLU를 적용하여 Top-1 정확도를 72.0%에서 76.2%로 (4.2%p) 향상시켰습니다.
  - COCO 키포인트 검출에서 MobileNetV2에 DY-ReLU를 적용하여 AP(Average Precision)를 3.5p 향상시켰습니다.
- **다양한 공유 전략:** 활성화 함수를 공간 및 채널에 걸쳐 다르게 공유하는 세 가지 DY-ReLU 변형(A, B, C)을 탐구하고, 각 변형이 다양한 작업에 어떻게 적합한지 분석했습니다.
- **기존 연구와의 관계:** DY-ReLU가 ReLU, LeakyReLU, PReLU, SE(Squeeze-and-Excitation)의 특수 사례이며, Maxout보다 훨씬 효율적이고 성능이 우수한 동적 버전임을 입증했습니다.

## 📎 Related Works

- **활성화 함수:**
  - **정적 활성화 함수:** ReLU [27,17], LeakyReLU [25], PReLU [10], RReLU [40], Maxout [7], Softplus [6], ELU [4], SELU [19], Mish [26], PELU [33], Swish [28] 등이 있습니다. 이들은 모두 입력 독립적으로 작동합니다.
- **동적 신경망:**
  - D²NN [23], SkipNet [34], BlockDrop [37] (강화 학습을 통한 모델 일부 건너뛰기), MSDNet [15] (예측 신뢰도 기반 조기 종료), Slimmable Net [42] (다양한 너비로 실행 가능한 단일 네트워크), Once-for-all [1] (다중 서브 네트워크 지원), Hypernetworks [8] (다른 네트워크를 사용하여 매개변수 생성), SENet [14] (전역 컨텍스트를 압축하여 채널 재가중), Dynamic Convolution [41,3] (입력에 따라 컨볼루션 커널 조정). DY-ReLU는 커널 가중치 대신 활성화 함수에 초점을 맞춘다는 점에서 차별화됩니다.
- **효율적인 CNNs:**
  - MobileNetV1/V2/V3 [13,30,12], ShuffleNet [44,24], ShiftNet [36] 등 경량 및 효율적인 CNN 아키텍처 연구. DY-ReLU는 이러한 네트워크에 쉽게 통합되어 표현 능력을 향상시킬 수 있습니다.

## 🛠️ Methodology

DY-ReLU는 매개변수가 입력에 따라 달라지는 동적 조각 선형 함수입니다.

1. **동적 활성화 정의:**
   입력 벡터(또는 텐서) $x$에 대해, 동적 활성화는 $f_{\theta(x)}(x)$로 정의되며, 학습 가능한 매개변수 $\theta(x)$는 입력 $x$에 적응합니다.

   - **하이퍼 함수 ($\theta(x)$):** 활성화 함수의 매개변수를 계산하며, 모든 입력 요소의 전역 컨텍스트를 인코딩합니다.
   - **활성화 함수 ($f_{\theta(x)}(x)$):** 하이퍼 함수에서 계산된 매개변수 $\theta(x)$를 사용하여 활성화를 생성합니다.

2. **DY-ReLU의 정의 및 구현:**
   전통적인 ReLU는 $y = \max\{x, 0\}$입니다. DY-ReLU는 이를 일반화하여 $K$개의 선형 함수의 최댓값을 취합니다.
   채널 $c$의 입력 $x_c$에 대한 활성화는 다음과 같이 계산됩니다:
   $$y_c = f_{\theta(x)}(x_c) = \max_{1 \le k \le K} \{a_k^c(x)x_c + b_k^c(x)\}$$
   여기서 계수 $(a_k^c, b_k^c)$는 하이퍼 함수 $\theta(x)$의 출력입니다:
   $$[a_1^1,...,a_1^C,...,a_K^1,...,a_K^C,b_1^1,...,b_1^C,...,b_K^1,...,b_K^C]^T = \theta(x)$$

   - **하이퍼 함수 $\theta(x)$ 구현:** SE(Squeeze-and-Excitation) [14]와 유사한 경량 네트워크를 사용합니다.
     - 입력 텐서 $C \times H \times W$에 대해, 먼저 전역 평균 풀링(global average pooling)으로 공간 정보를 압축합니다.
     - 이어서 두 개의 완전 연결(FC) 계층(사이에 ReLU 포함)과 정규화 계층이 따릅니다.
     - 출력은 $2KC$개의 요소로 구성되며, 이는 $a_{1:K}^{1:C}$ 및 $b_{1:K}^{1:C}$의 잔차 $\Delta a_k^c, \Delta b_k^c$에 해당합니다.
     - 잔차는 $2\sigma(x)-1$ (여기서 $\sigma(x)$는 시그모이드 함수)을 사용하여 -1과 1 사이로 정규화됩니다.
     - 최종 출력은 초기화 값과 잔차의 합으로 계산됩니다:
       $$a_k^c(x) = \alpha_k + \lambda_a \Delta a_k^c(x)$$
       $$b_k^c(x) = \beta_k + \lambda_b \Delta b_k^c(x)$$
       여기서 $\alpha_k, \beta_k$는 초기화 값이며, $\lambda_a, \lambda_b$는 잔차 범위를 제어하는 스칼라 하이퍼 매개변수입니다.
       - $K=2$의 경우 기본값: $\alpha_1=1, \alpha_2=\beta_1=\beta_2=0$ (정적 ReLU에 해당). $\lambda_a=1.0, \lambda_b=0.5$.

3. **DY-ReLU 변형:** 활성화 함수 공유 방식에 따라 세 가지 변형이 있습니다.
   - **DY-ReLU-A (Spatial and Channel-shared):** 모든 공간 위치와 채널에서 동일한 조각 선형 활성화 함수를 공유합니다. 하이퍼 함수 출력은 $2K$개.
   - **DY-ReLU-B (Spatial-shared and Channel-wise):** 공간적으로 공유되지만 채널별로 다릅니다. 하이퍼 함수 출력은 $2KC$개. (기본 DY-ReLU)
   - **DY-ReLU-C (Spatial and Channel-wise):** 각 입력 요소 $x_{c,h,w}$마다 고유한 활성화 함수를 가집니다. 공간과 채널을 분리하기 위해 공간 어텐션 $\pi_{h,w}$ 브랜치를 추가하며, 이는 1x1 컨볼루션과 스케일된 소프트맥스 함수로 계산됩니다.
     $$\pi_{h,w} = \min\left\{\frac{\gamma\exp(z_{h,w}/\tau)}{\sum_{h,w}\exp(z_{h,w}/\tau)}, 1\right\}$$
     여기서 $z_{h,w}$는 1x1 컨볼루션 출력, $\tau$는 온도, $\gamma$는 스케일러.

## 📊 Results

- **ImageNet 분류 (주로 DY-ReLU-B, K=2 사용):**
  - **MobileNetV2:**
    - $\times1.0$: Top-1 72.0% $\to$ 76.2% (+4.2%p) (FLOPs +5%)
    - $\times0.35$: Top-1 60.3% $\to$ 66.4% (+6.1%p) (FLOPs +9.8%)
  - **MobileNetV3:**
    - Large: Top-1 75.2% $\to$ 75.9% (+0.7%p)
    - Small: Top-1 67.4% $\to$ 69.7% (+2.3%p)
  - **ResNet:**
    - ResNet-50: Top-1 76.2% $\to$ 77.2% (+1.0%p) (FLOPs +0.5%)
    - ResNet-10: Top-1 63.0% $\to$ 66.3% (+3.3%p)
  - **기존 활성화 함수 대비 우수성:** RReLU, LeakyReLU, PReLU, SE, Maxout (Maxout은 훨씬 더 많은 계산 비용에도 불구하고 DY-ReLU가 더 높은 성능을 보임)보다 우수한 성능을 달성합니다.
- **COCO 키포인트 검출 (DY-ReLU-C, K=2 사용):**
  - **MobileNetV2 $\times1.0$ (백본):** AP 64.6 $\to$ 68.1 (+3.5p) (MAdds +3.3%)
  - **MobileNetV2 $\times0.5$ (백본):** AP 59.3 $\to$ 63.3 (+4.0p)
  - **MobileNetV3 Small (백본):** AP 57.1 $\to$ 60.7 (+3.6p)
- **DY-ReLU 변형 분석:**
  - ImageNet 분류에서는 채널별 변형(DY-ReLU-B 및 C)이 채널 공유(DY-ReLU-A)보다 효과적이며, 공간별 변형(DY-ReLU-C)은 추가 개선을 가져오지 않습니다.
  - COCO 키포인트 검출에서는 백본에서 채널별 변형(B, C)이 효과적이며, 헤드 네트워크에서는 공간별 변형(C)이 중요합니다. DY-ReLU-A나 B를 헤드에 사용하면 성능이 오히려 저하될 수 있습니다.

## 🧠 Insights & Discussion

- **활성화 함수의 동적 특성:** 학습된 DY-ReLU는 주어진 입력 $x$에 대해 활성화 값 $y$가 다양한 범위에서 변동하는 것을 보여, 정적 ReLU와 달리 실제로 입력에 따라 동적으로 작동함을 확인했습니다. 특히 음수 입력에 대해 양수 활성화가 나타나거나, 기울기가 1을 초과하는 경우가 51%, 절편이 0.05를 초과하는 경우가 37% 발생하여, DY-ReLU가 정적 함수로는 모델링할 수 없는 복잡한 패턴을 학습하고 있음을 시사합니다.
- **경량 모델에 대한 효과:** DY-ReLU는 모델 크기가 작아 과소 적합(underfitted)되기 쉬운 경량 모델(예: MobileNetV2 $\times0.35$, ResNet-10)에서 특히 큰 성능 향상을 보였습니다. 이는 DY-ReLU가 모델의 표현 능력을 효율적으로 증대시키는 데 기여하기 때문입니다.
- **계층별 동적 특성 변화:** DY-ReLU에서 두 세그먼트 간의 기울기 차이(|a_1^c - a_2^c|)는 낮은 계층에서 높은 계층으로 갈수록 감소합니다. 이는 활성화 함수가 낮은 계층에서는 더 많은 "굽힘"을 가지고 높은 계층에서는 더 부드러워지는 경향이 있음을 나타내며, 네트워크의 다른 수준에서 동적 기능의 역할이 다를 수 있음을 시사합니다.
- **변형의 작업 특이성:** 다양한 DY-ReLU 변형(A, B, C)은 특정 작업 및 네트워크 부분에 따라 다른 효과를 보였습니다. 특히 키포인트 검출과 같은 공간적으로 민감한 작업의 헤드 네트워크에서는 공간별 DY-ReLU-C가 필수적입니다. 이는 하이퍼 함수가 공간적으로 정보를 학습해야 할 필요성을 강조합니다.
- **하이퍼파라미터의 안정성:** 기울기 초기화 값($\alpha_k$)과 절편 초기화 값($\beta_k$)은 특정 범위 내에서 성능에 크게 민감하지 않았으며, 기울기 범위($\lambda_a$)도 1~2 사이에서 좋은 성능을 보였습니다. 이는 DY-ReLU의 구현이 비교적 견고함을 나타냅니다.

## 📌 TL;DR

**문제:** 기존 ReLU와 그 변형들은 입력에 관계없이 고정된 활성화 함수를 사용하여, 특히 경량 신경망에서 모델의 표현 능력을 제한합니다.

**제안 방법:** 본 논문은 **Dynamic ReLU (DY-ReLU)**를 제안합니다. DY-ReLU는 입력의 전역 컨텍스트를 활용하여 활성화 함수의 매개변수를 동적으로 생성하는 경량 "하이퍼 함수"를 통해 조각 선형 활성화 함수를 적응시킵니다.

**주요 결과:** DY-ReLU는 MobileNetV2의 ImageNet 분류에서 Top-1 정확도를 4.2%p, COCO 키포인트 검출에서 AP를 3.5%p 향상시키는 등, 경미한 계산 비용 증가(약 5% FLOPs)만으로도 이미지 분류 및 자세 추정 작업에서 상당한 성능 향상을 달성했습니다. 또한, DY-ReLU는 기존의 정적 및 일부 동적 활성화 함수들보다 우수하며, 특히 Maxout보다 훨씬 효율적이고 강력한 성능을 보여주었습니다. 이는 DY-ReLU가 효율적인 CNN 아키텍처를 위한 유용한 구성 요소임을 입증합니다.
