# Evolutionary Optimization of Deep Learning Activation Functions

Garrett Bingham, William Macke, Risto Miikkulainen

## 🧩 Problem to Solve

딥러닝 신경망에서 활성화 함수(activation function)의 선택은 네트워크의 성능에 지대한 영향을 미칩니다. 현재 Rectified Linear Unit (ReLU)이 가장 널리 사용되고 있지만, 수동으로 새로운 활성화 함수를 설계하려는 시도는 ReLU만큼 광범위하게 채택되지 못했습니다. 이는 활성화 함수 설계가 신경망 최적화의 미개척 영역임을 시사하며, 메타 학습(metalearning)을 통해 활성화 함수를 자동으로 최적화하여 성능을 향상시킬 수 있는 기회를 탐색하는 것이 본 연구의 주요 문제입니다.

## ✨ Key Contributions

- 진화 알고리즘이 ReLU를 능가하는 성능을 가진 새로운 활성화 함수를 발견할 수 있음을 입증했습니다.
- 후보 활성화 함수를 위한 트리 기반 탐색 공간을 정의하고, 변이(mutation), 교차(crossover), 완전 탐색(exhaustive search)을 활용하여 효과적으로 탐색했습니다.
- CIFAR-10 및 CIFAR-100 이미지 데이터셋에 대해 Wide Residual Networks를 훈련하는 실험을 통해 제안된 접근 방식의 효과를 확인했습니다.
- 진화된 활성화 함수가 네트워크의 정확도를 통계적으로 유의미하게 증가시켰음을 보였습니다.
- 최적의 성능은 특정 태스크에 맞춰 활성화 함수를 진화시킬 때 달성되지만, 이렇게 발견된 새로운 활성화 함수들이 다른 태스크에서도 높은 성능을 보여 일반화될 수 있음을 입증했습니다.

## 📎 Related Works

- **신경망 구조 최적화 (Neuroevolution)**: Real et al. [18]은 아키텍처 쌍을 샘플링하고 변이시켜 더 나은 아키텍처를 훈련하는 방식을 제안했으며, Xie and Yuille [21]는 교차를 포함한 유전 알고리즘을 사용하여 아키텍처를 탐색했습니다. Real et al. [17]은 에이징 진화(aging evolution)를 통해 ImageNet에서 인간이 설계한 아키텍처보다 높은 정확도를 달성했습니다.
- **다른 신경망 요소 최적화**:
  - **손실 함수(Loss Functions)**: Gonzalez and Miikkulainen [8]은 유전 알고리즘을 사용하여 새로운 손실 함수를 구축하고 최적화했습니다.
  - **활성화 함수 선택/설계**: Marchisio et al. [12]는 각 레이어에 대한 활성화 함수를 자동으로 선택하는 방법을 제시했으며, Hagg et al. [9]은 NEAT 알고리즘을 확장하여 뉴런별 활성화 함수와 네트워크 토폴로지를 동시에 진화시켰습니다.
  - **강화 학습(RL) 기반 활성화 함수 설계**: Ramachandran et al. [16]은 RL을 사용하여 Swish($x \cdot \sigma(x)$)와 같은 새로운 활성화 함수를 설계했습니다. 본 연구는 이들의 연구를 확장하여 미리 정의된 목록에서 선택하는 것이 아닌, 방대한 후보 함수 공간을 탐색하며 특정 태스크에 특화된 함수를 진화시키는 데 중점을 둡니다.

## 🛠️ Methodology

본 연구는 진화 알고리즘을 사용하여 활성화 함수를 자동으로 발견합니다.

1. **탐색 공간 정의 (Search Space)**

   - 각 활성화 함수는 단항(unary) 및 이항(binary) 연산자로 구성된 트리로 표현됩니다.
   - **단항 연산자**: $0, 1, x, -x, |x|, x^2, x^3, \sqrt{x}, e^x, e^{-x^2}, \log(1+e^x), \log(|x+\epsilon|), \sin(x), \sinh(x), \text{arcsinh}(x), \cos(x), \cosh(x), \tanh(x), \text{arctanh}(x), \max\{x,0\}, \min\{x,0\}, \sigma(x), \text{erf}(x), \text{sinc}(x)$
   - **이항 연산자**: $x_1+x_2, x_1-x_2, x_1 \cdot x_2, x_1/(x_2+\epsilon), \max\{x_1,x_2\}, \min\{x_1,x_2\}$
   - "코어 유닛(core unit)"은 $binary(unary_1(x), unary_2(x))$ 형태의 활성화 함수입니다.
   - 탐색 공간 $S_d$는 균형 잡힌 코어 유닛 트리의 집합으로 정의되며, $d$는 트리의 깊이를 나타냅니다. (예: $S_1$은 하나의 코어 유닛, $S_2$는 $core\_unit_1(core\_unit_2(x), core\_unit_3(x))$ 형태)

2. **변이 (Mutation)**

   - 활성화 함수 트리에서 노드를 균일하게 무작위로 선택합니다.
   - 선택된 노드의 연산자를 탐색 공간 내의 다른 무작위 연산자(동일한 타입: 단항 $\leftrightarrow$ 단항, 이항 $\leftrightarrow$ 이항)로 교체합니다.

3. **교차 (Crossover)**

   - 두 개의 부모 활성화 함수를 선택합니다.
   - 두 부모 함수에서 동일한 깊이를 가진 무작위 서브트리를 교환하여 새로운 자식 활성화 함수를 생성합니다. 이는 자식 함수가 부모와 동일한 탐색 공간($S_d$)에 속하도록 보장합니다.

4. **진화 알고리즘 (Evolution Algorithm)**

   - $N$개의 활성화 함수로 구성된 초기 모집단으로 시작합니다.
   - 각 함수를 사용하여 신경망을 훈련하고, 유효성 검사 데이터셋에서 얻은 평가 메트릭 $L_i$ (정확도 또는 음수 손실)를 기반으로 적합도 $p_i$를 할당합니다.
     $$ p*i = \frac{e^{L_i}}{\sum*{j=1..N} e^{L_j}} $$
   - 번식을 위해 모집단에서 $2(N-m)$개의 함수를 적합도에 비례하는 확률로 대체하여 선택합니다.
   - 선택된 함수에 교차 및 변이를 적용하여 $N-m$개의 새로운 함수를 생성합니다.
   - 탐색을 증가시키기 위해 $m$개의 무작위로 생성된 함수를 추가하여 다음 세대의 모집단 크기를 다시 $N$으로 만듭니다.
   - 이 과정은 여러 세대 동안 반복되며, 탐색 과정에서 최고의 성능을 보인 활성화 함수가 최종 결과로 반환됩니다.

5. **실험 설정**
   - **아키텍처**: 깊이 28, 폭 10인 Wide Residual Network (WRN-28-10)를 사용하며, 모든 ReLU 활성화 함수를 후보 함수로 대체합니다. 일부 비교 실험에서는 WRN-40-4도 사용합니다.
   - **데이터셋**: CIFAR-10 및 CIFAR-100 이미지 데이터셋을 사용하며, 과적합 방지를 위해 훈련 세트에서 밸리데이션 세트를 분리합니다.
   - **훈련 스케줄**: 활성화 함수 탐색 시 각 함수를 50 epoch 동안 훈련하며, 최종 성능 평가 시에는 200 epoch 동안 훈련합니다.
   - **탐색 전략**:
     - **완전 탐색 (Exhaustive Search)**: 작은 $S_1$ 공간 내의 모든 3,456개 함수를 평가합니다.
     - **무작위 탐색 (Random Search)**: $S_2$ 공간에서 500개의 무작위 함수를 10개의 "세대"로 나누어 평가합니다.
     - **진화 (Evolution)**: $S_2$ 공간에서 50개 함수($N=50$)의 초기 모집단으로 시작하여 10세대에 걸쳐 실행됩니다. 각 세대는 이전 세대의 상위 5개 함수, 10개의 무작위 함수($m=10$), 그리고 교차 및 변이를 통해 생성된 35개의 함수로 구성됩니다.
       - **정확도 기반 적합도 (Accuracy-based Fitness)**: 탐색(exploration)에 유리하도록 설계되었습니다.
       - **손실 기반 적합도 (Loss-based Fitness)**: 성능이 낮은 함수에 더 큰 페널티를 부여하여 고성능 함수를 더 빠르게 찾도록 합니다.

## 📊 Results

- **성능 향상**:
  - **완전 탐색($S_1$)**: $S_1$에서 완전 탐색으로 발견된 상위 활성화 함수들(예: $(\arctan(x)) \cdot \min\{x,0\}$)은 ReLU와 Swish보다 일관되게 뛰어난 성능을 보였습니다.
  - **진화($S_2$, 손실 기반 적합도)**: $S_2$와 같이 방대한 탐색 공간에서 손실 기반 적합도를 사용한 진화는 ReLU 및 Swish보다 우수한 성능을 보이는 새로운 활성화 함수들을 성공적으로 발견했습니다. 예를 들어, $(e^{(\min\{\text{erf}(x),0\})-(\max\{x,0\})}) \cdot (\min\{(\text{arctan}((x)^3)) \cdot (\max\{|x|,0\}),0\})$와 같은 함수가 발견되었습니다.
  - **통계적 유의미성**: WRN-28-10을 CIFAR-100에서 200 epoch 동안 훈련했을 때, $S_1$에서 발견된 최적 함수와 $S_2$에서 발견된 최적 함수 모두 ReLU 및 Swish에 비해 통계적으로 유의미한 정확도 향상(p-value $4.64 \times 10^{-16}$ 및 $2.91 \times 10^{-5}$)을 보였습니다.
  - **효과가 적었던 전략**: 무작위 탐색 및 정확도 기반 적합도를 사용한 진화는 기준 활성화 함수를 능가하지 못하거나, 특정 함수에서 훈련이 완료되지 않는 등의 문제를 보였습니다.
- **활성화 함수 특화 및 일반화**:
  - CIFAR-10용 WRN-28-10에서 진화된 활성화 함수(예: $\tanh(x) \cdot \min\{x,0\}$)는 CIFAR-100용 WRN-40-4 태스크로 성공적으로 전이되어 ReLU 및 Swish보다 나은 성능을 보였습니다.
  - 그러나 CIFAR-100용 WRN-40-4 태스크를 위해 특별히 진화된 함수(예: $\sigma(x) \cdot \text{erf}(x)$)는 전이된 함수보다 더 높은 성능을 달성했습니다. 이는 활성화 함수 메타 학습의 가장 큰 강점이 특정 아키텍처와 데이터셋에 특화된 함수를 발견하는 데 있음을 보여줍니다.

## 🧠 Insights & Discussion

- 발견된 상위 활성화 함수 중 상당수는 부드럽고 단조로운 특성을 공유했지만, 복잡하고 직관적이지 않은 형태의 함수도 효과적일 수 있음을 확인했습니다. 이는 "간단한 활성화 함수가 항상 우수하다"는 기존의 주장과 상충될 수 있으며, 복잡한 함수가 특정 태스크에 특화될 때 강력한 성능을 발휘할 수 있음을 시사합니다.
- 진화된 활성화 함수는 일반적으로 자신이 진화된 아키텍처와 데이터셋에서 가장 잘 작동하지만, 동일 도메인 내의 다른 아키텍처나 더 어려운 데이터셋으로도 합리적인 수준으로 일반화될 수 있습니다.
- 활성화 함수 메타 학습의 가장 큰 이점은 각 특정 아키텍처 및 데이터셋에 특화된 함수를 발견하는 능력에 있으며, 이를 통해 가장 큰 성능 향상을 기대할 수 있습니다. 미래에는 여러 아키텍처와 데이터셋에 걸쳐 함수를 평가하여 더 일반적인 고성능 함수를 발견하는 연구가 가능할 것입니다.

## 📌 TL;DR

이 논문은 딥러닝에서 활성화 함수의 중요성에 주목하여 진화 알고리즘을 이용한 자동 최적화 방법을 제안합니다. 트리 기반 탐색 공간에서 변이와 교차를 활용한 진화적 탐색을 통해, ReLU와 같은 기존 활성화 함수를 능가하는 새로운 함수들을 발견했습니다. 특히 손실 기반 적합도 사용이 효과적이었으며, 발견된 함수들은 Wide Residual Networks를 CIFAR-10/100 데이터셋에 훈련했을 때 통계적으로 유의미한 정확도 향상을 보였습니다. 연구 결과는 진화된 활성화 함수가 특정 태스크에 최적화될 때 최고의 성능을 발휘하지만, 다른 태스크로도 성공적으로 일반화될 수 있음을 보여주며, 활성화 함수 메타 학습의 강력한 잠재력을 입증합니다.
