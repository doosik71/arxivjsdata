# Weighted Sigmoid Gate Unit for an Activation Function of Deep Neural Network

Masayuki Tanaka,Member, IEEE

## 🧩 Problem to Solve

심층 신경망에서 활성화 함수는 매우 중요한 역할을 합니다. 현재 널리 사용되는 Rectified Linear Unit (ReLU)은 단순하고 효과적이지만, 성능을 개선하기 위한 많은 활성화 함수들이 제안되었습니다. 그러나 이러한 활성화 함수들은 주로 객체 인식 작업에서만 평가되었고, 더 다양한 작업에서 ReLU의 성능을 능가하는 일반적인 활성화 함수에 대한 필요성이 있었습니다. 특히, 기존의 시그모이드 게이팅 방식은 스칼라 입력에 초점을 맞추는 경향이 있었고, 벡터 입력에 대한 보다 유연하고 강력한 활성화 함수가 요구되었습니다.

## ✨ Key Contributions

- **가중 시그모이드 게이트 유닛(Weighted Sigmoid Gate Unit, WiG) 제안**: 입력과 가중 시그모이드 게이트의 곱으로 구성된 새로운 활성화 함수를 제안했습니다.
- **기존 활성화 함수의 일반화**: 제안된 WiG가 ReLU, Swish, Sigmoid-weighted Linear Unit (SiL) 등 여러 기존 활성화 함수들을 특별한 경우로 포함함을 보여주었습니다.
- **다중 작업 성능 평가**: 객체 인식(cifar10/cifar100) 작업뿐만 아니라 이미지 복원(노이즈 제거) 작업에서도 WiG의 성능을 평가했습니다.
- **최첨단 성능 달성**: 객체 인식 및 이미지 복원 두 가지 작업에서 기존의 활성화 함수(ReLU 포함)들을 능가하는 최상의 성능을 달성했습니다.

## 📎 Related Works

- **ReLU (Rectified Linear Unit)** [1]: 현재 가장 널리 사용되는 활성화 함수 $f(x) = \max(x, 0)$.
- **SiL (Sigmoid-weighted Linear Unit)** [3]: $f(x) = x \cdot \sigma(x)$로 정의되며, 강화 학습에서 ReLU의 근사치로 제안되었습니다.
- **Swish unit** [2]: $f(x) = x \cdot \sigma(\beta x)$로 정의되며, $\beta$는 훈련 가능한 매개변수입니다. SiL은 Swish의 특수한 경우 ($\beta=1$)입니다.
- **다른 활성화 함수들**: selu [5], ELU [8], softplus [4], Leaky ReLU [6], PReLU [7] 등이 성능 비교를 위해 사용되었습니다.
- **시그모이드 게이팅**: Long Short-Term Memory (LSTM) [11]과 같은 강력한 모델에서 사용되는 접근 방식입니다.

## 🛠️ Methodology

1. **WiG 정의**: 제안된 WiG는 N차원 입력 벡터 $x \in \mathbb{R}^{N}$에 대해 다음과 같이 정의됩니다.
   $$f(x) = x \odot \sigma(W_{g}x + b_{g})$$
   여기서 $W_{g} \in \mathbb{R}^{N \times N}$는 $N \times N$ 크기의 가중치 행렬, $b_{g} \in \mathbb{R}^{N}$는 $N$차원 바이어스 벡터, $\sigma(\cdot)$는 요소별 시그모이드 함수, $\odot$는 요소별 곱셈을 나타냅니다.

2. **도함수**: WiG의 도함수는 다음과 같습니다.
   $$\frac{\partial f}{\partial x} = \sigma(W_{g}x + b_{g}) + W_{g} \cdot \{x \odot (1 - \sigma(W_{g}x + b_{g})) \odot \sigma(W_{g}x + b_{g})\}$$

3. **컨볼루션 신경망을 위한 WiG**: 특징 맵 $X$에 대해 다음과 같이 표현됩니다.
   $$F(X) = X \odot \sigma(w_{g} * X + B_{g})$$
   여기서 $w_{g}$는 컨볼루션 커널, $B_{g}$는 바이어스 맵, $*$는 컨볼루션 연산자입니다.

4. **단순 스칼라 입력 예시**: $f(x) = x \times \sigma(wx + b)$의 경우, $w$가 양의 큰 값일 때 ReLU처럼 작동하고, 음의 큰 값일 때 negative-ReLU처럼 작동합니다. $w=0$일 때 선형 활성화와 동일하며, $b=0$일 때 Swish가 됩니다.

5. **초기화**:

   - 가중치 매개변수 $W_{g}$는 스케일링된 항등 행렬 $sI$로, 바이어스 매개변수 $b_{g}$는 0으로 초기화합니다.
   - $s$ 값이 클 경우, WiG는 근사된 ReLU로 초기화되어 ReLU 네트워크로부터의 전이 학습에 유용합니다.
   - $s=1$일 경우, WiG는 SiL로 초기화되어 스크래치 학습에 사용됩니다.

6. **희소성 제약**: 손실 함수에 게이트 출력의 $L_{1}$ 노름($L = \lambda_{g} |\sigma(W_{g}x + b_{g})|_{1}$)을 추가하여 게이트와 WiG 출력의 희소성을 강제합니다.

## 📊 Results

- **객체 인식 작업 (cifar10 및 cifar100)**:

  - VGG와 유사한 네트워크 구조를 사용하여 평가했습니다.
  - WiG는 cifar10에서 0.949, cifar100에서 0.742의 검증 정확도를 달성하여 모든 비교 활성화 함수 중 최고 성능을 기록했습니다.
  - 특히 cifar100 데이터셋에서 WiG는 ReLU (0.653), Swish (0.689) 대비 큰 폭의 성능 향상을 보였습니다.
  - 학습 교차 엔트로피를 통해 WiG가 더 빠르게 학습하고 더 낮은 최종 엔트로피에 도달함을 확인했습니다.

- **이미지 복원 작업 (이미지 노이즈 제거)**:
  - 확장된 컨볼루션 및 스킵 연결을 사용하는 노이즈 제거 네트워크를 사용하여 평가했습니다.
  - WiG는 다양한 노이즈 레벨 (5, 10, 15, 20, 25, 30)에서 PSNR과 SSIM 모두에서 지속적으로 가장 높은 성능을 보였습니다.
  - 다른 활성화 함수들은 대부분 노이즈 제거 작업에서 단순 ReLU보다 나은 성능을 보여주지 못했으나, WiG만이 PSNR과 SSIM 측면에서 ReLU보다 우수했습니다.

## 🧠 Insights & Discussion

- WiG는 학습 가능한 가중치 행렬 $W_{g}$와 바이어스 벡터 $b_{g}$를 통해 입력에 따라 동적으로 게이트를 조절함으로써, ReLU, Swish, SiL 등의 고정된 형태의 활성화 함수보다 훨씬 유연하고 강력한 비선형성을 제공합니다.
- 다양한 활성화 함수를 특수 사례로 포함하는 WiG의 일반화 능력은 모델 설계의 유연성을 높입니다. 특히, $W_{g}$와 $b_{g}$의 값을 조정하여 ReLU나 Swish와 같은 형태를 모방하거나 학습 과정에서 자동으로 최적화할 수 있습니다.
- 객체 인식과 이미지 복원이라는 매우 다른 두 가지 핵심 딥러닝 작업 모두에서 뛰어난 성능을 보인 것은 WiG의 강력한 일반화 능력과 안정성을 입증합니다. 이는 WiG가 다양한 신경망 아키텍처와 작업에 적용될 수 있음을 시사합니다.
- 적절한 초기화 전략 (큰 스케일 매개변수로 ReLU 근사 또는 $s=1$로 SiL 초기화)과 희소성 제약은 WiG의 효과적인 학습과 성능 향상에 기여합니다.
- 바이어스 매개변수 $b$가 도함수의 임계값을 제어할 수 있다는 점은 활성화 함수의 응답을 미세 조정할 수 있는 추가적인 제어권을 제공합니다.

## 📌 TL;DR

**문제**: 널리 쓰이는 ReLU를 포함한 기존 활성화 함수들은 한정된 유연성과 평가 환경(주로 객체 인식)을 가졌다.
**방법**: 저자는 입력 벡터 $x$와 가중 시그모이드 게이트 $\sigma(W_{g}x + b_{g})$의 요소별 곱으로 구성된 새로운 활성화 함수인 WiG($f(x) = x \odot \sigma(W_{g}x + b_{g})$)를 제안한다. WiG는 학습 가능한 가중치 행렬 $W_{g}$와 바이어스 $b_{g}$를 통해 ReLU, Swish, SiL 등을 특수한 경우로 포함하며, 적절한 초기화 및 희소성 제약을 적용한다.
**결과**: WiG는 객체 인식(cifar10/cifar100) 및 이미지 노이즈 제거 작업을 포함한 광범위한 실험에서 기존의 모든 활성화 함수들을 능가하는 최고 성능을 달성했으며, 더 빠른 학습 수렴과 높은 정확도/품질을 입증했다.
