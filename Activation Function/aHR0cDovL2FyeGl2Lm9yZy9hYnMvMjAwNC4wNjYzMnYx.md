# 활성화 함수 및 Xavier, He 정규 초기화와의 관계에 대한 조사

Leonid Datta

## 🧩 Problem to Solve

인공신경망 훈련 및 성능에서 활성화 함수와 가중치 초기화 방법은 매우 중요한 역할을 합니다. 이 논문은 다음 질문들을 해결하는 것을 목표로 합니다:

- 어떤 속성들이 효과적인 활성화 함수에 필수적이거나 중요한가?
- 널리 사용되는 활성화 함수(시그모이드, tanh, ReLU, LReLU, PReLU)는 어떤 특성을 가지며 어떤 문제점에 직면하는가?
- 주요 가중치 초기화 방법인 Xavier 및 He 정규 초기화는 이러한 활성화 함수들과 어떤 근본적인 관계를 가지는가?

궁극적으로, 이 조사는 활성화 함수와 가중치 초기화 사이의 상호작용을 이해하여 신경망의 훈련 효율성과 성능을 최적화하기 위한 지침을 제공하고자 합니다.

## ✨ Key Contributions

이 조사의 주요 기여는 다음과 같습니다:

- 활성화 함수가 가져야 할 필수적이고 중요한 속성과 그 이유를 논의합니다.
- 시그모이드, tanh, ReLU, LReLU, PReLU 등 널리 사용되는 다섯 가지 활성화 함수와 그들이 직면하는 문제점들을 다룹니다.
- Xavier 및 He 정규 가중치 초기화 방법과 위에서 언급된 활성화 함수들과의 관계를 탐구합니다.

## 📎 Related Works

이 논문은 활성화 함수 및 가중치 초기화 분야의 주요 선행 연구들을 참조합니다:

- **Cybenko Theorem [16]:** 시그모이드 활성화 함수를 가진 단일 은닉층 신경망이 모든 연속 함수를 근사할 수 있음을 보입니다.
- **Glorot et al. [10, 31, 34]:** Xavier 초기화 방법을 제안하고, 깊은 신경망에서 시그모이드 활성화 함수의 문제점을 지적하며, 깊은 희소 정류 신경망(deep sparse rectifier neural networks)을 탐구합니다.
- **Nair et al. [22]:** ReLU(Rectified Linear Unit) 함수를 제안했습니다.
- **Maas et al. [28]:** Leaky ReLU(LReLU) 함수를 도입하여 ReLU의 '죽은 뉴런' 문제를 해결하고자 했습니다.
- **He et al. [11]:** PReLU(Parametric ReLU) 함수와 He 정규 가중치 초기화 방법을 제안했습니다. 이들은 ImageNet 분류에서 인간 수준의 성능을 뛰어넘는 결과를 달성했습니다.
- **Krizhevsky et al. [24]:** ReLU가 시그모이드/tanh보다 훈련에 필요한 에포크 수 면에서 6배 더 빠르다는 것을 입증했습니다.
- **Ronneberger et al. [13], Huang et al. [14]:** ReLU와 He 정규 초기화의 조합이 U-Net과 같은 최신 신경망 아키텍처에서 우수한 성능을 보이는 사례를 제시합니다 (예: 생의학 영상 분할 챌린지).

## 🛠️ Methodology

이 조사는 활성화 함수와 가중치 초기화에 대한 광범위한 문헌 검토를 통해 다음과 같은 접근 방식을 사용합니다:

1. **활성화 함수의 속성 정의:** 활성화 함수가 가져야 할 필수/중요 속성(비선형성, 미분 가능성, 연속성, 유계성, 제로 중심성, 계산 비용)을 설명하고 각 속성의 중요성을 논의합니다.
2. **활성화 함수의 주요 문제점 분석:** 경사 소실(vanishing gradient) 문제와 죽은 뉴런(dead neuron) 문제의 원리와 영향을 설명합니다.
3. **주요 활성화 함수 검토:** 시그모이드, tanh, ReLU, LReLU, PReLU의 정의, 특성, 장점, 단점, 그리고 직면하는 문제점들을 개별적으로 상세히 분석합니다.
4. **가중치 초기화 방법 검토:** Xavier 초기화와 He 정규 초기화 방법의 원리, 공식($U[(-\frac{\sqrt{6}}{\sqrt{n_i+n_j}}, \frac{\sqrt{6}}{\sqrt{n_i+n_j}})]$ for Xavier, $N[(-\frac{\sqrt{6}}{\sqrt{n_i(1+a^2)}}, \frac{\sqrt{6}}{\sqrt{n_i(1+a^2)}})]$ for He, where $n_i$ is incoming connections, $n_j$ is outgoing, $a$ is PReLU parameter) 및 제안 배경을 설명합니다.
5. **속성 및 문제점 비교:** 활성화 함수들의 계산 비용, 유계성, 시그모이드 여부, 연속성, 제로 중심성, 미분 가능성, 죽은 뉴런 문제 직면 여부, 경사 소실 문제 직면 여부를 표 형식으로 정리하여 비교 분석합니다.
6. **활성화 함수와 초기화 방법 간의 관계 탐구:** 각 초기화 방법이 어떤 활성화 함수와 잘 작동하는지, 그리고 왜 그런지에 대한 이론적, 실험적 논의를 제시하며, 깊은 신경망에서의 성능 차이를 설명합니다.

## 📊 Results

- **활성화 함수 비교:**

  - **시그모이드/Tanh:** 연속적, 미분 가능, 유계(각각 (0,1), (-1,1) 범위), 시그모이드 형태. Tanh는 시그모이드의 제로 중심성 부족 문제를 해결하지만, 둘 다 높은 계산 비용과 경사 소실 문제에 취약합니다.
  - **ReLU:** 연속적(x=0 제외), 비유계, 비제로 중심. 매우 낮은 계산 비용을 가지며 양수 입력에 대해 경사 소실 문제가 없습니다. 그러나 음수 입력을 0으로 만들어 '죽은 뉴런' 문제를 유발할 수 있습니다.
  - **LReLU/PReLU:** LReLU는 음수 입력에 작은 기울기(예: $0.01x$)를 부여하여 죽은 뉴런 문제를 완화하고, PReLU는 이 기울기($a$)를 학습 가능한 매개변수로 만듭니다. 둘 다 계산 효율적이고 제로 중심적이지만, 음수 부분의 작은 기울기 때문에 경사 소실 위험이 부분적으로 존재합니다.

- **가중치 초기화 비교:**

  - **Xavier 초기화:** 주로 선형 영역을 가정하며 시그모이드 및 tanh와 같은 제로 중심 활성화 함수에 적합합니다. 깊은 신경망에서 ReLU와 함께 사용할 경우, 깊은 층에서 분산이 기하급수적으로 작아져 수렴에 실패할 수 있습니다.
  - **He 정규 초기화:** ReLU 계열 활성화 함수(ReLU, PReLU)를 위해 특별히 설계되었습니다. 이는 깊은 네트워크에서 분산을 보존하고 오류를 더 빠르게 줄여 Xavier보다 우수한 성능을 보입니다.

- **조합 성능:**
  - 깊은 신경망에서 **ReLU 계열 활성화 함수와 He 정규 초기화의 조합**이 가장 효과적인 것으로 나타났습니다. 이는 U-Net과 같은 최신 아키텍처에서 생의학 영상 분할 챌린지에서 가장 낮은 오류율을 달성하는 등 뛰어난 성능을 입증했습니다.
  - Xavier 초기화는 얕은 네트워크나 tanh 활성화 함수와 함께 사용할 때 여전히 유용할 수 있지만, 깊은 신경망에서는 성능이 떨어집니다.

## 🧠 Insights & Discussion

이 조사는 활성화 함수와 가중치 초기화 방법의 선택이 신경망, 특히 깊은 신경망의 훈련과 성능에 결정적인 영향을 미친다는 중요한 통찰을 제공합니다.

- **시그모이드에서 정류 비선형성으로의 전환:** 경사 소실 문제와 높은 계산 비용 때문에 시그모이드 계열 활성화 함수(시그모이드, tanh)의 사용은 감소하고 있습니다. 대신, ReLU, LReLU, PReLU와 같은 정류 비선형성(rectifier nonlinearities)이 계산 효율성과 경사 소실 문제에 대한 강점으로 인해 인기를 얻고 있습니다.
- **초기화와 활성화 함수의 조화:** Xavier 초기화는 시그모이드 계열 함수(특히 tanh)와 얕은 네트워크에 적합하지만, 깊은 네트워크에서는 분산 유지에 실패하여 ReLU와 함께 사용 시 수렴하지 못할 수 있습니다. 반면, He 정규 초기화는 ReLU 계열 함수와 깊은 네트워크를 위해 특별히 설계되었으며, 이 조합은 더 빠르고 안정적인 훈련을 가능하게 합니다.
- **문제점 극복:** LReLU 및 PReLU는 ReLU의 '죽은 뉴런' 문제를 개선하지만, 여전히 음수 입력에 대한 작은 기울기로 인해 경사 소실의 위험을 완전히 제거하지는 못합니다. 최적의 활성화 함수는 여전히 활발히 연구되고 있는 분야입니다.
- **실제 적용의 중요성:** U-Net의 사례에서 볼 수 있듯이, 적절한 활성화 함수와 초기화 방법의 조합은 실제 문제 해결에서 SOTA(state-of-the-art) 성능을 달성하는 데 필수적입니다.

결론적으로, 깊은 신경망을 설계할 때는 ReLU와 같은 정류 비선형성을 사용하고, 이에 맞춰 He 정규 초기화를 적용하는 것이 현재까지 가장 권장되는 접근 방식입니다.

## 📌 TL;DR

인공신경망의 **활성화 함수**와 **가중치 초기화**는 훈련 성능에 핵심적입니다. 이 조사는 활성화 함수의 **필수 속성** (비선형성, 미분 가능성 등)과 **주요 문제점** (경사 소실, 죽은 뉴런)을 분석합니다. **시그모이드, tanh**는 경사 소실에 취약하지만, **ReLU**는 계산 효율적이며 경사 소실에 강하지만 '죽은 뉴런' 문제가 있습니다. **LReLU, PReLU**는 이 문제를 완화합니다. **Xavier 초기화**는 시그모이드/tanh 및 얕은 네트워크에 적합한 반면, **He 정규 초기화**는 ReLU 계열 활성화 함수 및 깊은 네트워크에서 우수한 성능을 보여, 현재 깊은 신경망 훈련에 더 선호됩니다.
