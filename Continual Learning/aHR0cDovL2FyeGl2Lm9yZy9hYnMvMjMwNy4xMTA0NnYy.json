{
  "url": "http://arxiv.org/abs/2307.11046v2",
  "title": "A Definition of Continual Reinforcement Learning",
  "authors": "David Abel, Andr√© Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh",
  "year": 2023,
  "abstract": "In a standard view of the reinforcement learning problem, an agent's goal is\nto efficiently identify a policy that maximizes long-term reward. However, this\nperspective is based on a restricted view of learning as finding a solution,\nrather than treating learning as endless adaptation. In contrast, continual\nreinforcement learning refers to the setting in which the best agents never\nstop learning. Despite the importance of continual reinforcement learning, the\ncommunity lacks a simple definition of the problem that highlights its\ncommitments and makes its primary concepts precise and clear. To this end, this\npaper is dedicated to carefully defining the continual reinforcement learning\nproblem. We formalize the notion of agents that \"never stop learning\" through a\nnew mathematical language for analyzing and cataloging agents. Using this new\nlanguage, we define a continual learning agent as one that can be understood as\ncarrying out an implicit search process indefinitely, and continual\nreinforcement learning as the setting in which the best agents are all\ncontinual learning agents. We provide two motivating examples, illustrating\nthat traditional views of multi-task reinforcement learning and continual\nsupervised learning are special cases of our definition. Collectively, these\ndefinitions and perspectives formalize many intuitive concepts at the heart of\nlearning, and open new research pathways surrounding continual learning agents.",
  "citation": 158
}