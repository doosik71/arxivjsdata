# A Closer Look at Rehearsal-Free Continual Learning

James Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, Zsolt Kira

## 🧩 Problem to Solve

기존의 기계 학습 모델은 지속적으로 변화하는 데이터 환경에서 새로운 개념을 학습하면서도 이전에 학습했던 지식을 잊어버리는 **재앙적 망각(catastrophic forgetting)** 문제에 직면합니다. 이를 해결하기 위한 최신 연속 학습(Continual Learning) 방법론들은 일반적으로 과거 데이터를 재연(rehearsal)하거나 생성하여 사용하는 방식을 요구하지만, 이는 높은 메모리 비용과 데이터 프라이버시 침해 위험을 수반합니다. 본 논문은 과거 데이터를 저장하거나 생성하지 않는 **재연-없는(rehearsal-free) 연속 학습** 환경에서 재앙적 망각 문제를 효과적으로 해결할 수 있는 방법을 탐색합니다.

## ✨ Key Contributions

- **재연-없는 연속 학습 분석:** 재연-없는 연속 학습에서 망각이 주로 모델의 후반 레이어에서 발생함을 밝히고, 사전 학습(pre-training)이 없는 경우 예측 증류(prediction distillation)가 가장 효과적인 완화 전략임을 제시합니다.
- **사전 학습 모델 활용:** 사전 학습된 모델이 있는 시나리오에서는 예측 증류보다 **매개변수 정규화(parameter regularization)**가 더 효과적임을 발견하여, 연속 학습 문제 설정에 따라 방법론의 효율성이 극적으로 변할 수 있음을 보여줍니다. 특히, L2 정규화가 EWC(Elastic Weight Consolidation)보다 우수한 성능을 보입니다.
- **최첨단 성능 달성:** 제안된 방법을 통해 재연-없는 설정에서 최첨단(SOTA) 결과를 달성했으며, 최근 각광받는 프롬프트(prompting) 기반 연속 학습 방법론들보다도 뛰어난 성능을 입증했습니다.

## 📎 Related Works

- **연속 학습 범주:** 모델 아키텍처 확장, 가중치 공간 정규화(예: EWC), 예측 공간 정규화(예: 지식 증류(Knowledge Distillation)의 LwF) 등 다양한 접근 방식이 존재합니다.
- **재연 기반 방법론:** 과거 데이터를 저장하거나 생성 모델로 샘플링하여 망각을 완화하는 방식(예: Experience Replay, Generative Replay)은 효과적이지만, 메모리, 연산 비용, 데이터 프라이버시 및 잠재적 민감 데이터 기억 문제로 인해 많은 실제 응용에 제약이 있습니다.
- **온라인 재연-없는 학습:** 고정된 사전 학습 모델을 사용하는 스트리밍 학습 관점의 연구가 있으나, 본 연구는 모델이 새로운 작업 데이터에 대해 "수렴할 때까지" 학습하는 비고정 모델에 중점을 둡니다.
- **프로토타입 기반 접근법:** 데이터 저장 없이 프로토타입을 학습하여 망각을 피하려 하지만, 임베딩 네트워크 학습의 어려움이나 큰 첫 번째 작업에 대한 가정 등 추가적인 가정이 필요할 수 있습니다.
- **최근 재연-없는 학습:** 동결된 트랜스포머 모델에서 프롬프트를 학습하는 방식(예: L2P, DualPrompt)이나, 딥 모델 반전을 통해 이미지를 생성하는 방식(예: DeepInversion)이 있으나, 전자는 데이터가 사전 학습된 인코더로 분리 가능하다는 가정을, 후자는 높은 계산 비용과 데이터 프라이버시 위험을 안고 있습니다. 본 연구는 이러한 방식들과 달리 재연을 _완전히_ 배제하는 데 집중합니다.

## 🛠️ Methodology

본 논문은 체크포인트 모델($\theta_{n-1}$)에서 새로운 작업 학습 모델($\theta_n$)으로 지식을 전달하는 세 가지 재연-없는 정규화 방식을 심층적으로 탐구합니다.

- **매개변수 공간 정규화 (Parameter Space Regularization):**
  - **EWC (Elastic Weight Consolidation):** 이전 작업의 Fisher 정보 행렬($F_{jj}^{n-1}$)을 사용하여 모델 매개변수 변화에 페널티를 부여합니다.
    $$L_{ewc} = \sum_{j=1}^{N_{params}} F_{jj}^{n-1} (\theta_{j}^{n} - \theta_{j}^{n-1})^2$$
  - **L2 정규화 (L2 regularization):** EWC에서 Fisher 정보 행렬이 항등 행렬인 경우와 같으며, 단순히 이전 모델 매개변수와의 L2 거리에 페널티를 부여합니다. 사전 학습 시 첫 번째 작업부터 적용 가능합니다.
- **특징 공간 정규화 (Feature Space Regularization, 지식 증류):**
  - **예측 증류 (PredKD, Prediction Distillation):** LwF(Learning without Forgetting)에서 도입된 방식으로, 이전 모델($\theta_{n-1}$)의 예측 분포와 현재 모델($\theta_n$)의 예측 분포 간의 교차 엔트로피를 최소화합니다.
    $$L_{PredKD} = CE(p_{\theta_{n,1:n-1}}(x), p_{\theta_{n-1,1:n-1}}(x))$$
  - **특징 증류 (FeatKD, Feature Distillation):** 모델의 중간 특징 공간에서 이전 모델과 현재 모델의 특징 벡터 간의 L2 거리를 최소화하여 특징 공간의 드리프트(drift)를 방지합니다.
    $$L_{FeatKD} = ||\theta_{l}^{n}(x) - \theta_{l}^{n-1}(x)||_{2}^{2}$$
- **분류기 헤드 및 태스크 편향 완화:**
  - **Sigmoid 이진 교차 엔트로피 (BCE):** 추론 시 작업 레이블이 주어지지 않는 환경에서 재연 데이터 없이 발생하는 태스크 편향을 완화하기 위해, 일반적인 소프트맥스(softmax) 교차 엔트로피 대신 다중 클래스 시그모이드 이진 교차 엔트로피(BCE)를 사용합니다. 이는 이전 클래스에 대한 편향을 줄여 성능을 크게 향상시킵니다.
- **실험 설정:** CIFAR-100 (ResNet-18) 및 ImageNet-R (ViT-B/16) 데이터셋을 사용하여, 사전 학습 유무에 따른 각 정규화 방법론의 성능을 비교합니다. CKA(Centered Kernel Alignment) 유사도를 통해 특징 드리프트를 분석하고, 최종 정확도($A_{1:N}$), 전역 망각($F_N^G$), 지역 망각($F_N^L$) 지표로 평가합니다.

## 📊 Results

- **사전 학습 없는 시나리오 (CIFAR-100):**
  - PredKD와 BCE 분류기를 함께 사용할 때 강력한 재연-없는 연속 학습 기준선을 형성합니다.
  - 매개변수 정규화(EWC, L2)는 망각을 줄이는 데 효과적이지만, 새로운 태스크 학습 능력(가소성, plasticity)이 낮아 최종 정확도는 PredKD 단독보다 낮습니다 (PredKD: 25.2%, PredKD + EWC: 22.7%, PredKD + L2: 21.6%). FeatKD는 최종 정확도를 낮추면서도 망각 개선에는 큰 이득이 없었습니다.
- **사전 학습 모델 활용 시나리오 (CIFAR-100 with ImageNet pre-training):**
  - 매개변수 정규화 방법론의 성능이 역전됩니다. L2 + PredKD (35.6%)가 EWC + PredKD (31.1%)를 능가하며, PredKD 단독(26.6%)보다 훨씬 높은 최종 정확도를 달성합니다.
  - 사전 학습은 망각 자체에는 큰 영향을 주지 않지만, 모델이 새로운 태스크를 학습하는 능력(가소성)을 크게 향상시켜 매개변수 정규화의 단점을 보완합니다.
- **SOTA 비교 (ResNet on CIFAR-100):**
  - 사전 학습을 활용한 PredKD + L2 (34.4%)는 모델 반전 기반의 합성 데이터 재연 방식(ABD: 33.7%)이나 소규모 이미지 코어셋 재연 방식(LwF+2k IMG: 27.4%)보다 우수한 성능을 보여줍니다.
- **ViT 모델 및 프롬프트 기반 방법론과의 비교 (ImageNet-R):**
  - ViT 트랜스포머의 셀프 어텐션 블록 내 QKV(Query, Key, Value) 투영 행렬에 L2 매개변수 정규화를 적용하여 미세 조정(fine-tuning)한 결과, L2 (76.06%)는 CODA-Prompt (75.45%), DualPrompt (71.32%) 등 최신 프롬프트 기반 연속 학습 방법론들을 능가하며 최첨단 성능을 달성합니다. EWC는 저조한 성능을 보였습니다.

## 🧠 Insights & Discussion

- **매개변수 정규화의 재평가:** 일반적으로 재연-없는 연속 학습에서 실패한다고 여겨졌던 EWC나 L2와 같은 매개변수 정규화 방법론이 소프트맥스 대신 BCE 분류기를 사용할 경우, 특히 사전 학습된 모델과 결합될 때 매우 효과적일 수 있음을 입증했습니다.
- **사전 학습의 역할:** 사전 학습은 모델의 특징 추출 능력을 향상시켜 새로운 태스크를 학습하는 데 필요한 가소성을 줄여줍니다. 이로 인해 낮은 가소성을 대가로 망각을 줄이던 매개변수 정규화 방법론들이 사전 학습 환경에서는 낮은 망각과 높은 정확도를 동시에 달성할 수 있게 됩니다. 특히 L2 정규화는 첫 태스크부터 적용되어 사전 학습 상태를 효과적으로 보존할 수 있다는 장점이 있습니다.
- **L2 정규화의 우수성:** 사전 학습된 모델을 활용하는 환경에서 L2 정규화가 EWC 및 특징 증류보다 일관되게 우수한 성능을 보임을 발견했습니다. 이는 L2가 사전 학습된 풍부한 초기 상태에 매개변수를 가깝게 유지함으로써 과거뿐만 아니라 미래 태스크에도 유용한 매개변수 공간을 장려하기 때문입니다.
- **프롬프트 방식 능가:** ViT 모델의 QKV 매트릭스에 L2 정규화를 적용한 미세 조정 방식이 최근 인기를 끄는 프롬프트 기반 연속 학습 방법론들을 능가함으로써, 간단하고 잘 확립된 정규화 기법의 잠재력을 재조명했습니다.
- **향후 연구 방향:** 본 연구는 재연-없는 연속 학습에서 다양한 정규화 기법의 효용성에 대한 귀중한 통찰을 제공했으며, 문제 설정(사전 학습 유무)에 따라 최적의 전략이 달라질 수 있음을 강조합니다. 이는 향후 재연-없는 환경에 대한 연구에 중요한 기반이 될 것입니다.

## 📌 TL;DR

데이터 프라이버시 및 메모리 제약으로 인해 재연이 불가능한 연속 학습 환경에서 재앙적 망각을 해결하기 위해, 본 논문은 매개변수 정규화(L2, EWC)와 지식 증류(PredKD, FeatKD)의 효과를 심층 분석했습니다. BCE 분류기와 결합 시 매개변수 정규화가 재연-없는 환경에서도 성공할 수 있음을 보였고, 특히 **사전 학습된 모델에서는 L2 정규화가 EWC와 지식 증류를 능가하며 가장 효과적**임을 발견했습니다. 나아가, ViT 모델의 QKV 블록에 L2 정규화를 적용한 미세 조정 방식이 최신 프롬프트 기반 연속 학습 방법론들을 뛰어넘는 최첨단 성능을 달성하며, 재연-없는 연속 학습에서 정규화의 강력한 잠재력을 입증했습니다.
