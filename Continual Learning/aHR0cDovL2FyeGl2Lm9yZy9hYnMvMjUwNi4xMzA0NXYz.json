{
  "url": "http://arxiv.org/abs/2506.13045v3",
  "title": "A Comprehensive Survey on Continual Learning in Generative Models",
  "authors": "Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu",
  "year": 2025,
  "abstract": "The rapid advancement of generative models has enabled modern AI systems to\ncomprehend and produce highly sophisticated content, even achieving human-level\nperformance in specific domains. However, these models remain fundamentally\nconstrained by catastrophic forgetting - a persistent challenge where adapting\nto new tasks typically leads to significant degradation in performance on\npreviously learned tasks. To address this practical limitation, numerous\napproaches have been proposed to enhance the adaptability and scalability of\ngenerative models in real-world applications. In this work, we present a\ncomprehensive survey of continual learning methods for mainstream generative\nmodels, including large language models, multimodal large language models,\nvision language action models, and diffusion models. Drawing inspiration from\nthe memory mechanisms of the human brain, we systematically categorize these\napproaches into three paradigms: architecture-based, regularization-based, and\nreplay-based methods, while elucidating their underlying methodologies and\nmotivations. We further analyze continual learning setups for different\ngenerative models, including training objectives, benchmarks, and core\nbackbones, offering deeper insights into the field. The project page of this\npaper is available at\nhttps://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models."
}