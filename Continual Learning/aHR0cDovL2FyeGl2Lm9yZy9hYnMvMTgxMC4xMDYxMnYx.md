# Continual Classification Learning Using Generative Models

Frantzeska Lavda, Jason Ramapuram, Magda Gregorova, Alexandros Kalousis

## 🧩 Problem to Solve

신경망은 한 번에 하나의 태스크를 순차적으로 학습할 때 이전에 학습했던 태스크에 대한 성능을 유지하지 못하고 급격히 감소하는 "파국적 망각(catastrophic forgetting)" 문제에 직면합니다. 본 연구는 이 문제를 해결하고, 이전에 학습한 지식을 유지하면서 새로운 지식을 수용하여 순차적으로 관찰되는 태스크로부터 지속적으로 학습하는 분류 모델을 제안합니다.

## ✨ Key Contributions

- **새로운 변분 경계(Variational Bound) 도출:** 결합 로그-우도 $log p(x,y)$에 대한 새로운 변분 하한을 도출하여 생성 모델(Generative Model)과 판별 모델(Discriminative Model) 학습을 동시에 가능하게 합니다.
- **학생-교사(Student-Teacher) 아키텍처:** 과거 데이터나 모델을 저장할 필요 없이 이전 태스크의 분포를 요약하고 데이터를 생성하는 교사 모델과, 이를 활용하여 새로운 태스크를 학습하는 학생 모델로 구성된 아키텍처를 제안합니다.
- **파국적 망각 완화:** 생성된 과거 태스크 데이터를 현재 태스크 학습에 통합하여 파국적 망각을 효과적으로 방지합니다.
- **동시 학습 능력:** 입력 데이터 $x$를 생성하고 레이블 $y$를 분류하는 두 가지 문제를 공통 잠재 변수 $z$를 사용하여 동시에 해결합니다.

## 📎 Related Works

- **동적 아키텍처 (Dynamic Architectures) [11, 1]:** 새로운 태스크에 따라 모델의 아키텍처가 진화하며 과거 모델을 보존하는 방식입니다. 본 연구는 과거 모델을 보관할 필요가 없습니다.
- **정규화 접근 방식 (Regularization Approaches) [7, 13]:** 이전 태스크에 중요한 매개변수의 변경을 최소화하도록 목적 함수에 제약을 가합니다. 본 연구는 이전 태스크의 매개변수를 저장할 필요가 없습니다.
- **변분 연속 학습 (Variational Continual Learning, VCL) [9]:** 판별 또는 생성 모델 중 하나에 적용 가능하지만, 본 연구와 달리 둘 다 동시에 적용하지는 않습니다. VCL은 "코어셋(core-sets)"을 통해 일부 과거 데이터를 재사용하고, 태스크별 "헤드 네트워크(head networks)"를 유지하여 과거 데이터 접근 및 태스크별 모델 저장 제한을 완화합니다. 본 연구는 이러한 제한을 완전히 따릅니다.
- **평생 생성 모델링 (Lifelong Generative Modelling) [10]:** 본 연구의 생성 능력의 기반이 되는 선행 연구입니다.

## 🛠️ Methodology

1. **모델 구성:** 변분 오토인코더(VAE)의 인코더 및 디코더에 분류기(classifier)를 결합합니다.
2. **그래픽 모델:** 각 관측치 $x$와 해당 레이블 $y$는 잠재 변수 $z$에 의해 생성된다고 가정하여 $p(x,y,z) = p(x|z)p(y|z)p(z)$로 모델링합니다. 여기서 $x, y$는 잠재 변수 $z$가 주어졌을 때 조건부 독립($x \perp y | z$)입니다.
3. **새로운 변분 하한 도출:** $log p(x,y)$에 대한 새로운 하한 $L(x,y)$을 다음과 같이 도출합니다.
   $$L(x,y) = E_{q_{\phi}(z|x)}[log p(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z)) + E_{q_{\phi}(z|x)}[log p(y|z)]$$
   이 하한은 표준 VAE의 ELBO (생성 손실)와 분류 손실 $E_{q_{\phi}(z|x)}[log p(y|z)]$을 포함하여 생성과 분류를 동시에 학습할 수 있게 합니다.
4. **학생-교사 아키텍처:**
   - **교사 모델:** 이전 태스크의 지식을 요약하고, 해당 태스크의 입력-출력 쌍 $(\tilde{x}, \tilde{y})$을 생성합니다.
   - **학생 모델:** 현재 태스크의 새로운 레이블 데이터 $(x,y)$와 교사 모델이 생성한 이전 태스크의 데이터 $(\tilde{x}, \tilde{y})$를 함께 사용하여 학습합니다.
   - **지식 전달:** 새로운 태스크가 시작될 때마다 학생 모델은 최신 매개변수를 교사 모델에 전달합니다.
5. **최적화 손실 함수 (Eq. 5):**
   $$E_{q_{\phi_s}(z|x_s)}[log p_{\theta_s^x}(x|z_s) + log p_{\theta_s^y}(y|z_s)] - D_{KL}(q(z|x)||p(z)) - D_{KL}[q_{\phi_s}(z_s|\tilde{x})||q_{\phi_t}(z_t|\tilde{x})] - L_I(z, \tilde{x})$$
   이 손실 함수는 재구성(reconstruction) 손실, 분류(classification) 손실, 사전 분포와의 KL 발산, 교사 모델의 잠재 표현을 보존하기 위한 KL 발산, 그리고 음의 정보 이득 정규화 항을 포함합니다.

## 📊 Results

- **실험 설정:** Permuted MNIST (원본 MNIST + 4개의 무작위 순열, 총 5개 태스크) 및 MNIST, FashionMNIST, MNIST 순열의 3개 태스크 시퀀스에서 성능을 평가했습니다.
- **비교 모델:**
  - `vae-cl`: 제안된 분류기 변분 경계를 사용하지만 학생-교사 아키텍처가 없는 표준 VAE.
  - `EWC`: EWC(Elastic Weight Consolidation) 정규화 방식을 적용한 모델 (교사 모델이 분포 요약을 유지하지만 데이터를 생성하지는 않음).
- **결과 요약:**
  - `vae-cl`은 첫 번째 태스크 이후 성능이 급격히 저하되었습니다.
  - `EWC`는 성능 저하가 덜했지만 여전히 이전 태스크를 망각했습니다.
  - **CCL-GM (제안 모델):** 태스크 수가 증가함에 따라 높은 평균 분류 정확도와 낮은 평균 음의 재구성 ELBO를 일관되게 유지했습니다.
- **결론:** 제안된 CCL-GM 모델은 파국적 망각을 성공적으로 완화하며, 생성 및 분류 태스크 모두에서 우수한 성능을 유지함을 보여주었습니다.

## 🧠 Insights & Discussion

- **파국적 망각 해결:** 본 모델은 과거 데이터나 태스크별 모델을 저장하지 않고도 순차적 학습 환경에서 파국적 망각을 효과적으로 해결합니다. 이는 엄격한 연속 학습 패러다임을 충족시킵니다.
- **새로운 변분 경계의 중요성:** 새로 도출된 결합 로그-우도에 대한 변분 경계는 VAE가 분류 태스크를 자연스럽게 통합하여 생성과 분류를 동시에 수행할 수 있도록 합니다. 기존 방식들은 VAE에 분류 관련 항을 추가하는 `ad-hoc` 방식이었습니다.
- **학생-교사 아키텍처의 효율성:** 교사 모델의 생성 능력을 활용하여 이전 태스크의 "경험"을 학생 모델에게 전달함으로써, 망각 없이 지속적인 학습이 가능해집니다. 이는 모델이 여러 다른 태스크에 걸쳐 데이터를 분류하고 재구성하는 것을 지속적으로 학습하게 합니다.

## 📌 TL;DR

순차적 태스크 학습에서 발생하는 파국적 망각 문제를 해결하기 위해, VAE 기반의 학생-교사 아키텍처를 활용한 연속 분류 학습 모델(CCL-GM)을 제안합니다. 이 모델은 결합 로그-우도 $log p(x,y)$에 대한 새로운 변분 하한을 도출하여 생성 및 분류를 동시에 학습합니다. 교사 모델은 과거 태스크 데이터를 생성하여 학생 모델의 학습을 돕고, 이를 통해 이전 데이터를 저장하지 않고도 망각을 방지합니다. 실험 결과, CCL-GM은 permuted MNIST 및 혼합 데이터셋에서 기존 `vae-cl` 및 `EWC` 대비 월등히 높은 분류 정확도와 재구성 성능을 유지하며 파국적 망각을 성공적으로 완화했습니다.
