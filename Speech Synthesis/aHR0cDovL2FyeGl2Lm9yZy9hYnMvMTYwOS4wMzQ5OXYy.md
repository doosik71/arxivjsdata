# WAVENET: A GENERATIVE MODEL FOR RAW AUDIO

A ̈aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu

## 🧩 Problem to Solve

이 논문은 초당 수만 개의 샘플을 포함하는 원시 오디오 파형과 같은 매우 높은 시간 해상도를 가진 신호를 자연스럽게 생성하는 것을 목표로 합니다. 기존 방식은 주로 음성 생성 과정에서 발생하는 인공물(artifacts)과 부자연스러움으로 인해 자연스러운 오디오 생성이 어려웠습니다. 특히, 복잡한 분포를 모델링하고 오디오 신호의 장거리 시간 의존성을 효과적으로 포착하는 것이 주요 과제입니다.

## ✨ Key Contributions

- **최고 수준의 음성 합성 (TTS) 성능 달성**: 사람의 평가 결과, WaveNet이 기존의 파라메트릭(parametric) 및 연결(concatenative) 시스템보다 영어와 중국어 모두에서 훨씬 더 자연스러운 음성 합성을 제공함을 입증했습니다.
- **확장된 수용 필드 (Receptive Field)를 위한 새로운 아키텍처**: 원시 오디오 생성에 필요한 장거리 시간 의존성을 처리하기 위해 **Dilated Causal Convolutions** 기반의 새로운 아키텍처를 개발하여 계산 비용을 크게 늘리지 않으면서도 매우 넓은 수용 필드를 달성했습니다.
- **다중 화자 모델링 능력**: 단일 WaveNet 모델이 화자 ID를 조건으로 부여하여 여러 다른 화자의 특성을 동일한 충실도로 포착하고 그 사이를 전환할 수 있음을 보였습니다.
- **다양한 오디오 모달리티에 대한 유연성**: 음악 모델링 시 새롭고 사실적인 음악 조각을 생성했으며, 음소 인식(phoneme recognition)과 같은 차별적인(discriminative) 모델로서도 유망한 결과를 보였습니다.

## 📎 Related Works

- **신경망 기반 자기회귀 생성 모델**: 이미지(PixelRNN, PixelCNN; van den Oord et al., 2016a;b) 및 텍스트(Józefowicz et al., 2016)와 같은 복잡한 분포 모델링에 대한 최근 발전에 영감을 받았습니다. WaveNet은 특히 PixelCNN 아키텍처에 기반을 둡니다.
- **Dilated Convolutions**: 신호 처리(Holschneider et al., 1989; Dutilleux, 1989) 및 이미지 분할(Chen et al., 2015; Yu & Koltun, 2016) 분야에서 이전에 사용되었습니다.
- **Residual 및 Skip Connections**: 깊은 모델의 훈련 가속화에 활용되는 기법(He et al., 2015)입니다.
- **$\mu$-law companding**: ITU-T (1988) 권고에 따라 오디오 데이터의 동적 범위를 압축하는 데 사용되는 비선형 양자화 기법입니다.
- **기존 음성 합성 시스템**: HMM 기반 단위 선택 연결형(Gonzalvo et al., 2016) 및 LSTM-RNN 기반 통계 파라메트릭(Zen et al., 2016) 합성기가 WaveNet의 성능 비교를 위한 베이스라인으로 사용되었습니다.
- **원시 오디오 기반 음성 인식**: Palaz et al., 2013; Tüske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015 등에서 연구되었습니다.

## 🛠️ Methodology

WaveNet은 원시 오디오 파형에 직접 작동하는 심층 생성 모델입니다.

1. **자기회귀 모델 (Autoregressive Model)**: 파형 $x = \{x_{1},...,x_{T}\}$의 결합 확률은 다음과 같이 조건부 확률의 곱으로 분해됩니다:
   $$p(x) = \prod_{t=1}^{T} p(x_{t}|x_{1},...,x_{t-1})$$
   각 오디오 샘플 $x_{t}$는 이전 모든 타임스텝의 샘플에 조건화됩니다.
2. **인과적 컨볼루션 (Causal Convolutions)**: 예측 $p(x_{t+1}|x_{1},...,x_{t})$이 미래 타임스텝에 의존하지 않도록 보장합니다. 일반 컨볼루션의 출력을 몇 타임스텝 시프트하여 구현됩니다.
3. **Dilated Causal Convolutions**: WaveNet의 핵심 요소로, 인과적 컨볼루션 스택의 수용 필드 크기를 기하급수적으로 확장합니다. 필터는 특정 간격으로 입력 값을 건너뛰며 적용됩니다. 이 논문에서는 각 레이어마다 dilation이 1, 2, 4, ..., 512와 같이 두 배로 증가한 후 반복되는 구성을 사용합니다. 이는 적은 수의 레이어로도 매우 넓은 수용 필드를 가질 수 있게 합니다.
4. **Softmax 분포**: 조건부 확률 분포 $p(x_{t}|x_{1},...,x_{t-1})$를 모델링하기 위해 256개의 가능한 값에 대한 범주형 분포를 출력하는 softmax 레이어를 사용합니다.
   - 원시 16비트 오디오 데이터는 먼저 $\mu$-law companding 변환을 거친 후 256개의 값으로 양자화됩니다:
     $$f(x_{t}) = \mathrm{sign}(x_{t}) \frac{\mathrm{ln} (1 + \mu|x_{t}|)}{\mathrm{ln} (1 + \mu)}$$
     여기서 $-1 < x_{t} < 1$ 이고 $\mu = 255$ 입니다. 이 비선형 양자화는 선형 양자화보다 훨씬 더 나은 재구성을 제공합니다.
5. **게이팅 활성화 유닛 (Gated Activation Units)**: Gated PixelCNN에서 사용된 활성화 함수($z = \mathrm{tanh} (W_{f,k} * x) \odot \sigma(W_{g,k} * x)$)를 사용하며, ReLU보다 오디오 신호 모델링에 더 효과적임이 확인되었습니다.
6. **잔차 (Residual) 및 스킵 (Skip) 연결**: 수렴 속도를 높이고 더 깊은 모델 훈련을 가능하게 하기 위해 네트워크 전체에 걸쳐 사용됩니다.
7. **조건부 WaveNets (Conditional WaveNets)**: 추가 입력 $h$를 받아 오디오의 조건부 분포 $p(x|h)$를 모델링할 수 있습니다.
   - **전역 조건화 (Global Conditioning)**: 단일 잠재 표현 $h$ (예: 화자 임베딩)가 모든 타임스텝에 영향을 줍니다.
   - **지역 조건화 (Local Conditioning)**: 오디오 신호보다 낮은 샘플링 주파수를 가질 수 있는 두 번째 시계열 $h_{t}$ (예: 언어적 특징)를 전치 컨볼루션 네트워크를 통해 오디오 신호와 동일한 해상도로 변환하여 사용합니다.

## 📊 Results

- **다중 화자 음성 생성**: 109명의 화자로 구성된 VCTK 데이터셋을 사용하여, WaveNet은 텍스트에 조건화되지 않은 상태에서 비실재하지만 인간 언어와 유사한 단어를 자연스러운 억양으로 생성했습니다. 단일 WaveNet이 모든 화자의 특성을 성공적으로 학습했으며, 더 많은 화자를 추가했을 때 모델 성능이 향상되는 것을 관찰했습니다.
- **음성 합성 (TTS)**: Google의 북미 영어 및 만다린 중국어 TTS 시스템 구축에 사용된 데이터셋으로 실험했습니다.
  - **주관적 청취 테스트**: WaveNet은 베이스라인(LSTM-RNN 파라메트릭 및 HMM 기반 연결형) 시스템을 모든 면에서 능가했습니다. 특히 언어적 특징과 기본 주파수(logF0) 값 모두에 조건화된 WaveNet(L+F) 모델이 가장 우수한 성능을 보였습니다.
  - **평균 의견 점수 (MOS)**: WaveNet(L+F)은 4.0 이상의 MOS 점수를 달성하여 기존 최고 시스템에 비해 자연스러운 음성과의 격차를 크게 줄였습니다 (미국 영어 51%, 만다린 중국어 69% 감소).
- **음악 생성**: MagnaTagATune 및 YouTube 피아노 데이터셋에 WaveNet을 훈련시켰습니다. 수용 필드를 늘리는 것이 음악적으로 들리는 샘플을 얻는 데 중요했습니다. 비록 장거리 일관성(장르, 악기, 볼륨 등)은 부족했지만, 종종 조화롭고 미학적으로 만족스러운 음악 조각을 생성했습니다. 태그에 조건화된 모델은 원하는 음악적 특성을 제어할 수 있었습니다.
- **음성 인식**: TIMIT 데이터셋에서 WaveNet을 음성 인식에 적용하여, 원시 오디오에 직접 훈련된 모델 중 최고 점수인 18.8%의 PER(Phone Error Rate)을 달성했습니다. 이는 다음 샘플 예측과 프레임 분류라는 두 가지 손실 항으로 훈련되었습니다.

## 🧠 Insights & Discussion

WaveNet은 원시 오디오 파형 수준에서 직접 작동함으로써 기존의 수작업으로 추출된 특징이나 보코더에 의존하지 않고도 매우 풍부하고 자연스러운 오디오 특성을 학습할 수 있는 강력하고 유연한 프레임워크를 제공합니다. Dilated Causal Convolutions의 사용은 고해상도 오디오 신호의 장거리 시간 의존성을 효율적으로 포착하는 데 결정적인 역할을 합니다.

$\mu$-law 양자화를 사용한 범주형 분포는 연속적인 오디오 신호에 효과적으로 적용될 수 있음을 보여주었습니다. 또한, 화자 ID, 언어적 특징, 음악 태그와 같은 다양한 입력에 대한 조건부 모델링은 WaveNet의 활용도를 크게 확장시킵니다.

한계점으로는 모델의 수용 필드 크기가 매우 긴 범위의 일관성(예: F0 조건화 없는 음성 운율, 음악의 전체적인 구조)을 포착하는 데 여전히 제한적일 수 있다는 점이 있습니다. 그럼에도 불구하고, WaveNet은 음성 합성, 음악 생성뿐만 아니라 음성 향상, 음성 변환, 음원 분리 등 다양한 오디오 관련 응용 분야에서 큰 잠재력을 가지고 있음을 입증했습니다.

## 📌 TL;DR

**문제**: 고품질의 자연스러운 원시 오디오 파형을 생성하는 것.
**방법**: WaveNet은 Dilated Causal Convolutions을 활용하는 자기회귀(autoregressive) 심층 신경망입니다. $\mu$-law 양자화와 조건부 메커니즘을 통해 원시 오디오 샘플을 조건부 확률로 직접 모델링하여 복잡한 오디오 특성을 포착합니다.
**발견**: WaveNet은 TTS에서 기존 시스템을 능가하는 최고 수준의 자연스러움을 달성했으며, 사실적인 음악 조각을 생성하고, 다중 화자 모델링 및 음성 인식에서도 강력한 성능을 보여주며 다양한 오디오 응용 분야에서의 잠재력을 입증했습니다.
