# Deep Voice: Real-time Neural Text-to-Speech

Sercan Ö. Arık, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, Shubho Sengupta, Mohammad Shoeybi

## 🧩 Problem to Solve

기존의 텍스트-음성 변환(TTS) 시스템은 복잡한 다단계 파이프라인으로 구성되어 있으며, 각 구성 요소가 수작업으로 설계된 특징(feature engineering)과 도메인 전문 지식에 크게 의존하여 개발이 노동 집약적이고 어려웠습니다. 특히, 인간 수준의 음성 합성 품질을 제공하는 WaveNet과 같은 신경망 모델은 높은 컴퓨팅 요구 사항으로 인해 실시간 추론이 어려워 실제 제품 환경에 적용하기에 한계가 있었습니다. 이 논문은 이러한 복잡성을 줄이고, 실시간으로 고품질 음성 합성이 가능한 완전한 신경망 기반 TTS 시스템을 구축하는 것을 목표로 합니다.

## ✨ Key Contributions

- **완전한 신경망 기반 TTS 시스템:** 텍스트에서 음성으로의 변환 전체 과정을 딥러닝 모델로 대체하여, 기존 TTS 시스템보다 단순하고 유연하며, 특징 공학(feature engineering)의 필요성을 최소화했습니다.
- **CTC 손실을 활용한 음소 경계 감지:** 음소 경계를 정밀하게 탐지하기 위해 Connectionist Temporal Classification (CTC) 손실 함수를 사용한 새로운 신경망 기반 분할(segmentation) 모델을 제안했습니다.
- **최적화된 WaveNet 변형 구현:** 원본 WaveNet보다 적은 파라미터와 빠른 훈련 속도를 가지는 WaveNet 변형 모델을 오디오 합성 모델로 사용했습니다.
- **실시간보다 빠른 추론 달성:** WaveNet 추론을 위한 최적화된 CPU 및 GPU 커널을 개발하여 기존 구현 대비 최대 400배 빠른 속도를 달성하며, 실시간 음성 합성이 가능함을 입증했습니다.
- **새로운 데이터셋 적응 용이성:** 수동적인 데이터 주석이나 추가적인 특징 공학 없이도 새로운 데이터셋과 음성, 도메인에 시스템을 쉽게 재훈련하고 높은 품질의 음성을 생성할 수 있음을 보여주었습니다.

## 📎 Related Works

- **신경망 기반 TTS 구성 요소:** 이전 연구들은 음소 변환(G2P), 음소 지속 시간 예측, 기본 주파수(F0) 예측, 오디오 합성 등 TTS 시스템의 특정 구성 요소에 신경망을 적용했지만, 전체 TTS 문제를 해결하거나 도메인 특화된 수작업 특징에 의존했습니다 (e.g., Rao et al., 2015; Zen & Sak, 2015; Pascual & Bonafonte, 2016; van den Oord et al., 2016).
- **파라메트릭 오디오 합성 모델:** WaveNet (van den Oord et al., 2016), SampleRNN (Mehri et al., 2016), Char2Wav (Sotelo et al., 2017) 등이 고품질 오디오 합성을 보여주었지만, Deep Voice는 이러한 시스템들과 달리 기존 TTS 시스템의 특징 추출 없이 완전히 독립적으로 작동하며, 최소한의 특징(원-핫 인코딩된 문자/음소, 음소 지속 시간, 정규화된 로그 F0)만을 사용합니다.
- **실시간 추론의 부재:** 이전 WaveNet 구현은 1초의 오디오를 합성하는 데 수 분이 소요되어 실시간 애플리케이션에 부적합했습니다 (Paine et al., 2016). Deep Voice는 이 문제를 해결하고 생산 준비(production-ready) 시스템으로서 실시간 추론에 중점을 둡니다.

## 🛠️ Methodology

Deep Voice는 5가지 주요 구성 요소로 이루어져 있으며, 각 구성 요소는 신경망으로 구현됩니다:

1. **Grapheme-to-Phoneme (G2P) 모델:**

   - 인코더-디코더 아키텍처를 기반으로 하며, GRU(Gated Recurrent Unit)를 사용합니다.
   - 다층 양방향 인코더와 단방향 디코더를 사용하며, 디코더의 초기 상태는 인코더의 마지막 은닉 상태로 초기화됩니다.
   - 빔 서치(beam search)를 사용하여 디코딩을 수행합니다.

2. **Segmentation 모델:**

   - 음성 발화와 대상 음소 시퀀스 간의 정렬을 출력하도록 훈련됩니다.
   - 음성 인식 분야에서 사용되는 CTC(Connectionist Temporal Classification) 손실 함수를 음소 경계 감지에 적용합니다.
   - 정확한 음소 경계 감지를 위해 단일 음소 대신 "음소 쌍(phoneme pair)" 시퀀스를 예측하도록 훈련합니다.
   - 입력 오디오는 멜 주파수 켑스트럼 계수(MFCCs)로 특징화되며, CNN-RNN 아키텍처를 사용합니다.

3. **Phoneme Duration 및 Fundamental Frequency (F0) 모델:**

   - 단일 신경망 아키텍처를 사용하여 음소 지속 시간과 시간 종속적 기본 주파수(F0)를 공동으로 예측합니다.
   - 입력은 스트레스 정보가 포함된 음소 시퀀스이며, 각 음소와 스트레스는 원-핫 벡터로 인코딩됩니다.
   - 완전 연결(fully connected) 및 단방향 GRU 계층으로 구성됩니다.
   - 음소 지속 시간 오차, F0 오차, 음성 여부 확률의 음의 로그 우도, F0의 시간에 대한 절대 변화에 비례하는 페널티 항을 결합한 공동 손실 함수를 최소화하여 최적화됩니다.

4. **Audio Synthesis 모델:**

   - WaveNet의 변형 모델을 사용합니다.
   - WaveNet의 컨디셔닝 네트워크는 언어적 특징을 원하는 주파수로 업샘플링하고, 자기회귀(autoregressive) 네트워크는 이산화된 오디오 샘플에 대한 확률 분포를 생성합니다.
   - 기존 WaveNet의 전치 컨볼루션(transposed convolution) 대신 양방향 Quasi-RNN (QRNN) 계층 스택을 통해 입력 특징을 인코딩한 후 반복(repetition)을 통해 원하는 주파수로 업샘플링합니다. 이를 통해 더 나은 성능, 빠른 훈련, 적은 파라미터를 달성합니다.

5. **Inference 최적화:**
   - 실시간 추론을 위해 CPU 및 GPU에서 고속 최적화된 추론 커널을 구현합니다.
   - **CPU 구현:** 재계산 방지, 캐시 친화적인 메모리 접근, 멀티스레딩을 통한 병렬화, 효율적인 동기화, 비선형성 FLOP 최소화, 캐시 스래싱 방지, 맞춤형 하드웨어 최적화 루틴(AVX 어셈블리 커널) 사용.
   - **GPU 구현:** Persistent RNN 기술(Diamos et al., 2016)을 사용하여 단일 커널 실행으로 전체 오디오 샘플을 생성하며, 모델 가중치를 레지스터 파일에 로드하여 재로딩 없이 사용합니다. `int16` 양자화(quantization)도 지원합니다.

## 📊 Results

- **훈련 데이터:** 약 20시간 분량의 내부 영어 음성 데이터셋 (13,079개 발화) 및 Blizzard 2013 데이터셋의 일부를 사용했습니다.
- **Segmentation 결과:** 음소 쌍(phoneme pair) 오차율 7%를 달성했습니다. 음소 경계의 작은 오차(10-30ms)는 오디오 품질에 영향을 미치지 않았습니다.
- **Grapheme-to-Phoneme 결과:** 음소 오차율 5.8%, 단어 오차율 28.7%를 달성하여 기존 연구와 유사한 성능을 보였으며, 언어 모델을 사용하지 않고 다중 발음 단어를 제외한 결과입니다.
- **Phoneme Duration 및 F0 결과:** 음소 지속 시간 예측에서 평균 절대 오차(MAE) 38ms, F0 예측에서 MAE 29.4 Hz를 기록했습니다.
- **Audio Synthesis (MOS) 결과:**
  - 지상 진실(ground truth) 지속 시간 및 F0를 사용했을 때 MOS 점수가 높게 나타났으며, 지상 진실 오디오 샘플의 95% 신뢰 구간과 교차했습니다.
  - **합성된 F0를 사용하면 MOS가 감소했고, 합성된 지속 시간을 포함하면 MOS가 크게 감소했습니다.** 이는 현재 자연스러운 TTS를 향한 주요 병목 현상이 지속 시간 및 F0 예측에 있음을 시사합니다.
  - 모델 크기를 조절하여 합성 품질과 추론 속도 간의 균형을 조절할 수 있음을 보여주었습니다 (예: 2배 빠른 실시간 추론 모델에서 2.74 MOS, 1배 빠른 실시간 추론 모델에서 3.35 MOS).
  - 원본 WaveNet의 모든 특징을 사용한 모델과 비교했을 때, 축소된 특징 세트를 사용한 모델 간에 인지적 품질 차이는 없었습니다.
- **추론 속도:**
  - CPU 커널은 일부 모델에서 실시간보다 빠르거나(최대 2.7배) 실시간에 근접한 성능을 보였습니다.
  - GPU 커널은 아직 실시간 성능에 미치지 못했지만(최대 0.39배), 기존 WaveNet 구현 대비 최대 400배의 속도 향상을 달성했습니다.

## 🧠 Insights & Discussion

- **신경망 기반 TTS의 실현 가능성:** Deep Voice는 고품질 텍스트-음성 변환 엔진의 모든 구성 요소에 딥러닝 접근 방식이 실현 가능함을 입증했습니다.
- **실시간 오디오 생성의 돌파구:** 최적화된 추론 기술을 통해 스트리밍 방식으로 실시간 오디오 생성이 가능함을 보여주었습니다.
- **높은 유연성 및 쉬운 적응성:** 수동 개입 없이 시스템을 훈련할 수 있어 TTS 시스템 생성 과정을 크게 단순화하고, 새로운 데이터셋에 쉽게 적용할 수 있습니다.
- **주요 병목 현상:** 현재 시스템에서 음성 합성의 자연스러움을 저해하는 주요 요인은 음소 지속 시간 및 기본 주파수(F0) 예측 모델의 성능에 있음을 확인했습니다.
- **향후 연구 방향:**
  - 모델 양자화(quantization) 및 다른 하드웨어 아키텍처(예: Xeon Phi)를 통한 추론 성능 추가 개선.
  - 음소 분할, 지속 시간 예측, F0 예측 모델을 오디오 합성 모델에 직접 통합하여 완전한 종단 간(end-to-end) 시퀀스-투-시퀀스 TTS 시스템으로 발전시키는 것.
  - 더 큰 훈련 데이터셋이나 생성 모델링 기법을 통해 지속 시간 및 F0 모델의 성능을 향상시키는 것.

## 📌 TL;DR

Deep Voice는 모든 구성 요소를 신경망으로 구축하고 실시간 추론을 가능하게 한 고품질 TTS 시스템입니다. 기존 TTS의 복잡성과 수작업 특징 공학의 필요성을 줄였으며, WaveNet 변형 및 CTC 기반 음소 분할 모델을 포함합니다. 최적화된 CPU/GPU 커널을 통해 WaveNet 추론 속도를 400배 향상시켜 실시간 음성 합성을 달성했습니다. MOS 점수 분석 결과, 음소 지속 시간 및 F0 예측이 음성 품질의 주요 병목임을 밝혔으며, 이는 향후 연구의 중요한 방향을 제시합니다.
