{
  "title": "FloWaveNet : A Generative Flow for Raw Audio",
  "authors": "Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, Sungroh Yoon",
  "year": 2018,
  "url": "http://arxiv.org/abs/1811.02155v3",
  "abstract": "Most modern text-to-speech architectures use a WaveNet vocoder for\nsynthesizing high-fidelity waveform audio, but there have been limitations,\nsuch as high inference time, in its practical application due to its ancestral\nsampling scheme. The recently suggested Parallel WaveNet and ClariNet have\nachieved real-time audio synthesis capability by incorporating inverse\nautoregressive flow for parallel sampling. However, these approaches require a\ntwo-stage training pipeline with a well-trained teacher network and can only\nproduce natural sound by using probability distillation along with auxiliary\nloss terms. We propose FloWaveNet, a flow-based generative model for raw audio\nsynthesis. FloWaveNet requires only a single-stage training procedure and a\nsingle maximum likelihood loss, without any additional auxiliary terms, and it\nis inherently parallel due to the characteristics of generative flow. The model\ncan efficiently sample raw audio in real-time, with clarity comparable to\nprevious two-stage parallel models. The code and samples for all models,\nincluding our FloWaveNet, are publicly available.",
  "citation": 227
}