# LOW BIT-RATE SPEECH CODING WITH VQ-VAE AND A WAVENET DECODER

Cristina Gârbacea, Aäron van den Oord, Yazhe Li, Felicia S C Lim, Alejandro Luebs, Oriol Vinyals, Thomas C Walters

## 🧩 Problem to Solve

효율적인 음성 신호 전송 및 저장을 위해, 기존의 음성 코덱들은 수작업으로 설계된 복잡한 파이프라인을 사용하며 일반적으로 높은 비트율(16 kbps 이상)에서 작동합니다. 이 연구는 딥러닝 기반의 종단간(end-to-end) 학습 방법을 사용하여 기존 코덱보다 훨씬 낮은 비트율(low bit-rate)에서도 높은 지각적(perceptual) 재구성 품질을 유지하면서 음성 코딩을 수행하는 것이 가능한지 탐구하는 것을 목표로 합니다. 특히, VQ-VAE (Vector Quantized-Variational AutoEncoder) 아키텍처를 스피커 독립적이며 운율(prosody)을 보존하는 방식으로 개량하여, 매우 낮은 비트율의 음성 코덱으로서의 적합성을 평가하고자 합니다.

## ✨ Key Contributions

- **VQ-VAE 기반 초저비트율 음성 코덱 제안:** VQ-VAE와 WaveNet 디코더를 결합한 아키텍처를 초저비트율 음성 코딩에 적용하고 그 효율성을 입증했습니다.
- **스피커 및 운율 투명성 달성:** 기존 VQ-VAE 아키텍처를 수정하여 스피커의 고유성과 발화의 운율을 보존하도록 개선했습니다.
  - 스피커 독립성을 위해 발화 전체에서 얻은 시간 불변(time-invariant) 잠재 코드북을 도입했습니다.
  - 운율(피치, $f_0$) 보존을 위해 별도의 $f_0$ 예측 디코더를 추가하고 해당 손실 항을 사용하여 잠재 표현이 피치 정보를 포함하도록 유도했습니다.
- **경쟁력 있는 재구성 품질:** 1.6 kbps에서 작동하는 모델이 LibriSpeech 데이터셋 학습 시 2.4 kbps의 MELP 코덱과 23.05 kbps의 AMR-WB 코덱 사이의 지각적 품질을 달성했습니다.
- **고품질 조건에서의 우수한 성능:** 트레이닝 데이터에 테스트 스피커가 포함된 스튜디오 품질 음성 데이터로 학습 시, 1.6 kbps의 VQ-VAE 모델이 23.05 kbps의 AMR-WB와 유사한 지각적 품질을 제공했습니다.
- **스피커 유사성 보존:** MUSHRA 테스트 외에도 MOS(Mean Opinion Score) 평가를 통해 VQ-VAE 기반 코덱이 다른 저비트율 코덱(MELP, Speex)보다 스피커 유사성을 더 잘 보존함을 보였습니다.

## 📎 Related Works

- **WaveNet [8]:** 원시 오디오 데이터로부터 직접 학습하여 고품질 음성을 생성하는 능력을 보여준 텍스트-음성 변환 모델.
- **VQ-VAE (Vector-Quantized Variational AutoEncoder) [12]:** 이산적인 잠재 표현을 학습하는 오토인코더 모델로, 고품질 오디오 재구성을 위해 WaveNet과 유사한 디코더와 함께 사용되었습니다. 이미지, 오디오, 비디오 데이터에서 중요한 고수준 특징을 매우 압축된 형태로 캡처할 수 있음을 입증했습니다.
- **딥러닝 기반 이미지 압축 [1-4]:** 오토인코더를 활용하여 이미지를 효율적으로 압축하는 연구들.
- **딥러닝 기반 음성 코딩 [5, 6]:** 음향 특징을 활용하거나 종단간 최적화된 DNN 기반 코덱을 제안한 선행 연구들.
- **WaveNet 디코더를 활용한 저비트율 음성 코딩 [9]:** Codec2 인코더의 비트 스트림을 WaveNet 디코더로 재구성하여 고품질 오디오를 생성한 연구.
- **기존 음성 코덱:** AMR-WB [10], MELP [17], Codec2 [11], Speex [16] 등 다양한 비트율에서 작동하는 기존의 수작업으로 설계된 코덱들.

## 🛠️ Methodology

이 연구는 기존 VQ-VAE 아키텍처를 음성 코딩 작업에 더 적합하도록 다음과 같은 방식으로 수정 및 평가했습니다.

1. **기본 아키텍처:**

   - **인코더 (Encoder):** 입력 오디오 파형을 이산적인 잠재 표현으로 변환하는 컨볼루션 네트워크.
   - **벡터 양자화 (Vector Quantization, VQ) 레이어:** 인코더 출력을 고정된 코드북 내의 벡터로 양자화하여 이산 잠재 코드를 생성합니다. 이 코드들은 압축된 음성 신호에 해당하며, 낮은 비트율 코딩의 핵심입니다.
   - **디코더 (Decoder):** 이산 잠재 코드와 추가적인 조건부 정보(스피커 및 운율 관련)를 받아 원시 오디오 파형을 재구성하는 WaveNet 기반 네트워크.

2. **스피커 고유성 유지 (Speaker Identity Maintenance):**

   - 기존 VQ-VAE에서 스피커 식별을 위해 사용되던 명시적인 원-핫(one-hot) 코딩 조건을 제거했습니다.
   - 대신, 인코더 출력의 시간 차원($t$)에 대해 평균 풀링(mean pooling)을 수행하여 전체 발화에서 하나의 시간 불변 잠재 표현(time-invariant latent representation)을 추출합니다.
   - 이 시간 불변 잠재 표현은 별도의 코드북으로 양자화되어 스피커 관련 정보를 요약하고 전달하는 역할을 합니다. 이는 시간 가변적인(time-varying) 메시지 콘텐츠 코드와 분리되어 스피커 독립적인 모델을 가능하게 합니다.

3. **운율(Prosody) 제약 (Prosody Constraining):**

   - 네트워크가 피치($f_0$) 및 타이밍 정보를 병목(bottleneck) 표현을 통해 전달하도록 유도하기 위해 훈련 체계를 설계했습니다.
   - WaveNet 오디오 디코더와 병렬로 두 번째 디코더, 즉 $f_0$ 예측 디코더를 추가했습니다.
   - 이 $f_0$ 예측 디코더는 오디오 디코더와 공통의 잠재 표현을 사용하여 학습 발화의 $f_0$ 트랙을 예측합니다.
   - 손실 함수에 $f_0$ 예측 항을 추가하여 잠재 표현이 피치 트랙 정보를 포함하도록 강제합니다. 이 정보는 웨이브폼 디코더가 원본과 동일한 피치 트랙을 가진 발화를 생성하는 데 사용됩니다.
   - $f_0$ 정보는 200 Hz로 샘플링되었으며, 오디오 샘플(16 kHz)과의 호환성을 위해 인코더 측에 스트라이드 5의 추가 컨볼루션 레이어를 도입했습니다.
   - 코덱으로 사용될 때 $f_0$ 예측 네트워크는 제거되고 오디오 디코더만 파형 재구성에 사용됩니다.

4. **실험 설정:**
   - **데이터셋:** LibriSpeech (LS, 1000시간, 2302 스피커) 및 LSplus (LS에 테스트 스피커 포함) 데이터셋, 그리고 스튜디오 품질의 비공개 Studio 데이터셋을 사용했습니다.
   - **평가:** MUSHRA (Multiple Stimuli with Hidden Reference and Anchor) 청취 테스트를 통해 지각적 오디오 품질을 평가했습니다. 스피커 유사성은 MOS 평가를 사용했습니다.

## 📊 Results

- **Studio 데이터셋 학습 시 (테스트 스피커 포함):**

  - 1.6 kbps의 VQ-VAE 모델이 2.4 kbps의 MELP, Codec2, Speex 코덱보다 훨씬 높은 MUSHRA 점수를 기록했습니다.
  - 특히, 23.05 kbps의 AMR-WB 코덱, 128 kbps의 8비트 $\mu$-law와 함께 최고 품질 클러스터에 속하며, AMR-WB와 유사한 지각적 품질을 달성했습니다 (그림 1). 이는 표준 광대역 오디오 코덱 대비 압축률이 14배 향상되었음을 의미합니다.

- **LibriSpeech 데이터셋 학습 시 (스피커 독립적):**

  - 1.6 kbps의 VQ-VAE 모델은 2.4 kbps의 MELP 코덱과 23.05 kbps의 AMR-WB 코덱 중간 정도의 MUSHRA 점수를 기록했습니다 (그림 2).
  - 800 bps 모델도 2.4 kbps의 MELP보다 우수한 성능을 보였고, 400 bps에 이르러서야 Speex 2.4 kbps보다 성능이 저하되었습니다.

- **테스트 스피커 포함 여부의 영향:**

  - LibriSpeech에 테스트 스피커 발화를 추가하여 학습한 LSplus 모델(1.6 kbps)이 일반 LibriSpeech 모델(1.6 kbps)보다 약간 더 높은 MUSHRA 점수를 보였습니다 (그림 3).

- **스피커 유사성 MOS 평가:**
  - VQ-VAE LSplus (1600 bps)는 3.794, VQ-VAE LS (1600 bps)는 3.703의 MOS를 기록하여, MELP (2400 bps, 3.138) 및 Speex (2400 bps, 2.534)보다 스피커 유사성 보존에서 더 우수함을 보였습니다 (표 I).
  - 테스트 스피커가 학습 세트에 포함된 경우(LSplus) MOS가 약간 더 높았지만, VQ-VAE LS 모델의 오차 범위가 넓은 것은 특정 스피커에 대해 매우 낮은 MOS가 할당되었기 때문으로 분석됩니다.

## 🧠 Insights & Discussion

- **저비트율 코딩의 잠재력:** VQ-VAE 기반 아키텍처는 매우 낮은 비트율(1.6 kbps)에서도 인상적인 음성 재구성 품질을 보여주며, 이는 기존의 저비트율 코덱들을 능가하는 성능입니다. 특히, 스튜디오 품질의 데이터와 학습 스피커가 포함된 경우 AMR-WB와 거의 동등한 수준에 도달하여 딥러닝 기반 코덱의 잠재력을 강력하게 시사합니다.
- **스피커 및 운율 투명성:** 제안된 아키텍처 수정(시간 불변 코드 및 $f_0$ 예측 디코더)은 VQ-VAE 모델이 스피커 고유성과 발화의 운율을 효과적으로 보존할 수 있도록 합니다. 이는 범용 음성 코덱으로서 VQ-VAE의 실용성을 높이는 중요한 진전입니다.
- **데이터셋의 중요성:** 학습 데이터셋의 품질과 다양성(LibriSpeech vs. Studio)이 모델의 일반화 및 재구성 품질에 큰 영향을 미친다는 것을 MUSHRA 결과가 보여줍니다. 특정 스피커에 대한 학습은 성능 향상에 도움이 되지만, 스피커 독립성을 달성하는 것이 범용 코덱의 핵심입니다.
- **한계 및 향후 연구:**
  - 현재 1.6 kbps에서 경쟁력 있는 결과를 보였지만, 더 낮은 비트율(예: 400 bps)에서는 품질 저하가 관찰됩니다. 향후 모델 성능을 1.6 kbps 이상으로 개선할 여지가 있습니다.
  - 스피커 유사성 보존의 경우, 일부 스피커에서 성능의 큰 편차가 관찰되었습니다. 스피커 고유성 보존의 가변성을 이해하고 개선하기 위한 추가 연구가 필요합니다.
  - 시간 불변 코드의 긴 윈도우(전체 발화)는 온라인 코딩에 비현실적입니다. 미래에는 과거 정보만을 기반으로 느리게 변하는 코드로 대체하는 연구가 필요합니다.
- **기술적 의미:** 이 연구는 딥러닝, 특히 생성 모델이 음성 코딩 분야에서 수작업으로 설계된 시스템의 성능을 뛰어넘을 수 있음을 보여주는 중요한 증거를 제공합니다. 이는 통신 및 저장 효율성을 크게 향상시킬 수 있는 기반이 됩니다.

## 📌 TL;DR

이 논문은 VQ-VAE와 WaveNet 디코더를 기반으로 한 초저비트율(1.6 kbps) 음성 코덱을 제안합니다. 스피커 고유성을 유지하고 운율($f_0$) 정보를 보존하도록 아키텍처를 수정하여, 1.6 kbps에서 2.4 kbps의 MELP 코덱보다 우수하며 23.05 kbps의 AMR-WB 코덱에 준하는 지각적 품질을 달성합니다. MUSHRA 및 MOS 청취 테스트를 통해 VQ-VAE 코덱이 스피커 독립성과 스피커 유사성 보존 능력에서 기존 저비트율 코덱보다 뛰어남을 입증하며, 딥러닝 기반 음성 코딩의 큰 잠재력을 보여줍니다.
