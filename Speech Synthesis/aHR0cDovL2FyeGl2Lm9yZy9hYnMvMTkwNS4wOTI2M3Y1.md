# FastSpeech: Fast, Robust and Controllable Text to Speech

Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu

## 🧩 Problem to Solve

신경망 기반의 종단간(end-to-end) TTS 모델(예: Tacotron 2)은 합성 음성의 품질을 크게 향상시켰지만, 다음과 같은 문제점들이 있습니다:

- **느린 추론 속도:** 멜-스펙트로그램을 자기회귀(autoregressive) 방식으로 생성하기 때문에 추론 속도가 느립니다. 멜-스펙트로그램 시퀀스의 길이가 수백 또는 수천에 달할 수 있기 때문입니다.
- **낮은 견고성(Robustness):** 자기회귀 생성 과정에서 발생하는 오류 전파 및 텍스트와 음성 간의 잘못된 어텐션 정렬로 인해 단어 건너뛰기(word skipping) 또는 반복(repeating) 문제가 발생하기 쉽습니다.
- **제어 부족:** 음성 속도나 운율(prosody)을 직접 제어하기 어렵습니다.

## ✨ Key Contributions

FastSpeech는 병렬 멜-스펙트로그램 생성을 위한 새로운 피드포워드 네트워크를 제안하며, 주요 기여는 다음과 같습니다:

- **획기적인 추론 속도 향상:** 자기회귀 Transformer TTS 대비 멜-스펙트로그램 생성 속도를 270배, 종단간 음성 합성 속도를 38배 가속화했습니다.
- **견고성 향상:** 단어 건너뛰기 및 반복 문제를 거의 제거하여 합성 음성의 견고성을 크게 개선했습니다.
- **음성 제어 가능성:** 음성 속도(0.5배에서 1.5배까지)와 단어 사이의 휴지(break)를 부드럽게 조절하여 운율 제어를 가능하게 했습니다.
- **높은 음성 품질 유지:** 자기회귀 Transformer TTS 모델과 거의 동등한 수준의 음성 품질을 달성했습니다.

## 📎 Related Works

- **자기회귀(Autoregressive) TTS 모델:** Tacotron [27], Tacotron 2 [22], Deep Voice 3 [19], Transformer TTS [14] 등 대부분의 신경망 기반 TTS 시스템은 멜-스펙트로그램을 자기회귀 방식으로 생성합니다.
- **보코더(Vocoder):** Griffin-Lim [6], WaveNet [24], Parallel WaveNet [16], WaveGlow [20] 등이 멜-스펙트로그램에서 음성을 합성하는 데 사용됩니다.
- **비자기회귀(Non-Autoregressive) 시퀀스 생성:** 신경망 기계 번역 [7,8,26] 및 오디오 합성(예: Parallel WaveNet [16], ClariNet [18], WaveGlow [20]) 분야에서 추론 속도 향상을 위해 연구되어 왔습니다. 그러나 이들은 멜-스펙트로그램을 여전히 자기회귀 방식으로 생성하여 본 연구가 다루는 문제를 해결하지 못했습니다.
- **동시 연구:** 멜-스펙트로그램을 병렬로 생성하는 동시 연구 [17]가 있으나, FastSpeech는 더 적은 파라미터로 더 빠른 속도 향상을 이루고 단어 건너뛰기/반복 문제를 거의 완전히 해결한다는 점에서 차이가 있습니다.

## 🛠️ Methodology

FastSpeech는 인코더-어텐션-디코더 기반 아키텍처 대신 새로운 피드포워드 구조를 사용하여 멜-스펙트로그램을 병렬로 생성합니다. 주요 구성 요소는 다음과 같습니다:

- **피드포워드 트랜스포머 (Feed-Forward Transformer, FFT):**

  - Transformer [25]의 셀프-어텐션(self-attention)과 1D 컨볼루션 [5,19]을 기반으로 하는 피드포워드 구조입니다.
  - 여러 개의 FFT 블록을 텍스트(음소) 입력 측과 멜-스펙트로그램 출력 측에 쌓고, 그 사이에 길이 조절기(Length Regulator)를 배치하여 두 시퀀스 간의 길이 불일치를 해결합니다.
  - 각 FFT 블록은 멀티-헤드 어텐션(multi-head attention) 네트워크와 2계층 1D 컨볼루션 네트워크(ReLU 활성화)로 구성됩니다. 1D 컨볼루션은 음성 작업에서 인접 은닉 상태 간의 긴밀한 관계를 활용합니다.
  - 잔차 연결(residual connections), 계층 정규화(layer normalization), 드롭아웃(dropout)이 적용됩니다.

- **길이 조절기 (Length Regulator):**

  - 음소 시퀀스와 멜-스펙트로그램 시퀀스 간의 길이 불일치 문제를 해결하고, 음성 속도 및 운율을 제어하는 데 사용됩니다.
  - 각 음소에 해당하는 멜-스펙트로그램의 길이를 '음소 지속 시간(phoneme duration)'이라고 정의합니다.
  - 주어진 음소 지속 시간 $D = [d_1, d_2, ..., d_n]$에 따라 음소 은닉 상태 $H_{\text{pho}} = [h_1, h_2, ..., h_n]$를 각 $d_i$만큼 확장하여 멜-스펙트로그램 길이와 일치하는 확장된 시퀀스 $H_{\text{mel}}$을 생성합니다.
  - $\alpha$ 파라미터를 조절하여 음소 지속 시간을 변경함으로써 음성 속도를 제어할 수 있습니다. (예: $\alpha=1.0$은 일반 속도, $\alpha=1.3$은 느린 속도, $\alpha=0.5$는 빠른 속도)
  - 문장 내 공백 문자의 지속 시간을 조절하여 단어 사이에 휴지를 추가함으로써 운율을 제어할 수도 있습니다.

- **지속 시간 예측기 (Duration Predictor):**

  - 길이 조절기에 필요한 음소 지속 시간을 예측합니다.
  - 2계층 1D 컨볼루션 네트워크(ReLU 활성화, 계층 정규화, 드롭아웃)와 하나의 선형 계층으로 구성됩니다.
  - 로그 도메인에서 MSE(Mean Square Error) 손실을 사용하여 FastSpeech 모델과 함께 공동으로 훈련됩니다.

- **훈련 과정 (교사-학생 학습 및 지식 증류):**
  - **지속 시간 추출:** 먼저 자기회귀 인코더-어텐션-디코더 기반 Transformer TTS 교사 모델을 훈련합니다. 이 교사 모델의 디코더-인코더 어텐션 정렬(attention alignments)에서 각 음소에 대한 실제 지속 시간을 추출합니다. 어텐션 헤드 중 대각선 특성(diagonal property)에 가장 가까운 헤드를 선택하기 위해 '초점 비율(focus rate, $F$)'을 사용합니다.
  - **시퀀스 수준 지식 증류 (Sequence-Level Knowledge Distillation):** 교사 모델이 생성한 멜-스펙트로그램을 FastSpeech 모델 훈련을 위한 목표(target)로 사용하여 교사 모델의 지식을 학생 모델(FastSpeech)로 전달합니다.

## 📊 Results

FastSpeech는 LJSpeech 데이터셋에 대한 실험을 통해 다음과 같은 결과를 보였습니다.

- **음성 품질 (MOS):**

  - FastSpeech (Mel + WaveGlow)의 평균 의견 점수(MOS)는 3.84$\pm$0.08로, 자기회귀 Transformer TTS (3.88$\pm$0.09) 및 Tacotron 2 (3.86$\pm$0.09)와 거의 동등한 품질을 달성했습니다.
  - 원본 음성(GT)의 MOS는 4.41$\pm$0.08입니다.

- **추론 속도 향상:**

  - 멜-스펙트로그램 생성: 자기회귀 Transformer TTS 대비 269.40배 빠릅니다 (0.025초 vs 6.735초).
  - 종단간 음성 합성 (WaveGlow 사용): 자기회귀 Transformer TTS 대비 38.30배 빠릅니다 (0.180초 vs 6.895초).
  - FastSpeech의 추론 지연 시간은 생성되는 멜-스펙트로그램 길이에 거의 영향을 받지 않는 반면, Transformer TTS는 길이에 따라 크게 증가합니다.

- **견고성:**

  - TTS 시스템에 특히 어려운 50개 문장(예: 단일 문자, 반복 숫자, 긴 문장)에 대한 테스트에서, Transformer TTS는 34%의 오류율(단어 반복 및 건너뛰기)을 보인 반면, FastSpeech는 0%의 오류율로 이러한 문제를 거의 완벽하게 제거했습니다.

- **제어 가능성:**

  - **음성 속도 조절:** $\alpha$ 하이퍼파라미터를 조절하여 0.5배(빠르게)에서 1.5배(느리게)까지 음성 속도를 부드럽게 조절할 수 있으며, 피치(pitch)는 안정적으로 유지됩니다.
  - **단어 간 휴지 조절:** 문장 내 공백 문자의 지속 시간을 조절하여 단어 사이에 휴지를 추가함으로써 운율을 개선할 수 있습니다.

- **어블레이션 연구 (CMOS):**
  - FFT 블록에서 1D 컨볼루션을 제거하면 CMOS가 -0.113 감소하여 1D 컨볼루션의 효과를 입증했습니다.
  - 시퀀스 수준 지식 증류를 제거하면 CMOS가 -0.325 감소하여 지식 증류의 중요성을 강조했습니다.

## 🧠 Insights & Discussion

- FastSpeech는 병렬 멜-스펙트로그램 생성을 위한 독창적인 피드포워드 네트워크 구조를 통해 기존 자기회귀 TTS 모델의 주요 한계점(느린 추론 속도, 낮은 견고성, 제어 부족)을 성공적으로 해결했습니다.
- 교사 모델로부터 추출된 음소 지속 시간을 활용하는 길이 조절기와 지속 시간 예측기는 텍스트와 음성 간의 명시적인 정렬을 제공함으로써 견고성과 제어 가능성을 동시에 확보하는 핵심적인 역할을 합니다.
- 시퀀스 수준 지식 증류는 학생 모델이 교사 모델의 높은 품질을 유지하면서 병렬 생성의 이점을 누릴 수 있게 하는 효과적인 학습 전략임을 보여줍니다.
- 이 연구는 고품질의 음성 합성을 유지하면서 실시간 애플리케이션에 적합한 빠르고 안정적이며 제어 가능한 TTS 시스템의 가능성을 제시합니다.
- **향후 연구:** 합성 음성의 품질을 더욱 향상시키고, 다화자 및 저자원(low-resource) 설정에 FastSpeech를 적용하며, 병렬 신경망 보코더와 FastSpeech를 통합하여 완전한 종단간 병렬 TTS 시스템을 구축할 계획입니다.

## 📌 TL;DR

자기회귀 TTS 모델의 느린 속도, 낮은 견고성, 제어 부족 문제를 해결하기 위해, 이 논문은 병렬 멜-스펙트로그램 생성을 위한 새로운 피드포워드 Transformer 기반 모델인 FastSpeech를 제안합니다. FastSpeech는 교사 모델에서 추출한 음소 지속 시간을 활용하는 길이 조절기와 지속 시간 예측기를 통해 텍스트와 음성 길이의 불일치를 해소하고 명시적인 정렬을 제공합니다. 결과적으로 FastSpeech는 자기회귀 Transformer TTS와 유사한 음성 품질을 유지하면서 멜-스펙트로그램 생성 속도를 270배, 종단간 합성 속도를 38배 가속화하고, 단어 건너뛰기/반복 문제를 거의 제거하며, 음성 속도와 운율을 효과적으로 제어할 수 있음을 입증했습니다.
