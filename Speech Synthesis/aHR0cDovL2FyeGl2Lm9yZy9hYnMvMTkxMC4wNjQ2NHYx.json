{
  "title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder",
  "authors": "Cristina Gârbacea, Aäron van den Oord, Yazhe Li, Felicia S C Lim, Alejandro Luebs, Oriol Vinyals, Thomas C Walters",
  "year": 2019,
  "url": "http://arxiv.org/abs/1910.06464v1",
  "abstract": "In order to efficiently transmit and store speech signals, speech codecs\ncreate a minimally redundant representation of the input signal which is then\ndecoded at the receiver with the best possible perceptual quality. In this work\nwe demonstrate that a neural network architecture based on VQ-VAE with a\nWaveNet decoder can be used to perform very low bit-rate speech coding with\nhigh reconstruction quality. A prosody-transparent and speaker-independent\nmodel trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits\nperceptual quality which is around halfway between the MELP codec at 2.4 kbps\nand AMR-WB codec at 23.05 kbps. In addition, when training on high-quality\nrecorded speech with the test speaker included in the training set, a model\ncoding speech at 1.6 kbps produces output of similar perceptual quality to that\ngenerated by AMR-WB at 23.05 kbps.",
  "citation": 178
}