# Unsupervised speech representation learning using WaveNet autoencoders

Jan Chorowski, Ron J. Weiss, Samy Bengio, Aäron van den Oord

## 🧩 Problem to Solve

이 논문은 의미 있는 음성 잠재 표현을 비지도 학습 방식으로 추출하는 문제를 다룹니다. 목표는 음성 신호에서 음소 정체성과 같은 고수준의 의미론적 내용을 포착하는 동시에, 기저 피치 윤곽이나 배경 소음과 같은 혼란스러운 저수준 세부 정보에 불변하는 표현을 학습하는 것입니다. 이는 대규모 레이블 데이터셋의 필요성, 지도 학습 모델의 제한된 통찰력, 저자원 자동 음성 인식(ASR) 시나리오에서의 비지도 표현 학습의 중요성 때문에 필요합니다.

## ✨ Key Contributions

- **음성 표현의 분리**: 발화의 음성 내용과 화자 정체성 같은 화자 특성을 분리하는 비지도 음성 표현 학습 방법을 평가하고 개선했습니다.
- **WaveNet 디코더의 활용**: 고용량 WaveNet 디코더를 사용하여 인코더가 버린 정보를 재구성함으로써, 인코더가 저수준 세부 정보 대신 고수준 의미적 특징에 집중하도록 유도했습니다.
- **최적의 입출력 구성 발견**: 인코더 입력으로 MFCC(Mel-Frequency Cepstral Coefficients)를 사용하고 디코더 목표로 원시 파형을 재구성하는 것이 최상의 표현을 생성함을 발견했습니다. 이는 시스템이 특징 추출 과정에서 제거된 샘플 수준의 세부 정보도 생성하도록 강제합니다.
- **VQ-VAE의 우수성 입증**: 세 가지 오토인코더 변형(차원 축소, 가우시안 VAE, 이산 VQ-VAE) 중, VQ-VAE가 음향 내용과 화자 정보 간의 가장 강력한 분리를 달성함을 입증했습니다.
- **시간-지터 정규화 도입**: 잠재 표현이 발화의 음성 내용에 집중하도록 강제하는 새로운 시간-지터(time-jitter) 정규화 기법을 제안하여, 잠재 공간의 붕괴를 방지하고 표현의 시간적 안정성을 높였습니다.
- **ZeroSpeech 2017 벤치마크 성능**: ZeroSpeech 2017 비지도 음향 단위 발견(acoustic unit discovery) 태스크에서 최상위 참가자들과 견줄 만한 강력한 성능을 달성했습니다.

## 📎 Related Works

- **계층적 표현 학습**: Stacked Restricted Boltzmann Machines [1], Denoising Autoencoders [2].
- **지도 특징 학습**: ImageNet [28]에서 학습된 시각 특징, 음소 예측 태스크에서 추출된 병목 특징 [30], [31], 기계 번역에서 추출된 텍스트 표현 [32].
- **비지도 특징 학습**: 차원 축소 오토인코더 [35], Variational Autoencoder (VAE) [38], Vector Quantized VAE (VQ-VAE) [19].
- **순차 데이터용 오토인코더**: WaveNet [18]과 같은 자가회귀 모델, 잠재 공간 붕괴 문제 및 해결책 [49].
- **불변 표현 학습**: Moyer et al. [54]의 증명, Domain-Adversarial Training [65].
- **정규화 기법**: Slow Features Analysis [48], Dropout [50], Zoneout [51].
- **계층적 잠재 변수 모델**: 순차 데이터용 계층적 VAE [66], [67].
- **음향 단위 발견 시스템**: DP-GMM [68], HDP-HMM [69], [70], [71], HMM-VAE [72], [73].
- **비지도 음성 인식**: Segmental autoencoder [75], 음성에서 단어 임베딩 학습 [76], ZeroSpeech 2017 참가 시스템 [58], [59], [60], [61].

## 🛠️ Methodology

본 논문은 WaveNet 오토인코더 기반의 음성 표현 학습 모델을 제안하며, 크게 인코더, 병목(bottleneck), WaveNet 디코더의 세 부분으로 구성됩니다 (그림 1 참조).

1. **인코더 (Encoder)**:

   - 입력으로 원시 오디오 샘플 또는 오디오 특징(예: MFCC + 첫 번째 및 두 번째 시간 미분값)을 받습니다.
   - 잔여 컨볼루션 네트워크(residual convnet)로 구성되며, 입력 신호에서 잠재 벡터 시퀀스를 추출합니다.
   - 스트라이드 컨볼루션(strided convolutions)을 통해 잠재 벡터 추출 주파수를 제어합니다 (예: 50Hz 또는 25Hz).
   - 실험을 통해 13 MFCC 특징을 사용하는 것이 가장 좋은 결과를 보였습니다.

2. **병목 (Bottleneck)**:

   - 인코더가 추출한 은닉 벡터를 잠재 표현으로 변환하는 핵심 부분입니다. 세 가지 변형을 비교했습니다.
     - **단순 차원 축소 (AE)**: 잠재 표현의 차원을 단순히 줄이는 선형 레이어 (예: 64차원).
     - **가우시안 변이형 오토인코더 (VAE)**: 인코더가 잠재 표현 $z$에 대한 분포 $q(z|x;\varphi)$를 생성하고, 이 분포에서 샘플링합니다. 다음 목적 함수를 사용합니다:
       $$J_{\text{VAE}}(\theta,\varphi;x) = \text{E}_{q(z|x;\varphi)}[\text{log} p(x|z;\theta)] - \text{max} (B,D_{\text{KL}}(q(z|x;\varphi)||p(z)))$$
       여기서 $B$는 "자유 정보(free information)"의 양을 나타내어 KL 다이버전스(KL divergence)에 대한 페널티를 제어합니다.
     - **벡터 양자화 변이형 오토인코더 (VQ-VAE)**: 이산적인 잠재 표현을 학습합니다.
       - 사전 정의된 $K$개의 프로토타입 벡터 $\{e_i\}_{i=1}^K$를 유지합니다.
       - 인코더 출력 $z_e(x)$에 가장 가까운 프로토타입 $q(x) = \text{argmin}_i \|z_e(x)-e_i\|_2^2$를 찾아 잠재 표현 $z_q(x) = e_{q(x)}$으로 사용합니다.
       - 손실 함수는 재구성 손실, 프로토타입과 할당된 벡터 간의 거리, 그리고 commitment loss를 포함합니다:
         $$L= \text{log}p(x|z_q(x)) + \|sg(z_e(x)) - e_{q(x)}\|_2^2 + \gamma\|z_e(x)-sg(e_{q(x)})\|_2^2$$
         여기서 $sg(\cdot)$는 역전파 시 그래디언트를 0으로 만드는 stop-gradient 연산입니다.

3. **WaveNet 디코더 (WaveNet Decoder)**:

   - WaveNet [18] 기반의 자가회귀(autoregressive) 네트워크로, 원시 파형 샘플을 재구성합니다.
   - 병목에서 나온 잠재 표현과 별도의 화자 임베딩(one-hot vector)을 조건으로 사용합니다.
   - 과거 샘플에 대한 자가회귀 정보, 화자에 대한 전역 정보, 인코더에서 추출된 잠재 정보를 결합하여 샘플을 생성합니다.
   - 화자 정체성에 디코더를 명시적으로 조건화함으로써 인코더는 화자 종속적인 정보를 잠재 표현에 담을 필요가 없어집니다.

4. **시간-지터 정규화 (Time-jitter Regularization)**:

   - 훈련 중에 각 잠재 벡터가 이웃한 벡터(이전 또는 다음 시간 스텝)로 대체될 확률을 0.12로 설정합니다.
   - 드롭아웃(dropout)에서 영감을 받았으며, 잠재 벡터의 상호 적응(co-adaptation)을 방지하고 시간 경과에 따른 표현 안정성을 촉진합니다. 병목 바로 뒤에 적용됩니다.

5. **훈련**: LibriSpeech (clean subset) 및 ZeroSpeech 2017 데이터셋을 사용했습니다. Adam 옵티마이저와 Polyak averaging을 적용했습니다.

## 📊 Results

- **병목 비교**:
  - **VQ-VAE**: 음성 내용(phoneme) 예측 정확도는 높게 유지하면서, 화자 정체성 및 성별 예측 정확도는 크게 낮추어(화자 0% 근접, 성별 50% 근접) 가장 강력한 정보 분리 능력을 보였습니다. 이는 VQ-VAE가 음성 내용에 불변하는 표현을 학습하는 데 가장 적합함을 시사합니다.
  - **AE (단순 차원 축소)**: 가장 적은 정보를 버리며, 화자와 음성 정보 모두에 대한 예측력이 높게 유지되어 다양한 속성이 혼합된 표현을 생성했습니다.
  - **VAE**: AE보다 정보 분리 능력이 좋지만, VQ-VAE만큼 효과적이지는 않았습니다. 음소 예측 정확도는 낮아지고 성별 예측 정확도는 높아지는 경향을 보였습니다.
- **VQ-VAE 토큰 해석 가능성**:
  - VQ-VAE 토큰을 음소에 매핑하는 프레임 단위 음소 인식 정확도를 측정했습니다.
  - 32768개의 토큰을 사용한 모델은 64.5%의 최고 정확도를 달성했으며, 토큰 수가 증가할수록 정확도가 향상되는 경향을 보였습니다 (4096개 토큰에서 가장 큰 정확도 향상).
- **ZeroSpeech 2017 음향 단위 발견**:
  - ABX 테스트에서 VQ-VAE 모델이 영어와 프랑스어(충분히 큰 훈련 데이터셋)에서 이전 최고 점수를 능가하는 성능을 보였습니다.
  - 만다린어(중국어)에서는 훈련 데이터셋 부족과 MFCC가 성조 정보를 버리는 특성 때문에 성능이 상대적으로 낮았으나, 다국어 훈련을 통해 일부 개선되었습니다.
  - 화자 간(across-speaker) 평가에서 VQ-VAE의 양자화(quantization) 단계가 화자 종속적인 정보를 제거하여 성능 향상에 기여했습니다.
- **하이퍼파라미터 영향**:
  - **시간-지터 정규화**: 음소 매핑 정확도와 ZeroSpeech ABX 점수를 크게 향상시켰으며, 특히 50Hz와 같은 높은 토큰 프레임 레이트에서 좋은 성능을 유지하는 데 중요했습니다. 또한, 표현을 화자에 덜 민감하게 만들었습니다.
  - **입력 표현**: 원시 파형보다 MFCC 또는 log-mel spectrogram 특징을 사용하는 것이 더 나은 성능을 보였습니다.
  - **디코더 WaveNet 수용 필드**: WaveNet의 수용 필드가 10ms 이상으로 커질 때 조건부 신호의 성별 불변성이 크게 증가했습니다.
  - **인코더 수용 필드**: 토큰 당 약 0.3초의 입력 신호를 보는 것이 최적의 성능을 보였습니다.
  - **병목 비트레이트**: 토큰 레이트(token rate)는 중요한 매개변수였으며, 25Hz 또는 50Hz에서 가장 좋은 결과를 얻었습니다.

## 🧠 Insights & Discussion

이 논문은 WaveNet 오토인코더와 VQ-VAE 병목을 결합하여 비지도 방식으로 음성 표현을 학습하는 데 있어 중요한 통찰력을 제공합니다.

- **정보 분리의 중요성**: 디코더에 화자 정보를 명시적으로 조건화하고, 인코더에 병목(bottleneck)을 적용하는 것이 음성 내용과 화자 특성을 효과적으로 분리하는 데 필수적임을 입증했습니다. 이는 ASR과 같은 후속 작업에서 더 유용하고 일반화 가능한 표현을 제공합니다.
- **VQ-VAE의 역할**: VAE가 노이즈 주입을 통해 정보를 제한하는 반면, VQ-VAE는 양자화 메커니즘을 통해 인코딩이 특정 프로토타입과 동일하도록 강제합니다. 실험 결과, VQ-VAE의 이산적이고 결정론적인 양자화가 VAE보다 더 나은 정보 분리를 가능하게 했습니다.
- **시간-지터 정규화의 효과**: 제안된 시간-지터 정규화는 잠재 공간 붕괴 문제를 방지하고, 잠재 코드의 용량을 제한하면서도 시간적 안정성을 촉진하여 표현의 품질을 크게 향상시켰습니다. 이러한 기법은 다른 자가회귀 디코더를 사용하는 잠재 변수 모델에도 적용될 수 있는 일반성을 가집니다.
- **하이퍼파라미터 최적화의 중요성**: 인코더와 디코더의 수용 필드(receptive field) 크기를 신중하게 최적화하는 것이 모델 성능에 결정적인 영향을 미친다는 것을 발견했습니다. 이는 모델 설계 시 중요한 고려사항입니다.
- **제한 사항 및 향후 연구**:
  - VQ-VAE 토큰이 너무 많아 각 토큰에 개별 의미를 부여하기 어려운 경우가 있습니다. 더 구조화된 잠재 표현이 필요할 수 있습니다.
  - 만다린어와 같은 성조 언어에서 MFCC가 피치 정보를 버리는 문제로 인해 성능이 저하될 수 있습니다. 다중 스케일 모델링이나 음성 처리에 특화된 페널티(예: HMM 사전 정보)를 도입하여 운율 정보를 더 잘 포착하는 것이 필요합니다.
  - 더 큰 비레이블 데이터셋을 활용하기 위한 모델 용량 증대 방법 연구가 필요합니다.

## 📌 TL;DR

**문제**: 음소 정보를 포착하면서 화자 정체성 및 저수준 음향 세부 사항에 불변하는 음성 표현을 비지도 학습으로 얻는 것이 목표입니다.
**방법**: WaveNet 오토인코더를 사용하여 이 문제를 해결했습니다. 인코더는 MFCC 특징을 입력으로 받고, 디코더는 원시 파형을 재구성합니다. 특히, VQ-VAE(Vector Quantized Variational Autoencoder)를 병목(bottleneck)으로 사용하고, 잠재 표현의 시간적 안정성을 높이고 잠재 공간 붕괴를 막기 위한 시간-지터(time-jitter) 정규화를 도입했습니다. 디코더는 화자 정보에 명시적으로 조건화되어 인코더가 화자 불변 특징에 집중하게 합니다.
**결과**: VQ-VAE 모델은 음성 내용과 화자 정보 간의 뛰어난 분리 능력을 보였으며, 추출된 토큰들은 음소에 높은 정확도로 매핑 가능했습니다. 또한, ZeroSpeech 2017 음향 단위 발견 챌린지에서 최고 수준의 성능을 달성하여, 제안된 접근 방식이 비지도 음성 표현 학습에 효과적임을 입증했습니다.
