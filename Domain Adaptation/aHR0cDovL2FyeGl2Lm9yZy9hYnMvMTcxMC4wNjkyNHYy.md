# VisDA: The Visual Domain Adaptation Challenge

Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, Kate Saenko

## 🧩 Problem to Solve

기계 학습 모델은 훈련 데이터와 다른 분포를 가진 새로운 도메인의 데이터("도메인 시프트")에 적용될 때 성능이 크게 저하되는 문제가 있습니다. 특히, 실제 이미지 데이터셋을 구축하는 데 필요한 막대한 라벨링 비용과 시간이 걸리기 때문에, 레이블이 풍부한 합성(synthetic) 데이터로 훈련한 모델을 레이블이 없는 실제(real) 데이터 도메인에 효과적으로 적응시키는 비지도 도메인 적응(Unsupervised Domain Adaptation, UDA) 기술의 발전이 중요합니다. 그러나 기존의 도메인 적응 벤치마크는 데이터셋 규모가 작거나, 작업 다양성이 부족하거나, 도메인 시프트의 정도가 미미하여 실제 문제 해결을 위한 진전을 이끌어내기 어려웠습니다.

## ✨ Key Contributions

- **VisDA 2017 데이터셋 및 챌린지 소개:** 시뮬레이션-현실(simulation-to-reality) 도메인 시프트를 다루는 대규모 비지도 도메인 적응(UDA) 벤치마크를 제시합니다.
- **대규모 크로스 도메인 데이터셋 구축:**
  - **VisDA-C (이미지 분류):** 12개 범주에 걸쳐 총 28만 개 이상의 이미지를 포함하는 현재까지 가장 큰 크로스 도메인 객체 분류 데이터셋입니다.
  - **VisDA-S (이미지 분할):** 18개 범주에 걸쳐 총 3만 개 이상의 이미지를 포함하는 대규모 의미론적 분할 데이터셋입니다.
- **현실적인 평가 프로토콜 제시:** 하이퍼파라미터 튜닝을 위한 **검증(validation) 도메인**과 최종 모델 평가를 위한 **테스트(test) 도메인**을 별도로 제공하여, 레이블이 없는 목표 도메인에서의 실제 배포 시나리오를 반영합니다.
- **베이스라인 성능 분석 및 챌린지 결과 공개:** 다양한 도메인 적응 알고리즘(DAN, D-CORAL 등)의 베이스라인 성능을 제공하고, 챌린지 참가자들의 상위 결과를 공개하여 현재 기술 수준을 파악하고 향후 연구 방향을 제시합니다.
- **향후 연구를 위한 도전 과제 제시:** 데이터셋 난이도 증가 방안(VisDA-C-ext, ImageNet 사전 훈련 없이 학습 등)을 제안하고, 데이터 생성 도구 및 메타데이터 공개를 통해 커뮤니티의 추가 연구를 장려합니다.

## 📎 Related Works

- **시각 도메인 적응 연구:** 초기 얕은 특징(shallow feature) 기반 방법부터 최근 딥러닝 기반 접근 방식에 이르기까지 광범위한 연구가 진행되었습니다.
- **기존 벤치마크 데이터셋:**
  - **분류 데이터셋:** MNIST, USPS, SVHN (숫자), Office (실세계 객체), Cross-Dataset Testbed (혼합 객체). 이러한 데이터셋은 크기가 작거나 도메인 시프트가 미미하다는 한계가 있습니다.
  - **의미론적 분할 데이터셋:** SYNTHIA, CityScapes, GTA5, Virtual KITTI (도시 경관 대시캠). 이들을 조합하여 도메인 적응 실험에 활용했습니다.
  - **합성 데이터셋:** ModelNet, PASCAL3D+, ObjectNet3D, ShapeNet 등 3D 모델 기반 데이터셋이 존재하지만, VisDA-C는 특정 클래스에 대해 균형 잡힌 대규모 컬렉션을 제공합니다.
- **챌린지 기반 혁신:** 대부분의 기존 챌린지는 동일 도메인 내에서 데이터를 분할하지만, VisDA는 도메인 시프트를 핵심으로 다룹니다.

## 🛠️ Methodology

본 논문은 새로운 방법론을 제안하기보다는 대규모 데이터셋과 챌린지 환경을 구축하는 데 중점을 둡니다.

### 데이터셋 구축

- **VisDA-C (Classification) 데이터셋:**
  - **훈련 도메인(소스):** ShapenetCore, NTU 3D, SHREC 2010, SketchUp 3D Warehouse에서 수집된 1,907개의 3D 모델을 사용하여 152,397개의 합성 이미지를 렌더링했습니다. 다양한 카메라 앵글(20개 조합), 조명 조건(4개 방향)을 적용하고, 객체 회전, 스케일 조절, 불필요한 면 제거 등의 전처리 과정을 거쳤습니다.
  - **검증 도메인(타겟):** Microsoft COCO [25] 데이터셋에서 12개 객체 카테고리에 해당하는 55,388개의 실제 이미지 패치를 추출했습니다. 이미지 크롭 시 패딩을 추가하고, 작은 이미지를 제외했으며, "person" 카테고리 이미지는 균형을 위해 수를 줄였습니다.
  - **테스트 도메인(타겟):** YouTube Bounding Boxes (YT-BB) [32] 데이터셋에서 12개 카테고리에 해당하는 72,372개의 실제 이미지 패치를 추출했습니다. YT-BB 이미지는 COCO보다 해상도가 낮은 특징이 있습니다.
- **VisDA-S (Semantic Segmentation) 데이터셋:**
  - **훈련 도메인(소스):** GTA5 [33] 데이터셋에서 24,966개의 고품질 합성 대시캠 이미지와 픽셀 수준의 의미론적 라벨을 사용했습니다.
  - **검증 도메인(타겟):** CityScapes [7] 데이터셋에서 5,000개의 실제 도시 대시캠 이미지와 라벨을 사용했습니다.
  - **테스트 도메인(타겟):** Berkeley Deep Drive/Nexar [1] 데이터셋에서 1,500개의 라벨이 없는 실제 대시캠 이미지를 사용했습니다.

### 베이스라인 실험 및 챌린지 평가

- **분류 작업:** AlexNet [20] 및 ResNext-152 [54] CNN 아키텍처를 기본 모델로 사용했습니다. ImageNet [35] 사전 훈련된 가중치로 초기화하고, 마지막 분류 레이어는 무작위 초기화 후 학습률을 10배 높였습니다.
  - **비교 알고리즘:** DAN (Deep Adaptation Network) [26] 및 Deep CORAL [40]을 비지도 도메인 적응 베이스라인으로 평가했습니다.
  - **평가 지표:** 범주별 평균 정확도.
- **분할 작업:** Dilated Fully Convolutional Network를 기본 모델로 사용하고, Hoffman et al. (2016) [16]에서 제안된 전역 및 카테고리별 적응 기법을 적용했습니다.
  - **평가 지표:** Intersection over Union (IoU), 특히 mean IoU (mIoU).

## 📊 Results

### VisDA-C (Classification)

- **베이스라인 결과:**
  - AlexNet을 사용한 소스-온리(Source-only) 모델은 검증 도메인에서 28.12%, 테스트 도메인에서 30.81%의 평균 정확도를 기록했습니다.
  - DAN은 검증 도메인에서 51.62% (83.6% 상대 개선), 테스트 도메인에서 49.78% (61.57% 상대 개선)를 달성했습니다.
  - Deep CORAL은 검증 도메인에서 45.53% (61.9% 상대 개선), 테스트 도메인에서 45.29% (47.0% 상대 개선)를 달성했습니다.
  - 오라클(In-domain) 성능은 합성 도메인에서 99.92%, 실제 검증 도메인에서 87.62%, 실제 테스트 도메인(ResNext)에서 93.40%였습니다.
- **챌린지 상위 결과:**
  - **GFColourLabUEA 팀**이 테스트 도메인에서 92.8%의 정확도를 달성하며 최고 성능을 기록했습니다. 이는 소스 성능(45.3%) 대비 104%의 놀라운 상대 개선입니다. 이 팀은 semi-supervised learning의 $\Pi$-model [21]과 mean teacher model [43]에 기반한 label propagation 알고리즘 [10]을 사용했습니다.
  - NLEDA 및 BUPTOVERFIT 팀은 각각 87.7%, 85.4%를 달성하며 MMD(Maximum Mean Discrepancy) 변형 및 앙상블 기법을 활용했습니다.

### VisDA-S (Semantic Segmentation)

- **베이스라인 결과:**
  - 소스-온리 모델은 검증 도메인에서 21.4% mIoU, 테스트 도메인에서 25.9% mIoU를 기록했습니다.
  - [16]의 적응 방법은 검증 도메인에서 mIoU를 25.5%로 개선했습니다.
  - 오라클(In-domain) 성능은 검증 도메인에서 64.0% mIoU였습니다.
- **챌린지 상위 결과:**
  - **MSRA 팀**이 테스트 도메인에서 47.5% mIoU를 달성하며 최고 성능을 기록했습니다. 이 팀은 프레임 수준의 판별자(discriminator)를 사용한 스타일 트랜스퍼와 픽셀 수준의 판별자를 위한 앙상블 모델(ResNet-101, ResNet-152, SE-ResNeXt-101) 및 피라미드 공간 풀링(Pyramid Spatial Pooling)을 결합한 다단계 절차를 사용했습니다.

## 🧠 Insights & Discussion

- **챌린지의 성공:** VisDA 챌린지 참가자들은 분류 작업에서 소스-온리 모델보다 훨씬 우수한 성능을 달성했으며, 이는 대규모 합성-현실 도메인 적응 문제 해결에 큰 진전이 있었음을 보여줍니다.
- **새로운 도전 과제:**
  - **난이도 증가:** 챌린지 참가자들이 매우 높은 성능을 달성했으므로, 연구자들에게 더 큰 도전을 주기 위해 검증/테스트 도메인 교환(MS COCO가 더 어려움), 작은/불규칙한 이미지를 포함하는 VisDA-C-ext 확장 버전 데이터셋, 그리고 ImageNet 사전 훈련 없는 설정 등을 제안합니다.
  - **사전 훈련의 한계:** 대부분의 참가자가 ImageNet 사전 훈련 모델을 활용했는데, 이는 의료 영상이나 로봇 공학 등 ImageNet과 같은 대규모 레이블 데이터셋이 없는 도메인에는 적용하기 어렵습니다. 따라서 사전 훈련에 의존하지 않는 도메인 적응 알고리즘 개발이 중요합니다.
  - **다양한 변형:** 배경, 텍스처, 시점(POV) 변화에 대한 강건성을 테스트할 수 있는 데이터셋 생성 도구 및 메타데이터를 공개하여 연구자들이 특정 가설을 검증하고 새로운 작업을 탐구할 수 있도록 지원합니다.
- **실용적 중요성:** 합성 데이터로부터 학습하여 실제 도메인에 적용하는 능력은 레이블링 비용이 높은 분야에서 매우 중요합니다. VisDA 벤치마크는 이러한 실질적인 문제를 해결하기 위한 모델 개발의 기반을 제공합니다.

## 📌 TL;DR

VisDA는 대규모 **합성-현실 도메인 시프트**를 다루는 **비지도 도메인 적응(UDA) 챌린지**를 위한 데이터셋입니다. **이미지 분류(VisDA-C)** 및 **의미론적 분할(VisDA-S)** 두 가지 작업으로 구성되며, 각각 합성 소스 도메인과 두 개의 서로 다른 실제 타겟 도메인(검증, 테스트)을 제공하여 현실적인 평가를 가능하게 합니다. 챌린지 결과는 베이스라인을 크게 뛰어넘는 인상적인 성능 개선을 보여주었으며, ImageNet 사전 훈련 없이 학습하는 등 **더 어려운 도전 과제**를 제시하여 UDA 연구의 미래 방향을 설정합니다.
