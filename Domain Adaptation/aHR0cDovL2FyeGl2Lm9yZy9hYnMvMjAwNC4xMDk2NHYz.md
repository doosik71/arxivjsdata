# Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks

Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith

## 🧩 Problem to Solve

이 논문은 방대한 양의 이질적인 텍스트로 사전 학습된 최신 언어 모델(예: RoBERTa)이 특정 도메인이나 작업에 대해 추가적으로 조정될 때 여전히 성능 향상에 도움이 되는지 여부를 탐구합니다. 즉, 광범위하게 사전 학습된 모델이 보편적으로 작동하는지, 아니면 특정 도메인에 맞는 별도의 사전 학습 모델을 구축하는 것이 여전히 유용한지 밝히는 것이 목표입니다.

## ✨ Key Contributions

- 낮은 리소스 및 높은 리소스 환경 모두에서 4개 도메인 및 8개 분류 태스크에 걸쳐 **도메인 적응형 사전 학습(DAPT)** 및 **태스크 적응형 사전 학습(TAPT)** 에 대한 철저한 분석을 제공합니다.
- 적응형 언어 모델의 도메인 및 태스크 간 **전이성(transferability)** 에 대한 조사를 수행합니다.
- 인간이 큐레이션한 데이터셋을 이용한 사전 학습의 중요성과 이 성능에 근접하기 위한 간단한 **데이터 선택 전략(kNN-TAPT)** 의 효과를 강조하는 연구를 제시합니다.
- 다단계 적응형 사전 학습이 태스크 성능에 상당한 이득을 제공함을 일관되게 보여줍니다.

## 📎 Related Works

- **도메인 적응을 위한 전이 학습:** 도메인 내에서 지속적인 사전 학습의 이점을 보여준 기존 연구들(예: Alsentzer et al., 2019; Lee et al., 2019)이 있으며, 이 논문은 대규모 사전 학습 말뭉치와 목표 도메인 간의 변화가 태스크 성능에 미치는 영향에 대한 추가 조사를 제공합니다.
- **태스크 적응형 사전 학습:** 주어진 태스크의 비지도 데이터에 대한 지속적인 사전 학습(TAPT)이 최종 태스크 성능에 유익하다는 점(예: Howard and Ruder, 2018; Sun et al., 2019)이 제시되었습니다. 또한, DAPT와 TAPT의 상호작용 및 데이터셋 크기와의 관계를 비교합니다.
- **전이 학습을 위한 데이터 선택:** 전이 학습을 위한 데이터 선택은 NLP 분야에서 탐구되어 왔으며(Moore and Lewis, 2010; Ruder and Plank, 2017), 이 연구는 특정 태스크에 관련된 예시를 선택하는 데 중점을 둡니다.

## 🛠️ Methodology

이 연구는 RoBERTa를 기반 모델로 사용하여 여러 단계의 적응형 사전 학습을 탐구합니다.

1. **RoBERTa (Baseline):** 기본 RoBERTa-base 모델을 사용하며, 각 분류 태스크에 대해 파라미터 미세 조정을 수행합니다.
2. **도메인 적응형 사전 학습 (DAPT):** RoBERTa를 대규모 도메인 특정 비지도 텍스트 코퍼스(BIOMED, CS, NEWS, REVIEWS)에서 추가 사전 학습합니다.
3. **태스크 적응형 사전 학습 (TAPT):** RoBERTa를 특정 태스크에 사용 가능한 비지도 학습 데이터셋에서 추가 사전 학습합니다. 이는 DAPT보다 훨씬 작은 코퍼스를 사용하지만, 태스크와 더욱 직접적으로 관련됩니다.
4. **DAPT + TAPT:** 먼저 DAPT를 적용한 후, 이어서 TAPT를 적용합니다. 이 방식은 도메인 적응과 태스크 적응을 결합한 것입니다.
5. **인간 큐레이션 TAPT (Curated-TAPT):** 태스크 분포와 유사한 더 큰 규모의 인간이 큐레이션한 비지도 데이터 풀을 활용하여 TAPT를 수행합니다.
6. **자동 데이터 선택 TAPT (kNN-TAPT):** 대규모 도메인 내 코퍼스에서 태스크 관련 비지도 텍스트를 자동으로 검색하는 방법을 제안합니다. 경량형 bag-of-words 언어 모델인 VAMPIRE를 사용하여 태스크 및 도메인 텍스트를 공유 벡터 공간에 임베딩한 다음, 태스크 데이터에 대한 질의를 기반으로 도메인에서 $k$개의 최인접 이웃(nearest neighbors)을 선택하여 TAPT 코퍼스를 증강합니다.

## 📊 Results

- **DAPT:** RoBERTa의 사전 학습 도메인과 거리가 먼 도메인(BIOMED, CS, REVIEWS)에서 모든 태스크에 걸쳐 일관된 성능 향상을 보였습니다. 무관한 도메인에 적응하는 것(¬DAPT)은 대부분의 경우 최종 태스크 성능에 해로웠습니다.
- **TAPT:** 모든 도메인에 걸쳐 RoBERTa baseline을 일관되게 개선했으며, 일부 태스크에서는 DAPT 성능과 일치하거나 능가했습니다. TAPT는 DAPT보다 훨씬 적은 컴퓨팅 자원을 필요로 합니다.
- **DAPT + TAPT:** 도메인 적응 후에 태스크 적응을 적용하는 다단계 사전 학습이 모든 태스크에서 최고의 성능을 달성했습니다.
- **Curated-TAPT:** 인간이 큐레이션한 대규모 비지도 데이터를 활용한 TAPT는 이전의 TAPT 결과를 더욱 향상시켰습니다. 특히, RCT-500 태스크에서 Curated-TAPT는 전체 레이블링된 데이터의 0.3%만 가지고도 DAPT+TAPT의 95% 성능을 달성하여 레이블링된 데이터가 적은 상황에서의 엄청난 이점을 보여주었습니다.
- **kNN-TAPT:** 자동 데이터 선택 방식인 kNN-TAPT는 모든 경우에서 TAPT보다 뛰어났으며, $k$가 증가함에 따라 DAPT 성능에 근접했습니다. 무작위 선택(RAND-TAPT)은 일반적으로 kNN-TAPT보다 성능이 낮았습니다.
- **태스크 간 전이:** TAPT는 단일 태스크 성능에 최적화되어 동일 도메인 내 다른 태스크로의 전이에는 해로울 수 있음을 보여주었습니다. 이는 도메인 내 태스크 데이터 분포가 다를 수 있음을 시사합니다.

## 🧠 Insights & Discussion

- **특정 도메인/태스크 적응의 중요성:** 수억 개의 매개변수를 가진 대규모 사전 학습 언어 모델조차도 단일 텍스트 도메인의 복잡성을 완전히 인코딩하기는 어렵습니다. 따라서 모델을 특정 도메인이나 태스크에 맞춰 추가적으로 사전 학습하는 것이 상당한 이점을 제공합니다.
- **다단계 적응의 효과:** DAPT와 TAPT를 결합하는 다단계 적응형 사전 학습은 도메인과 태스크 인식을 모두 활용하여 최고의 성능을 달성하는 효과적인 전략입니다.
- **비용-효과성:** TAPT는 DAPT보다 훨씬 적은 컴퓨팅 자원을 요구하면서도 종종 DAPT에 필적하는 성능을 제공하여 비용 효율적인 적응 기술임을 입증합니다.
- **데이터 큐레이션 및 선택의 가치:** 태스크 분포에서 대규모 비지도 데이터를 큐레이션하는 것이 최종 태스크 성능에 매우 유익합니다. 또한, kNN-TAPT와 같은 자동화된 데이터 선택 방법은 DAPT보다 저렴한 비용으로 DAPT에 근접하는 성능을 달성할 수 있어 리소스가 제한된 환경에서 유용합니다.
- **도메인 경계의 모호성:** 도메인 간의 어휘 중첩 및 상호 전이 가능성은 도메인 경계가 명확하지 않고 미묘한 스펙트럼으로 존재할 수 있음을 시사합니다.

## 📌 TL;DR

대규모 언어 모델(RoBERTa)은 광범위한 사전 학습에도 불구하고, **도메인 적응형 사전 학습(DAPT)** 과 **태스크 적응형 사전 학습(TAPT)** 을 통해 특정 도메인 및 태스크에 추가로 조정될 때 상당한 성능 향상을 보입니다. 특히, DAPT 후 TAPT를 적용하는 다단계 접근 방식이 가장 효과적이며, TAPT는 계산 효율성이 높습니다. 또한, 인간이 큐레이션한 데이터나 자동 선택된 데이터를 활용하여 태스크 관련 비지도 코퍼스를 확장하는 것이 모델 성능을 크게 향상시킬 수 있음을 보여주며, 이는 더욱 전문화된 언어 모델 구축의 중요성을 강조합니다.
