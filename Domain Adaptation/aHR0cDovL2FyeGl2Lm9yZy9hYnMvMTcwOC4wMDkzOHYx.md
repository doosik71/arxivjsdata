# Associative Domain Adaptation

Philip Haeusser, Thomas Frerix, Alexander Mordvintsev, Daniel Cremers

## 🧩 Problem to Solve

신경망 훈련 시 대규모의 레이블링된 데이터가 필요한데, 이를 확보하는 데 비용이 많이 들거나 불가능한 경우가 많습니다. 본 논문은 이러한 문제를 해결하기 위한 도메인 적응(Domain Adaptation)에 초점을 맞춥니다. 도메인 적응은 레이블이 있는 소스 도메인(Source Domain) 데이터를 사용하여, 훈련 시 레이블이 없는 타겟 도메인(Target Domain) 데이터에 대한 모델의 테스트 오류를 최소화하는 것을 목표로 합니다. 소스 및 타겟 도메인의 데이터 분포가 서로 다르기 때문에, 이 두 도메인 간의 차이를 줄이면서도 분류 성능을 유지하는 것이 핵심 과제입니다.

## ✨ Key Contributions

- 신경망을 이용한 도메인 적응을 위한 직관적이고 효과적인 훈련 방안인 연관 도메인 적응(Associative Domain Adaptation)을 제안합니다.
- 기존 도메인 적응 방법론에 제안하는 접근 방식을 통합하고, 가장 널리 사용되는 명시적 유사도 손실 $L_{sim}$인 최대 평균 불일치(Maximum Mean Discrepancy, MMD)와 상세히 비교 분석합니다.
- 임의의 신경망 아키텍처에 쉽게 적용할 수 있는 간단한 구현 방안을 제시하며, 구조적 또는 계산적 오버헤드가 거의 없습니다.
- 다양한 도메인 적응 벤치마크에서 관련 딥러닝 방법들을 능가하는 최첨단(state-of-the-art) 성능을 달성합니다.
- 연관 도메인 적응이 타겟 도메인 샘플 분류에 더 효과적인 임베딩을 생성함을 상세한 분석을 통해 입증합니다.

## 📎 Related Works

- **CORAL ([24, 25])**: 타겟 데이터의 공분산을 소스 데이터에 명시적으로 강제하여 통계적 정렬을 수행합니다.
- **적대적 훈련 기반 방법 (Adversarial Training) ([9, 3])**: 특징 추출기 위에 클래스 레이블 및 도메인 레이블 예측기를 구축하여 특징 분포를 유사하게 만듭니다. $DANN [9]$은 특징 공간에서 도메인 불변 특징을 학습하고, $DSN [3]$은 데이터 공간에서 적대적 접근 방식을 사용합니다.
- **최대 평균 불일치 (MMD) 기반 방법 ([11, 15, 20, 4])**: Reproducing Kernel Hilbert Space에서 두 확률 분포의 평균 임베딩 간 거리를 최소화하여 잠재 특징 불일치를 줄입니다. $Deep Adaptation Network (DAN) [15]$은 MMD를 사용하여 여러 계층 간의 불일치를 최소화하고, $[20]$ 및 $[4]$는 공유 및 개별 표현을 사용하여 도메인 불변 특징을 학습합니다.

## 🛠️ Methodology

본 논문은 반지도 학습(semi-supervised learning)을 위한 "Learning by Association" [12] 방법을 도메인 적응에 맞게 일반화한 **연관 도메인 적응(Associative Domain Adaptation)**을 제안합니다.

1. **전체 손실 함수**:
   전체 손실 $L$은 분류 손실 $L_{classification}$과 연관 손실 $L_{assoc}$의 합으로 구성됩니다.
   $$L = L_{classification} + \alpha L_{assoc}$$
   $L_{classification}$은 레이블링된 소스 도메인에 대한 분류 오류를 최소화하여 판별(discrimination)을 강화합니다. $L_{assoc}$는 소스 및 타겟 샘플에 대해 통계적으로 도메인 불변적인 임베딩을 생성하여 동화(assimilation)를 유도합니다.

2. **연관 손실 $L_{assoc}$**:
   $L_{assoc}$는 두 가지 구성 요소로 이루어집니다.
   $$L_{assoc} = \beta_1 L_{walker} + \beta_2 L_{visit}$$

   - **워커 손실 ($L_{walker}$)**:
     소스 임베딩 $A_i$에서 타겟 임베딩 $B_j$를 거쳐 다시 소스 임베딩 $A_j$로 돌아오는 두 단계 왕복 확률 $P_{aba}$를 기반으로 합니다. $P_{aba}$는 같은 클래스 내의 모든 연관 순환이 균일한 확률을 갖도록 강제하는 교차 엔트로피 손실로 정의됩니다.
     $$P_{ab}(B_j|A_i) = \frac{\exp(\langle A_i, B_j \rangle)}{\sum_{j'} \exp(\langle A_i, B_{j'} \rangle)}$$
     $$P_{aba} = (P_{ab} P_{ba})$$
     $$T_{ij} = \begin{cases} 1/|A_{class(A_i)}| & \text{class}(A_i) = \text{class}(A_j) \\ 0 & \text{else} \end{cases}$$
     $$L_{walker} = H(T, P_{aba})$$
     여기서 $H$는 교차 엔트로피를 나타냅니다. 이 손실은 소스 레이블 정보를 활용하여 클래스 클러스터 간의 원치 않는 동화를 방지합니다.
   - **방문 손실 ($L_{visit}$)**:
     $L_{walker}$만으로는 네트워크가 쉽게 연관될 수 있는 타겟 샘플만 방문하게 되어 일반화가 저하될 수 있습니다. 이를 방지하기 위해 $L_{visit}$은 각 타겟 샘플이 균일한 확률로 방문되도록 하는 정규화 역할을 합니다.
     $$P_{visit}(B_j) = \sum_{x_i \in D_s} P_{ab}(B_j|A_i)$$
     $$V_j = \frac{1}{|D_t|}$$
     $$L_{visit} = H(V, P_{visit})$$

3. **훈련 설정**:
   - **네트워크 아키텍처**: 모든 실험에 대해 $C(32,3) → C(32,3) → P(2) → C(64,3) → C(64,3) → P(2) → C(128,3) → C(128,3) → P(2) → FC(128)$와 같은 일반적인 CNN 아키텍처를 사용합니다. 임베딩 크기는 128입니다.
   - **하이퍼파라미터**: 초기 학습률 $1e-4$를 사용하고, $L_{assoc}$의 $$\alpha$$는 훈련 시작 후 일정 반복(delay) 후에 활성화되는 스텝 함수로 설정하여, 네트워크가 먼저 기본적인 클래스 구조를 학습하도록 합니다. 미니 배치 크기는 모든 클래스가 충분히 대표되도록 조절합니다. $L_{visit}$의 가중치 $$\beta_2$$는 도메인 간 클래스 분포가 다를 경우 낮게 설정합니다.

## 📊 Results

- **도메인 적응 벤치마크 성능**: MNIST→MNIST-M, Synth→SVHN, SVHN→MNIST, SynthSigns→GTSRB의 4가지 벤치마크에서 모두 기존의 최첨단 딥러닝 방법들을 능가하는 성능(가장 낮은 타겟 테스트 오류)을 달성했습니다. 예를 들어, SVHN→MNIST에서 2.40%의 테스트 오류를 기록하여 이전 최고 성능을 크게 상회했습니다.
- **커버리지(Coverage)**: 소스 전용 훈련(Source only) 대비 타겟 전용 훈련(Target only) 간의 격차를 도메인 적응이 얼마나 효과적으로 메우는지 보여주는 커버리지 지표에서도 평균 87.17%의 향상을 보였습니다.
- **임베딩 품질 분석 (t-SNE)**: t-SNE 시각화 결과, 소스 전용 훈련 시 타겟 도메인 샘플이 흩어져 있는 반면, 연관 도메인 적응(DA$_{assoc}$) 훈련 후에는 소스 및 타겟 도메인 샘플이 모두 잘 클러스터링되고 명확히 분리되는 것을 확인했습니다. MMD 손실(DA$_{MMD}$)로 훈련했을 때는 분포는 유사했지만 클래스 판별력이 덜 명확했습니다.
- **임베딩 품질 분석 (MMD 값)**: MMD 값을 명시적으로 최소화하는 DA$_{MMD}$가 가장 낮은 MMD 값을 보였지만, DA$_{assoc}$는 SVHN→MNIST와 같은 일부 경우에서 DA$_{MMD}$보다 높은 MMD 값을 보였음에도 불구하고 훨씬 낮은 테스트 오류를 달성했습니다. 이는 낮은 MMD 값이 항상 낮은 타겟 테스트 오류와 상관관계가 있는 것은 아님을 시사합니다.

## 🧠 Insights & Discussion

- **L$_{assoc}$의 장점**: MMD는 소스 및 타겟 임베딩이 통계적으로 유사하게 되도록 강제하지만, 소스 레이블 정보를 활용하지 않아 클래스 판별력을 손상시킬 수 있습니다. 반면, $L_{assoc}$는 소스 도메인 클래스에 대한 지식을 통합하여, 통계적 유사성을 유지하면서도 클래스 클러스터 간의 원치 않는 동화를 방지하고 같은 클래스 내에서의 유사성을 강화합니다. 이로 인해 임베딩이 분류에 더욱 효과적으로 됩니다.
- **분포 이동 모델링**: $L_{assoc}$는 소스-타겟 분포 간의 이동을 명시적으로 모델링하여 도메인 적응 문제를 해결하는 데 필수적인 역할을 합니다. 이는 단순히 반지도 학습 방법을 도메인 적응에 적용하는 것과 차별화됩니다.
- **실용성**: 제안된 방법은 기존 신경망 아키텍처에 간단하게 추가할 수 있으며, 구조적, 계산적 오버헤드가 거의 없어 매우 실용적입니다. 일반적인 CNN 아키텍처로도 최첨단 결과를 달성하여, 방법론 자체의 효과를 입증했습니다.
- **MMD의 한계**: MMD는 잠재 특징 표현의 정렬에 유용하지만, 도메인 적응의 궁극적인 목표인 타겟 도메인 분류 정확도를 항상 보장하지는 않습니다. 본 연구는 MMD 값이 낮다고 해서 항상 분류 오류가 낮아지는 것은 아님을 실험적으로 보여주었습니다.

## 📌 TL;DR

본 논문은 신경망 기반의 새로운 도메인 적응 기법인 **연관 도메인 적응(Associative Domain Adaptation)**을 제안합니다. 이 방법은 레이블이 있는 소스 도메인에서의 분류 손실과, 소스 레이블 정보를 활용하여 소스-타겟 임베딩 간의 일관성을 강제하는 연관 손실 $L_{assoc}$($L_{walker}$와 $L_{visit}$으로 구성)을 결합한 통합 손실 함수를 최적화합니다. 이는 임베딩 공간에서 클래스 간의 구별력을 유지하면서 도메인 불변 특징을 학습하는 데 효과적입니다. 간단한 구현과 낮은 오버헤드로 다양한 도메인 적응 벤치마크에서 최첨단 성능을 달성했으며, MMD와 같은 기존 방법보다 타겟 도메인 분류에 더 효과적인 임베딩을 생성함을 정량적/정성적 분석으로 입증했습니다.
