# Multi-task Domain Adaptation for Sequence Tagging

Nanyun Peng, Mark Dredze

## 🧩 Problem to Solve

많은 자연어 처리(NLP) 작업은 정형 도메인(예: 뉴스 기사)에 풍부한 주석이 있지만, 주석이 적은 새로운 도메인(예: 소셜 미디어)에 적용될 때 성능이 크게 저하되는 문제가 있습니다. 기존의 도메인 적응(Domain Adaptation, DA) 접근 방식은 주로 단일 작업을 여러 도메인에 걸쳐 적응하는 데 초점을 맞추어 왔습니다. 이 연구는 도메인 적응 시나리오에서 여러 작업을 동시에 학습하는 다중 작업 학습(Multi-task Learning, MTL)이 더 강력하고 일반화된 데이터 표현을 학습하는 데 도움이 될 수 있는지 탐구합니다. 즉, 여러 작업을 함께 학습함으로써 도메인 적응의 성능을 향상시킬 수 있는지를 묻습니다.

## ✨ Key Contributions

- 도메인 적응 시 여러 작업을 동시에 고려하는 새로운 신경망 기반 다중 작업 도메인 적응(MTDA) 프레임워크를 제안했습니다.
- 서로 다른 도메인과 서로 다른 작업이라는 새로운 "도메인/작업 불일치(mismatch)" 설정을 제시하고 탐구했습니다.
- 중국어 단어 분리(Chinese Word Segmentation, CWS) 및 개체명 인식(Named Entity Recognition, NER) 작업에서 소셜 미디어 데이터에 대해 최신(state-of-the-art) 결과를 달성했습니다.

## 📎 Related Works

- **도메인 적응(Domain Adaptation):**
  - 매개변수 연결(parameter tying): Dredze and Crammer (2008), Daum ́e III (2007, 2009), Finkel and Manning (2009) 등.
  - 교차 도메인 표현 학습: Blitzer et al. (2006, 2007), Glorot et al. (2011), Chen et al. (2012), Yang and Eisenstein (2015) 등.
  - 감독(supervised) 및 비감독(unsupervised) DA.
- **다중 작업 학습(Multi-task Learning, MTL):**
  - 여러 작업을 동시에 학습하여 성능을 개선하는 접근 방식 (Caruana, 1997; Ando and Zhang, 2005).
  - 신경망을 활용한 MTL: 공유 표현 학습 (Collobert et al., 2011; Liu et al., 2016c; Peng and Dredze, 2016; Yang et al., 2016; Liu et al., 2016a).
- **가장 유사한 연구:** Yang and Hospedales (2015)는 MTL과 다중 도메인 학습을 통합적인 관점으로 제시했으나, MTL이 도메인 적응을 직접 돕는 가능성은 탐구하지 않았습니다.

## 🛠️ Methodology

제안하는 신경망 프레임워크는 시퀀스 태깅 문제에 대해 BiLSTM-CRF(Bi-directional Long Short-Term Memory with Conditional Random Fields)를 기반으로 하며, 다음과 같은 세 가지 주요 계층으로 구성됩니다.

1. **공유 표현 학습기 (Shared Representation Learner):**

   - 입력 시퀀스의 각 토큰에 대해 은닉 벡터 $h_{t} = \overrightarrow{h}_{t} \oplus \overleftarrow{h}_{t}$를 생성하는 BiLSTM을 사용합니다.
   - 모든 도메인과 작업에 걸쳐 공유되어 견고한 데이터 표현을 학습합니다.

2. **도메인 투영 계층 (Domain Projection Layer):**

   - 학습된 공유 표현을 각 도메인에 맞는 공통 공간으로 변환합니다.
   - **도메인 마스크(Domain Masks):** Daum ́e III (2007)에서 영감을 받아 은닉 벡터 차원을 공유 영역과 도메인 특정 영역으로 분할합니다. 도메인 $d$의 마스크 $m_{d}$를 사용하여 $\hat{h} = m_{d} \odot h$와 같이 요소별 곱셈을 적용합니다.
   - **선형 투영(Linear Projection):** 각 도메인에 대해 $k \times k$ 행렬 $T_{d}$를 사용하여 은닉 벡터에 선형 변환을 적용합니다: $\hat{h} = T_{d}h$.

3. **작업별 Neural-CRF 모델 (Task Specific Neural-CRF Models):**
   - 시퀀스 태깅을 위해 CRF를 디코더로 사용합니다.
   - 각 작업에 대해 단일 CRF를 사용하며, 이 CRF는 모든 도메인에 걸쳐 공유됩니다. 이는 데이터 양을 늘리고 데이터 분포의 변화를 더 잘 처리하기 위함입니다.
   - 주어진 입력 $x_{k}$에 대한 레이블 시퀀스 $y_{k}$의 조건부 확률 $p(y_{k}|x_{k};W)$를 정의합니다.

**매개변수 추정 (Parameter Estimation):**

- 모든 데이터셋의 로그 우도(log-likelihood)를 최대화하는 방식으로 엔드-투-엔드(end-to-end) 학습을 수행합니다.
- 확률적 경사 하강법(SGD)을 사용한 교대 최적화(alternating optimization)를 적용합니다.
- 과적합 방지를 위해 드롭아웃(dropout)과 조기 종료(early stopping)를 사용하고, 각 데이터셋에 대해 별도의 학습률과 학습률 감소 전략을 적용합니다.

## 📊 Results

- **주요 결과:** 다중 작업 도메인 적응(MTDA)은 CWS 및 NER 작업 모두에서 소셜 미디어 도메인에 대해 단일 작업 도메인 적응 모델 및 베이스라인(Separate, Mix)보다 뛰어난 성능을 보였습니다. 특히, 중국어 CWS에서 89.0%, NER에서 59.9%의 F1 점수로 새로운 최신(state-of-the-art) 결과를 달성했습니다.
- **도메인 적응 효과:** 학습 데이터의 도메인 정보를 명시적으로 모델링하는 것이 효과적인 표현 학습에 도움이 되며, 단순히 모든 데이터를 섞어 학습하는 것보다 더 큰 이점을 제공합니다.
- **학습 데이터 규모 영향:** 인-도메인(in-domain) 학습 데이터의 양이 증가함에 따라 베이스라인에 비해 MTDA 방식은 성능 감소가 적고 더 견고한(robust) 특성을 보였습니다. 인-도메인 데이터가 없는 비감독(unsupervised) 도메인 적응 시나리오에서도 잘 작동합니다.
- **모델 변형:**
  - 추가적인 학습 데이터(다른 작업, 다른 도메인, 또는 둘 다)를 포함하는 모든 설정에서 성능 향상이 관찰되었습니다.
  - "도메인/작업 불일치(Mismatch)" 설정(다른 도메인 + 다른 작업)은 "다중 작업 학습(Multi-task)" 설정(동일 도메인 + 다른 작업)보다 더 나은 성능을 보였는데, 이는 대규모 소스 도메인 데이터의 이점 때문일 것으로 분석됩니다.
  - "도메인 적응(Domain Adaptation)" 설정(동일 작업 + 다른 도메인)은 많은 양의 추가 데이터와 정렬된 작업의 이점 덕분에 두 데이터셋 설정 중에서 가장 좋은 결과를 보였습니다.
- **통계적 유의미성:** 제안된 MTDA 모델은 베이스라인 및 단일 작업 도메인 적응 모델에 비해 통계적으로 유의미한(p < 0.01) 성능 향상을 보였습니다.

## 🧠 Insights & Discussion

- **MTL과 DA의 시너지:** 이 연구는 다중 작업 학습과 도메인 적응을 결합함으로써 더 많은 데이터를 효과적으로 활용하고 더 나은 귀납적 편향(inductive bias)을 제공하여, 더욱 견고하고 일반화 가능한 표현을 학습할 수 있음을 입증합니다.
- **일반화 능력 향상:** MTL은 공유 표현 학습기가 도메인 간에 더 잘 일반화되는 표현을 유도하도록 돕습니다. 이는 특히 타겟 도메인 데이터가 부족할 때 큰 이점이 됩니다.
- **유연성:** 제안된 프레임워크는 작업 및 도메인의 수에 유연하게 대응하며, 매개변수는 선형적으로 증가합니다. 단일 도메인 또는 단일 작업의 경우 각각 표준 MTL 또는 DA 프레임워크로 축소됩니다.
- **새로운 불일치 설정:** 다른 도메인과 다른 작업이라는 새로운 설정을 탐구한 것은 향후 연구 방향을 제시합니다.
- **한계 및 향후 연구:**
  - 도메인 투영 전략으로 두 가지 간단한 방법만을 탐구했습니다. 향후 더 정교한 도메인 적응 기법들을 통합할 수 있습니다.
  - 실험은 시퀀스 태깅 문제에만 국한되었습니다. 텍스트 분류, 구문 분석, 기계 번역과 같은 다른 NLP 문제로의 적용 가능성을 탐구할 계획입니다.
  - 매개변수 공유 및 표현 학습과 같은 다중 도메인 학습의 다른 전통적인 방법들이 딥러닝 아키텍처에서 어떻게 구현될 수 있는지 추가 연구가 필요합니다.

## 📌 TL;DR

이 논문은 시퀀스 태깅을 위한 **다중 작업 도메인 적응(Multi-task Domain Adaptation, MTDA)** 프레임워크를 제안합니다. 전통적인 도메인 적응은 단일 작업에 집중하지만, 이 연구는 여러 작업을 동시에 학습하여 도메인 간 일반화 능력이 더 뛰어난 공유 표현을 학습하는 데 초점을 맞춥니다. BiLSTM-CRF 기반의 신경망 구조는 공유 표현 학습기, 도메인 투영 계층(마스크 또는 선형 투영), 작업별 CRF 모델로 구성됩니다. 중국어 단어 분리(CWS) 및 개체명 인식(NER)에 적용한 결과, MTDA가 기존 도메인 적응 및 베이스라인보다 우수한 성능을 보이며 소셜 미디어 도메인에서 두 작업 모두 최신(state-of-the-art) 결과를 달성했습니다. 이는 다중 작업 학습이 도메인 적응을 위한 더 강력하고 일반화된 표현을 유도하는 데 효과적임을 시사합니다.
