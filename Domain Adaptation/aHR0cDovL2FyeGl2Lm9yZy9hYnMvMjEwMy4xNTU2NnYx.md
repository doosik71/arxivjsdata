# Contrastive Domain Adaptation

Mamatha Thota, Georgios Leontidis

## 🧩 Problem to Solve

심층 학습(Deep Learning) 모델은 대량의 레이블링된 데이터에 의존하지만, 이러한 데이터는 확보하기 어렵고 비용이 많이 듭니다. 반면, 레이블이 없는 데이터는 풍부하게 존재하며, 자기 지도 학습(self-supervised learning), 특히 대조 학습(contrastive learning)은 레이블 없는 데이터에서 시각적 표현을 학습하는 데 유망합니다.

하지만 다음과 같은 문제들이 있습니다:

1. **도메인 적응(Domain Adaptation)의 필요성**: 한 도메인(소스 도메인)에서 학습된 모델을 다른 도메인(타겟 도메인)에 적용할 때 데이터 분포의 차이(도메인 시프트)로 인해 성능 저하가 발생합니다.
2. **기존 도메인 적응 방법의 한계**: 많은 도메인 적응 방법은 소스 도메인의 레이블에 의존하여 적용 범위가 제한됩니다.
3. **레이블 없는 대조 학습의 과제**: 레이블 없이 대조 학습을 도메인 적응에 효과적으로 적용하는 것은 미개척 분야이며, 특히 '거짓 부정 샘플(false negatives)' 문제는 해결하기 어렵습니다. 거짓 부정 샘플은 실제로는 같은 클래스에 속하지만 잘못된 가정으로 인해 부정 샘플로 간주되어 학습을 방해하는 경우를 의미합니다.

## ✨ Key Contributions

- **레이블 없는 도메인 적응을 위한 대조 학습 탐구**: 소스 및 타겟 도메인 간의 분포 차이를 가진 환경에서 일반화 성능을 극대화하기 위해 대조 학습을 활용합니다.
- **레이블 및 사전 학습 모델 없는 도메인 적응 방법 제안**: 레이블링된 데이터나 ImageNet 사전 학습 모델을 사용하지 않는 도메인 적응 접근 방식을 제시합니다.
- **거짓 부정 샘플 제거 도입**: 도메인 적응 설정에 거짓 부정 샘플 제거 메커니즘을 통합하여 추가적인 계산 오버헤드 없이 정확도를 향상시킵니다.
- **다중 뷰 학습 프레임워크 확장**: 두 개 이상의 뷰(view)를 사용하여 학습하는 도메인 적응 프레임워크를 확장하고 실험을 수행합니다.

## 📎 Related Works

- **도메인 적응 (Domain Adaptation)**:
  - **적대적 방법**: DANN (Domain-Adversarial Neural Network), ADDA (Adversarial Discriminative Domain Adaptation) 등 소스 및 타겟 분포 정렬.
  - **분포 차이 최소화**: MMD (Maximum Mean Discrepancy), 상관 관계 정렬(correlation alignment), Wasserstein 거리 등.
  - **딥 도메인 혼란(Deep Domain Confusion)**: Tzeng et al.의 DDC, Long et al.의 DAN 및 JAN.
- **대조 학습 (Contrastive Learning)**:
  - **표현 학습 방법**: SimCLR, MoCo (Momentum Contrast), Wu et al. (메모리 뱅크), Tian et al. (멀티 뷰).
  - 이전 연구들은 도메인 시프트를 고려하지 않거나, 소스 레이블/ImageNet 사전 학습 모델에 의존했습니다. 본 논문은 레이블이나 사전 학습 모델 없이 무작위로 초기화된 인코더를 훈련합니다.
- **거짓 부정 샘플 제거 (Removal of False Negatives)**:
  - **하드 부정 샘플 마이닝**: Kalantidis et al., Robinson et al.
  - **거짓 부정 샘플 제거 및 유인**: Huynh et al. (본 논문의 영감).
  - **편향 제거 대조 학습**: Chuang et al.

## 🛠️ Methodology

본 논문은 레이블 없는 소스 및 타겟 데이터를 사용하여 도메인 간의 시각적 특징을 일반화하는 **대조 도메인 적응(Contrastive Domain Adaptation, CDA)** 방법을 제안합니다.

1. **모델 개요 (CDA)**:
   - **아키텍처**: ResNet-50 인코더($f(\cdot)$)와 두 개의 비선형 MLP 투영 헤드($g(\cdot)$)로 구성됩니다. 인코더는 무작위로 초기화되어 처음부터 학습됩니다.
   - **데이터 증강**: 각 소스 및 타겟 이미지에 대해 두 개의 증강된 뷰를 생성합니다. (예: $a_s_1$, $a_s_2$).
   - **독립적인 대조 손실**: SimCLR에서 영감을 받아, 대조 손실을 소스 도메인과 타겟 도메인에 대해 독립적으로 계산합니다.
     $$L_{CONT\_DA} = L_{CONT\_S} + L_{CONT\_T}$$
     여기서 $L_{CONT\_S}$와 $L_{CONT\_T}$는 각 도메인에서 계산된 NT-Xent 손실의 변형입니다.
2. **거짓 부정 샘플 제거 (False Negative Removal, FNR)**:
   - 레이블이 없으므로 미니 배치 내에서 앵커 이미지와 동일한 클래스에 속할 수 있는 부정 샘플이 존재할 수 있습니다.
   - 이러한 '거짓 부정 샘플'은 모델 수렴을 방해하고 성능을 저하시킬 수 있습니다.
   - **메커니즘**: 미니 배치 내에서 앵커와 부정 샘플 간의 유사도를 계산하고, 앵커와 유사한 부정 샘플($S_i$)을 식별하여 대조 손실 계산에서 제거합니다. 예를 들어, $FNR_1$은 앵커당 하나의 거짓 부정 샘플을 제거합니다.
   - FNR이 적용된 대조 손실($L_{FNR}$)은 다음과 같습니다:
     $$L_{FNR} = -log \frac{exp(sim(z_i, z_j)/T)}{\sum_{k=1, (k \neq i, k \neq S_i)}^{2N} sim(z_i, z_k)/T}$$
   - 도메인 적응을 위해 FNR 손실도 소스 및 타겟 도메인에 대해 독립적으로 계산됩니다:
     $$L_{FNR\_DA} = L_{FNR\_S} + L_{FNR\_T}$$
3. **최대 평균 불일치 (Maximum Mean Discrepancy, MMD) 재검토**:
   - MMD는 두 분포의 RKHS (Reproducing Kernel Hilbert Space) 평균 임베딩 간의 거리를 측정하여 도메인 불일치를 명시적으로 최소화합니다.
   - MMD 손실($L_{MMD}$)은 다음과 같이 계산됩니다:
     $$L_{MMD} = \left\| \frac{1}{N} \sum_{i=1}^{N} \phi(x_s_i) - \frac{1}{M} \sum_{j=1}^{M} \phi(x_t_j) \right\|^2_H$$
     이는 최종 손실 함수에 통합되어 소스 및 타겟 특징 분포를 정렬하는 데 사용됩니다.
4. **학습 및 평가**:
   - 인코더와 투영 헤드는 $L_{FNR\_DA}$와 $L_{MMD}$를 조합한 총 손실을 역전파하여 업데이트됩니다.
   - 사전 학습 후에는 투영 헤드를 버리고, 고정된 인코더 특징 위에 선형 분류기를 학습시켜 성능을 평가합니다 (선형 평가).
   - **다중 뷰 확장**: 두 개의 증강 뷰 외에 네 개의 증강 뷰를 사용하는 실험도 수행됩니다.

## 📊 Results

본 논문은 MNIST->USPS, SVHN->MNIST, MNIST->MNISTM과 같은 숫자 데이터셋에 대해 다양한 실험을 수행했으며, 정확도를 평가 지표로 사용했습니다.

- **CDA-Base (대조 학습만 적용)**: SimCLR-Base (소스 데이터로만 학습 후 타겟에 테스트) 대비 평균 19%의 정확도 향상을 보이며, 도메인 불변 특징 학습의 효과를 입증했습니다.
- **CDA$_{FNR}$ (거짓 부정 샘플 제거 적용)**:
  - CDA-Base 대비 $FNR_1$ (하나의 거짓 부정 제거)은 평균 2.3%의 정확도 향상.
  - $FNR_2$ (두 개의 거짓 부정 제거)는 CDA-Base 대비 3.8%의 정확도 향상을 달성하여, 거짓 부정 제거가 수렴 속도 및 정확도를 높임을 보여주었습니다.
- **CDA-MMD (CDA-Base + MMD)**: CDA-Base 대비 평균 4.5%의 정확도 향상을 보여 MMD가 도메인 정렬에 효과적임을 입증했습니다.
- **CDA$_{FNR}$-MMD (FNR + MMD)**:
  - 두 가지 방법의 조합으로, CDA-Base 대비 평균 5.1%의 가장 큰 정확도 향상을 보였습니다.
  - MNIST->USPS 및 SVHN->MNIST 태스크에서 DANN, DAN, ADDA 등 기존 SOTA(State-Of-The-Art) 방법들과 비교했을 때 경쟁력 있거나 우수한 성능을 나타냈습니다.
- **네 개의 뷰를 사용한 CDA (CDAx4aug)**:
  - **CDAx4aug**: 두 개의 뷰를 사용하는 CDA-Base 대비 평균 5.1%의 정확도 향상을 가져왔습니다.
  - **CDAx4aug$_{FNR}$**: CDAx4aug 대비 0.7% 추가 정확도 향상.
  - **CDAx4aug-MMD** 및 **CDAx4aug$_{FNR}$-MMD**: 네 개의 뷰 설정에서는 MMD 추가가 두 개의 뷰 설정만큼 큰 성능 향상을 가져오지 못하거나 일부 경우 성능이 약간 저하되는 경향을 보였는데, 이는 추가 증강으로 인한 노이즈가 수렴에 영향을 미쳤을 가능성이 있습니다.

## 🧠 Insights & Discussion

- **레이블 없는 대조 학습의 잠재력**: 본 연구는 레이블이 없는 소스 및 타겟 데이터만으로도 대조 학습이 도메인 적응 문제에서 경쟁력 있는 성능을 달성할 수 있음을 보여주었습니다. 이는 기존 레이블 의존적 또는 사전 학습 모델 의존적 방법의 한계를 극복합니다.
- **도메인 독립적인 대조 학습의 중요성**: 소스 및 타겟 도메인에 대해 대조 손실을 독립적으로 적용함으로써, 모델은 도메인 시프트에 강인한 일반화 가능한 특징을 학습할 수 있습니다.
- **거짓 부정 샘플 제거의 핵심 역할**: 레이블 없는 환경에서 거짓 부정 샘플을 식별하고 제거하는 것은 대조 학습의 성능을 크게 향상시키고 모델의 효율적인 수렴을 돕는 필수적인 요소입니다.
- **MMD를 통한 명시적 도메인 정렬**: MMD는 특징 공간에서 소스 및 타겟 분포를 명시적으로 정렬하여 도메인 간의 불일치를 효과적으로 줄이고 모델의 성능을 더욱 향상시킵니다.
- **다중 뷰 학습의 효과**: 일반적으로 더 많은 증강 뷰를 사용하는 것은 더 풍부한 긍정/부정 샘플을 제공하여 표현 학습을 개선합니다. 하지만 MMD와 결합 시에는 추가적인 뷰가 생성하는 노이즈로 인해 수렴이 느려지거나 성능 향상이 미미할 수 있음을 시사합니다.
- **의미**: 본 연구는 레이블 없는 도메인 적응 분야에서 대조 학습의 가능성을 열고, 미래 연구들이 대조 학습을 도메인 적응에 적용하도록 장려할 것으로 기대됩니다.

## 📌 TL;DR

**문제**: 레이블링된 데이터 없이 도메인 시프트 문제를 해결하는 것은 어려우며, 특히 레이블 없는 대조 학습에서 발생하는 거짓 부정 샘플은 성능 저하를 야기합니다.
**제안 방법**: 본 논문은 레이블 없는 소스 및 타겟 데이터만을 활용하는 **대조 도메인 적응(CDA)** 프레임워크를 제안합니다. 이 방법은 각 도메인에 대해 독립적으로 대조 손실을 적용하고, **거짓 부정 샘플 제거(FNR)** 메커니즘을 통합하며, **최대 평균 불일치(MMD)**를 통해 도메인 간의 특징 분포를 명시적으로 정렬합니다. 또한 두 개 또는 네 개의 증강 뷰를 사용하여 학습하는 방안도 탐구합니다.
**주요 발견**: CDA, 특히 FNR 및 MMD를 결합한 방식($CDA_{FNR}$-MMD)은 레이블이나 ImageNet 사전 학습 없이도 기준선 모델 대비 도메인 적응 성능을 크게 향상시켰습니다. 거짓 부정 샘플 제거는 학습 수렴과 정확도에 필수적이었으며, MMD는 도메인 정렬에 효과적이었습니다. 이 방법은 기존의 최신 비지도 도메인 적응 방법들과 비교했을 때 경쟁력 있거나 우수한 성능을 보였습니다.
