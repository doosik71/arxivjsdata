# Domain Adaptation for Neural Machine Translation: A Survey

Danielle Saunders

## 🧩 Problem to Solve

최근 딥러닝 기술의 발전으로 신경망 기계 번역(NMT) 모델은 충분한 훈련 데이터와 시간을 확보하면 강력한 성능을 발휘합니다. 하지만 이 시스템들은 훈련 데이터와 스타일 및 어휘가 다른 새로운 도메인의 텍스트를 번역할 때 어려움을 겪습니다. 새로운 도메인에 맞춰 모델을 처음부터 다시 훈련하는 것은 비실용적이며, 기존 모델을 인-도메인(in-domain) 데이터로 단순 파인튜닝(fine-tuning)하는 방식은 새로운 데이터에 과적합되거나 이전에 학습된 지식을 '치명적으로 망각(catastrophic forgetting)'하는 문제를 야기할 수 있습니다. 본 논문은 이러한 한계를 극복하고, 특히 시스템이 여러 도메인에 걸쳐 번역해야 할 때 견고한 도메인 적응 접근 방식을 모색하는 것을 핵심 문제로 다룹니다.

## ✨ Key Contributions

- 신경망 기계 번역(NMT)을 위한 도메인 적응(Domain Adaptation, DA) 기술에 대한 철저하고 최신 설문조사를 제공합니다.
- NMT 시스템 개발 과정에 맞춰 데이터 중심, 아키텍처 중심, 훈련 스키마, 추론 스키마 등 적응 접근 방식의 체계적인 분류법을 제시합니다.
- 단일 도메인 적응과 다중 도메인 적응을 명확히 구분하고, 각 시나리오에 적합한 기술을 상세히 설명합니다.
- 조사된 모든 적응 기술에 걸쳐 발생할 수 있는 '망각' 및 '과적합'과 같은 주요 문제점과 그 해결책에 중점을 둡니다.
- NMT의 다른 연구 영역(예: 저자원 언어 번역, 성별 편향 처리, 문서 수준 번역)을 도메인 적응 문제로 재해석하여 적용 가능성을 탐구합니다.

## 📎 Related Works

이전에는 Chu and Wang (2018, 2020)의 NMT 도메인 적응에 대한 간략한 설문조사가 있었으나, 이는 주로 혼합 파인튜닝, 인스턴스 가중치, 아키텍처적 도메인 제어 등 특정 몇 가지 방법에 초점을 맞췄습니다. 본 논문은 이전 연구와 달리 훨씬 광범위한 적응 기술 개요를 제공하며, 특정 시나리오에서의 각 방법의 상대적인 장단점, 다중 도메인 적응의 중요성, 그리고 전통적인 도메인 범주에 속하지 않는 NMT 과제에 대한 도메인 적응 기술의 적용 가능성을 강조합니다.

## 🛠️ Methodology

본 논문은 NMT 도메인 적응 기술을 다음과 같은 네 가지 주요 범주로 분류하여 설명합니다.

1. **데이터 중심 적응 방법 (Data-Centric Adaptation Methods):**

   - **추가 자연어 데이터 선택:** n-그램, TF-IDF, 퍼지 매칭과 같은 개별 토큰 수준 또는 연속 문장 임베딩(embedding) 유사성을 통해 인-도메인과 유사한 문장을 검색합니다. 외부 도메인 분류 모델(예: 교차 엔트로피 차이, SVM, 비지도 클러스터링)을 사용하여 도메인 관련성을 점수화하여 데이터를 선택하기도 합니다.
   - **기존 자연어 데이터 필터링:** 기존 인-도메인 말뭉치에서 노이즈를 제거하고 실제 도메인 특성을 대표하는 데이터를 선별합니다.
   - **단일 언어 데이터로부터 합성 병렬 적응 데이터 생성:**
     - **역번역 (Back Translation):** 인-도메인 단일 언어 타겟 문서를 타겟-소스 NMT 모델로 역번역하여 합성 소스 문장을 생성합니다.
     - **정방향 번역 (Forward Translation):** 인-도메인 단일 언어 소스 문서를 소스-타겟 NMT 모델로 번역하여 합성 타겟 문장을 생성합니다.
   - **자연어 데이터에 인위적 노이즈 추가 및 단순화:** 소스 문장에 삭제, 대체, 순열 등의 노이즈를 추가하여 모델의 과적합을 줄이고 견고성을 높입니다. 문장 단순화는 번역을 용이하게 합니다.
   - **순수 합성 병렬 적응 데이터:** 외부 사전(lexicon)이나 템플릿을 사용하여 희귀 단어 및 특정 용어 번역을 위한 합성 데이터를 생성합니다.

2. **아키텍처 중심 적응 (Architecture-Centric Adaptation):**

   - **도메인 레이블 및 제어:** 도메인 레이블을 소스 또는 타겟 문장에 인라인 태그로 삽입하거나 별도의 임베딩 특징으로 주어 모델이 도메인 정보를 활용하도록 합니다.
   - **도메인별 서브 네트워크:** 어휘 임베딩, 인코더, 디코더 등 NMT 아키텍처의 특정 구성 요소를 도메인별로 만들거나, 도메인 분류기 서브 네트워크를 함께 학습시켜 도메인을 추론합니다.
   - **경량 파라미터 추가:** '어댑터(adapter) 레이어'(예: Bapna and Firat, 2019b)와 같이 사전 학습된 모델의 파라미터를 고정한 채 소수의 새로운 파라미터만 추가하고 튜닝하여 망각 없이 효율적으로 적응합니다.

3. **적응을 위한 훈련 스키마 (Training Schemes for Adaptation):**

   - **목표 함수 정규화 (Objective Function Regularization):**
     - **파라미터 고정 (Freezing Parameters):** 모델 파라미터의 일부를 고정하여 사전 학습된 값을 유지하고 망각을 방지합니다.
     - **파라미터 정규화 (Regularizing Parameters):** L2 정규화 또는 EWC(Elastic Weight Consolidation) (Eq. 9, 10)를 사용하여 파라미터가 사전 학습된 값에서 크게 벗어나지 않도록 제어합니다.
     - **지식 증류 (Knowledge Distillation):** 대규모 '교사(teacher)' 모델의 출력 분포를 '학생(student)' 모델에 증류하여 사전 학습 도메인에서의 성능을 유지하도록 유도합니다.
   - **커리큘럼 학습 (Curriculum Learning):** 훈련 예제를 난이도(문장 길이, 어휘 희귀성) 또는 도메인 관련성에 따라 순서를 지정하여 점진적으로 인-도메인 데이터에 적응하며 망각 및 과적합을 완화합니다.
   - **인스턴스 가중치 (Instance Weighting):** 손실 함수를 조정하여 각 훈련 예제에 타겟 도메인 관련성에 따른 가중치를 부여함으로써 특정 예제에 더 많은 또는 적은 중요도를 부여합니다.
   - **비-MLE 훈련 (Non-MLE Training):**
     - **최소 위험 훈련 (Minimum Risk Training, MRT):** 번역 품질 평가 지표(예: BLEU)에 대한 기대 비용을 최소화하여 과적합 및 노출 편향 문제를 완화합니다 (Eq. 12).
     - **메타 학습 (Meta-Learning):** 소수의 훈련 단계와 예제로 새로운 작업에 쉽게 적응할 수 있도록 모델 파라미터를 튜닝하는 '학습 방법 학습' 방식으로, 어댑터 레이어 튜닝 등에 활용됩니다 (Eq. 13).

4. **추론 스키마 (Inference Schemes for Adaptation):**
   - **다중 도메인 앙상블 (Multi-Domain Ensembling):**
     - **도메인 적응형 앙상블 (Domain Adaptive Ensembling):** 여러 NMT 모델(일반 및 인-도메인)의 예측을 입력 문장에 따라 동적으로 가중치를 부여하여 통합합니다 (Eq. 15, 16).
     - **검색 기반 앙상블 (Retrieval-Based Ensembling):** k-NN(k-Nearest Neighbor) 기계 번역을 사용하여 NMT 예측을 정적 인-도메인 데이터스토어에서 검색된 가장 가까운 이웃 토큰 분포와 보간합니다.
   - **제약 조건부 추론 및 재스코어링 (Constrained Inference and Rescoring):** 초기 번역을 생성한 후 다른 모델로 N-best 리스트를 재스코어하거나, 특정 용어 포함 등 제약 조건을 사용하여 NMT 디코딩이 특정 번역만 생성하도록 유도합니다.
   - **사전 및 사후 처리 통한 도메인 용어 제어 (Domain Terminology Via Pre- and Post-Processing):**
     - **용어 태깅:** 소스 문장의 특정 용어에 인라인 태그를 추가하고, 번역 후 해당 타겟 언어 용어로 교체합니다.
     - **인-도메인 프라이밍 (In-Domain Priming):** 유사한 소스 문장이나 번역을 입력의 컨텍스트 단서로 활용하여 모델이 도메인별 용어 및 구문을 생성하도록 유도합니다.

## 📊 Results

본 설문조사는 특정 실험 결과보다는 다양한 도메인 적응 기술의 효과와 잠재력을 종합적으로 제시합니다.

- **데이터 중심 방법**은 적절한 데이터 선택(예: n-그램 매칭) 및 생성(예: 역번역)을 통해 인-도메인 번역 품질을 크게 향상시키며, 저자원 도메인에서도 효과적임을 보여줍니다.
- **아키텍처 중심 방법** 중 '어댑터 레이어'는 모델의 망각 없이 새로운 도메인에 효율적으로 적응하고 과적합을 줄이는 데 기여합니다. 도메인 태그 사용도 인-도메인 번역 성능을 개선합니다.
- **훈련 스키마**에서 EWC(Elastic Weight Consolidation)와 같은 파라미터 정규화 기법은 치명적인 망각을 완화하고, 최소 위험 훈련(MRT) 및 메타 학습은 과적합 및 노출 편향 문제를 해결하면서 새로운 도메인에 대한 성능을 향상시킵니다.
- **추론 스키마**는 모델을 재훈련하지 않고도 추론 시 도메인별 번역을 유도할 수 있는 유연성을 제공하며, 도메인 적응형 앙상블, k-NN 번역, 제약 조건부 디코딩, 용어 태깅 및 프라이밍 등이 효과적입니다.
- **사례 연구**는 도메인 적응 기술이 저자원 언어 번역(기존 모델 튜닝, 가상 병렬 데이터 생성), 기계 번역의 성별 편향 완화(판별 손실, 성별 태그), 문서 수준 번역(문서 컨텍스트 통합, 태그) 등 다양한 NMT 과제에 성공적으로 적용될 수 있음을 보여줍니다.

## 🧠 Insights & Discussion

본 설문조사는 도메인 적응이 NMT 모델이 제한된 훈련 데이터로 관심 언어에 대해 우수한 성능을 달성하게 하며, 모델을 처음부터 재훈련하는 것보다 비용 효율적인 접근 방식을 제공한다는 점을 강조합니다. 때로는 도메인 적응이 처음부터 훈련하는 것보다 더 나은 성능을 낼 수도 있습니다.

- **광범위한 적용 가능성:** 전통적인 '도메인' 개념을 넘어, 저자원 언어 번역, 성별 편향 처리, 문서 수준 번역과 같은 NMT 연구의 다른 영역에서도 도메인 적응 기술이 성공적으로 적용될 수 있음을 보여줍니다. 이는 '도메인' 개념을 확장하여 다양한 문제를 해결하는 데 활용될 수 있음을 시사합니다.
- **망각 및 과적합 완화:** 단순 파인튜닝의 주요 문제점인 치명적인 망각과 과적합을 해결하기 위한 다양한 기법(파라미터 정규화, 지식 증류, 어댑터 레이어, 커리큘럼 학습 등)의 중요성을 강조합니다.
- **미래 방향:**
  - **초미세(Extremely Fine-Grained) 적응:** 사용자 또는 문장 수준의 적응이 인기를 얻고 있으며, 관련 데이터 추출 및 모델 변경 없는 추론 시 적응 방식에 대한 효율적인 연구가 더 필요합니다.
  - **비지도 적응 (Unsupervised Adaptation):** 병렬 데이터가 부족한 도메인에서 단일 언어 데이터를 활용한 의사 병렬 데이터 생성 및 효과적인 추출 방법에 대한 연구의 중요성이 커지고 있습니다.
  - **효율성:** 모델 크기가 커지고 적응 수준이 미세해지면서 훈련 및 저장 비용이 증가함에 따라, 튜닝 단계 감소, 부분 모델 적응(어댑터), 파라미터 튜닝 없는 출력 적응 방식 등 효율적인 접근 방식에 대한 수요가 증가할 것입니다.
  - **의도적 망각 (Intentional Forgetting):** 치명적인 망각을 '기능'으로 활용하여 원치 않는 동작(예: 시대에 뒤떨어진 번역, 성별 편향)을 의도적으로 학습하지 않거나, 개인 정보 보호를 위해 특정 데이터를 '삭제'하는 머신 언러닝(machine unlearning) 연구의 잠재력을 언급합니다.

## 📌 TL;DR

본 논문은 NMT의 도메인 적응(DA) 문제를 종합적으로 탐구하며, 모델이 새로운 도메인의 텍스트를 번역할 때 발생하는 성능 저하, 재훈련의 비실용성, 그리고 파인튜닝의 망각 및 과적합 문제를 해결하고자 합니다. 이를 위해 데이터 선택/생성, 모델 아키텍처 조정(예: 어댑터 레이어), 훈련 방식(예: EWC, MRT, 메타 학습), 추론 절차(예: 앙상블, 제약 조건부 디코딩) 등 다양한 DA 기술을 체계적으로 분류하고 설명합니다. 저자들은 DA가 저자원 언어 번역, 성별 편향 완화, 문서 수준 번역과 같은 다른 NMT 연구 영역에도 성공적으로 확장 적용될 수 있음을 보여주며, 미래에는 초미세 적응, 비지도 적응, 효율성, 의도적 망각이 주요 연구 방향이 될 것이라고 제안합니다.
