# Transferability in Deep Learning: A Survey

Junguang Jiang, Yang Shu, Jianmin Wang, Mingsheng Long

## 🧩 Problem to Solve

딥러닝 알고리즘의 성공은 일반적으로 대규모 데이터에 의존하지만, 인간은 이전 학습 경험에서 관련 지식을 인식하고 새로운 작업에 적용하는 타고난 지식 전이 능력을 가지고 있습니다. 이처럼 딥러닝에서 지식을 습득하고 재활용하는 능력을 '전이성(transferability)'이라고 합니다. 이 논문은 딥러닝을 인간 학습만큼 '데이터 효율적'으로 만들고 더 강력한 알고리즘을 설계하려는 장기적인 목표를 달성하기 위해, 단편적으로 연구되어 온 딥러닝 내 다양한 전이성 관련 분야를 연결하고 통일된 관점으로 분석하여 전이성의 개념, 원리, 방법론 및 도전 과제를 포괄적으로 제시하는 것을 목표로 합니다.

## ✨ Key Contributions

- **전이성의 통일된 관점 제공**: 딥러닝의 전체 생애 주기(사전 학습, 적응, 평가)에 걸쳐 전이성을 탐구하는 통합적이고 완전한 시각을 제시합니다.
- **다양한 분야 연결**: 딥러닝의 고립된 여러 분야(예: 메타 학습, 인과 학습, 도메인 적응 등)를 전이성의 관점에서 연결합니다.
- **핵심 원리 및 방법론 분석**: 딥 아키텍처, 사전 학습(지도/비지도), 태스크 적응, 도메인 적응 등 최근의 중요한 연구들을 아우르며 근본적인 목표와 도전 과제를 핵심 원리 및 방법론과 병행하여 설명합니다.
- **미해결 질문 강조**: 전이 가능한 지식을 학습하고 새로운 태스크 및 도메인에 적응하는 데 적절한 목표, 치명적 망각 및 부정적 전이 방지 등 미해결 질문들을 조명합니다.
- **벤치마크 및 오픈소스 라이브러리 제공**: 딥러닝 방법론의 전이성을 공정하게 평가하기 위한 벤치마크와 오픈소스 라이브러리 TLlib을 구현하여 제공합니다.

## 📎 Related Works

이 서베이 논문은 전이성을 중심으로 딥러닝의 광범위한 분야를 다루며, 다음을 포함한 다양한 선행 연구들을 참조합니다.

- **사전 학습 아키텍처**: ResNet, BatchNorm, Transformer, Vision Transformer (ViT) 등
- **지도 사전 학습**: Meta-Learning (MANN, MAML, Meta Transfer), Causal Learning (RIM, IRM) 등
- **비지도 사전 학습**: Generative Learning (GPT, BERT, MAE, T5), Contrastive Learning (InstDisc, MoCo, SimCLR, BYOL, SimSiam, CLIP) 등
- **태스크 적응**: Catastrophic Forgetting (EWC, LWF), Negative Transfer (BSS, LEEP), Parameter Efficiency (Side-Tuning, Adapter Tuning, Diff Pruning), Data Efficiency (Matching Net, ProtoNet, Prompt Learning) 등
- **도메인 적응**: Statistics Matching (DDC, DAN, JAN, deep CORAL), Domain Adversarial Learning (DANN, CDAN, ADDA), Hypothesis Adversarial Learning (MCD, DD, MDD), Domain Translation (CoGAN, PixelDA, CycleGAN, CyCADA), Semi-Supervised Learning (Self-Ensemble, MMT, Entropy Minimization, Pseudo-Labeling) 등

## 🛠️ Methodology

이 서베이 논문은 딥러닝 애플리케이션의 전 생애 주기를 **사전 학습(Pre-Training)**, **적응(Adaptation)**, **평가(Evaluation)** 세 단계로 나누어 전이성을 획득하고 활용하는 방법을 체계적으로 분석합니다.

1. **사전 학습 (Pre-Training)**
   - **모델 아키텍처**: ResNet, Transformer와 같이 전이 가능한 표현 학습에 중요한 영향을 미치는 아키텍처를 검토합니다.
   - **지도 사전 학습**: 대규모 레이블링된 데이터를 사용하여 전이성을 획득하는 방법을 설명하며, 메타 학습(Meta-Learning)과 인과 학습(Causal Learning)과 같은 고급 방법을 다룹니다.
   - **비지도 사전 학습**: 대규모 레이블링되지 않은 데이터를 활용하여 전이 가능한 지식을 습득하는 방법을 다루며, 생성 학습(Generative Learning)과 대조 학습(Contrastive Learning)에 초점을 맞춥니다.
2. **적응 (Adaptation)**
   - **태스크 적응 (Task Adaptation)**: 사전 학습된 모델을 새로운 하위 태스크에 재활용하는 방법을 탐구합니다.
     - **문제점**: 치명적 망각(Catastrophic Forgetting), 부정적 전이(Negative Transfer).
     - **해결책**: 도메인 적응 튜닝, 정규화 튜닝, 파라미터 효율성(잔차 튜닝, 파라미터 차이 튜닝), 데이터 효율성(메트릭 학습, 프롬프트 학습).
   - **도메인 적응 (Domain Adaptation)**: 레이블링된 소스 도메인에서 레이블링되지 않은 타겟 도메인으로 지식을 전이하는 방법을 다룹니다.
     - **이론적 기반**: $H\Delta H$-발산($H\Delta H$-Divergence), 불일치 불일치(Disparity Discrepancy) 등의 이론적 일반화 경계를 제시합니다.
     - **방법론**: 통계 매칭(Statistics Matching), 도메인 적대 학습(Domain Adversarial Learning), 가설 적대 학습(Hypothesis Adversarial Learning), 도메인 변환(Domain Translation), 준지도 학습(Semi-Supervised Learning).
3. **평가 (Evaluation)**
   - **데이터셋**: GLUE, ImageNet-R, DomainNet과 같이 전이성 평가에 적합한 대규모 데이터셋을 제시합니다.
   - **TLlib 라이브러리**: 다양한 전이성 알고리즘을 통합된 방식으로 구현한 오픈소스 라이브러리를 소개합니다.
   - **벤치마크**: TLlib을 활용하여 사전 학습 및 적응 방법의 교차 태스크 및 교차 도메인 전이성에 대한 벤치마크 결과를 제공합니다.

## 📊 Results

이 논문은 TLlib 라이브러리를 통해 사전 학습 및 적응 방법론의 전이성을 대규모 데이터셋에서 벤치마크한 결과를 제시합니다.

- **사전 학습의 전이성**:
  - **교차 태스크 전이성(GLUE, ImageNet)**: GPT, BERT, T5, ERNIE와 같은 다양한 아키텍처와 사전 학습 방법이 여러 NLP 및 이미지 인식 태스크에서 서로 다른 성능을 보이며, 아키텍처와 사전 학습 방식이 전이성에 큰 영향을 미친다는 것을 확인했습니다 (표 8, 9).
  - **교차 도메인 전이성(ImageNet-Sketch, ImageNet-R)**: ResNet, ViT, ResNext 등 다양한 모델과 사전 학습 전략(표준, SSP, WSP)이 도메인 간 전이성에 미치는 영향을 분석했으며, 대규모 데이터와 모델 용량이 더 나은 교차 도메인 전이성을 가져올 수 있음을 보여줍니다 (표 10).
- **태스크 적응의 전이성**:
  - **GLUE, 이미지 분류 데이터셋**: SMART, Adapter-Tuning, Diff Pruning, LWF, DELTA, BSS, Bi-Tuning과 같은 다양한 태스크 적응 방법들을 벤치마크했습니다 (표 11, 12). 개별 데이터셋에서는 큰 개선을 보이는 방법도 있었지만, 여러 데이터셋에 걸친 평균적인 성능 개선은 제한적이었습니다. 이는 태스크 적응 알고리즘의 효과가 타겟 태스크와 사전 학습 태스크 간의 관련성에 크게 의존함을 시사합니다.
- **도메인 적응의 전이성**:
  - **DomainNet, ImageNet-scale 데이터셋**: DAN, DANN, JAN, CDAN, MCD, MDD 등 다양한 도메인 적응 방법들의 성능을 평가했습니다 (표 13, 14). 소규모 데이터셋에서 최첨단 성능을 보이던 방법들이 대규모 데이터셋에서는 기대만큼의 성능을 발휘하지 못하는 경우가 많다는 것을 발견했습니다. 이는 대규모 데이터셋에서의 교차 도메인 전이성 개선에 더 많은 연구 노력이 필요함을 강조합니다.

## 🧠 Insights & Discussion

- **전이성의 근본적인 역할**: 전이성은 딥러닝의 데이터 효율성 문제를 해결하고 인간 학습에 가까워지기 위한 핵심적인 능력임을 재확인합니다.
- **사전 학습의 한계와 도전**:
  - **태스크 불일치**: 표준 지도 사전 학습은 사전 학습 태스크와 타겟 태스크 간의 관계에 따라 전이성이 달라지며, 모든 하위 태스크에 지배적인 사전 학습 태스크는 없습니다.
  - **메타/인과 학습의 확장성**: 메타 학습 및 인과 학습은 소규모 데이터셋에서 검증되었지만, 대규모 데이터 사전 학습을 통해 더 강력한 전이성을 획득할 수 있을지는 불분명합니다.
  - **비지도 사전 학습의 휴리스틱**: 비지도 사전 학습 태스크 설계는 여전히 휴리스틱하며, 태스크 불일치 해결 및 전이성 원리에 대한 견고한 분석이 부족합니다.
- **적응 단계의 미해결 과제**:
  - **치명적 망각 및 부정적 전이**: 태스크 적응 시 사전 학습된 지식을 보존하면서 새로운 태스크에 효과적으로 적응하는 것은 여전히 중요하며, 치명적 망각 및 부정적 전이 문제를 피하는 것이 관건입니다. 공격적인 전이 전략은 더 큰 긍정적 전이를 촉진하지만, 심각한 부정적 전이를 초래할 수 있습니다.
  - **파라미터/데이터 효율성**: 대규모 모델에서 파라미터 및 데이터 효율성을 높이는 방법들이 연구되고 있으나, 그 효과는 태스크 간의 관련성에 따라 크게 달라집니다.
- **도메인 적응의 복잡성**:
  - **이론과 실천의 격차**: 도메인 적응은 이론적 보장이 있지만, 실제 "오픈 도메인" 시나리오(예: 타겟 도메인의 범주가 소스 도메인과 정확히 같다고 보장할 수 없는 경우)에서는 이러한 제한을 만족하기 어렵습니다.
  - **사전 학습과의 시너지**: 사전 학습은 도메인 적응의 전이성을 높이는 데 필수적이며, 도메인 적응은 사전 학습된 모델의 치명적 망각 문제를 야기할 수 있으므로 학습률 조절 등 신중한 접근이 필요합니다.
- **향후 연구 방향**: 사전 학습과 적응 단계 간의 태스크/도메인 불일치를 효과적으로 해결하고, 대규모 데이터셋에서 전이성을 획득 및 활용하는 더 강력하고 일반화 가능한 방법론을 개발하는 것이 중요합니다.

## 📌 TL;DR

이 서베이 논문은 딥러닝의 고질적인 '데이터 비효율성' 문제를 해결하기 위한 핵심 능력인 '전이성'을 딥러닝 모델의 **사전 학습**, **적응**, **평가** 전 생애 주기에 걸쳐 포괄적으로 분석합니다. 사전 학습 단계에서는 전이 가능한 지식을 효과적으로 습득하기 위한 아키텍처 설계와 지도/비지도 학습 방법론(메타 학습, 인과 학습, 생성/대조 학습)을 탐구하고, 적응 단계에서는 습득된 지식을 새로운 태스크(치명적 망각, 부정적 전이, 파라미터/데이터 효율성)와 도메인(통계 매칭, 도메인 적대 학습, 가설 적대 학습, 도메인 변환, 준지도 학습)에 재활용하는 전략들을 다룹니다. 또한, 전이성 연구의 공정한 평가를 위해 대규모 데이터셋과 오픈소스 라이브러리 TLlib 기반의 벤치마크 결과를 제공하며, 대규모 데이터셋에서 최첨단 도메인 적응 방법들의 성능 한계를 지적하고 미해결 과제들을 제시합니다. 궁극적으로 딥러닝의 데이터 효율성을 향상시키기 위한 전이성의 근본적인 중요성을 강조합니다.
