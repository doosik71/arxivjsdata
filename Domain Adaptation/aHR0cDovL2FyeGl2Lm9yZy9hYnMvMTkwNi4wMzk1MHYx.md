# Domain-Specific Batch Normalization for Unsupervised Domain Adaptation

Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, Bohyung Han

## 🧩 Problem to Solve

비지도 도메인 적응(Unsupervised Domain Adaptation, UDA)은 레이블이 지정된 소스 도메인에서 학습된 지식을 레이블이 없는 타겟 도메인으로 전이하는 것을 목표로 합니다. 이 작업의 주요 문제는 소스 및 타겟 데이터셋 간의 **도메인 시프트(domain shift)** 현상입니다. 기존의 많은 UDA 기법들은 소스 및 타겟 도메인 간에 전체 네트워크 파라미터를 공유하는데, 이는 두 도메인이 분명히 다른 특성을 가지고 있음에도 불구하고 단일 모델 내에서 호환되지 않는 정보를 처리해야 하는 한계를 가집니다. 본 연구는 이러한 도메인 시프트 문제를 해결하기 위해 도메인 고유 정보를 도메인 불변 정보로부터 분리하여 모델의 일반화 성능을 향상시키는 것을 목표로 합니다.

## ✨ Key Contributions

- **새로운 DSBN 기반 UDA 프레임워크 제안:** 다양한 심층 신경망 모델에 적용 가능한 일반적인 도메인 적응 방법인 도메인별 배치 정규화(DSBN)를 기반으로 하는 UDA 프레임워크를 제안합니다.
- **DSBN을 활용한 2단계 학습 방법 도입:** 가상 레이블(pseudo-label) 추정 후 다중 작업 분류(multi-task classification)를 포함하는 2단계 학습 방법으로, 기존 비지도 도메인 적응 기법에 자연스럽게 통합됩니다.
- **다중 소스 도메인 적응을 위한 원칙적인 알고리즘 제공:** DSBN의 간단한 확장을 통해 다중 소스 도메인 적응 시나리오에도 적용 가능한 알고리즘을 제시합니다.
- **최첨단 성능 달성:** 제안하는 프레임워크를 최근 도메인 적응 기법(MSTN, CPUA)과 결합하여 Office-31 및 VisDA-C 데이터셋을 포함한 표준 벤치마크에서 최첨단(state-of-the-art) 정확도를 달성합니다.

## 📎 Related Works

- **도메인 불변 학습 (Domain-invariant Learning):**
  - **전역 정렬 (Global Alignment):** 최대 평균 불일치(Maximum Mean Discrepancy, MMD) [1, 28], 중심 모멘트 불일치(Central Moment Discrepancy, CMD) [29], Wasserstein 거리 [21], 상관 관계 정렬(CORAL) [22], 도메인 적대 신경망(DANN) [3], 적대적 판별 도메인 적응(ADDA) [24], 공동 적응 네트워크(JAN) [12] 등이 있습니다.
  - **지역 정렬 (Local Alignment):** Cycle-Consistent Adversarial Domain Adaptation (CyCADA) [4], 조건부 적대 도메인 적응(CDAN) [10] 등이 있습니다.
- **도메인별 학습 (Domain-specific Learning):**
  - 도메인 분리 네트워크(DSN) [2], 협력 및 적대 네트워크(CAN) [30] 등은 도메인별 구성 요소를 분리하여 학습합니다.
  - AdaBN [7]은 타겟 샘플을 사용하여 배치 정규화 통계를 재추정하며, [15]는 도메인 분포를 기준 가우시안 분포에 정렬합니다.
- **가상 레이블을 이용한 학습 (Learning with Pseudo Labels):**
  - Moving Semantic Transfer Network (MSTN) [27], Class Prediction Uncertainty Alignment (CPUA) [14], 비대칭 트리-트레이닝 네트워크(ATN) [18] 등은 타겟 도메인의 가상 레이블을 추정하여 도메인별 모델을 학습합니다. 본 연구는 DSBN을 통해 더 신뢰할 수 있는 가상 레이블을 추정합니다.

## 🛠️ Methodology

본 논문은 도메인별 배치 정규화(Domain-Specific Batch Normalization, DSBN) 개념과 이를 활용한 2단계 학습 프레임워크를 제안합니다.

**1. 도메인별 배치 정규화 (DSBN):**

- **배경:** 일반적인 배치 정규화(BN)는 미니 배치 내 활성화의 평균 $\mu$와 분산 $\sigma^2$을 계산하고, 학습 중 전체 활성화의 이동 평균(exponential moving average) $\bar{\mu}$, $\bar{\sigma}^2$을 추정하여 테스트 시 사용합니다. 그러나 도메인 시프트가 클 경우 소스와 타겟 도메인이 동일한 평균과 분산을 공유하는 것은 부적절합니다.
- **구현:** DSBN은 각 도메인 $d \in \{S, T\}$에 대해 별도의 배치 정규화 파라미터($\gamma_d$, $\beta_d$) 및 통계치($\mu_d$, $\sigma^2_d$) 세트를 할당합니다.
- **수식:** 도메인 $d$에 속하는 활성화 $x_d$에 대해 DSBN은 다음과 같이 표현됩니다.
  $$ \text{DSBN}\_d(x_d[i,j,n];\gamma_d,\beta_d) = \gamma_d \cdot \frac{x_d[i,j,n] - \mu_d}{\sqrt{\sigma^2_d + \epsilon}} + \beta_d $$
    여기서 $\mu_d$와 $\sigma^2_d$는 각 도메인의 미니 배치 내에서 별도로 계산되며, 학습 중에는 각 도메인별 이동 평균($\bar{\mu}_d$, $\bar{\sigma}^2_d$)이 유지됩니다.
- **이점:** DSBN은 도메인별 통계 및 affine 파라미터를 학습하여 도메인 고유 정보를 효과적으로 캡처하고, 네트워크가 도메인 불변 특징을 더 잘 학습할 수 있도록 합니다.
- **다중 소스 확장:** 다중 소스 도메인 적응 시나리오에서는 각 소스 도메인에 추가적인 DSBN 브랜치를 할당하여 쉽게 확장될 수 있습니다.

**2. 2단계 학습 프레임워크:**

- **1단계: 초기 가상 레이블러 학습 (Training Initial Pseudo Labeler):**
  - 기존 비지도 도메인 적응 네트워크(예: MSTN [27], CPUA [14])의 모든 배치 정규화(BN) 계층을 DSBN 계층으로 대체합니다.
  - 이렇게 DSBN이 통합된 네트워크($F_1^T$로 표기)를 해당 네트워크의 원래 손실 함수와 학습 전략을 사용하여 훈련하고, 이를 통해 타겟 도메인 데이터에 대한 초기 가상 레이블을 생성합니다.
- **2단계: 가상 레이블을 통한 자기 학습 (Self-training with Pseudo Labels):**
  - DSBN 계층을 포함하는 최종 네트워크($F_2^d$로 표기)를 학습합니다. 이 단계에서는 소스 도메인에는 실제 레이블, 타겟 도메인에는 1단계에서 얻은 가상 레이블을 사용하여 다중 작업 분류 손실(두 도메인에 대한 교차 엔트로피 손실의 합)을 최소화합니다.
  - **가상 레이블 정제:** 타겟 도메인 샘플 $x$에 대한 가상 레이블 $y'$은 1단계 모델 $F_1^T$와 2단계 모델 $F_2^T$의 예측 점수를 가중 조합하여 점진적으로 정제됩니다.
    $$ y' = \text{argmax}\_{c \in C} \{ (1-\lambda)F_1^T(x)[c] + \lambda F_2^T(x)[c] \} $$
        여기서 $\lambda$는 훈련 진행도에 따라 0에서 1로 점진적으로 증가하는 가중치 팩터($\lambda = \frac{2}{1+\text{exp}(-\gamma \cdot p)} - 1$)입니다. 이는 훈련 초기에 $F_1^T$의 안정적인 예측에 더 의존하고, $F_2^T$가 개선됨에 따라 점차 $F_2^T$의 예측에 더 큰 비중을 두게 합니다.
  - **반복적 정제:** 2단계 학습 절차는 반복적으로 수행될 수 있으며, 각 반복에서 이전 반복의 모델로 업데이트된 가상 레이블을 사용하여 다음 학습을 진행함으로써 추가적인 정확도 향상을 달성합니다.

## 📊 Results

- **VisDA-C 데이터셋:** MSTN 및 CPUA를 기반 모델로 사용했을 때, DSBN을 통합한 2단계 학습 프레임워크는 현저한 정확도 향상을 보이며 최첨단(SOTA) 성능을 달성했습니다. 특히 MSTN 기반 DSBN 모델은 80.2%의 평균 정확도를 기록하여 재현된 MSTN(65.0%) 및 다른 SOTA 방법들을 능가했습니다.
- **Office-31 데이터셋:** DSBN을 적용한 모델은 2단계 학습을 통해 88.3%의 평균 정확도를 달성하며 기존 최첨단 방법(CDAN-M 87.7%)을 넘어섰습니다. 이는 DSBN이 기존 도메인 적응 알고리즘과 성공적으로 결합되어 성능을 크게 향상시킬 수 있음을 보여줍니다.
- **다중 소스 도메인 적응 (Office-31, Office-Home):** 여러 소스 도메인을 활용하는 "Merged" 및 "Separate" 시나리오 모두에서 DSBN 모델은 일반 BN 모델보다 지속적으로 우수한 성능을 보였습니다. 특히 어려운 도메인 적응 작업에서 "Separate" DSBN이 "Merged" BN보다 우수한 결과를 보여, DSBN이 다중 소스 환경에서도 효과적임을 입증했습니다.
- **Ablation Study:** VisDA-C 데이터셋에서 수행된 실험 결과, DSBN은 1단계와 2단계 학습 과정 모두에서 결정적인 역할을 하며 상당한 정확도 향상을 가져왔습니다. DSBN을 활용한 2단계 학습이 성능을 크게 향상시키는 반면, 일반 BN을 사용한 2단계 학습은 도움이 되지 않거나 오히려 성능이 감소할 수 있음을 확인했습니다.
- **반복 학습 (Iterative Learning):** 2단계 학습을 반복적으로 수행했을 때 VisDA-C 데이터셋에서 분류 정확도가 점진적으로 향상됨을 확인했습니다(예: 1회 반복 80.2% $\rightarrow$ 4회 반복 82.7%).

## 🧠 Insights & Discussion

- **DSBN의 효과:** DSBN은 각 도메인의 고유한 통계($\mu_d, \sigma^2_d$)와 아핀 변환 파라미터($\gamma_d, \beta_d$)를 독립적으로 학습함으로써, 도메인별 정보를 효과적으로 포착합니다. 이는 네트워크의 다른 공유 파라미터들이 도메인 간의 공통적이고 불변적인 특징을 더욱 명확하게 학습할 수 있도록 돕습니다.
- **향상된 도메인 불변 표현:** t-SNE 시각화 결과는 DSBN이 동일 클래스 내의 소스 및 타겟 도메인 샘플 표현을 더 잘 정렬시켜 도메인 불변 특징 학습 능력을 향상시킴을 명확히 보여줍니다.
- **2단계 학습 전략의 강점:** 첫 번째 단계에서 DSBN을 통해 신뢰할 수 있는 초기 가상 레이블을 생성하고, 두 번째 단계에서 이 가상 레이블을 점진적으로 정제하는 자기 학습 방식은 모델의 최종 성능을 크게 향상시킵니다. 특히, 2단계 학습을 반복적으로 적용하는 것이 추가적인 성능 개선으로 이어진다는 점은 가상 레이블 정제의 중요성을 강조합니다.
- **다중 소스 적응에 대한 확장성:** DSBN은 다중 소스 도메인 적응 시나리오에도 유연하게 적용될 수 있으며, 각 소스 도메인의 특성을 개별적으로 처리함으로써 병합된 소스 도메인 접근 방식보다 우수한 성능을 달성할 수 있습니다.
- **일반화 가능성:** DSBN은 배치 정규화 계층을 사용하는 모든 심층 신경망 기반 UDA 기법에 쉽게 통합될 수 있는 범용적인 방법입니다.

## 📌 TL;DR

본 논문은 심층 신경망 기반 비지도 도메인 적응(UDA)의 **도메인 시프트 문제**를 해결하기 위해 **도메인별 배치 정규화(DSBN)**를 제안한다. DSBN은 각 도메인에 대해 독립적인 배치 정규화 파라미터 및 통계를 사용하여 도메인 고유 정보를 분리하고, 나머지 네트워크는 도메인 불변 특징을 학습한다. 이 프레임워크는 기존 UDA 기법에 쉽게 통합되는 **2단계 학습 방식**을 채택하는데, 1단계에서 DSBN으로 강화된 네트워크로 초기 가상 레이블을 생성하고, 2단계에서 이 가상 레이블을 점진적으로 정제하며 자기 학습을 수행한다. 이 방법은 VisDA-C 및 Office-31 벤치마크에서 **최첨단 성능**을 달성했으며, 다중 소스 도메인 적응 시나리오에서도 DSBN의 효과를 입증하였다.
