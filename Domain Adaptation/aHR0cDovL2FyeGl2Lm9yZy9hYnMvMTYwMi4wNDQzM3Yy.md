# Unsupervised Domain Adaptation with Residual Transfer Networks

Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan

## 🧩 Problem to Solve

최근 딥러닝의 성공은 방대한 양의 레이블링된 데이터에 의존합니다. 그러나 레이블된 데이터가 없는 타겟 태스크의 경우, 도메인 적응(Domain Adaptation)은 다른 소스 도메인으로부터 학습된 모델을 전이시키는 데 사용됩니다. 이 과정에서 소스 도메인과 타겟 도메인 간의 데이터 분포 차이(Domain Shift)로 인해 예측 모델을 타겟 태스크에 적응시키는 데 어려움이 있습니다. 특히, 이전의 딥 도메인 적응 방법들은 소스 분류기와 타겟 분류기가 동일하다는 강한 가정을 하였으나, 실제 적용에서는 이 가정이 유효하지 않은 경우가 많습니다. 본 논문은 이러한 분류기 불일치 문제를 해결하고, 레이블된 소스 데이터와 레이블 없는 타겟 데이터를 사용하여 적응형 분류기(adaptive classifiers)와 전이 가능한 특징(transferable features)을 동시에 학습하는 것을 목표로 합니다.

## ✨ Key Contributions

- **잔여 전이 네트워크(Residual Transfer Network, RTN) 제안:** 딥 네트워크에서 적응형 분류기와 전이 가능한 특징을 동시에 학습하는 새로운 비지도 도메인 적응 접근 방식인 RTN을 제안합니다.
- **분류기 적응(Classifier Adaptation) 도입:** 이전 방법들이 가정한 '공유 분류기(shared-classifier)' 가정을 완화하고, 소스 분류기($f_S(x)$)와 타겟 분류기($f_T(x)$)가 잔여 함수($\Delta f(x)$)만큼 다르다는 가정을 도입합니다 ($f_S(x) = f_T(x) + \Delta f(x)$). 딥 네트워크에 잔여 레이어(residual layers)를 삽입하여 이 잔여 함수를 명시적으로 학습함으로써 분류기 적응을 가능하게 합니다.
- **텐서 MMD(Tensor MMD)를 통한 다중 레이어 특징 적응:** 여러 레이어의 특징들을 텐서 곱(tensor product)으로 융합하고, 이를 재생 커널 힐베르트 공간(Reproducing Kernel Hilbert Spaces, RKHS)에 임베딩하여 최대 평균 불일치(Maximum Mean Discrepancy, MMD)를 최소화함으로써 특징 분포를 일치시킵니다. 이는 기존의 독립적인 다중 MMD 페널티보다 모델 선택을 용이하게 합니다.
- **엔트로피 최소화(Entropy Minimization) 활용:** 타겟 분류기($f_t(x)$)의 성능을 더욱 개선하기 위해, 레이블 없는 타겟 데이터에 대한 분류기 출력의 엔트로피를 최소화하는 원리를 적용하여 저밀도 분리(low-density separation)를 촉진합니다.
- **종단 간(End-to-End) 학습 프레임워크:** 특징 학습, 특징 적응, 분류기 적응을 통합하는 종단 간 딥러닝 프레임워크를 제시하며, 표준 역전파(back-propagation)를 통해 효율적으로 훈련할 수 있습니다.
- **최고 성능 달성:** 표준 도메인 적응 벤치마크(Office-31, Office-Caltech)에서 기존 최신 방법론들을 능가하는 성능을 보여줍니다.

## 📎 Related Works

- **도메인 적응 일반:** 도메인 적응은 다른 도메인이나 태스크 간의 모델 전이를 목표로 하며 [1], 레이블링 부담을 완화합니다 [9, 10, 11, 12, 13, 14, 15, 16]. 핵심은 도메인 간의 확률 분포 불일치를 줄이는 것입니다.
- **딥러닝 기반 도메인 적응:** 딥 신경망은 데이터 변동의 설명 요인을 분리하여 추상적 표현을 학습하며 [17], 전이성이 좋은 불변 요소를 나타냅니다 [3, 18, 19, 15, 16, 20]. 그러나 딥 특징도 도메인 간의 불일치를 "줄일" 뿐 "제거"하지 못한다는 연구 결과가 있습니다 [3, 4, 18].
- **기존 딥 도메인 적응 방법:**
  - **DDC [4], DAN [5]:** 딥 네트워크에 적응 레이어를 추가하여 분포의 평균 임베딩을 일치시킵니다. 특히 DAN은 다중 커널 MMD를 사용하여 여러 태스크별 레이어의 특징을 매칭합니다.
  - **RevGrad [6, 7]:** 적대적 훈련 패러다임을 통해 도메인 판별기를 혼란시켜 도메인 불변 특징을 학습합니다.
  - **한계:** 이 방법들은 학습된 도메인 불변 특징 표현에서 소스 분류기를 타겟 도메인으로 직접 전이할 수 있다는 가정(공유 분류기 가정)에 기반합니다. 이상적인 가설의 결합 오차가 클 때 이 가정이 성립하지 않을 수 있습니다 [22].
- **딥 잔여 학습(Deep Residual Learning):** ImageNet ILSVRC 2015 우승 모델인 ResNet [8]은 레이어를 잔여 함수($\Delta F(x)$)로 명시적으로 재구성하여 깊은 네트워크 훈련을 용이하게 합니다 ($F(x) = \Delta F(x) + x$). 본 논문은 이 아이디어를 분류기 불일치 모델링에 적용합니다.
- **기존 분류기 적응:** 분류기 적응은 섭동 함수(perturbation function)를 추가하여 소스 분류기를 타겟 도메인에 적응시키는 아이디어가 있었으나 [23, 24, 25], 이 방법들은 섭동 함수 학습에 타겟 레이블 데이터를 필요로 하므로 비지도 도메인 적응에는 적용할 수 없었습니다. 또한, 이전 연구들은 입력 공간 $x$에서 섭동 함수를 정의한 반면, 본 논문은 타겟 분류기 $f_T(x)$의 출력에 대한 잔여 함수를 사용합니다.

## 🛠️ Methodology

본 논문은 특징 적응과 분류기 적응을 통합한 종단 간 딥러닝 프레임워크인 RTN을 제안합니다.

1. **전반적인 문제 설정:**
   - 레이블된 소스 도메인 $D_s = \{(x_i^s, y_i^s)\}_{i=1}^{n_s}$ 와 레이블 없는 타겟 도메인 $D_t = \{x_j^t\}_{j=1}^{n_t}$ 가 주어집니다.
   - 두 도메인의 확률 분포는 $p \neq q$ 로 다릅니다.
   - 목표는 타겟 기대 위험 $R_t(f_t) = E_{(x,y)\sim q} [f_t(x) \neq y]$ 를 최소화하는 $f_t$ 를 학습하는 것입니다.
2. **특징 적응 (Feature Adaptation):**
   - 사전 훈련된 CNN(예: AlexNet)의 컨볼루션 레이어 특징을 미세 조정(fine-tuning)합니다.
   - 마지막 특징 레이어 위에 병목(bottleneck) 레이어 `fcb`를 추가하여 특징 차원을 줄입니다.
   - 여러 레이어 $L=\{\text{fcb}, \text{fcc}\}$ 의 특징들을 텐서 곱($\otimes$)으로 융합하여 $z_i^s, z_j^t$ 를 생성합니다.
   - 융합된 특징에 대해 소스와 타겟 도메인 간의 최대 평균 불일치(MMD)를 최소화합니다 (텐서 MMD):
     $$ \min*{f_S, f_T} D*{\mathcal{L}}(D*s, D_t) = \frac{1}{n_s^2} \sum*{i,j=1}^{n*s} k(z_i^s, z_j^s) + \frac{1}{n_t^2} \sum*{i,j=1}^{n*t} k(z_i^t, z_j^t) - \frac{2}{n_s n_t} \sum*{i=1}^{n*s} \sum*{j=1}^{n_t} k(z_i^s, z_j^t) $$
        여기서 $k(\cdot, \cdot)$는 가우시안 커널이며, 텐서의 벡터화에 정의됩니다.
3. **분류기 적응 (Classifier Adaptation):**
   - 소스 분류기 $f_S(x)$ 와 타겟 분류기 $f_T(x)$ 가 잔여 함수 $\Delta f(x)$ 만큼 다르다고 가정합니다:
     $$ f_S(x) = f_T(x) + \Delta f(x) $$
   - 딥 잔여 학습 [8]의 아이디어를 확장하여, `fcc` 레이어의 출력인 $f_T(x)$ 를 입력으로 받고 잔여 레이어(`fc1-fc2`)를 통해 $\Delta f(x)$ 를 학습한 뒤 $f_T(x)$ 에 더하여 $f_S(x)$ 를 출력하는 잔여 블록을 구성합니다.
   - $f_S(x)$ 를 잔여 블록의 출력으로 설정하여 소스 레이블 데이터로 더 잘 훈련될 수 있도록 합니다.
   - **엔트로피 최소화:** 타겟 레이블 데이터가 없으므로, 타겟 분류기 $f_t(x)$ 가 타겟 데이터의 저밀도 영역을 통과하도록 유도하기 위해 타겟 데이터에 대한 $f_t(x)$ 의 엔트로피를 최소화합니다:
     $$ \min*{f_t} \frac{1}{n_t} \sum*{i=1}^{n_t} H(f_t(x_i^t)) $$
        여기서 $H(\cdot)$는 엔트로피 함수입니다.
4. **잔여 전이 네트워크 (RTN) 최종 목적 함수:**
   - 특징 학습, 특징 적응, 분류기 적응을 통합하는 종합적인 목적 함수를 최적화합니다:
     $$ \min*{f_S=f_T+\Delta f} \frac{1}{n_s} \sum*{i=1}^{n*s} L(f_s(x_i^s), y_i^s) + \gamma \frac{1}{n_t} \sum*{i=1}^{n*t} H(f_t(x_i^t)) + \lambda D*{\mathcal{L}}(D_s, D_t) $$
        여기서 $L$ 은 교차 엔트로피 손실(cross-entropy loss), $\lambda$ 와 $\gamma$ 는 각각 텐서 MMD 페널티와 엔트로피 페널티에 대한 균형 매개변수입니다.
5. **훈련:**
   - ImageNet으로 사전 훈련된 AlexNet 모델을 사용하여 미세 조정합니다.
   - 표준 역전파(back-propagation) 알고리즘을 사용합니다.
   - 학습률은 RevGrad [6]에서 제시된 어닐링 전략을 따릅니다.
   - $\lambda, \gamma$ 매개변수는 특정 전이 태스크(A→W)에서 선택한 후 모든 다른 태스크에 고정하여 사용합니다.

## 📊 Results

- **데이터셋:** Office-31 (6가지 전이 태스크) 및 Office-Caltech (12가지 전이 태스크) 벤치마크.
- **비교 대상:** TCA [9], GFK [14] (전통적인 얕은 전이 학습), AlexNet [26] (표준 딥러닝), DDC [4], DAN [5], RevGrad [6] (최신 딥 도메인 적응).
- **RTN의 우월성:** RTN(mmd+ent+res) 모델은 모든 비교 방법론을 일관되게 능가하며, Office-31 및 Office-Caltech 데이터셋에서 새로운 최신 성능을 달성합니다. 특히 A→W, C→W와 같이 도메인 간 차이가 큰 어려운 전이 태스크에서 정확도를 크게 향상시켰습니다.
- **심층 분석 (RTN 변형):**
  - **RTN (mmd):** AlexNet에 텐서 MMD 모듈만 추가한 경우. DAN보다 약간 더 나은 성능을 보였으나, DAN이 여러 MMD 페널티를 사용하는 반면 RTN (mmd)는 단일 MMD 페널티를 사용하여 모델 선택이 훨씬 용이합니다. 이는 다중 레이어 특징 융합 및 적응의 효과를 입증합니다.
  - **RTN (mmd+ent):** RTN (mmd)에 엔트로피 페널티를 추가한 경우. RTN (mmd)보다 상당히 더 나은 성능을 보여, 레이블 없는 타겟 데이터의 저밀도 분리를 위한 엔트로피 최소화의 중요성을 강조합니다.
  - **RTN (mmd+ent+res):** RTN (mmd+ent)에 잔여 모듈을 추가한 경우. 모든 변형 중 최고의 성능을 달성합니다. 이는 분류기 레이어의 잔여 전이가 적응형 분류기 학습에 매우 중요하다는 것을 시사합니다.

## 🧠 Insights & Discussion

- **공유 분류기 가정의 한계 극복:** 기존 딥 도메인 적응 방법들이 암묵적으로 가정했던 소스-타겟 공유 분류기의 한계를 RTN은 잔여 함수를 명시적으로 학습함으로써 극복합니다. 이는 실용적인 도메인 적응 시나리오에서 중요한 진전입니다.
- **특징 및 분류기 동시 적응의 중요성:** 실험 결과는 전이 성능을 최적화하기 위해 특징 적응뿐만 아니라 분류기 적응도 필수적이며, 이 둘을 통합하여 종단 간으로 학습하는 것이 가장 효과적임을 보여줍니다.
- **텐서 MMD의 효율성:** 텐서 MMD는 여러 레이어의 특징을 융합하여 단일 MMD 페널티로 적응함으로써, 다중 MMD 페널티를 사용하는 방법보다 모델 선택을 간소화하고 효율적인 특징 적응을 가능하게 합니다.
- **엔트로피 최소화의 보완 역할:** 레이블 없는 타겟 데이터에 대한 엔트로피 최소화는 타겟 분류기가 타겟 데이터의 클러스터 구조에 더 잘 적응하도록 유도하여 분류기 적응의 효과를 보완하고 강화합니다.
- **잔여 학습의 확장성:** He et al. [8]의 잔여 학습 프레임워크가 특징 매핑을 학습하는 데 효과적이었던 것처럼, 본 연구에서는 이를 분류기 매핑에 적용하여 소스 및 타겟 분류기 간의 미묘한 차이를 효과적으로 모델링할 수 있음을 보여줍니다.
- **모듈 간 시너지:** 엔트로피 페널티와 잔여 모듈은 함께 사용될 때 가장 효과적입니다. 그렇지 않으면 잔여 함수가 무의미한 0 매핑을 학습하여 소스 및 타겟 분류기가 거의 동일해지는 경향이 있을 수 있습니다.

## 📌 TL;DR

**문제:** 딥러닝 기반 비지도 도메인 적응은 소스와 타겟 도메인 간의 데이터 분포 차이로 인해 발생하는 특징 및 분류기 불일치에 취약하며, 특히 기존 방법들의 공유 분류기 가정은 실제 상황에서 유효하지 않을 수 있습니다.

**제안 방법:** 본 논문은 **잔여 전이 네트워크(RTN)**를 제안하여 전이 가능한 특징과 적응형 분류기를 동시에 학습합니다. RTN은 **텐서 MMD**를 통해 다중 레이어 특징 분포를 효과적으로 일치시키고, **잔여 전이 모듈**을 도입하여 소스 분류기($f_S(x)$)와 타겟 분류기($f_T(x)$) 간의 차이를 잔여 함수($\Delta f(x)$)로 명시적으로 모델링($f_S(x) = f_T(x) + \Delta f(x)$)합니다. 또한, 타겟 분류기($f_T(x)$)의 성능을 개선하기 위해 레이블 없는 타겟 데이터에 대한 **엔트로피 최소화**를 활용합니다.

**주요 결과:** RTN은 Office-31 및 Office-Caltech 벤치마크에서 기존 최신 방법들을 능가하며 새로운 최고 성능을 달성했습니다. 심층 분석을 통해 텐서 MMD는 효율적인 특징 적응을, 엔트로피 최소화는 타겟 분류기 정제를, 그리고 잔여 전이 모듈은 분류기 불일치 문제를 해결하는 데 각기 중요한 역할을 하며, 이들의 시너지가 안전하고 효과적인 도메인 적응을 가능하게 함을 입증했습니다.
