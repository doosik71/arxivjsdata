# Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision

Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, In So Kweon

## 🧩 Problem to Solve

의미론적 분할(semantic segmentation) 모델은 픽셀 수준의 주석(annotation) 데이터에 크게 의존하지만, 이러한 데이터를 수집하는 것은 비용이 많이 들고 노동 집약적입니다. 합성 데이터(synthetic data)로 훈련된 모델은 실제 이미지(real images)로 전이할 때 도메인 간의 분포 차이(inter-domain gap)로 인해 성능이 저하되는 문제가 있습니다. 기존 연구들은 주로 이 **도메인 간(source-to-target) 간극**을 줄이는 데 집중했지만, 본 연구는 **대상 도메인 내(target-to-target)에서도 발생하는 큰 분포 차이(intra-domain gap)**, 즉 대상 데이터 자체의 난이도 차이(예: 깨끗한 장면과 노이즈가 많은 장면)를 해결해야 할 중요한 문제로 지적합니다.

## ✨ Key Contributions

- 대상 도메인 내 데이터 간에 **내부 도메인 간극(intra-domain gap)**이 존재함을 밝히고, 이를 해결하기 위한 엔트로피 기반 랭킹 함수를 제안하여 대상 도메인을 '쉬운' 및 '어려운' 하위 도메인으로 분리합니다.
- **두 단계의 자기 지도(self-supervised) 도메인 적응 접근 방식**을 제안하여 도메인 간 간극(inter-domain gap)과 내부 도메인 간극(intra-domain gap)을 동시에 최소화합니다.

## 📎 Related Works

- **비지도 도메인 적응(Unsupervised Domain Adaptation, UDA):** 레이블이 있는 소스 데이터와 레이블이 없는 대상 데이터 간의 분포 차이를 맞추는 것을 목표로 합니다. 적대적 학습(adversarial learning) 기반 UDA는 이미지 수준, 특징 수준, 출력 수준에서 도메인 불변 특징을 학습하는 데 효율적입니다. 본 논문은 [29]의 AdvEnt 방법을 도메인 간 적응에 활용합니다.
- **엔트로피를 통한 불확실성 측정(Uncertainty via Entropy):** 모델 출력의 엔트로피는 도메인 간극을 줄이거나(예: [29]) 샘플의 신뢰도를 측정하는 데 사용됩니다. 본 연구에서는 이를 활용하여 대상 이미지를 '쉬운' 및 '어려운' 분할로 나눕니다.
- **커리큘럼 도메인 적응(Curriculum Domain Adaptation):** 쉬운 샘플부터 먼저 처리하는 방식을 다룹니다. 기존 방법들은 도메인을 분해하기 위해 추가 정보가 필요하거나, 전역 및 지역 레이블 분포 학습에 초점을 맞추지만, 본 연구는 엔트로피 랭킹 시스템을 기반으로 더 간단하고 데이터 중심적인 접근 방식을 제안합니다.

## 🛠️ Methodology

본 연구는 도메인 간 및 내부 도메인 간극을 모두 최소화하는 **두 단계 자기 지도 도메인 적응** 접근 방식을 제안합니다.

1. **도메인 간 적응(Inter-domain Adaptation):**
   - `G_inter` (생성자)와 `D_inter` (판별자)로 구성됩니다.
   - 소스 도메인 이미지 $X_s$와 해당 정답 $Y_s$를 사용하여 `G_inter`는 교차 엔트로피 손실 $L_{seg}^{inter}(X_s, Y_s)$를 최소화하며 훈련됩니다.
   - 대상 도메인 이미지 $X_t$에 대해 `G_inter`는 예측 $P_t = G_{inter}(X_t)$를 생성하고, 이를 통해 엔트로피 맵 $I_t$를 계산합니다:
     $$I^{(h,w)}_t = \sum_c -P^{(h,w,c)}_t \log(P^{(h,w,c)}_t)$$
   - `D_inter`는 $I_s$와 $I_t$가 각각 소스 및 대상 도메인에서 왔는지 판별하도록 훈련되고, `G_inter`는 `D_inter`를 속여 두 도메인의 엔트로피 맵 분포를 정렬하도록 적대적 손실 $L_{adv}^{inter}(X_s, X_t)$를 최소화합니다.
2. **엔트로피 기반 랭킹 시스템(Entropy-based Ranking System):**
   - 도메인 간 적응이 완료된 `G_inter`를 사용하여 대상 도메인 내 모든 이미지 $X_t$에 대한 예측 엔트로피 맵 $I_t$를 생성합니다.
   - 각 이미지의 엔트로피 맵 평균 값 $R(X_t) = \frac{1}{HW} \sum_{h,w} I^{(h,w)}_t$를 사용하여 랭킹 점수를 매깁니다.
   - 하이퍼파라미터 $\lambda$ (예: 2/3)를 기준으로 전체 대상 이미지 세트를 '쉬운 분할'($X_{te}$)과 '어려운 분할'($X_{th}$)로 분리합니다.
3. **내부 도메인 적응(Intra-domain Adaptation):**
   - `G_intra` (생성자)와 `D_intra` (판별자)로 구성됩니다.
   - `G_inter`가 생성한 '쉬운 분할' $X_{te}$에 대한 예측 $P_{te}$를 의사 레이블(pseudo labels)로 활용합니다.
   - `G_intra`는 '쉬운 분할' $X_{te}$와 의사 레이블 $P_{te}$를 사용하여 교차 엔트로피 손실 $L_{seg}^{intra}(X_{te})$를 최소화하며 훈련됩니다.
   - '쉬운 분할' $X_{te}$의 엔트로피 맵 $I_{te}$와 '어려운 분할' $X_{th}$의 엔트로피 맵 $I_{th}$를 생성합니다.
   - `D_intra`는 $I_{te}$와 $I_{th}$가 각각 '쉬운' 및 '어려운' 분할에서 왔는지 판별하도록 훈련되고, `G_intra`는 `D_intra`를 속여 두 분할의 엔트로피 맵 분포를 정렬하도록 적대적 손실 $L_{adv}^{intra}(X_{te}, X_{th})$를 최소화합니다.

- **총 손실 함수:** $L = L_{seg}^{inter} + L_{adv}^{inter} + L_{seg}^{intra} + L_{adv}^{intra}$
- **훈련 과정:** 세 단계로 진행됩니다: 1) `G_inter`, `D_inter`를 최적화하는 도메인 간 적응, 2) `G_inter`로 의사 레이블 및 엔트로피 맵을 생성하고 대상 데이터를 분할, 3) `G_intra`, `D_intra`를 최적화하는 내부 도메인 적응.

## 📊 Results

- **GTA5 → Cityscapes:** 제안된 방법은 Cityscapes 검증 세트에서 46.3%의 평균 IoU(mIoU)를 달성하여 AdvEnt(43.8%) 대비 2.5%p 향상된 성능을 보였습니다. 엔트로피 정규화(entropy normalization)를 적용했을 때는 47.0%까지 향상되었습니다.
- **SYNTHIA → Cityscapes:** 16-클래스 및 13-클래스에서 각각 41.7%, 48.9%의 mIoU를 달성하여 기존 SOTA 기법들을 능가했습니다. 특히 자동차(car) 및 오토바이(motor bike) 클래스에서 현저한 개선을 보였습니다.
- **Synscapes → Cityscapes:** AdaptSegNet(52.7%) 대비 54.2%의 mIoU를 달성했습니다.
- **하이퍼파라미터 $\lambda$ 분석:** GTA5 → Cityscapes 실험에서 $\lambda=0.67$일 때 46.3%의 mIoU로 최적의 성능을 보였으며, 이는 대상 이미지의 약 2/3를 '쉬운' 분할로 할당함을 의미합니다.
- **자릿수 분류(Digit Classification):** MNIST → USPS, USPS → MNIST, SVHN → MNIST 작업에서도 기존 CyCADA [11] 모델보다 높은 정확도(예: MNIST→USPS에서 95.8±0.1%)를 달성하여 방법론의 일반성을 입증했습니다.

## 🧠 Insights & Discussion

- **내부 도메인 간극의 중요성:** 기존 도메인 간 적응만으로는 해결할 수 없었던 대상 데이터 내의 다양성을 성공적으로 다루어 성능을 향상시켰습니다. 특히 복잡하거나 노이즈가 많은 "어려운" 이미지에 대한 분할 정확도가 개선되었습니다.
- **이론적 분석:** 벤-데이비드(Ben-David)의 이론 [1]을 기반으로, 대상 도메인 오류의 상한은 소스 도메인 오류와 도메인 발산(divergence)에 의해 결정된다고 설명합니다. 제안된 모델은 도메인 간 및 내부 도메인 정렬을 통해 도메인 발산을 최소화합니다.
- **한계:** 소스-대상 도메인 간극이 크거나(즉, $d_H(S,T)$ 값이 높은 경우), 모델의 소스 도메인 오류($\epsilon_S(h)$)가 높을 때(예: 소형 신경망 사용 시) 모델의 효율성이 감소할 수 있습니다. 이는 엔트로피 랭킹 시스템과 내부 도메인 적응 과정에 영향을 미칩니다.
- **일반성:** 의미론적 분할뿐만 아니라 자릿수 분류와 같은 다른 작업에서도 효과를 보여 제안된 두 단계 자기 지도 적응 방식이 다양한 도메인 적응 문제에 적용될 수 있음을 시사합니다.

## 📌 TL;DR

본 논문은 의미론적 분할에서 합성-실제 데이터 간의 **도메인 간 간극(inter-domain gap)**과 함께, 실제 **대상 데이터 내의 다양성으로 인한 내부 도메인 간극(intra-domain gap)**이라는 새로운 문제를 제기합니다. 이를 해결하기 위해 두 단계의 자기 지도 도메인 적응 접근 방식을 제안합니다. 첫째, 기존 도메인 적응 방법론을 통해 도메인 간 간극을 줄입니다. 둘째, 엔트로피 기반 랭킹 시스템을 활용하여 대상 도메인 데이터를 '쉬운' 분할과 '어려운' 분할로 나눈 후, '쉬운' 분할의 의사 레이블을 사용하여 '어려운' 분할로 자기 지도 학습을 수행함으로써 내부 도메인 간극을 추가로 줄입니다. 이 방법은 Cityscapes 벤치마크 및 자릿수 분류 태스크에서 SOTA 성능을 능가하며, 도메인 적응의 새로운 방향을 제시합니다.
