{
  "url": "http://arxiv.org/abs/2010.03978v1",
  "title": "A Brief Review of Domain Adaptation",
  "authors": "Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, Hamid R. Arabnia",
  "year": 2020,
  "abstract": "Classical machine learning assumes that the training and test sets come from\nthe same distributions. Therefore, a model learned from the labeled training\ndata is expected to perform well on the test data. However, This assumption may\nnot always hold in real-world applications where the training and the test data\nfall from different distributions, due to many factors, e.g., collecting the\ntraining and test sets from different sources, or having an out-dated training\nset due to the change of data over time. In this case, there would be a\ndiscrepancy across domain distributions, and naively applying the trained model\non the new dataset may cause degradation in the performance. Domain adaptation\nis a sub-field within machine learning that aims to cope with these types of\nproblems by aligning the disparity between domains such that the trained model\ncan be generalized into the domain of interest. This paper focuses on\nunsupervised domain adaptation, where the labels are only available in the\nsource domain. It addresses the categorization of domain adaptation from\ndifferent viewpoints. Besides, It presents some successful shallow and deep\ndomain adaptation approaches that aim to deal with domain adaptation problems."
}