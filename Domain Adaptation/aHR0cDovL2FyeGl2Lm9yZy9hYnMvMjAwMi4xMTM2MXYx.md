# Understanding Self-Training for Gradual Domain Adaptation

Ananya Kumar, Tengyu Ma, Percy Liang

## 🧩 Problem to Solve

머신러닝 시스템은 배포 후 시간이 지남에 따라 데이터 분포가 점진적으로 변화하는 "점진적 도메인 적응(gradual domain adaptation)" 시나리오에 직면합니다. 기존 모델은 이러한 변화로 인해 성능이 저하되지만, 매번 레이블된 데이터를 새로 수집하는 것은 비실용적입니다. 이 연구의 목표는 소스 도메인에서 훈련된 초기 분류기를, 타겟 도메인으로 점진적으로 변화하는 중간 도메인들의 레이블 없는 데이터만을 활용하여 효율적으로 적응시키는 것입니다. 기존의 도메인 적응 방법은 이러한 점진적 변화 구조를 무시하는 경향이 있으며, 타겟 도메인에 직접 적응하는 것은 종종 무한한 오류를 초래할 수 있습니다.

## ✨ Key Contributions

- **최초의 이론적 분석:** 점진적 도메인 적응이 직접적인 도메인 적응보다 성능 향상을 제공함을 이론적으로 입증했습니다.
- **비공허(Non-Vacuous) 오류 상한선 증명:** 점진적 변화 환경에서 자기 훈련(self-training) 오류에 대한 최초의 비공허 상한선을 제시했습니다. 이는 직접적인 타겟 도메인 적응 시 오류가 무한할 수 있는 상황에서도 유효합니다.
- **알고리즘적 통찰:** 이론적 분석을 통해 정규화(regularization) 및 레이블 선명화(label sharpening)가 심지어 무한한 데이터가 있을 때도 점진적 자기 훈련에 필수적임을 밝혀냈습니다. 또한, 자기 훈련이 Wasserstein-infinity 거리($W_\infty$)가 작은 변화에 특히 효과적임을 시사합니다.
- **실험적 성능 향상:** 점진적 변화 구조를 활용하여 회전 MNIST 및 현실적인 Portraits 데이터셋에서 높은 정확도를 달성했습니다.
- **이론적 한계 및 조건 제시:** 점진적 자기 훈련의 오류가 지수적으로 증가할 수 있음을 보이며, 특정 데이터 분포(예: 가우시안 설정) 하에서는 베이즈 최적 분류기를 복구할 수 있음을 분석했습니다.

## 📎 Related Works

- **자기 훈련(Self-training):** 반지도 학습(semi-supervised learning) 및 도메인 적응에서 널리 사용되는 방법으로, 엔트로피 최소화(entropy minimization) 및 공동 훈련(co-training)과 관련이 있습니다.
- **비지도 도메인 적응(Unsupervised domain adaptation):** 레이블된 소스 도메인에서 레이블 없는 타겟 도메인으로 직접 적응하는 것을 목표로 합니다. 밀도 비율(density ratio)에 의존하는 중요도 가중치 기반 방법과 $H\Delta H$-divergence 이론이 있으며, 이들은 도메인 간의 겹침을 가정합니다.
- **점진적 도메인 적응(Gradual domain adaptation):** Hoffman et al., Michael et al., Markus et al., Bobu et al. 등이 접근 방식을 제안했으나, 이 논문은 점진적 도메인 적응에 대한 최초의 이론을 개발했습니다.
- **다른 학습 패러다임과의 구분:** 온라인 학습(online learning), 평생 학습(lifelong learning), 개념 드리프트(concept drift)와는 달리, 변화된 분포에서 레이블 없는 데이터만 가지고 있다는 점에서 차이가 있습니다.

## 🛠️ Methodology

본 연구는 자기 훈련(self-training) 알고리즘을 점진적 도메인 적응 설정에 맞게 변형하여 사용하며, 두 가지 이론적 설정 하에서 분석합니다.

### 1. 점진적 자기 훈련(Gradual Self-Training) 알고리즘

- **초기 분류기 훈련:** 소스 도메인 $P_0$의 레이블된 예제 $S_0$로 초기 분류기 $\theta_0$를 훈련합니다.
- **순차적 적응:** 각 연속적인 도메인 $P_t$에 대해 다음 단계를 수행합니다 (여기서 $t$는 $1$부터 $T$까지 증가):
  - 이전 단계에서 학습된 분류기 $\theta_{t-1}$를 사용하여 현재 도메인 $P_t$의 레이블 없는 예제 $S_t$에 **가상 레이블(pseudolabel)**을 생성합니다.
  - 가상 레이블된 예제를 사용하여 정규화된 지도 학습 분류기 $\theta_t$를 훈련합니다.
- **출력:** 최종적으로 타겟 도메인 $P_T$에 대한 분류기 $\theta_T$를 얻습니다.

### 2. 이론적 분석 설정

**a) 마진 설정(Margin Setting):**

- **모델:** $\ell_2$ norm이 제한된 정규화된 선형 모델 $\Theta_R = \{(w,b) : w \in \mathbb{R}^d, b \in \mathbb{R}, \Vert w \Vert_2 \le R\}$을 사용합니다.
- **손실 함수:** 램프 손실(ramp loss) $r(m) = \min(\max(1-m, 0), 1)$을 사용합니다. 이는 아웃라이어에 강하고 상한이 있는 손실입니다.
- **분포 간 거리:** Wasserstein-infinity 거리 $W_\infty$를 사용하여 도메인 간의 점진적 변화를 정의합니다. $\rho(P_t, P_{t+1}) \le \rho < 1/R$.
- **가정:**
  - $\alpha^*$-분리 가정: 각 도메인 $P_t$에서 낮은 손실 $\alpha^*$를 달성하는 분류기 $\theta_t \in \Theta_R$가 존재합니다.
  - 점진적 변화 가정: 연속적인 도메인 간의 $\rho(P_t, P_{t+1})$가 작습니다.
  - 바운드된 데이터 가정: 데이터의 $\mathbb{E}_{X \sim P}[ \Vert X \Vert_2^2 ] \le B^2$.
  - 레이블 이동 없음 가정: $P_t(Y)$는 모든 $t$에 대해 동일합니다.

**b) 가우시안 설정(Gaussian Setting):**

- **모델:** 각 클래스 $y \in \{-1,1\}$에 대해 $P_t(X|Y=y)$가 $d$차원 등방성 가우시안 분포 $N(y\mu_t, \sigma_t^2 I)$를 따르는 이상적인 상황을 가정합니다.
- **목표:** 레이블 없는 손실 함수 $U(w,P) = \mathbb{E}_{X \sim P}[\varphi(|w^T X|)]$를 최소화합니다.
- **알고리즘:** $w_0$를 레이블된 데이터에서 선택한 후, $w_t = \arg\min_{\Vert w \Vert_2 \le 1, \Vert w - w_{t-1} \Vert_2 \le 1/2} U(w,P_t)$를 통해 매개변수를 업데이트합니다. 여기서 근처에서의 국소 최소화에 제약이 있습니다.

## 📊 Results

- **점진적 시프트 활용의 효과:** Gradual Self-Training은 Gaussian, Rotating MNIST, Portraits 세 가지 데이터셋 모두에서 비적응적(non-adaptive) 소스 분류기, 타겟 도메인에 직접 자기 훈련(Target ST), 모든 레이블 없는 데이터를 풀링하여 자기 훈련(All ST)하는 기준선(baseline)보다 지속적으로 높은 정확도를 보였습니다.
  - **Portraits 데이터셋 예시:** Source (75.3%) → Target ST (76.9%) → All ST (78.9%) → Gradual ST (83.8%).
  - **Rotating MNIST 데이터셋 예시:** Source (31.9%) → Target ST (33.0%) → All ST (38.0%) → Gradual ST (87.9%).
- **필수 요소의 중요성:**
  - **정규화(Regularization):** 명시적 정규화(예: Dropout, Batchnorm)가 신경망의 암묵적 정규화만으로는 충분하지 않으며, 점진적 자기 훈련 성능에 필수적임을 확인했습니다. 정규화를 사용하지 않으면 정확도가 크게 하락했습니다 (Portraits: 82.6% vs 76.5%, Rot MNIST: 83.8% vs 45.8%). 데이터 양이 증가해도 정규화의 중요성은 줄어들지 않았습니다.
  - **레이블 선명화(Label Sharpening):** 모델의 확률적 출력 대신 '하드(hard)' 레이블(-1 또는 1)을 사용하여 가상 레이블을 생성하는 것이 더 나은 정확도를 보였습니다 ('소프트(soft)' 레이블 사용 시 정확도 하락: Portraits: 82.6% vs 80.1%).
- **점진적 시프트가 도움이 되는 시점:** 이론적 예측과 같이, 도메인 간의 변화가 Wasserstein-infinity 거리에서 작을 때 점진적 자기 훈련이 효과적이었습니다. 총 변동 거리(total variation distance)는 작지만 Wasserstein-infinity 거리는 큰 방식으로 중간 도메인을 구성했을 때 (예: MNIST 이미지를 점진적으로 섞는 방식), 점진적 자기 훈련은 직접적인 타겟 적응보다 나은 성능을 보이지 못했습니다.

## 🧠 Insights & Discussion

- **점진적 변화의 가치:** 이 연구는 데이터 분포 변화의 점진적 특성을 활용하는 것이 모델 성능을 향상시키는 데 중요하며, 심지어 직접적인 적응이 불가능한 시나리오에서도 오류를 제어할 수 있음을 이론적으로 증명했습니다.
- **정규화와 마진의 중요성:** 정규화는 단순히 데이터가 적을 때 일반화 성능을 높이는 것을 넘어, 도메인 이동에 적응하고 결정 경계에 충분한 마진(margin)을 유지하는 데 필수적입니다. 이는 무한한 데이터가 있는 상황에서도 마찬가지입니다.
- **레이블링 전략의 영향:** 자기 훈련 시 소프트 레이블은 모델이 매개변수를 업데이트할 동기를 부여하지 않을 수 있으므로, 하드 레이블을 사용하는 '레이블 선명화'가 모델 업데이트에 중요합니다.
- **손실 함수의 선택:** 램프 손실은 이론적으로 아웃라이어에 강하고 오류를 바운드하는 데 도움이 되어 힌지 손실보다 이점(특히 이론적 보증 측면)을 가집니다.
- **도메인 거리 척도의 중요성:** 점진적 자기 훈련의 효과는 도메인 간의 변화를 측정하는 거리 척도에 민감합니다. 특히 Wasserstein-infinity 거리가 작을 때 가장 잘 작동하며, 이는 실제 적용 시 점진적 도메인 적응을 설계하는 데 중요한 지침이 됩니다.
- **한계:** 마진 설정에서의 지수적 오류 증가는 이론적 한계를 보여주며, 더 나은(예: sub-exponential) 바운드를 얻기 위해서는 추가적인 데이터 분포 가정이 필요할 수 있습니다. Portraits 데이터셋에서의 레이블 이동과 같은 현실적인 복잡성은 이론에 완전히 포괄되지 않은 부분입니다.

## 📌 TL;DR

**문제:** 점진적으로 변하는 데이터 분포에 레이블 없는 데이터로 머신러닝 모델을 적응시키는 것이 도전 과제입니다. 직접 적응은 실패할 수 있습니다.
**방법:** "점진적 자기 훈련"이라는 새로운 방법을 제안합니다. 이는 이전 단계에서 학습한 분류기를 사용하여 다음 단계의 레이블 없는 데이터에 가상 레이블을 생성하고, 이를 통해 모델을 순차적으로 업데이트합니다.
**발견:**

1. **이론적 보증:** 직접 적응이 실패하는 상황에서도 점진적 자기 훈련은 오류를 유한하게 제어할 수 있음을 최초로 이론적으로 증명했습니다.
2. **핵심 요소:** 정규화와 하드 레이블(레이블 선명화)이 모델 성능 유지 및 향상에 필수적임을 밝혀냈습니다.
3. **적용 조건:** 도메인 변화가 Wasserstein-infinity 거리에서 작을 때 점진적 자기 훈련이 특히 효과적임을 실험적으로 검증했습니다.
4. **실용적 효과:** 합성 및 실제 데이터셋(Rotating MNIST, Portraits)에서 기준선 대비 높은 정확도를 달성하며 그 유용성을 입증했습니다.
