{
  "title": "What is Left After Distillation? How Knowledge Transfer Impacts Fairness\n  and Bias",
  "authors": "Aida Mohammadshahi, Yani Ioannou",
  "year": 2024,
  "url": "http://arxiv.org/abs/2410.08407v2",
  "abstract": "Knowledge Distillation is a commonly used Deep Neural Network (DNN)\ncompression method, which often maintains overall generalization performance.\nHowever, we show that even for balanced image classification datasets, such as\nCIFAR-100, Tiny ImageNet and ImageNet, as many as 41% of the classes are\nstatistically significantly affected by distillation when comparing class-wise\naccuracy (i.e. class bias) between a teacher/distilled student or distilled\nstudent/non-distilled student model. Changes in class bias are not necessarily\nan undesirable outcome when considered outside of the context of a model's\nusage. Using two common fairness metrics, Demographic Parity Difference (DPD)\nand Equalized Odds Difference (EOD) on models trained with the CelebA,\nTrifeature, and HateXplain datasets, our results suggest that increasing the\ndistillation temperature improves the distilled student model's fairness, and\nthe distilled student fairness can even surpass the fairness of the teacher\nmodel at high temperatures. Additionally, we examine individual fairness,\nensuring similar instances receive similar predictions. Our results confirm\nthat higher temperatures also improve the distilled student model's individual\nfairness. This study highlights the uneven effects of distillation on certain\nclasses and its potentially significant role in fairness, emphasizing that\ncaution is warranted when using distilled models for sensitive application\ndomains."
}