{
  "title": "Adaptive Instance Distillation for Object Detection in Autonomous\n  Driving",
  "authors": "Qizhen Lan, Qing Tian",
  "year": 2022,
  "url": "http://arxiv.org/abs/2201.11097v2",
  "abstract": "In recent years, knowledge distillation (KD) has been widely used to derive\nefficient models. Through imitating a large teacher model, a lightweight\nstudent model can achieve comparable performance with more efficiency. However,\nmost existing knowledge distillation methods are focused on classification\ntasks. Only a limited number of studies have applied knowledge distillation to\nobject detection, especially in time-sensitive autonomous driving scenarios. In\nthis paper, we propose Adaptive Instance Distillation (AID) to selectively\nimpart teacher's knowledge to the student to improve the performance of\nknowledge distillation. Unlike previous KD methods that treat all instances\nequally, our AID can attentively adjust the distillation weights of instances\nbased on the teacher model's prediction loss. We verified the effectiveness of\nour AID method through experiments on the KITTI and the COCO traffic datasets.\nThe results show that our method improves the performance of state-of-the-art\nattention-guided and non-local distillation methods and achieves better\ndistillation results on both single-stage and two-stage detectors. Compared to\nthe baseline, our AID led to an average of 2.7% and 2.1% mAP increases for\nsingle-stage and two-stage detectors, respectively. Furthermore, our AID is\nalso shown to be useful for self-distillation to improve the teacher model's\nperformance."
}