{
  "title": "Collaborative Teacher-Student Learning via Multiple Knowledge Transfer",
  "authors": "Liyuan Sun, Jianping Gou, Baosheng Yu, Lan Du, Dacheng Tao",
  "year": 2021,
  "url": "http://arxiv.org/abs/2101.08471v2",
  "abstract": "Knowledge distillation (KD), as an efficient and effective model compression\ntechnique, has been receiving considerable attention in deep learning. The key\nto its success is to transfer knowledge from a large teacher network to a small\nstudent one. However, most of the existing knowledge distillation methods\nconsider only one type of knowledge learned from either instance features or\ninstance relations via a specific distillation strategy in teacher-student\nlearning. There are few works that explore the idea of transferring different\ntypes of knowledge with different distillation strategies in a unified\nframework. Moreover, the frequently used offline distillation suffers from a\nlimited learning capacity due to the fixed teacher-student architecture. In\nthis paper we propose a collaborative teacher-student learning via multiple\nknowledge transfer (CTSL-MKT) that prompts both self-learning and collaborative\nlearning. It allows multiple students learn knowledge from both individual\ninstances and instance relations in a collaborative way. While learning from\nthemselves with self-distillation, they can also guide each other via online\ndistillation. The experiments and ablation studies on four image datasets\ndemonstrate that the proposed CTSL-MKT significantly outperforms the\nstate-of-the-art KD methods."
}