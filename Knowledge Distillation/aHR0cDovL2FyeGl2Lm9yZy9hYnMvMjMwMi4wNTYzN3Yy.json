{
  "title": "Dual Relation Knowledge Distillation for Object Detection",
  "authors": "Zhenliang Ni, Fukui Yang, Shengzhao Wen, Gang Zhang",
  "year": 2023,
  "url": "http://arxiv.org/abs/2302.05637v2",
  "abstract": "Knowledge distillation is an effective method for model compression. However,\nit is still a challenging topic to apply knowledge distillation to detection\ntasks. There are two key points resulting in poor distillation performance for\ndetection tasks. One is the serious imbalance between foreground and background\nfeatures, another one is that small object lacks enough feature representation.\nTo solve the above issues, we propose a new distillation method named dual\nrelation knowledge distillation (DRKD), including pixel-wise relation\ndistillation and instance-wise relation distillation. The pixel-wise relation\ndistillation embeds pixel-wise features in the graph space and applies graph\nconvolution to capture the global pixel relation. By distilling the global\npixel relation, the student detector can learn the relation between foreground\nand background features, and avoid the difficulty of distilling features\ndirectly for the feature imbalance issue. Besides, we find that instance-wise\nrelation supplements valuable knowledge beyond independent features for small\nobjects. Thus, the instance-wise relation distillation is designed, which\ncalculates the similarity of different instances to obtain a relation matrix.\nMore importantly, a relation filter module is designed to highlight valuable\ninstance relations. The proposed dual relation knowledge distillation is\ngeneral and can be easily applied for both one-stage and two-stage detectors.\nOur method achieves state-of-the-art performance, which improves Faster R-CNN\nbased on ResNet50 from 38.4% to 41.6% mAP and improves RetinaNet based on\nResNet50 from 37.4% to 40.3% mAP on COCO 2017."
}