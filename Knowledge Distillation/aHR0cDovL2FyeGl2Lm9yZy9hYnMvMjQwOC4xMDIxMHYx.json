{
  "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
  "authors": "Kamal Acharya, Alvaro Velasquez, Houbing Herbert Song",
  "year": 2024,
  "url": "http://arxiv.org/abs/2408.10210v1",
  "abstract": "This survey paper delves into the emerging and critical area of symbolic\nknowledge distillation in Large Language Models (LLMs). As LLMs like Generative\nPre-trained Transformer-3 (GPT-3) and Bidirectional Encoder Representations\nfrom Transformers (BERT) continue to expand in scale and complexity, the\nchallenge of effectively harnessing their extensive knowledge becomes\nparamount. This survey concentrates on the process of distilling the intricate,\noften implicit knowledge contained within these models into a more symbolic,\nexplicit form. This transformation is crucial for enhancing the\ninterpretability, efficiency, and applicability of LLMs. We categorize the\nexisting research based on methodologies and applications, focusing on how\nsymbolic knowledge distillation can be used to improve the transparency and\nfunctionality of smaller, more efficient Artificial Intelligence (AI) models.\nThe survey discusses the core challenges, including maintaining the depth of\nknowledge in a comprehensible format, and explores the various approaches and\ntechniques that have been developed in this field. We identify gaps in current\nresearch and potential opportunities for future advancements. This survey aims\nto provide a comprehensive overview of symbolic knowledge distillation in LLMs,\nspotlighting its significance in the progression towards more accessible and\nefficient AI systems."
}