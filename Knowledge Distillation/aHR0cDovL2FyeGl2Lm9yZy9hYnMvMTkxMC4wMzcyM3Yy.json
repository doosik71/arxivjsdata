{
  "url": "http://arxiv.org/abs/1910.03723v2",
  "title": "Knowledge Distillation from Internal Representations",
  "authors": "Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, Chenlei Guo",
  "year": 2019,
  "abstract": "Knowledge distillation is typically conducted by training a small model (the\nstudent) to mimic a large and cumbersome model (the teacher). The idea is to\ncompress the knowledge from the teacher by using its output probabilities as\nsoft-labels to optimize the student. However, when the teacher is considerably\nlarge, there is no guarantee that the internal knowledge of the teacher will be\ntransferred into the student; even if the student closely matches the\nsoft-labels, its internal representations may be considerably different. This\ninternal mismatch can undermine the generalization capabilities originally\nintended to be transferred from the teacher to the student. In this paper, we\npropose to distill the internal representations of a large model such as BERT\ninto a simplified version of it. We formulate two ways to distill such\nrepresentations and various algorithms to conduct the distillation. We\nexperiment with datasets from the GLUE benchmark and consistently show that\nadding knowledge distillation from internal representations is a more powerful\nmethod than only using soft-label distillation."
}