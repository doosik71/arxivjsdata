{
  "title": "Attention Distillation: self-supervised vision transformer students need\n  more guidance",
  "authors": "Kai Wang, Fei Yang, Joost van de Weijer",
  "year": 2022,
  "url": "http://arxiv.org/abs/2210.00944v1",
  "abstract": "Self-supervised learning has been widely applied to train high-quality vision\ntransformers. Unleashing their excellent performance on memory and compute\nconstraint devices is therefore an important research topic. However, how to\ndistill knowledge from one self-supervised ViT to another has not yet been\nexplored. Moreover, the existing self-supervised knowledge distillation (SSKD)\nmethods focus on ConvNet based architectures are suboptimal for ViT knowledge\ndistillation. In this paper, we study knowledge distillation of self-supervised\nvision transformers (ViT-SSKD). We show that directly distilling information\nfrom the crucial attention mechanism from teacher to student can significantly\nnarrow the performance gap between both. In experiments on ImageNet-Subset and\nImageNet-1K, we show that our method AttnDistill outperforms existing\nself-supervised knowledge distillation (SSKD) methods and achieves\nstate-of-the-art k-NN accuracy compared with self-supervised learning (SSL)\nmethods learning from scratch (with the ViT-S model). We are also the first to\napply the tiny ViT-T model on self-supervised learning. Moreover, AttnDistill\nis independent of self-supervised learning algorithms, it can be adapted to ViT\nbased SSL methods to improve the performance in future research. The code is\nhere: https://github.com/wangkai930418/attndistill"
}