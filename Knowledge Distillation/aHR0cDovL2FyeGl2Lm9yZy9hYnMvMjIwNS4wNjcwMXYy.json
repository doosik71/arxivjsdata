{
  "title": "Knowledge Distillation Meets Open-Set Semi-Supervised Learning",
  "authors": "Jing Yang, Xiatian Zhu, Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos",
  "year": 2022,
  "url": "http://arxiv.org/abs/2205.06701v2",
  "abstract": "Existing knowledge distillation methods mostly focus on distillation of\nteacher's prediction and intermediate activation. However, the structured\nrepresentation, which arguably is one of the most critical ingredients of deep\nmodels, is largely overlooked. In this work, we propose a novel {\\em\n\\modelname{}} ({\\bf\\em \\shortname{})} method dedicated for distilling\nrepresentational knowledge semantically from a pretrained teacher to a target\nstudent. The key idea is that we leverage the teacher's classifier as a\nsemantic critic for evaluating the representations of both teacher and student\nand distilling the semantic knowledge with high-order structured information\nover all feature dimensions. This is accomplished by introducing a notion of\ncross-network logit computed through passing student's representation into\nteacher's classifier. Further, considering the set of seen classes as a basis\nfor the semantic space in a combinatorial perspective, we scale \\shortname{} to\nunseen classes for enabling effective exploitation of largely available,\narbitrary unlabeled training data. At the problem level, this establishes an\ninteresting connection between knowledge distillation with open-set\nsemi-supervised learning (SSL). Extensive experiments show that our\n\\shortname{} outperforms significantly previous state-of-the-art knowledge\ndistillation methods on both coarse object classification and fine face\nrecognition tasks, as well as less studied yet practically crucial binary\nnetwork distillation. Under more realistic open-set SSL settings we introduce,\nwe reveal that knowledge distillation is generally more effective than existing\nOut-Of-Distribution (OOD) sample detection, and our proposed \\shortname{} is\nsuperior over both previous distillation and SSL competitors. The source code\nis available at \\url{https://github.com/jingyang2017/SRD\\_ossl}."
}