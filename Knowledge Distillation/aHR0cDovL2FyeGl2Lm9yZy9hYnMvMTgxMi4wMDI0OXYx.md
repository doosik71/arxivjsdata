# On Compressing U-net Using Knowledge Distillation
Karttikeya Mangalam, Dr. Mathieu Salzamann

## 🧩 Problem to Solve
최근 딥러닝 모델들은 이미지 인식 및 분할과 같은 컴퓨터 비전 분야에서 탁월한 성능을 보여주고 있습니다. 특히 U-net은 생체 의료 영상 분할에서 효과적이지만, 그 거대한 파라미터 수는 모바일 장치와 같이 리소스가 제한된 환경에 배포하기 어렵게 만듭니다. 이 연구는 대형 모델과 유사한 성능을 유지하면서 더 압축된 네트워크를 학습시키는 방법을 모색합니다.

## ✨ Key Contributions
*   표준 지식 증류(Knowledge Distillation, KD)만으로는 U-net을 안정적으로 압축하는 데 불충분함을 보여주었습니다.
*   지식 증류에 배치 정규화(Batch Normalization)와 클래스 재가중치(Class Re-weighting)를 도입하여 학습 과정을 크게 개선했습니다.
*   제안된 방법을 통해 U-net의 파라미터 수를 1000배 이상 (원본의 0.1% 수준) 압축하면서도 성능 저하를 거의 발생시키지 않았습니다.

## 📎 Related Works
*   **네트워크 압축 기법:** 가중치 양자화(Weight Quantization), 아키텍처 가지치기(Architecture Pruning), 지식 증류(Knowledge Distillation)의 세 가지 범주로 나뉩니다.
*   **초기 지식 증류:**
    *   Caruana et al. [2]: 다층 퍼셉트론에 RMSE 기반 오차 지표를 사용한 초기 증류 방법.
    *   Hinton et al. [5]: 교사(teacher) 네트워크의 예측 확률(soft probabilities)을 활용하여 학생(student) 네트워크를 학습시키는 접근 방식으로 이 개념을 대중화했습니다.
    *   Romero et al. [4]: 교사 네트워크의 중간 표현(intermediate representations)을 활용하여 학생 네트워크를 지도하는 방법을 제안했습니다.
*   **U-net:** Ronneberger et al. [12]이 생체 의료 영상 분할을 위해 도입한 아키텍처.
*   **배치 정규화:** Ioffe and Szegedy [6]에 의해 내부 공변량 변화(internal covariate shift)를 줄여 딥 네트워크 학습을 가속화하는 방법으로 제안되었습니다.

## 🛠️ Methodology
*   **U-net 아키텍처:** [12]에서 소개된 U-net은 축소 경로(contracting path)와 확장 경로(expansive path)로 구성된 완전 컨볼루션 네트워크이며 스킵 연결(skip connections)을 포함합니다.
*   **지식 증류 (힌튼 방식 기반):**
    *   입력 $W \times H$ 이미지 $x$와 출력 $W \times H$ 이진 레이블 맵 $y$를 사용하는 이진 이미지 분할 문제를 다룹니다.
    *   학습 과정은 교차 엔트로피 손실 $H(y^{t}, \hat{y}^{t}) = -\sum_{(i,j)} y^{t}_{ij} \log(\hat{y}^{t}_{ij}) + (1-y^{t}_{ij}) \log(1-\hat{y}^{t}_{ij})$을 사용합니다.
    *   교사 네트워크는 온도 매개변수 $T > 1$를 사용하여 더 부드러운 확률 $\hat{y}^{t*}$을 생성합니다.
    *   학생 네트워크는 이 $\hat{y}^{t*}$를 사용하여 $H(y^{t*}, \hat{y}^{t})$ (바닐라 증류) 또는 원본 교차 엔트로피와 결합하여 (혼합 증류) 학습됩니다. 학습 후 학생 네트워크의 소프트맥스 온도는 1로 다시 낮춰집니다.
*   **증류 개선 (제안된 수정):**
    표준 증류로는 U-net을 충분히 압축하기 어려웠기에 다음 두 가지를 제안했습니다. (학생 네트워크에만 적용)
    1.  **배치 정규화 [6]:** 축소 경로의 모든 컨볼루션 레이어에 배치 정규화를 추가하여 내부 공변량 변화를 줄입니다.
    2.  **클래스 재가중치:** 학습 데이터셋 내 클래스 비율에 따라 각 클래스에 가중치를 부여합니다. 전경(foreground) 픽셀의 손실 기여도는 배경 픽셀 수에 대한 전경 픽셀 수의 비율($w_f = 17.8$)로 곱해지고, 배경(background) 픽셀의 기여도는 변경되지 않습니다($w_b = 1.0$). 이는 클래스 불균형 문제를 해결하는 데 도움을 줍니다.

## 📊 Results
*   **증류 없는 압축:** 증류 없이도 초기 U-net의 채널 수를 64에서 4로 줄인 4-Unet이 원본 64-Unet과 유사한 테스트 손실을 달성했습니다. 이 4-Unet이 이후 증류 실험에서 교사 네트워크로 사용되었습니다.
*   **표준 증류의 실패:** 4-Unet으로부터 2-Unet을 학습시키기 위한 표준 증류(바닐라, 혼합, 순차 증류)는 클래스 재가중치를 적용했음에도 불구하고 모두 실패했습니다.
*   **수정된 U-net 증류의 성공:** 배치 정규화와 클래스 재가중치가 통합된 수정된 2-Unet은 표준 64-Unet과 유사한 분할 정확도를 달성했습니다. 이 2-Unet은 64-Unet 용량의 약 0.1% (파라미터 수: 30,902 vs. 31,042,434)만을 필요로 했습니다.
*   이러한 수정이 적용되더라도 증류는 여전히 필수적이었습니다. 증류 없이 수정된 2-Unet을 학습시켰을 때는 성능이 유의미하게 저하되었습니다.

## 🧠 Insights & Discussion
*   원래 U-net은 매우 과잉 파라미터화되어 있어, 증류 없이도 상당한 압축이 가능합니다.
*   배치 정규화와 클래스 재가중치와 같은 보조 정규화 기법은 작은 학생 네트워크의 불안정한 학습을 안정화하는 데 결정적인 역할을 합니다.
*   이는 작은 네트워크가 학습하기 어렵다는 일반적인 문제를 해결하는 데 도움을 줍니다.
*   이 연구는 리소스 제약이 있는 환경에 대규모 딥러닝 모델을 배포하기 위한 실용적인 방법을 제시합니다.

## 📌 TL;DR
대규모 U-net 모델은 리소스 제약 환경에 부적합합니다. 이 연구는 표준 지식 증류만으로는 U-net의 효과적인 압축이 어렵다는 문제를 제기합니다. 해결책으로 학생 네트워크에 배치 정규화와 클래스 재가중치를 추가한 수정된 지식 증류 전략을 제안합니다. 이 방법을 통해 U-net의 파라미터 수를 1000배 이상 압축하면서도 원본 모델에 필적하는 분할 정확도를 유지할 수 있음을 입증했습니다.