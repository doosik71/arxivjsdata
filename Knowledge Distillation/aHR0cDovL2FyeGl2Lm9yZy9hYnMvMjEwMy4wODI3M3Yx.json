{
  "title": "Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge\n  Distillation",
  "authors": "Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, Il-Chul Moon",
  "year": 2021,
  "url": "http://arxiv.org/abs/2103.08273v1",
  "abstract": "Knowledge distillation is a method of transferring the knowledge from a\npretrained complex teacher model to a student model, so a smaller network can\nreplace a large teacher network at the deployment stage. To reduce the\nnecessity of training a large teacher model, the recent literatures introduced\na self-knowledge distillation, which trains a student network progressively to\ndistill its own knowledge without a pretrained teacher network. While\nSelf-knowledge distillation is largely divided into a data augmentation based\napproach and an auxiliary network based approach, the data augmentation\napproach looses its local information in the augmentation process, which\nhinders its applicability to diverse vision tasks, such as semantic\nsegmentation. Moreover, these knowledge distillation approaches do not receive\nthe refined feature maps, which are prevalent in the object detection and\nsemantic segmentation community. This paper proposes a novel self-knowledge\ndistillation method, Feature Refinement via Self-Knowledge Distillation\n(FRSKD), which utilizes an auxiliary self-teacher network to transfer a refined\nknowledge for the classifier network. Our proposed method, FRSKD, can utilize\nboth soft label and feature-map distillations for the self-knowledge\ndistillation. Therefore, FRSKD can be applied to classification, and semantic\nsegmentation, which emphasize preserving the local information. We demonstrate\nthe effectiveness of FRSKD by enumerating its performance improvements in\ndiverse tasks and benchmark datasets. The implemented code is available at\nhttps://github.com/MingiJi/FRSKD."
}