{
  "url": "http://arxiv.org/abs/1812.00249v1",
  "title": "On Compressing U-net Using Knowledge Distillation",
  "authors": "Karttikeya Mangalam, Mathieu Salzamann",
  "year": 2018,
  "abstract": "We study the use of knowledge distillation to compress the U-net\narchitecture. We show that, while standard distillation is not sufficient to\nreliably train a compressed U-net, introducing other regularization methods,\nsuch as batch normalization and class re-weighting, in knowledge distillation\nsignificantly improves the training process. This allows us to compress a U-net\nby over 1000x, i.e., to 0.1% of its original number of parameters, at a\nnegligible decrease in performance."
}