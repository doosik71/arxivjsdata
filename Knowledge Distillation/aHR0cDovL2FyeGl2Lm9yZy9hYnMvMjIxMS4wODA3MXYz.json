{
  "title": "KD-DETR: Knowledge Distillation for Detection Transformer with\n  Consistent Distillation Points Sampling",
  "authors": "Yu Wang, Xin Li, Shengzhao Weng, Gang Zhang, Haixiao Yue, Haocheng Feng, Junyu Han, Errui Ding",
  "year": 2022,
  "url": "http://arxiv.org/abs/2211.08071v3",
  "abstract": "DETR is a novel end-to-end transformer architecture object detector, which\nsignificantly outperforms classic detectors when scaling up. In this paper, we\nfocus on the compression of DETR with knowledge distillation. While knowledge\ndistillation has been well-studied in classic detectors, there is a lack of\nresearches on how to make it work effectively on DETR. We first provide\nexperimental and theoretical analysis to point out that the main challenge in\nDETR distillation is the lack of consistent distillation points. Distillation\npoints refer to the corresponding inputs of the predictions for student to\nmimic, which have different formulations in CNN detector and DETR, and reliable\ndistillation requires sufficient distillation points which are consistent\nbetween teacher and student.\n  Based on this observation, we propose the first general knowledge\ndistillation paradigm for DETR (KD-DETR) with consistent distillation points\nsampling, for both homogeneous and heterogeneous distillation. Specifically, we\ndecouple detection and distillation tasks by introducing a set of specialized\nobject queries to construct distillation points for DETR. We further propose a\ngeneral-to-specific distillation points sampling strategy to explore the\nextensibility of KD-DETR. Extensive experiments validate the effectiveness and\ngeneralization of KD-DETR. For both single-scale DAB-DETR and multis-scale\nDeformable DETR and DINO, KD-DETR boost the performance of student model with\nimprovements of $2.6\\%-5.2\\%$. We further extend KD-DETR to heterogeneous\ndistillation, and achieves $2.1\\%$ improvement by distilling the knowledge from\nDINO to Faster R-CNN with ResNet-50, which is comparable with homogeneous\ndistillation methods.The code is available at\nhttps://github.com/wennyuhey/KD-DETR."
}