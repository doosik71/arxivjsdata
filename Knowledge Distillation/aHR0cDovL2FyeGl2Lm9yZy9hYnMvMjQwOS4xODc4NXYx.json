{
  "title": "Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation",
  "authors": "Chaomin Shen, Yaomin Huang, Haokun Zhu, Jinsong Fan, Guixu Zhang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.18785v1",
  "abstract": "Knowledge distillation has become widely recognized for its ability to\ntransfer knowledge from a large teacher network to a compact and more\nstreamlined student network. Traditional knowledge distillation methods\nprimarily follow a teacher-oriented paradigm that imposes the task of learning\nthe teacher's complex knowledge onto the student network. However, significant\ndisparities in model capacity and architectural design hinder the student's\ncomprehension of the complex knowledge imparted by the teacher, resulting in\nsub-optimal performance. This paper introduces a novel perspective emphasizing\nstudent-oriented and refining the teacher's knowledge to better align with the\nstudent's needs, thereby improving knowledge transfer effectiveness.\nSpecifically, we present the Student-Oriented Knowledge Distillation (SoKD),\nwhich incorporates a learnable feature augmentation strategy during training to\nrefine the teacher's knowledge of the student dynamically. Furthermore, we\ndeploy the Distinctive Area Detection Module (DAM) to identify areas of mutual\ninterest between the teacher and student, concentrating knowledge transfer\nwithin these critical areas to avoid transferring irrelevant information. This\ncustomized module ensures a more focused and effective knowledge distillation\nprocess. Our approach, functioning as a plug-in, could be integrated with\nvarious knowledge distillation methods. Extensive experimental results\ndemonstrate the efficacy and generalizability of our method."
}