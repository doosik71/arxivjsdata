{
  "title": "Small Scale Data-Free Knowledge Distillation",
  "authors": "He Liu, Yikai Wang, Huaping Liu, Fuchun Sun, Anbang Yao",
  "year": 2024,
  "url": "http://arxiv.org/abs/2406.07876v1",
  "abstract": "Data-free knowledge distillation is able to utilize the knowledge learned by\na large teacher network to augment the training of a smaller student network\nwithout accessing the original training data, avoiding privacy, security, and\nproprietary risks in real applications. In this line of research, existing\nmethods typically follow an inversion-and-distillation paradigm in which a\ngenerative adversarial network on-the-fly trained with the guidance of the\npre-trained teacher network is used to synthesize a large-scale sample set for\nknowledge distillation. In this paper, we reexamine this common data-free\nknowledge distillation paradigm, showing that there is considerable room to\nimprove the overall training efficiency through a lens of ``small-scale\ninverted data for knowledge distillation\". In light of three empirical\nobservations indicating the importance of how to balance class distributions in\nterms of synthetic sample diversity and difficulty during both data inversion\nand distillation processes, we propose Small Scale Data-free Knowledge\nDistillation SSD-KD. In formulation, SSD-KD introduces a modulating function to\nbalance synthetic samples and a priority sampling function to select proper\nsamples, facilitated by a dynamic replay buffer and a reinforcement learning\nstrategy. As a result, SSD-KD can perform distillation training conditioned on\nan extremely small scale of synthetic samples (e.g., 10X less than the original\ntraining data scale), making the overall training efficiency one or two orders\nof magnitude faster than many mainstream methods while retaining superior or\ncompetitive model performance, as demonstrated on popular image classification\nand semantic segmentation benchmarks. The code is available at\nhttps://github.com/OSVAI/SSD-KD."
}