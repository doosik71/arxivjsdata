{
  "title": "Progressive Class-level Distillation",
  "authors": "Jiayan Li, Jun Li, Zhourui Zhang, Jianhua Xu",
  "year": 2025,
  "url": "http://arxiv.org/abs/2505.24310v1",
  "abstract": "In knowledge distillation (KD), logit distillation (LD) aims to transfer\nclass-level knowledge from a more powerful teacher network to a small student\nmodel via accurate teacher-student alignment at the logits level. Since\nhigh-confidence object classes usually dominate the distillation process,\nlow-probability classes which also contain discriminating information are\ndownplayed in conventional methods, leading to insufficient knowledge transfer.\nTo address this issue, we propose a simple yet effective LD method termed\nProgressive Class-level Distillation (PCD). In contrast to existing methods\nwhich perform all-class ensemble distillation, our PCD approach performs\nstage-wise distillation for step-by-step knowledge transfer. More specifically,\nwe perform ranking on teacher-student logits difference for identifying\ndistillation priority from scratch, and subsequently divide the entire LD\nprocess into multiple stages. Next, bidirectional stage-wise distillation\nincorporating fine-to-coarse progressive learning and reverse coarse-to-fine\nrefinement is conducted, allowing comprehensive knowledge transfer via\nsufficient logits alignment within separate class groups in different\ndistillation stages. Extension experiments on public benchmarking datasets\ndemonstrate the superiority of our method compared to state-of-the-arts for\nboth classification and detection tasks.",
  "citation": 0
}