{
  "title": "Learning from Stochastic Teacher Representations Using Student-Guided\n  Knowledge Distillation",
  "authors": "Muhammad Haseeb Aslam, Clara Martinez, Marco Pedersoli, Alessandro Koerich, Ali Etemad, Eric Granger",
  "year": 2025,
  "url": "http://arxiv.org/abs/2504.14307v2",
  "abstract": "Advances in self-distillation have shown that when knowledge is distilled\nfrom a teacher to a student using the same deep learning (DL) architecture, the\nstudent performance can surpass the teacher particularly when the network is\noverparameterized and the teacher is trained with early stopping.\nAlternatively, ensemble learning also improves performance, although training,\nstoring, and deploying multiple models becomes impractical as the number of\nmodels grows. Even distilling an ensemble to a single student model or weight\naveraging methods first requires training of multiple teacher models and does\nnot fully leverage the inherent stochasticity for generating and distilling\ndiversity in DL models. These constraints are particularly prohibitive in\nresource-constrained or latency-sensitive applications such as wearable\ndevices. This paper proposes to train only one model and generate multiple\ndiverse teacher representations using distillation-time dropout. However,\ngenerating these representations stochastically leads to noisy representations\nthat are misaligned with the learned task. To overcome this problem, a novel\nstochastic self-distillation (SSD) training strategy is introduced for\nfiltering and weighting teacher representation to distill from task-relevant\nrepresentations only, using student-guided knowledge distillation (SGKD). The\nstudent representation at each distillation step is used as authority to guide\nthe distillation process. Experimental results on real-world affective\ncomputing, wearable/biosignal datasets from the UCR Archive, the HAR dataset,\nand image classification datasets show that the proposed SSD method can\noutperform state-of-the-art methods without increasing the model size at both\ntraining and testing time, and incurs negligible computational complexity\ncompared to state-of-the-art ensemble learning and weight averaging methods.",
  "citation": 0
}