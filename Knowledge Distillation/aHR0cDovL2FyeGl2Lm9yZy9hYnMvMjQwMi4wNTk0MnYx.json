{
  "title": "Cooperative Knowledge Distillation: A Learner Agnostic Approach",
  "authors": "Michael Livanos, Ian Davidson, Stephen Wong",
  "year": 2024,
  "url": "http://arxiv.org/abs/2402.05942v1",
  "abstract": "Knowledge distillation is a simple but powerful way to transfer knowledge\nbetween a teacher model to a student model. Existing work suffers from at least\none of the following key limitations in terms of direction and scope of\ntransfer which restrict its use: all knowledge is transferred from teacher to\nstudent regardless of whether or not that knowledge is useful, the student is\nthe only one learning in this exchange, and typically distillation transfers\nknowledge only from a single teacher to a single student. We formulate a novel\nform of knowledge distillation in which many models can act as both students\nand teachers which we call cooperative distillation. The models cooperate as\nfollows: a model (the student) identifies specific deficiencies in it's\nperformance and searches for another model (the teacher) who encodes learned\nknowledge into instructional virtual instances via counterfactual instance\ngeneration. Because different models may have different strengths and\nweaknesses, all models can act as either students or teachers (cooperation)\nwhen appropriate and only distill knowledge in areas specific to their\nstrengths (focus). Since counterfactuals as a paradigm are not tied to any\nspecific algorithm, we can use this method to distill knowledge between\nlearners of different architectures, algorithms, and even feature spaces. We\ndemonstrate that our approach not only outperforms baselines such as transfer\nlearning, self-supervised learning, and multiple knowledge distillation\nalgorithms on several datasets, but it can also be used in settings where the\naforementioned techniques cannot."
}