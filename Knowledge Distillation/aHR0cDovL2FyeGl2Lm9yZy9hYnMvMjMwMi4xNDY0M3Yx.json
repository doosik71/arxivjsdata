{
  "title": "Graph-based Knowledge Distillation: A survey and experimental evaluation",
  "authors": "Jing Liu, Tongya Zheng, Guanzheng Zhang, Qinfen Hao",
  "year": 2023,
  "url": "http://arxiv.org/abs/2302.14643v1",
  "abstract": "Graph, such as citation networks, social networks, and transportation\nnetworks, are prevalent in the real world. Graph Neural Networks (GNNs) have\ngained widespread attention for their robust expressiveness and exceptional\nperformance in various graph applications. However, the efficacy of GNNs is\nheavily reliant on sufficient data labels and complex network models, with the\nformer obtaining hardly and the latter computing costly. To address the labeled\ndata scarcity and high complexity of GNNs, Knowledge Distillation (KD) has been\nintroduced to enhance existing GNNs. This technique involves transferring the\nsoft-label supervision of the large teacher model to the small student model\nwhile maintaining prediction performance. This survey offers a comprehensive\noverview of Graph-based Knowledge Distillation methods, systematically\ncategorizing and summarizing them while discussing their limitations and future\ndirections. This paper first introduces the background of graph and KD. It then\nprovides a comprehensive summary of three types of Graph-based Knowledge\nDistillation methods, namely Graph-based Knowledge Distillation for deep neural\nnetworks (DKD), Graph-based Knowledge Distillation for GNNs (GKD), and\nSelf-Knowledge Distillation based Graph-based Knowledge Distillation (SKD).\nEach type is further divided into knowledge distillation methods based on the\noutput layer, middle layer, and constructed graph. Subsequently, various\nalgorithms' ideas are analyzed and compared, concluding with the advantages and\ndisadvantages of each algorithm supported by experimental results. In addition,\nthe applications of graph-based knowledge distillation in CV, NLP, RS, and\nother fields are listed. Finally, the graph-based knowledge distillation is\nsummarized and prospectively discussed. We have also released related resources\nat https://github.com/liujing1023/Graph-based-Knowledge-Distillation."
}