{
  "title": "A Survey on Recent Teacher-student Learning Studies",
  "authors": "Minghong Gao",
  "year": 2023,
  "url": "http://arxiv.org/abs/2304.04615v1",
  "abstract": "Knowledge distillation is a method of transferring the knowledge from a\ncomplex deep neural network (DNN) to a smaller and faster DNN, while preserving\nits accuracy. Recent variants of knowledge distillation include teaching\nassistant distillation, curriculum distillation, mask distillation, and\ndecoupling distillation, which aim to improve the performance of knowledge\ndistillation by introducing additional components or by changing the learning\nprocess. Teaching assistant distillation involves an intermediate model called\nthe teaching assistant, while curriculum distillation follows a curriculum\nsimilar to human education. Mask distillation focuses on transferring the\nattention mechanism learned by the teacher, and decoupling distillation\ndecouples the distillation loss from the task loss. Overall, these variants of\nknowledge distillation have shown promising results in improving the\nperformance of knowledge distillation.",
  "citation": 0
}