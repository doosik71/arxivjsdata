{
  "title": "Frequency Attention for Knowledge Distillation",
  "authors": "Cuong Pham, Van-Anh Nguyen, Trung Le, Dinh Phung, Gustavo Carneiro, Thanh-Toan Do",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.05894v1",
  "abstract": "Knowledge distillation is an attractive approach for learning compact deep\nneural networks, which learns a lightweight student model by distilling\nknowledge from a complex teacher model. Attention-based knowledge distillation\nis a specific form of intermediate feature-based knowledge distillation that\nuses attention mechanisms to encourage the student to better mimic the teacher.\nHowever, most of the previous attention-based distillation approaches perform\nattention in the spatial domain, which primarily affects local regions in the\ninput image. This may not be sufficient when we need to capture the broader\ncontext or global information necessary for effective knowledge transfer. In\nfrequency domain, since each frequency is determined from all pixels of the\nimage in spatial domain, it can contain global information about the image.\nInspired by the benefits of the frequency domain, we propose a novel module\nthat functions as an attention mechanism in the frequency domain. The module\nconsists of a learnable global filter that can adjust the frequencies of\nstudent's features under the guidance of the teacher's features, which\nencourages the student's features to have patterns similar to the teacher's\nfeatures. We then propose an enhanced knowledge review-based distillation model\nby leveraging the proposed frequency attention module. The extensive\nexperiments with various teacher and student architectures on image\nclassification and object detection benchmark datasets show that the proposed\napproach outperforms other knowledge distillation methods."
}