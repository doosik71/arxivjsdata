# Towards Oracle Knowledge Distillation with Neural Architecture Search

Minsoo Kang, Jonghwan Mun, Bohyung Han

## 🧩 Problem to Solve

기존의 지식 증류(Knowledge Distillation, KD) 방법론은 강력한 앙상블 교사 모델(ensemble teacher model)로부터 효율적인 학생 모델을 학습하는 데 있어 다음과 같은 문제점을 안고 있습니다:

1. **모델 용량(Capacity) 격차:** 교사 모델(특히 앙상블 모델)과 학생 모델 간의 모델 용량(예: 파라미터 수) 격차가 클수록 학생 모델의 학습 능력이 저해되어 교사 모델의 성능에 미치지 못합니다. 이는 지식 증류 과정의 병목 현상으로 작용합니다.
2. **앙상블 지식 활용의 비효율성:** 앙상블 교사 모델은 개별 모델들의 평균 예측(average prediction)이나 다수결 투표(majority voting)를 통해 지식을 전달하는데, 이는 앙상블의 최적 성능(오라클 예측)을 효과적으로 활용하지 못합니다. 앙상블 모델의 모든 구성원이 올바른 예측을 하지 못할 경우, 평균 예측은 틀릴 수 있습니다.

이러한 문제들로 인해, 대규모 앙상블 교사 모델로부터 효과적으로 지식을 추출하고, 이를 통해 경쟁력 있는 학생 모델을 훈련하는 것이 어려운 과제로 남아 있습니다.

## ✨ Key Contributions

본 논문의 주요 기여는 다음과 같습니다:

- **신경망 구조 탐색(Neural Architecture Search, NAS)을 통합한 새로운 지식 증류 프레임워크 제안:** 학생 모델의 용량 문제를 해결하고 지식 증류에 적합한 최적의 구조와 연산을 찾아냅니다. 이는 고정된 학생 모델 용량을 가정하는 기존 방식과 다릅니다.
- **새로운 오라클 지식 증류(Oracle Knowledge Distillation, OD) 손실 함수 도입:** 특히 앙상블 교사 모델에 유용하며, 학생 네트워크가 앙상블 교사의 오라클 예측(각 예제에 대해 정확히 예측한 모델들의 평균)을 모방하도록 학습하여 교사 모델을 능가하는 잠재력을 가집니다.
- **다양한 환경에서의 뛰어난 성능 입증:** 제안된 방법이 CIFAR-100 및 TinyImageNet 데이터셋에서 높은 정확도와 메모리 효율성을 동시에 달성함을 보여줍니다.
- **모델 용량 격차, 구조 탐색 목표 함수, 지식 증류 손실 함수 등 앙상블 교사 모델로부터의 지식 증류에 대한 포괄적인 분석 제공:** 이를 통해 지식 증류 분야에 대한 심층적인 이해를 돕습니다.

## 📎 Related Works

- **지식 증류(Knowledge Distillation, KD):**
  - **기본 KD:** Hinton et al. (2015)이 교사 네트워크의 출력 분포를 모방하도록 학생 네트워크를 학습시키는 방법을 제안.
  - **향상된 KD 방법:** 중간 계층 활성화(Romero et al. 2015), 어텐션 맵(Zagoruyko and Komodakis 2017), 인코딩된 전송 가능 요소(Kim et al. 2018), 적대적 샘플(Heo et al. 2019) 또는 훈련 예제 간의 관계 정보(Park et al. 2019)를 사용하여 정보 추출 및 전달 개선.
  - **상호 학습(Mutual Learning):** Zhang et al. (2018)은 양방향 상호작용을 통해 모델을 공동 학습.
  - **다른 목적의 KD:** 모델 압축 외에 점진적 학습(Chen et al. 2016), 연속 학습의 치명적 망각(Li and Hoiem 2016), 시각 질의 응답(Mun et al. 2018), 자체 지도 학습(Noroozi et al. 2018) 등에 활용.
- **신경망 구조 탐색(Neural Architecture Search, NAS):**
  - **강화 학습 기반:** Zoph and Le (2017)은 RNN 컨트롤러를 사용하여 최적의 모델을 탐색 (REINFORCE 사용).
  - **탐색 효율성 개선:** ENAS (Pham et al. 2018)는 가중치 공유를 통해 탐색 가속. MnasNet (Tan et al. 2019)은 정확도와 추론 지연 시간을 공동 최적화.
  - **경사 기반:** DARTS (Liu et al. 2018) 및 NAO (Luo et al. 2018)는 구조 표현의 연속적 이완을 통해 경사 기반 탐색 수행.
  - **점진적 탐색:** PNAS (Liu et al. 2018)는 순차적 모델 기반 최적화를 통해 탐색 비용 절감.
- **본 연구의 차별점:**
  - 기존 KD가 고정된 학생 모델 용량을 전제로 성능 향상에 초점을 맞춘 것과 달리, 본 연구는 학생 모델의 용량 문제를 해결하기 위해 적합한 증강 구조를 탐색합니다.
  - 앙상블 교사로부터의 지식 전달에서 기존의 단순 평균 방식과 달리, 오라클 지식 증류를 통해 보다 정교하고 효과적인 지식 전달 손실 함수를 제안합니다.

## 🛠️ Methodology

본 논문은 오라클 지식 증류 손실을 사용하는 신경망 구조 탐색(KDAS: Knowledge Distillation with Architecture Search) 프레임워크를 제안합니다.

1. **지식 증류(Knowledge Distillation, KD) 손실:**

   - 기본 KD는 학생 네트워크의 출력 분포($l^{(i)}_{s}$)가 교사 네트워크의 출력 분포($l^{(i)}_{t}$)를 모방하도록 학습합니다.
   - 손실 함수 $L_{\text{KD}}$는 교차 엔트로피 손실 $L_{\text{CE}}$와 쿨백-라이블러(KL) 발산 손실 $L_{\text{KL}}$의 가중합으로 정의됩니다:
     $$ L*{\text{KD}}(l^{(i)}*{s}, l^{(i)}_{t}, y^{(i)}) = \lambda L_{\text{CE}}(l^{(i)}_{s}, y^{(i)}) + (1-\lambda)L_{\text{KL}}(l^{(i)}_{s}, l^{(i)}_{t}) $$
        여기서 $L_{\text{CE}}(l^{(i)}_{s}, y^{(i)}) = H(\sigma(l^{(i)}_{s}), y^{(i)})$이고, $L_{\text{KL}}(l^{(i)}_{s}, l^{(i)}_{t}) = T^2 D_{\text{KL}}(\sigma(l^{(i)}_{t}/T)||\sigma(l^{(i)}_{s}/T))$입니다. $T$는 온도(temperature) 파라미터입니다.

2. **오라클 지식 증류(Oracle Knowledge Distillation, OD) 손실:**

   - 앙상블 교사 모델의 평균 예측 대신, 각 예제에 대해 올바르게 예측한 모델들($u^{(i)}_{j}=1$인 경우)의 예측만을 평균하여 학생 네트워크를 학습합니다.
   - 앙상블 내 $N$개 네트워크 중 $j$-번째 모델이 $x^{(i)}$에 대해 올바른 예측을 했는지 여부를 나타내는 이진 변수를 $u^{(i)}_{j}$라고 할 때, 교사 로그잇은 $\bar{l}^{(i)}_{t} = \frac{\sum_{j=1}^{N} u^{(i)}_{j} l^{(i)}_{t,j}}{\sum_{j=1}^{N} u^{(i)}_{j}}$로 정의됩니다.
   - OD 손실 $L_{\text{OD}}$는 다음과 같습니다:
     $$ L*{\text{OD}} = \begin{cases} L*{\text{KD}}(l^{(i)}_{s}, \bar{l}^{(i)}_{t}, y^{(i)}) & \text{if } \sum*{j=1}^{N} u^{(i)}*{j} > 0 \\ L*{\text{CE}}(l^{(i)}*{s}, y^{(i)}) & \text{otherwise} \end{cases} $$
     이는 학생 네트워크가 앙상블 모델의 오라클 예측을 모방하도록 유도하며, 모든 모델이 틀린 경우에는 정답 레이블에 맞춰 학습합니다.

3. **최적 모델 탐색 (KDAS):**
   - **백본 기반 구조 탐색:** 기존 NAS가 처음부터 구조를 탐색하는 것과 달리, 학생 네트워크의 기존 구조(백본)를 시작점으로 사용하여 여기에 추가 연산(add-on operations)을 증강하여 네트워크를 확장합니다. 이는 탐색 공간을 크게 줄이고 학습을 안정화합니다.
   - **탐색 공간:** 표준 컨볼루션 신경망의 개별 "단계(stages)" 끝에 추가 연산을 삽입합니다. 사용 가능한 연산은 7가지(identity, $3\times3$ 및 $5\times5$ 컨볼루션, $3\times3$ 및 $5\times5$ depthwise-separable 컨볼루션, $3\times3$ max pooling 및 average pooling)와 스킵 연결(skip connection)을 포함합니다.
   - **최적화:**
     - LSTM 컨트롤러를 훈련하여 사전 정의된 탐색 공간에서 아키텍처를 샘플링합니다.
     - REINFORCE (Williams 1992)를 사용하여 LSTM 컨트롤러를 업데이트합니다.
     - 샘플링된 아키텍처 $m$에 대한 보상 $R(m)$은 검증 세트의 정확도로 정의되며, $J(\theta) \equiv E_{m \sim \pi(m;\theta)}[R(m)]$를 최대화하는 방향으로 학습합니다.
     - $\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{j=1}^{N} [R(m_j) \nabla_{\theta} \log\pi(m_j;\theta)]$ 공식을 사용하여 기울기를 계산하고, 분산 감소를 위해 baseline $b$를 사용합니다.
     - ENAS (Pham et al. 2018)의 효율적인 학습 방식을 따라 여러 후보 네트워크에서 특정 계층의 파라미터를 공유합니다.
   - **메모리 제약 조건 하의 네트워크 선택:** 컨트롤러가 수렴한 후, 메모리 제약 $M$을 만족하면서 가장 정확한 모델 $m^* = \operatorname{argmax}_m R(m), \text{s.t. } |m| \le M$을 선택합니다. 선택된 모델은 $L_{\text{OD}}$ 손실 함수로 처음부터 다시 훈련됩니다.

## 📊 Results

- **주요 실험 결과 (CIFAR-100 및 TinyImageNet):**
  - 고정된 용량의 학생 모델(M3-4)은 교사 모델에 비해 성능이 낮습니다.
  - 수동으로 설계된 더 큰 용량의 네트워크(ResNet-62, ResNet-110)는 성능 향상을 보였지만, KDAS로 탐색된 모델(M14-19)이 더 적은 메모리 크기로 더 나은 성능을 달성합니다.
  - 특히, $L_{\text{OD}}$를 사용하여 학습된 KDAS 모델(M19)은 교사 모델을 포함하여 모든 모델 중 가장 좋은 정확도를 달성했습니다 (CIFAR-100에서 77.27%, TinyImageNet에서 63.04%).
- **전통적인 NAS와의 비교:**
  - 지식 증류 손실 없이 NAS로만 탐색된 네트워크(M12-13)는 KDAS (M18-19)보다 성능이 낮았습니다. 이는 NAS와 지식 증류의 결합이 효과적임을 시사합니다.
- **다양한 메모리 크기 분석 (CIFAR-100):**
  - KDAS 모델은 수동으로 설계된 모델(MMN)보다 정확도와 효율성 면에서 뛰어났습니다.
  - 특히, 백본 학생 모델(0.47M)보다 2배 이상 큰 KDAS 모델(0.89M 이상)은 오라클 지식 증류 손실 덕분에 교사 모델보다 더 높은 정확도를 달성했습니다.
- **다양한 네트워크 아키텍처에 대한 평가:**
  - ResNet-110, WideResNet-40-1, WideResNet-16-2, WideResNet-16-4 등 다양한 백본 아키텍처에 대해 KDAS는 수동 설계된 모델을 일관되게 능가하며, 교사 모델과 경쟁하거나 능가하는 성능을 보였습니다. 이는 KDAS가 모델에 구애받지 않는(model-agnostic) 특성을 가짐을 의미합니다.
- **다른 KD 방법들과의 비교:**
  - 단일 교사 모델(ResNet-110)을 사용한 비교에서, KDAS(0.91M)로 탐색된 모델은 KD, DML, BSS, TAKD 등 다른 KD 방법들보다 ResNet-62 (0.96M) 및 ResNet-68 (1.05M) 학생 모델에 대해 더 높은 정확도를 보였습니다. 이는 오라클 지식 증류 손실 없이도 구조 탐색 자체가 KD에 유망한 방향임을 입증합니다.
- **KD 대비 OD의 실질적 이점:**
  - 실제 데이터셋에서는 앙상블 교사 모델의 모든 구성원이 대부분의 훈련 예제에 대해 정답을 예측하지 못합니다 (CIFAR-100에서 5개 모델 모두 정답 예측 비율 94.04%, TinyImageNet에서 70.28%).
  - 이는 훈련 예제의 상당 부분(TinyImageNet의 경우 50.4%)에서 OD와 KD의 목적 함수가 다르다는 것을 의미하며, OD가 KD에 비해 상당한 정확도 향상을 가져올 수 있음을 보여줍니다.

## 🧠 Insights & Discussion

- **용량 격차 해소의 중요성:** 교사-학생 모델 간의 용량 격차가 KD 성능에 미치는 부정적인 영향을 확인했으며, NAS를 통해 학생 모델의 용량을 적절히 확장하는 것이 이 격차를 해소하고 성능을 향상시키는 데 필수적임을 입증했습니다.
- **오라클 지식 증류의 효과:** 앙상블 교사 모델의 오라클 예측을 모방하도록 학생 모델을 학습시키는 전략은 평균 예측을 모방하는 것보다 훨씬 효과적이며, 학생 모델이 교사 모델의 성능을 뛰어넘을 수 있는 잠재력을 제공합니다. 이는 앙상블의 완전한 능력을 학생 모델에 전이하는 데 중요한 통찰입니다.
- **NAS와 KD의 시너지:** 신경망 구조 탐색과 지식 증류를 결합하는 KDAS 프레임워크는 단순히 더 큰 모델을 수동으로 설계하는 것보다 더 효율적이고 성능이 우수한 학생 모델을 찾을 수 있음을 보여줍니다. 이는 KD의 새로운 연구 방향을 제시합니다.
- **모델-불가지론적(Model-Agnostic) 특성:** 제안된 방법이 다양한 백본 네트워크에 대해 일관되게 좋은 성능을 보인다는 것은 KDAS가 특정 아키텍처에 국한되지 않고 일반적인 적용 가능성을 가짐을 시사합니다.
- **미래 연구 방향:** 최적의 학생 네트워크를 탐색하는 본 프레임워크는 지식 증류 분야의 유망한 연구 방향이며, 향후 더 복잡한 교사-학생 관계나 다양한 태스크에 대한 적용 가능성을 모색할 수 있습니다.

## 📌 TL;DR

앙상블 교사 모델로부터 효율적인 학생 모델을 학습하는 데 있어 교사-학생 모델 간의 **용량 격차**와 앙상블 지식 활용의 비효율성 문제가 있습니다. 본 논문은 이 문제를 해결하기 위해 **신경망 구조 탐색(NAS)**과 **오라클 지식 증류(OD) 손실 함수**를 결합한 **KDAS(Knowledge Distillation with Architecture Search)** 프레임워크를 제안합니다. KDAS는 학생 네트워크의 백본을 기반으로 최적의 증강 구조를 탐색하며, OD 손실은 학생 모델이 앙상블 교사의 **정답 예측만을 모방**하도록 유도합니다. 실험 결과, KDAS로 탐색된 모델은 기존 지식 증류 방법이나 수동 설계된 더 큰 네트워크보다 **우수한 정확도와 효율성**을 달성했으며, 심지어 **교사 모델의 성능을 능가**하는 경우가 많았습니다. 이는 지식 증류에서 학생 네트워크 구조를 최적화하는 것이 매우 유망한 방향임을 시사합니다.
