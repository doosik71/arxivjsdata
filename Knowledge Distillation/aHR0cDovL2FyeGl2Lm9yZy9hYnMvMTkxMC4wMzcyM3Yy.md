# Knowledge Distillation from Internal Representations
Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, Chenlei Guo

## 🧩 Problem to Solve
기존 지식 증류(Knowledge Distillation, KD)는 주로 교사 모델의 출력 확률(soft-labels)을 사용하여 학생 모델을 학습시키는 방식입니다. 그러나 교사 모델(예: BERT와 같은 대규모 트랜스포머 모델)이 상당히 클 경우, 학생 모델이 soft-labels를 잘 모방하더라도 교사 모델의 내부 표현(internal representations)이 효과적으로 전달된다는 보장이 없습니다. 이러한 내부 불일치는 교사로부터 학생에게 전달되기를 의도했던 일반화 능력을 저해할 수 있습니다. 또한, 대규모 모델은 수억 개의 매개변수를 가지므로 학습 및 추론 시간이 길고 메모리 소비가 많아, 컴퓨팅 자원이 제한적인 실제 환경에 적용하기 어렵다는 문제가 있습니다.

## ✨ Key Contributions
*   대규모 모델(BERT)의 **내부 표현**을 간소화된 모델로 증류하는 새로운 지식 증류 방법을 제안합니다.
*   내부 표현 증류를 위한 두 가지 손실 함수를 정식화했습니다:
    1.  모든 트랜스포머 헤드의 self-attention 확률에 대한 **Kullback-Leibler (KL) 발산 손실**
    2.  `[CLS]` 활성화 벡터 간의 **코사인 유사도 손실**
*   내부 증류를 수행하기 위한 다양한 알고리즘을 제안했습니다:
    *   모든 레이어를 동시에 증류하는 방법
    *   **점진적 내부 증류(Progressive Internal Distillation, PID)**: 하위 레이어부터 순차적으로 지식을 증류하는 방식
    *   **누적 내부 증류(Stacked Internal Distillation, SID)**: 하위 레이어부터 시작하여 이전 레이어에서 생성된 손실을 누적하면서 지식을 증류하는 방식
*   내부 표현 증류를 추가하는 것이 soft-label만을 사용하는 기존 KD보다 훨씬 강력한 방법임을 GLUE 벤치마크 데이터셋에서 일관되게 입증했습니다.

## 📎 Related Works
*   **표준 지식 증류(Standard KD)** (Hinton et al., 2015): 교사-학생 설정에서 교사의 출력 확률을 사용하여 학생을 학습시킵니다.
*   **FitNets** (Romero et al., 2014): 내부 표현을 KD에 활용한 초기 연구로, 컨볼루션 학생 네트워크의 중간 지점에서 내부 표현을 비교했습니다. 본 연구는 FitNets와 다음 측면에서 차이점을 가집니다: 1) 단일 포인트별 손실 대신 모든 학생 레이어에 걸쳐 증류를 적용하여 교사 레이어 그룹을 학생 레이어 하나로 압축합니다. 2) 추가 매개변수 없이 내부 표현을 그대로 사용합니다. 3) 교사보다 더 깊은 모델에 중점을 두지 않습니다.
*   **최근 트랜스포머 기반 증류 기법**:
    *   **DistilBERT** (Sanh et al., 2019): 사전 학습 중 BERT를 압축합니다 (마스크드 언어 모델링 손실, 은닉 상태의 코사인 임베딩 손실, 교사-학생 증류 손실 사용).
    *   **Sun et al. (2019)**: 특정 작업 미세 조정 중 모델을 증류합니다 (은닉 상태의 MSE 손실 및 soft/hard-label 교차 엔트로피 손실 사용).
    *   본 연구는 self-attention 확률에 대한 KL-발산 손실을 사용하고, PID 및 SID와 같은 새로운 알고리즘을 도입한다는 점에서 차이가 있습니다.
*   **커리큘럼 학습(Curriculum Learning, CL)** (Bengio, 2009): 복잡한 작업을 단순한 개념부터 가르치는 데 초점을 맞추지만, 본 연구는 최적화된 복잡한 모델의 내부 표현을 가르치는 데 중점을 둡니다.
*   **다른 모델 압축 기술**: 양자화(Quantization)와 가중치 가지치기(Weights Pruning)는 본 논문에서 제안하는 방법과 상호 보완적으로 사용될 수 있습니다.

## 🛠️ Methodology
본 연구는 표준 지식 증류 프레임워크를 기반으로 트랜스포머 기반 모델의 내부 지식을 증류하기 위한 목적 함수와 알고리즘을 제안합니다.

*   **지식 증류(Knowledge Distillation)**:
    *   교사 모델의 soft-label과 원본 hard-label을 모두 사용하여 학생 모델을 학습시킵니다.
    *   손실 함수 $L_{KD}$는 다음과 같습니다:
        $$L_{KD} = -\frac{1}{N} \sum_{i=1}^{N} p(y_i|x_i,\theta_T) \log(\hat{y}_i) - \lambda \frac{1}{N} \sum_{i=1}^{N} y_i \log(\hat{y}_i)$$
        여기서 $p(y_i|x_i,\theta_T)$는 교사의 soft-labels, $\hat{y}_i$는 학생의 예측, $y_i$는 hard-labels, $\lambda$는 hard-label 손실의 가중치입니다.

*   **내부 표현 매칭**:
    *   학생 모델이 교사 모델처럼 내부적으로 동작하도록 최적화합니다. 이는 교사의 여러 레이어에서 오는 지식을 학생의 단일 레이어로 압축하는 것을 목표로 합니다.
    *   **KL-발산 손실($L_{kl}$)**: 트랜스포머 레이어의 self-attention 행렬 $A$에 적용됩니다.
        $$L_{kl} = \frac{1}{L} \sum_{i=1}^{L} A_{T_i} \log \frac{A_{T_i}}{A_{S_i}}$$
        여기서 $L$은 시퀀스 길이, $A_{T_i}$와 $A_{S_i}$는 각각 교사와 학생의 self-attention 행렬의 $i$번째 행을 나타냅니다.
    *   **코사인 유사도 손실($L_{cos}$)**: `[CLS]` 토큰의 은닉 벡터 표현에 적용됩니다.
        $$L_{cos} = 1 - \cos(h_T, h_S)$$
        여기서 $h_T$와 $h_S$는 각각 교사와 학생의 `[CLS]` 토큰에 대한 은닉 벡터 표현입니다. 이 손실은 네트워크를 통과하는 활성화가 유사하도록 정규화합니다.

*   **내부 지식 증류 방법**:
    1.  **모든 레이어 동시 내부 증류**: 학생의 모든 레이어가 매 에폭마다 교사의 해당 레이어와 일치하도록 최적화됩니다.
    2.  **점진적 내부 증류(Progressive Internal Distillation, PID)**: 입력에 가까운 하위 레이어부터 지식을 증류하고, 점진적으로 상위 레이어로 이동합니다. 한 번에 하나의 레이어만 최적화됩니다.
    3.  **누적 내부 증류(Stacked Internal Distillation, SID)**: 하위 레이어부터 지식을 증류하되, 상위 레이어로 이동하면서 이전 레이어에서 발생한 손실을 계속 누적합니다. 최종적으로는 분류 손실만 수행합니다.
    *   PID와 SID의 레이어 전환은 레이어별 정해진 에폭 수 또는 코사인 손실 임계값 중 먼저 도달하는 조건에 따라 결정됩니다.

## 📊 Results
*   **데이터셋**: GLUE 벤치마크의 CoLA, QQP, MRPC, RTE 네 가지 데이터셋에서 실험을 수행했습니다.
*   **모델 설정**: 교사 모델은 BERT$_{base}$(12개 트랜스포머 레이어)를 사용했으며, 학생 모델은 BERT$_6$(6개 트랜스포머 레이어)를 사용했습니다. 이는 트랜스포머 레이어에서 약 50%의 매개변수 감소를 의미합니다.
*   **핵심 결과**: 제안된 내부 증류 방법(Exp3.X)은 지식 증류를 적용하지 않은 BERT$_6$(Exp1.1)와 표준 soft-label 증류(Exp2.0)보다 모든 데이터셋에서 일관되게 우수한 성능을 보였습니다.
*   **성능 vs. 매개변수**: 내부 증류를 사용한 BERT$_6$는 BERT$_{base}$와 거의 유사한 성능을 달성하면서도 매개변수를 크게 줄였습니다(예: QQP F1에서 BERT$_{base}$가 91.45, 내부 증류 BERT$_6$가 91.38). 레이어 수를 더 줄일 때도 내부 증류 방법이 더 높은 성능을 유지하는 데 강건함을 보였습니다.
*   **데이터 크기의 영향**: 데이터 크기가 클 때는 표준 KD와 내부 KD 간의 성능 차이가 작았으나, 데이터 크기가 작을수록 내부 KD 방법이 더 큰 이점을 가졌습니다.
*   **학생 모델 수렴**: 내부 증류 알고리즘(특히 PID 및 SID)은 학생 모델이 더 높은 성능에 도달하도록 도왔으며, 심지어 분류 레이어를 학습하기 전에도 내부 지식 전달만으로 상당한 성능 향상을 이뤘습니다.
*   **어텐션 동작 분석**: 내부 증류를 거친 학생 모델의 어텐션 패턴은 교사 모델의 어텐션 패턴을 매우 가깝게 복제했으며, 표준 KD 학생 모델에 비해 교사 모델과의 KL-발산이 현저히 낮았습니다. 이는 내부 지식의 효과적인 압축을 시사합니다.
*   **오류 분석**: 내부 증류를 거친 학생 모델은 교사 모델의 예측(정답 및 오답)과 더 높은 일치율을 보여, 교사의 일반화 능력을 더 잘 전달받았음을 입증했습니다.

## 🧠 Insights & Discussion
*   **중요성**: 본 연구는 대규모 모델에서 소형 모델로 지식을 효과적으로 전달하는 데 있어 출력 확률뿐만 아니라 내부 표현을 증류하는 것이 얼마나 중요한지 보여줍니다. 특히 트랜스포머 기반 모델에서 교사의 언어적 특성과 일반화 능력을 학생에게 효과적으로 전달할 수 있습니다.
*   **향상된 일반화**: 교사의 내부 동작(어텐션 패턴, `[CLS]` 벡터 활성화)을 모방함으로써, 학생은 교사가 전달하고자 했던 언어적 속성과 일반화 능력을 더 잘 포착합니다. 이는 표준 KD 방식에서 발생할 수 있는 내부 표현 불일치 문제를 해결합니다.
*   **효율성**: BERT$_{base}$에서 BERT$_6$로의 매개변수 50% 감소와 같은 상당한 압축률에도 불구하고 성능 저하를 최소화하여, 자원 제약이 있는 환경에서도 대규모 모델의 배포 가능성을 높입니다.
*   **방법론적 시사점**: self-attention에 대한 KL-발산과 `[CLS]` 벡터에 대한 코사인 유사도 손실은 내부 KD에 효과적인 손실 함수임이 입증되었습니다. 점진적 또는 누적 증류 전략은 복잡한 지식을 체계적으로 구축하는 데 유용합니다.
*   **한계 및 향후 연구**: 매우 높은 압축률(예: BERT$_1$)의 경우 내부 KD의 추가적인 성능 향상 효과가 덜 두드러진다는 점이 있습니다. 다른 압축 기술(양자화, 가지치기)과의 결합 연구나, 상이한 아키텍처로의 증류 연구도 가능할 것으로 보입니다. 또한, 교사의 오류를 학생이 더 많이 모방한다는 점은, 더 나은 일반화와 교사의 오류 패턴 상속 사이의 트레이드오프를 보여줍니다.

## 📌 TL;DR
*   **Problem**: 기존 지식 증류(KD)는 교사 모델의 출력 확률(soft-labels)에만 의존하여, 내부 지식 전달이 충분치 않아 학생 모델의 일반화 능력이 저해될 수 있으며, 대규모 모델의 비효율성을 해소하기 위한 압축이 필요합니다.
*   **Proposed Method**: 교사 모델(BERT)의 **내부 표현(self-attention 확률 및 `[CLS]` 활성화 벡터)**을 학생 모델에 증류하는 새로운 KD 방법을 제안합니다. 이를 위해 KL-발산 손실과 코사인 유사도 손실을 사용하며, 모든 레이어를 동시에 증류하거나 점진적(PID) 또는 누적(SID) 방식으로 지식을 전달하는 알고리즘을 제시합니다.
*   **Key Findings**: 제안된 내부 지식 증류는 GLUE 벤치마크에서 기존 soft-label KD보다 지속적으로 우수한 성능을 보였습니다. BERT$_{base}$ 모델 대비 50%의 매개변수 감소에도 불구하고 유사한 성능을 달성했으며, 특히 데이터 크기가 작을수록 이점은 더욱 두드러졌습니다. 학생 모델의 내부 어텐션 패턴이 교사 모델과 더 유사하게 동작함을 확인하여, 일반화 능력이 효과적으로 전달됨을 입증했습니다.