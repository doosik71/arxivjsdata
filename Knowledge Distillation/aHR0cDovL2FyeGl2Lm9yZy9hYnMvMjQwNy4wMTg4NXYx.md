# Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application

CHUANPENG YANG, WANG LU, YAO ZHU, YIDONG WANG, QIAN CHEN, CHENLONG GAO, BINGJIE YAN, YIQIANG CHEN

## 🧩 Problem to Solve

대규모 언어 모델(LLM)은 다양한 분야에서 탁월한 성능을 보여주었지만, 막대한 모델 크기와 계산 요구량으로 인해 특히 자원이 제한된 환경에서의 실제 배포에 상당한 어려움을 야기합니다. 성능 저하를 최소화하면서 LLM을 압축하는 것이 핵심 연구 과제가 되었으며, 지식 증류(Knowledge Distillation, KD)는 성능을 크게 저하시키지 않으면서 추론 속도를 향상시키는 효과적인 기술로 부상했습니다. 이 서베이 논문은 LLM을 위한 지식 증류 기술을 체계적으로 탐구합니다.

## ✨ Key Contributions

이 논문은 LLM을 위한 지식 증류 기술에 대한 포괄적인 서베이를 제시하며, 주요 기여는 다음과 같습니다:

- **종합적인 분석:** 지식 증류 방법, 평가, 응용의 세 가지 측면에서 LLM에 특화된 지식 증류 기술을 심층적으로 탐구합니다.
- **체계적인 분류:** 지식 증류 방법론을 화이트박스(White-box) KD와 블랙박스(Black-box) KD로 명확하게 분류하여 각 접근 방식의 차이점을 상세히 설명합니다.
- **평가 및 효과 분석:** 다양한 증류 방법론 간의 평가 작업과 증류 효과를 분석하여, 특정 시나리오에서의 강점과 약점을 제시합니다. 특히 강건성(robustness) 관점에서 통합된 평가를 수행했습니다.
- **실제 응용 사례 제시:** LLM 압축 및 배포에 지식 증류 기술이 적용된 실제 사례들을 제시하여, 독자들이 다양한 규모의 LLM에 최적의 KD 방식을 선택하는 데 도움을 줍니다.
- **미래 연구 방향 제안:** LLM 지식 증류 분야의 현재 도전 과제를 식별하고, 향후 연구를 위한 유망한 방향을 제시합니다.

## 📎 Related Works

지식 증류에 대한 기존 연구들은 모델 압축에 중요한 기반을 제공해 왔습니다. Gou et al. [37]은 지식 범주, 훈련 방식, 교사-학생 아키텍처, 증류 알고리즘, 성능 비교 및 응용 등 여섯 가지 측면에서 지식 증류를 광범위하게 검토했습니다. Wang et al. [141]은 시각 작업과 관련된 지식 증류 기술의 연구 진행 상황과 기술적 세부 사항을 요약했습니다. Alkhulaifi et al. [4]는 새로운 증류 측정 지표를 도입하여 다양한 지식 압축 방법을 평가했습니다. Hu et al. [48]은 다양한 교사-학생 아키텍처, 지식 표현 및 최적화 목표를 탐구했습니다.

그러나 이 논문은 LLM의 등장으로 인해 지식 증류에 새롭게 발생한 도전 과제들을 강조합니다. 특히, LLM의 광범위한 일반화 능력에 대한 압축 모델의 평가, 그리고 KD 기술이 실제 LLM 압축 및 배포에 적용된 구체적인 사례 연구가 부족하다는 점을 지적하며, 이 서베이가 이러한 한계를 해결하고자 합니다.

## 🛠️ Methodology

이 서베이에서는 LLM 지식 증류를 다음 두 가지 주요 범주로 분류하고 세부 방법론을 설명합니다.

- **White-box Knowledge Distillation (화이트박스 지식 증류)**:
  교사 모델의 내부 데이터에 접근하여 지식을 전달하는 방법입니다.

  - **Logits-based methods (로짓 기반 방법)**:
    교사 모델의 최종 출력 로짓 레벨에서 지식을 전달합니다. 학생 모델의 출력 확률 분포가 교사 모델의 분포를 모방하도록 학습합니다.
    $$ L*{\text{logits}} = KL(p_t \,||\, p_s) = \sum*{j=1}^{C} p*{t_j} \log \left( \frac{p*{t*j}}{p*{s*j}} \right) $$
    여기서 $p*{s*i} = \frac{\exp(z*{s*i}/\tau)}{\sum*{j=1}^C \exp(z*{s_j}/\tau)}$, $p*{t*i} = \frac{\exp(z*{t*i}/\tau)}{\sum*{j=1}^C \exp(z\_{t_j}/\tau)}$ 이며, $\tau$는 로짓의 부드러움을 조절하는 온도 매개변수입니다. DistillBERT, MINILLM 등이 이 범주에 속합니다.
  - **Hint-based methods (힌트 기반 방법)**:
    교사 모델의 중간 레이어 특징을 통해 지식을 전달합니다. 학생 모델이 최종 결과뿐만 아니라 그 결과에 도달하는 과정까지 이해하도록 학습합니다.
    $$ L\_{\text{hint}} = H(F_s, F_t) = \|F_t - \phi(F_s)\|\_2 $$
        여기서 $F_s, F_t$는 학생 및 교사 네트워크의 중간 특징이며, $\phi$는 학생 특징을 교사 특징의 차원에 맞추는 함수입니다. TinyBERT, MobileBERT 등이 이 범주에 속합니다.

- **Black-box Knowledge Distillation (블랙박스 지식 증류)**:
  교사 모델의 내부 데이터에 접근할 수 없고, 오직 교사 모델의 출력(API를 통한 예측)만을 활용하여 지식을 전달하는 방법입니다. 주로 LLM의 emergent capabilities를 활용합니다.
  - **In-Context Learning (ICL, 인컨텍스트 학습)**:
    작업 설명과 소수의 예제(demonstrations)로 구성된 자연어 프롬프트를 사용하여 LLM의 소수샷(few-shot) 학습 능력을 학생 모델로 전달합니다.
    $$ \text{LLM}(\underbrace{I, f(x*1, y_1), \dots, f(x_k, y_k)}*{\text{demonstrations}}, \underbrace{f(x*{k+1})}*{\text{input}}, \underbrace{\_}_{\text{answer}}) \rightarrow \hat{y}_{k+1} $$
  - **Chain-of-Thought (CoT, 사고의 사슬)**:
    단순한 입력-출력 쌍 대신 중간 추론 단계(rationales)를 포함한 프롬프트를 통합하여 LLM의 복잡한 추론 능력을 학생 모델로 증류합니다.
    $$ \text{LLM}(\underbrace{I, f(x*1, r_1, y_1), \dots, f(x_k, r_k, y_k)}*{\text{demonstrations}}, \underbrace{f(x*{k+1})}*{\text{input}}, \underbrace{\_}_{\text{rational}}, \underbrace{\_}_{\text{answer}}) \rightarrow \hat{r}_{k+1}, \hat{y}_{k+1} $$
  - **Instruction Following (지시 따르기)**:
    자연어 설명이 포함된 구조화된 멀티태스크 데이터셋으로 교사 모델의 지시 따르기 능력을 파인튜닝하여 학생 모델에 전달합니다. Self-instruct, LaMini-LM 등이 이에 해당합니다.

또한, 이 서베이는 Dolly 데이터셋을 사용하여 학생 및 교사 모델을 파인튜닝하고, AdvGLUE 및 ANLI 벤치마크로 적대적 강건성을, Flipkart 및 DDXPlus 데이터셋으로 분포 외(OOD) 강건성을 평가했습니다.

## 📊 Results

- **화이트박스 KD 강건성 평가 결과**:

  - GPT-2 모델에서 **MINILLM**은 전반적으로 우수한 증류 성능을 보였으며, 특히 340M 규모의 GPT-2에서는 적대적 및 OOD 데이터셋 모두에서 최첨단 결과를 달성했습니다.
  - **OPT 모델**의 경우, 가장 간단한 KD 알고리즘(교사 분포를 각 토큰 단계의 지도 학습으로 사용하는 방식)이 전반적으로 최고의 성능을 보였습니다. MINILLM 또한 Flipkart 데이터셋에서 교사 모델의 성능을 능가했습니다.
  - **LLaMA**에서는 SeqKD가, **LLaMA2**에서는 JS가 비교적 우수한 증류 효과를 보였습니다. 이는 모델 크기가 동일하고 구조가 유사하더라도 동일한 증류 알고리즘의 효과가 크게 달라질 수 있음을 시사합니다.

- **블랙박스 KD (CoT) 강건성 평가 결과**:
  - GPT-2 모델(120M 및 340M)에서는 ANLI 및 e-SNLI 데이터셋의 해석을 사용한 증류가 더 나은 결과를 보였지만, 모델 크기가 커질수록 이들 데이터셋의 설명력이 감소하는 경향을 보였습니다. OPT 모델에서도 유사한 추세가 관찰되었습니다.
  - OPT 및 LLaMA 모델에서는 상식(CQA) 및 수학(SVAMP) 데이터셋을 사용한 CoT 증류가 Flipkart 및 DDXPlus 데이터셋에서 다른 두 데이터셋을 사용한 증류보다 우수했습니다. 이는 수학적 능력과 상식 지식의 증류가 모델의 OOD 일반화 능력을 향상시킴을 나타냅니다.

## 🧠 Insights & Discussion

- **화이트박스 KD의 장단점**: 로짓 기반 KD는 출력 분포 정렬에, 힌트 기반 KD는 중간 레이어 정렬을 통해 더 풍부한 정보를 전달하여 더 나은 결과를 얻을 수 있습니다. 그러나 두 방법 모두 증류 과정에서 상당한 GPU 메모리를 요구하며, 교사 모델의 아키텍처에 대한 깊은 이해가 필요합니다. Alpaca, Vicuna와 같은 다양한 오픈소스 LLM의 등장은 화이트박스 증류의 미래에 긍정적인 전망을 제공합니다.
- **블랙박스 KD의 장단점**: 블랙박스 KD는 주로 LLM이 생성한 설명이나 지시 쌍을 사용하여 학생 모델을 파인튜닝하며, 교사 모델은 데이터 생성에만 관여하므로 메모리 효율적입니다. 그러나 대부분의 현재 방법은 폐쇄형 교사 모델에 의존하며, 추가 데이터 생성 비용이 높고, 데이터 생성 기술이 공개되지 않아 공정한 평가에 어려움이 있습니다.
- **멀티모달 LLM(MLLM) 지식 증류**: MLLM에 대한 지식 증류는 아직 초기 단계이며, 주로 지시 따르기 능력 개선에 중점을 둡니다. 이는 LLM의 능력을 멀티모달 도메인으로 확장하는 데 중요한 역할을 합니다.
- **도전 과제 및 미래 방향**:
  - **통합된 평가 벤치마크**: GLUE, MMLU, BIG Bench, HELM 등 다양한 벤치마크가 존재하지만, 지식 증류를 위한 통일된 평가 기준이 부족합니다. LLM의 지속적인 발전에 맞춰 통합된 평가 표준을 개발하는 것이 중요합니다.
  - **고급 알고리즘**: 현재 방법론은 주로 학생 모델에 특정 능력을 부여하는 데 중점을 둡니다. 미래에는 오픈소스 LLM을 위한 화이트박스 증류 알고리즘과 멀티모달 LLM(MLLM) 증류 알고리즘에 대한 연구가 더 효과적인 다중 능력 통합을 촉진할 수 있습니다.
  - **해석 가능성**: 지식 증류 과정의 해석 가능성(예: CoT 증류가 학생 모델에 CoT 능력을 어떻게 부여하는지, 지시 파인튜닝에 필요한 데이터 양)을 통합하는 것이 중요합니다. 이는 모델 증류 평가뿐만 아니라 생산 환경에서의 모델 신뢰성과 예측 가능성을 향상시키는 데 기여할 것입니다.

## 📌 TL;DR

이 서베이 논문은 대규모 언어 모델(LLM)의 높은 계산 비용과 배포 문제를 해결하기 위한 지식 증류(KD) 기술을 종합적으로 분석합니다. 논문은 KD 방법론을 교사 모델의 내부 정보 접근 여부에 따라 화이트박스(로짓/힌트 기반)와 블랙박스(In-Context Learning, Chain-of-Thought, Instruction Following)로 분류하고, 각 방법론의 특징과 강건성 평가 결과를 제시합니다. 특히 MINILLM과 같은 특정 알고리즘의 우수성 및 모델/데이터셋에 따른 증류 효과의 가변성을 확인했습니다. 논문은 미래 연구 방향으로 통합된 평가 벤치마크, 고급 증류 알고리즘 개발, 그리고 지식 증류 과정의 해석 가능성 향상을 강조하며, LLM 압축 및 효과적인 배포를 위한 귀중한 통찰을 제공합니다.
