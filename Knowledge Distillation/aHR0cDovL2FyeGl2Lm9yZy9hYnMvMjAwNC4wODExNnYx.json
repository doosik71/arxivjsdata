{
  "url": "http://arxiv.org/abs/2004.08116v1",
  "title": "Triplet Loss for Knowledge Distillation",
  "authors": "Hideki Oki, Motoshi Abe, Junichi Miyao, Takio Kurita",
  "year": 2020,
  "abstract": "In recent years, deep learning has spread rapidly, and deeper, larger models\nhave been proposed. However, the calculation cost becomes enormous as the size\nof the models becomes larger. Various techniques for compressing the size of\nthe models have been proposed to improve performance while reducing\ncomputational costs. One of the methods to compress the size of the models is\nknowledge distillation (KD). Knowledge distillation is a technique for\ntransferring knowledge of deep or ensemble models with many parameters (teacher\nmodel) to smaller shallow models (student model). Since the purpose of\nknowledge distillation is to increase the similarity between the teacher model\nand the student model, we propose to introduce the concept of metric learning\ninto knowledge distillation to make the student model closer to the teacher\nmodel using pairs or triplets of the training samples. In metric learning, the\nresearchers are developing the methods to build a model that can increase the\nsimilarity of outputs for similar samples. Metric learning aims at reducing the\ndistance between similar and increasing the distance between dissimilar. The\nfunctionality of the metric learning to reduce the differences between similar\noutputs can be used for the knowledge distillation to reduce the differences\nbetween the outputs of the teacher model and the student model. Since the\noutputs of the teacher model for different objects are usually different, the\nstudent model needs to distinguish them. We think that metric learning can\nclarify the difference between the different outputs, and the performance of\nthe student model could be improved. We have performed experiments to compare\nthe proposed method with state-of-the-art knowledge distillation methods."
}