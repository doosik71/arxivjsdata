{
  "title": "Quantifying the Knowledge in a DNN to Explain Knowledge Distillation for\n  Classification",
  "authors": "Quanshi Zhang, Xu Cheng, Yilan Chen, Zhefan Rao",
  "year": 2022,
  "url": "http://arxiv.org/abs/2208.08741v1",
  "abstract": "Compared to traditional learning from scratch, knowledge distillation\nsometimes makes the DNN achieve superior performance. This paper provides a new\nperspective to explain the success of knowledge distillation, i.e., quantifying\nknowledge points encoded in intermediate layers of a DNN for classification,\nbased on the information theory. To this end, we consider the signal processing\nin a DNN as the layer-wise information discarding. A knowledge point is\nreferred to as an input unit, whose information is much less discarded than\nother input units. Thus, we propose three hypotheses for knowledge distillation\nbased on the quantification of knowledge points. 1. The DNN learning from\nknowledge distillation encodes more knowledge points than the DNN learning from\nscratch. 2. Knowledge distillation makes the DNN more likely to learn different\nknowledge points simultaneously. In comparison, the DNN learning from scratch\ntends to encode various knowledge points sequentially. 3. The DNN learning from\nknowledge distillation is often optimized more stably than the DNN learning\nfrom scratch. In order to verify the above hypotheses, we design three types of\nmetrics with annotations of foreground objects to analyze feature\nrepresentations of the DNN, \\textit{i.e.} the quantity and the quality of\nknowledge points, the learning speed of different knowledge points, and the\nstability of optimization directions. In experiments, we diagnosed various DNNs\nfor different classification tasks, i.e., image classification, 3D point cloud\nclassification, binary sentiment classification, and question answering, which\nverified above hypotheses."
}