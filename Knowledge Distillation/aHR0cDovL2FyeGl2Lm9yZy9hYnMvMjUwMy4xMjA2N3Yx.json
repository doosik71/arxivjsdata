{
  "url": "http://arxiv.org/abs/2503.12067v1",
  "title": "A Comprehensive Survey on Knowledge Distillation",
  "authors": "Amir M. Mansourian, Rozhan Ahmadi, Masoud Ghafouri, Amir Mohammad Babaei, Elaheh Badali Golezani, Zeynab Yasamani Ghamchi, Vida Ramezanian, Alireza Taherian, Kimia Dinashi, Amirali Miri, Shohreh Kasaei",
  "year": 2025,
  "abstract": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey",
  "citation": 33
}