{
  "title": "Wasserstein Distance Rivals Kullback-Leibler Divergence for Knowledge\n  Distillation",
  "authors": "Jiaming Lv, Haoyuan Yang, Peihua Li",
  "year": 2024,
  "url": "http://arxiv.org/abs/2412.08139v1",
  "abstract": "Since pioneering work of Hinton et al., knowledge distillation based on\nKullback-Leibler Divergence (KL-Div) has been predominant, and recently its\nvariants have achieved compelling performance. However, KL-Div only compares\nprobabilities of the corresponding category between the teacher and student\nwhile lacking a mechanism for cross-category comparison. Besides, KL-Div is\nproblematic when applied to intermediate layers, as it cannot handle\nnon-overlapping distributions and is unaware of geometry of the underlying\nmanifold. To address these downsides, we propose a methodology of Wasserstein\nDistance (WD) based knowledge distillation. Specifically, we propose a logit\ndistillation method called WKD-L based on discrete WD, which performs\ncross-category comparison of probabilities and thus can explicitly leverage\nrich interrelations among categories. Moreover, we introduce a feature\ndistillation method called WKD-F, which uses a parametric method for modeling\nfeature distributions and adopts continuous WD for transferring knowledge from\nintermediate layers. Comprehensive evaluations on image classification and\nobject detection have shown (1) for logit distillation WKD-L outperforms very\nstrong KL-Div variants; (2) for feature distillation WKD-F is superior to the\nKL-Div counterparts and state-of-the-art competitors. The source code is\navailable at https://peihuali.org/WKD"
}