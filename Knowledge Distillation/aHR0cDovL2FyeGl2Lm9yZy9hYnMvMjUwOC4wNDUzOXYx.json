{
  "title": "TopKD: Top-scaled Knowledge Distillation",
  "authors": "Qi Wang, Jinjia Zhou",
  "year": 2025,
  "url": "http://arxiv.org/abs/2508.04539v1",
  "abstract": "Recent advances in knowledge distillation (KD) predominantly emphasize\nfeature-level knowledge transfer, frequently overlooking critical information\nembedded within the teacher's logit distributions. In this paper, we revisit\nlogit-based distillation and reveal an underexplored yet critical element:\nTop-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge\nDistillation (TopKD), a simple, efficient, and architecture-agnostic framework\nthat significantly enhances logit-based distillation. TopKD consists of two\nmain components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies\nthe most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers\ntargeted and effective supervision. Notably, TopKD integrates seamlessly into\nexisting KD methods without introducing extra modules or requiring\narchitectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10,\nand Tiny-ImageNet demonstrate that TopKD consistently surpasses\nstate-of-the-art distillation methods. Moreover, our method demonstrates\nsubstantial effectiveness when distilling Vision Transformers, underscoring its\nversatility across diverse network architectures. These findings highlight the\nsignificant potential of logits to advance knowledge distillation."
}