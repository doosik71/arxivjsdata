{
  "title": "Sentence-Level or Token-Level? A Comprehensive Study on Knowledge\n  Distillation",
  "authors": "Jingxuan Wei, Linzhuang Sun, Yichong Leng, Xu Tan, Bihui Yu, Ruifeng Guo",
  "year": 2024,
  "url": "http://arxiv.org/abs/2404.14827v1",
  "abstract": "Knowledge distillation, transferring knowledge from a teacher model to a\nstudent model, has emerged as a powerful technique in neural machine\ntranslation for compressing models or simplifying training targets. Knowledge\ndistillation encompasses two primary methods: sentence-level distillation and\ntoken-level distillation. In sentence-level distillation, the student model is\ntrained to align with the output of the teacher model, which can alleviate the\ntraining difficulty and give student model a comprehensive understanding of\nglobal structure. Differently, token-level distillation requires the student\nmodel to learn the output distribution of the teacher model, facilitating a\nmore fine-grained transfer of knowledge. Studies have revealed divergent\nperformances between sentence-level and token-level distillation across\ndifferent scenarios, leading to the confusion on the empirical selection of\nknowledge distillation methods. In this study, we argue that token-level\ndistillation, with its more complex objective (i.e., distribution), is better\nsuited for ``simple'' scenarios, while sentence-level distillation excels in\n``complex'' scenarios. To substantiate our hypothesis, we systematically\nanalyze the performance of distillation methods by varying the model size of\nstudent models, the complexity of text, and the difficulty of decoding\nprocedure. While our experimental results validate our hypothesis, defining the\ncomplexity level of a given scenario remains a challenging task. So we further\nintroduce a novel hybrid method that combines token-level and sentence-level\ndistillation through a gating mechanism, aiming to leverage the advantages of\nboth individual methods. Experiments demonstrate that the hybrid method\nsurpasses the performance of token-level or sentence-level distillation methods\nand the previous works by a margin, demonstrating the effectiveness of the\nproposed hybrid method."
}