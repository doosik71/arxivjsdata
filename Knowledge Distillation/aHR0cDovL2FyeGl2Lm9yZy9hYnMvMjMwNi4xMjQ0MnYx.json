{
  "title": "Knowledge Distillation via Token-level Relationship Graph",
  "authors": "Shuoxi Zhang, Hanpeng Liu, Kun He",
  "year": 2023,
  "url": "http://arxiv.org/abs/2306.12442v1",
  "abstract": "Knowledge distillation is a powerful technique for transferring knowledge\nfrom a pre-trained teacher model to a student model. However, the true\npotential of knowledge transfer has not been fully explored. Existing\napproaches primarily focus on distilling individual information or\ninstance-level relationships, overlooking the valuable information embedded in\ntoken-level relationships, which may be particularly affected by the long-tail\neffects. To address the above limitations, we propose a novel method called\nKnowledge Distillation with Token-level Relationship Graph (TRG) that leverages\nthe token-wise relational knowledge to enhance the performance of knowledge\ndistillation. By employing TRG, the student model can effectively emulate\nhigher-level semantic information from the teacher model, resulting in improved\ndistillation results. To further enhance the learning process, we introduce a\ntoken-wise contextual loss called contextual loss, which encourages the student\nmodel to capture the inner-instance semantic contextual of the teacher model.\nWe conduct experiments to evaluate the effectiveness of the proposed method\nagainst several state-of-the-art approaches. Empirical results demonstrate the\nsuperiority of TRG across various visual classification tasks, including those\ninvolving imbalanced data. Our method consistently outperforms the existing\nbaselines, establishing a new state-of-the-art performance in the field of\nknowledge distillation."
}