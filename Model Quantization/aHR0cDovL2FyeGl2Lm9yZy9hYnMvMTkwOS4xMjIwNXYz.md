# Adaptive Binary-Ternary Quantization
Ryan Razani, Grégoire Morin, Eyyüb Sari, and Vahid Partovi Nia

## 🧩 Problem to Solve
*   **딥 신경망(DNN)의 자원 제약:** 최신 DNN 모델은 방대한 계산 자원과 메모리를 요구하여, 스마트 웨어러블, 휴대폰, 드론, 자율주행차와 같이 자원이 제한된 엣지 장치에 배포하기 어렵습니다.
*   **기존 저비트 양자화의 한계:**
    *   **이진 양자화(Binary Quantization, 1비트):** 메모리 절감 효과는 크지만, 정확도 손실이 발생할 수 있습니다.
    *   **삼진 양자화(Ternary Quantization, 2비트):** 이진 양자화보다 표현력이 풍부하여 정확도가 높지만, 메모리 사용량과 계산 비용이 두 배로 증가합니다.
    *   **혼합 정밀도 모델의 비효율성:** 이진과 삼진의 장점을 결합한 혼합 양자화 모델은 정확도와 메모리 효율성 사이의 절충점을 제공하지만, 양자화 비트(깊이)를 수동으로 설정하거나, 최적화를 위해 네트워크를 여러 번 훈련해야 하는 비효율적인 과정이 필요합니다.

## ✨ Key Contributions
*   **Smart Quantization (SQ) 방법론 제안:** 1비트 이진 양자화와 2비트 삼진 양자화를 적응적으로 결합하여 DNN 모델을 훈련하는 새로운 방법을 제시합니다.
*   **단일 훈련 프로세스:** 제안된 정규화 함수를 통해 모델 훈련 중 양자화 비트를 직접 조정함으로써, 모델을 단 한 번만 훈련하여 최적의 혼합 정밀도를 달성합니다. 이는 기존의 다중 훈련 방식이 가진 높은 비용 문제를 해결합니다.
*   **자동 계층별 양자화 비트 선택:** 학습 가능한 제어 매개변수를 통해 각 계층의 특성에 따라 이진(1비트) 또는 삼진(2비트) 양자화를 자동으로 선택합니다.
*   **향상된 성능과 효율성:** 실험 결과, SQ는 순수 삼진 네트워크(TWN)보다 더 높은 압축률(메모리 절감)을 달성하면서도 정확도를 유지하거나 오히려 개선하는 효과를 보였습니다.

## 📎 Related Works
*   **이진 양자화 (Binary Quantization):**
    *   **BinaryConnect (BC) [1]:** 순전파 시 가중치를 $\{-1, +1\}$로 이진화하며, 역전파 시 Straight-Through Estimator를 사용하여 기울기 문제를 해결합니다.
    *   **Binary Weight Network (BWN) / XNOR-Net [13]:** BC에 스케일링 인자($\mu$)를 추가하여 이진 모델과 전체 정밀도 모델 간의 정확도 격차를 줄입니다.
    *   **DoReFa-Net [19], ABCNet [9]:** 1비트 이상으로 가중치를 근사화하거나 이진 가중치 베이스의 선형 결합을 사용하는 등 다양한 이진 양자화 기법을 제안했습니다.
*   **삼진 양자화 (Ternary Quantization):**
    *   **Ternary Weight Network (TWN) [8]:** 가중치를 $\{-1, 0, +1\}$로 제한하여 2비트 정밀도를 사용하며, 이진 모델보다 표현력이 뛰어납니다.
    *   **Trained Ternary Quantization (TTQ) [20]:** 비대칭 값 $\{- \mu_1, 0, +\mu_2\}$로 가중치를 양자화하여 TWN보다 더 나은 정확도를 달성합니다.
*   **정규화 (Regularization):**
    *   **$L_1$ 및 $L_2$ 정규화:** 가중치를 원점 주변에 집중시킵니다.
    *   **특정 목적을 위한 정규화:** 이진 네트워크를 위한 $R_1(w, \mu) = ||w| - \mu|$와 같은 정규화 함수는 가중치를 $\{- \mu, +\mu\}$에 모이도록 유도합니다. 본 논문은 이를 삼진 양자화 $R_2(w, \mu) = |||w| - \frac{\mu}{2}| - \frac{\mu}{2}|$로 일반화하고, 이 두 함수 사이를 유연하게 전환하는 적응형 정규화 함수를 제안합니다.

## 🛠️ Methodology
Smart Quantization (SQ)는 적응형 정규화 함수를 신경망 훈련 과정에 통합하여, 각 계층의 양자화 비트(1비트 또는 2비트)를 자동으로 결정합니다.
1.  **적응형 정규화 함수 정의:**
    가중치 $w_{ij}$에 대한 새로운 정규화 함수 $R(W, \mu, \beta)$를 제안합니다. 이 함수는 이진 정규화와 삼진 정규화 사이를 유연하게 전환합니다.
    $$R(W, \mu, \beta) = \sum_{i=1}^{I} \sum_{j=1}^{J} \min \left( \left||w_{ij}|+\mu\right|^p, \left||w_{ij}|-\mu\right|^p, \tan(\beta)|w_{ij}|^p \right) + \gamma|\cot(\beta)|$$
    *   $\mu$: 각 계층에 대해 학습 가능한 스케일링 인자입니다.
    *   $p$: 정규화 함수의 차수 계수이며, 본 논문에서는 $p=1$로 설정합니다.
    *   $\beta$: $(\frac{\pi}{4}, \frac{\pi}{2})$ 범위의 학습 가능한 형태(shape) 매개변수입니다.
        *   $\beta \to \frac{\pi}{2}$일 때 이진 정규화($R_1$)에 가까워지며, 가중치를 $\{- \mu, +\mu\}$로 유도하여 0을 제외합니다.
        *   $\beta \to \frac{\pi}{4}$일 때 삼진 정규화($R_2$)에 가까워지며, 가중치를 $\{- \mu, 0, +\mu\}$로 유도하여 0을 포함합니다.
    *   $\gamma$: 이진 계층과 삼진 계층의 비율을 제어하는 튜닝 매개변수입니다. $\gamma$ 값이 클수록 $\beta$를 $\frac{\pi}{2}$에 가깝게 유도하여 이진 양자화를 선호합니다.
2.  **통합 목적 함수 최적화:**
    훈련 중 경험적 손실 함수 $L(.)$에 위에서 정의한 정규화 함수를 추가하여 가중치 $W$, 스케일링 인자 $\mu$, 형태 매개변수 $\beta$를 동시에 최적화합니다.
    $$L(W, \mu, \beta) = L(W) + \sum_{l=1}^{L} \lambda_l \sum_{k=1}^{K} R(W_{kl}, \mu_{kl}, \beta_l)$$
    *   $\lambda_l$: 각 계층의 가중치 수($\#W_l$)에 따라 $\lambda / \#W_l$로 설정하여 정규화 효과가 계층마다 균형을 이루도록 합니다.
3.  **최종 양자화 비트 결정:**
    훈련이 완료된 후, 각 계층에서 학습된 $\beta_l$ 값을 임계값 $\delta$ (약 $1.57 \approx \frac{\pi}{2}$)와 비교하여 해당 계층의 최종 양자화 비트를 결정합니다.
    *   $\beta_l \ge \delta$: 해당 계층은 이진(Binary) 양자화됩니다.
    *   $\beta_l < \delta$: 해당 계층은 삼진(Ternary) 양자화됩니다.
이 과정을 통해 각 계층의 가중치는 훈련 과정에서 자연스럽게 이진 또는 삼진 값으로 유도됩니다.

## 📊 Results
*   **MNIST 데이터셋 (LeNet-5 아키텍처):**
    *   SQ는 자동으로 1-1-2-2-2비트의 혼합 양자화를 적용하여 99.37%의 정확도(압축률 16.3)를 달성했습니다. 이는 전체 정밀도(FP, 99.44%)와 거의 동등하며, 순수 삼진 네트워크(TWN, 99.38%, 압축률 16)와 유사한 성능을 보였습니다. 단순한 모델에서는 이점이 제한적일 수 있지만, 첫 두 합성곱 계층을 1비트로, 나머지 완전 연결 계층을 2비트로 자동으로 양자화하는 능력을 입증했습니다.
*   **CIFAR10 데이터셋 (VGG-7 및 VGG-16 아키텍처):**
    *   **VGG-7:** SQ는 2-1-1-1-2-2-2비트의 혼합 양자화를 적용하여 92.94%의 정확도(압축률 18.3)를 달성했습니다. 이는 순수 삼진 네트워크(TWN, 92.74%, 압축률 16)를 능가하고, 전체 정밀도(FP, 93.72%)에 더 가까운 성능을 보였습니다. 특히 3개의 계층을 1비트로 양자화했습니다.
    *   **VGG-16:** SQ는 32-2-1-1-2-2-2-1-1-1-1-1-2-32비트의 혼합 양자화를 적용하여 92.38%의 정확도(압축률 25.1)를 달성했습니다. 이는 순수 삼진 네트워크(TWN, 92.14%, 압축률 15.9)를 능가하고, 전체 정밀도(FP, 92.53%)에 근접한 성능을 보였습니다. 7개의 계층을 1비트로 양자화하여 높은 압축률을 달성했습니다.
*   **결과 요약:** SQ는 순수 삼진 네트워크보다 현저히 높은 압축률을 달성하면서도 동등하거나 더 나은 정확도를 유지했습니다. 특히 복잡한 모델에서 일부 계층을 1비트로 양자화하여 메모리 사용량을 더욱 최적화하는 동시에 성능을 향상시키는 뛰어난 능력을 보여주었습니다.

## 🧠 Insights & Discussion
*   **자동화된 양자화 깊이 최적화:** SQ는 양자화 비트를 수동으로 튜닝하거나 여러 번의 훈련을 통해 최적화할 필요 없이, 단 한 번의 훈련으로 모델의 각 계층에 적합한 최적의 이진-삼진 혼합 양자화 깊이를 자동으로 찾아냅니다. 이는 특히 복잡한 네트워크의 양자화 비용을 크게 절감합니다.
*   **성능과 효율성 간의 최적화:** 이 방법은 메모리 사용량을 개선할 뿐만 아니라, 강력한 정규화 함수 덕분에 훈련된 가중치가 혼합 정밀도에 더 잘 적응하여 일부 경우에는 순수 삼진 네트워크보다 정확도가 더 높을 수 있음을 보여줍니다.
*   **계층별 중요도에 대한 통찰력 제공:** SQ는 어떤 계층이 더 공격적인 양자화(예: 1비트 이진 양자화)에 강한지 자동으로 식별함으로써, 네트워크 설계 및 최적화에 대한 귀중한 통찰력을 제공합니다.
*   **향후 연구 및 확장성:** ImageNet과 같은 더 복잡한 작업이나 더 깊은 아키텍처에 이 방법을 확장하는 것은 추가 연구가 필요합니다. 하지만 본 방법론은 계층별 양자화에 초점을 맞추었음에도 불구하고, 서브네트워크, 블록, 필터 또는 개별 가중치 단위의 혼합 양자화에도 적용될 수 있는 잠재력을 가지고 있습니다. 또한 음성 및 텍스트 관련 작업과 같은 다른 도메인의 신경망 모델에도 유사하게 적용될 수 있습니다.

## 📌 TL;DR
자원 제약이 있는 환경에 딥러닝 모델을 효율적으로 배포하기 위해, 본 논문은 이진 및 삼진 양자화를 적응적으로 결합하는 **Smart Quantization (SQ)** 방법을 제안합니다. SQ는 고유한 정규화 함수를 사용하여 모델을 단 한 번만 훈련하면서도 각 계층의 양자화 비트(1비트 또는 2비트)를 자동으로 최적화합니다. 실험 결과, SQ는 기존 순수 삼진 네트워크보다 더 높은 압축률을 달성하면서도 MNIST 및 CIFAR10 벤치마크에서 정확도를 유지하거나 향상시킴으로써, 수동 튜닝 없이 메모리 효율적인 혼합 정밀도 모델을 효과적으로 구축할 수 있음을 입증했습니다.