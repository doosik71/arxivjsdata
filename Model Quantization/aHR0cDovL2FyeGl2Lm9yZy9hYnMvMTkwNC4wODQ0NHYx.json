{
  "url": "http://arxiv.org/abs/1904.08444v1",
  "title": "Defensive Quantization: When Efficiency Meets Robustness",
  "authors": "Ji Lin, Chuang Gan, Song Han",
  "year": 2019,
  "abstract": "Neural network quantization is becoming an industry standard to efficiently\ndeploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and\nFPGAs. However, we observe that the conventional quantization approaches are\nvulnerable to adversarial attacks. This paper aims to raise people's awareness\nabout the security of the quantized models, and we designed a novel\nquantization methodology to jointly optimize the efficiency and robustness of\ndeep learning models. We first conduct an empirical study to show that vanilla\nquantization suffers more from adversarial attacks. We observe that the\ninferior robustness comes from the error amplification effect, where the\nquantization operation further enlarges the distance caused by amplified noise.\nThen we propose a novel Defensive Quantization (DQ) method by controlling the\nLipschitz constant of the network during quantization, such that the magnitude\nof the adversarial noise remains non-expansive during inference. Extensive\nexperiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization\nmethod can defend neural networks against adversarial examples, and even\nachieves superior robustness than their full-precision counterparts while\nmaintaining the same hardware efficiency as vanilla quantization approaches. As\na by-product, DQ can also improve the accuracy of quantized models without\nadversarial attack."
}