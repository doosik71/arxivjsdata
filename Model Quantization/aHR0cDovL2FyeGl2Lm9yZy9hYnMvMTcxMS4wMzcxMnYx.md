# Quantized Memory-Augmented Neural Networks
Seongsik Park, Seijoon Kim, Seil Lee, Ho Bae, and Sungroh Yoon

## 🧩 Problem to Solve
최근 제안된 메모리 증강 신경망(MANNs)은 긴 시퀀스 학습 및 질의응답(QnA)과 같은 복잡한 인공지능 작업을 효과적으로 해결합니다. 그러나 이러한 딥러닝 모델은 엄청난 계산 에너지와 하드웨어 자원을 요구하여, 제한된 자원을 가진 임베디드 시스템에 배포하기 어렵다는 문제가 있습니다. 양자화(Quantization)는 이러한 문제를 해결하기 위한 효과적인 방법으로, MLP, CNN, RNN 등 다른 신경망에 대해서는 성공적인 양자화가 이루어졌지만, MANN에 대한 성공적인 양자화 사례는 보고된 바 없습니다. 본 논문은 MANN 양자화 시 발생하는 과도한 양자화 오류와 그로 인한 성능 저하의 원인을 분석하고 이를 해결하는 방법을 제안합니다. 특히, 콘텐츠 기반 주소 지정(content-based addressing)이 성능 저하의 주요 원인임을 밝혀냅니다.

## ✨ Key Contributions
*   기존 MANN에 고정 소수점(fixed-point) 및 이진(binary) 양자화를 처음으로 적용하여, 후속 연구를 위한 중요한 기초를 마련했습니다.
*   MANN에 양자화를 적용했을 때 학습 성능이 크게 저하되는 이유를 이론적으로 분석하고, 다양한 실험을 통해 그 분석의 타당성을 검증했습니다.
*   작은 비트 너비의 양자화 환경에서도 양자화 오류에 대한 견고성과 학습 성능을 모두 향상시킬 수 있는 양자화된 MANN(Quantized MANN, Q-MANN)을 제안했습니다.

## 📎 Related Works
*   **메모리 증강 신경망 (MANN)**: 신경망과 외부 메모리를 결합한 모델로, Neural Turing Machines (NTMs), Memory Networks, Differentiable Neural Computers (DNCs) 등이 포함됩니다. 주로 콘텐츠 기반 주소 지정 $$C(M,k)[i] = \frac{\exp\{S(M_i,k)\}}{\sum_{j=1}^{L}\exp\{S(M_j,k)\}}$$ 특히 코사인 유사도를 사용합니다.
*   **고정 소수점 및 이진 양자화**: 부동 소수점 연산을 저비용의 정수 연산으로 대체하여 계산 에너지 이득을 얻는 기법입니다. 고정 소수점 표현 $$\hat{u}=S_{\hat{u}}2^{-FRAC}\sum_{k=0}^{n-2}2^{k}\hat{u}_{k}$$ 은 오버플로우 발생 시 양자화 오류 $$|\Delta \hat{u}| < \begin{cases} 2^{-FRAC} & \text{if }|u|<2^{IWL} \\ |2^{IWL}-|u|| & \text{if }|u|\ge 2^{IWL} \end{cases}$$ 가 크게 증가합니다.
*   **딥러닝에서의 양자화**: MLP, CNN, RNN에 대한 광범위한 양자화 연구가 진행되었습니다. BinaryConnect, BinaryNet, XNOR-Net, DoReFa-Net 등이 CNN의 파라미터와 활성화 함수를 이진화하여 계산 비용을 줄이는 데 초점을 맞추었으며, RNN 양자화 연구도 있었으나, MANN에 대한 양자화 시도는 본 연구가 처음입니다.

## 🛠️ Methodology
1.  **기존 MANN 양자화 문제 분석**:
    *   기존 MANN에 8비트 고정 소수점 양자화를 적용했을 때 테스트 에러율이 160% 이상 급증하는 것을 확인했습니다.
    *   이러한 성능 저하의 주된 원인은 콘텐츠 기반 주소 지정에 사용되는 **코사인 유사도**와 **소프트맥스 함수**에 내재된 문제점 때문입니다.
    *   학습이 진행됨에 따라 관련 벡터의 유사도는 커지고 관련 없는 벡터의 유사도는 작아지면서, 유사도 분포가 크게 넓어집니다.
    *   이 넓어진 분포는 제한된 고정 소수점 표현 범위 내에서 빈번한 **고정 소수점 오버플로우**를 유발하며, 이는 유사도 측정 오차 ($\Delta \hat{Z} \approx \sum(u_i \Delta \hat{v}_i + v_i \Delta \hat{u}_i)$)를 크게 증가시킵니다.
    *   소프트맥스 함수 ($\hat{y}_i = \frac{\exp(\hat{z}_i)}{\sum \exp(\hat{z}_k)}$)의 특성상 입력 양자화 오류($\Delta \hat{z}_{max}$)는 출력 오류($\Delta \hat{y}_i$)에 지수적으로($\exp(\Delta \hat{z}_{max})$) 영향을 미쳐, 학습 성능을 심각하게 저하시킵니다.

2.  **Q-MANN 아키텍처 제안**:
    *   **경계가 있는 유사도(Bounded Similarity) 사용**: 고정 소수점 오버플로우 문제를 해결하기 위해 코사인 유사도 대신 **Hamming 유사도**를 사용합니다.
        *   제안된 Hamming 유사도 함수는 다음과 같습니다: $$S_H(\hat{U},\hat{V}) = \sum_i S_{\hat{u}_i}S_{\hat{v}_i} \sum_{k=0}^{n-2}W_k \text{XNOR}(\hat{u}_{ik},\hat{v}_{ik})$$
        *   여기서 $S_{\hat{u}_i}, S_{\hat{v}_i}$는 부호, $\hat{u}_{ik}, \hat{v}_{ik}$는 각 비트, $W_k = 2^{k+\alpha-n}$는 가중치입니다. 이 유사도는 유사도 분포가 제한되어 있어 고정 소수점 오버플로우를 효과적으로 방지합니다.
    *   **미분 가능성 확보**: 순전파에서는 스텝 함수를 사용하고, 역전파에서는 미분 가능한 함수로 스무딩하여 SGD 및 역전파를 통한 End-to-End 학습을 가능하게 합니다. 근사화된 기울기 함수는 다음과 같습니다: $$\frac{\partial S_H(\hat{U},\hat{V})}{\partial \hat{u}_i} \approx S_{\hat{u}_i}2^{\alpha}(S_{\hat{u}_i}-S_{\hat{v}_i})-\sum_{k=0}^{n-2}(S_{\hat{v}_i}2^{\alpha}(\hat{u}_{ik}-\hat{v}_{ik}))$$

3.  **효과적인 학습 기법**:
    *   **메모리 컨트롤러 양자화 제어 (MQ)**: MANN의 메모리 컨트롤러로 사용되는 RNN은 동일한 파라미터를 여러 타임 스텝에 걸쳐 사용하므로 양자화 오류가 누적될 수 있습니다. 이를 완화하기 위해 각 타임 스텝마다 `IWL`과 `FRAC`를 미세하게 다르게 설정하여 양자화 오류의 상쇄 효과를 유도합니다.
    *   **조기 종료 (ES)**: 양자화 오류로 인해 학습이 불안정해지고 성능 저하 및 분산이 커지는 것을 막기 위해 조기 종료 기법을 활용합니다.

## 📊 Results
bAbI 데이터셋 (10k)에서 다양한 실험을 수행하여 제안된 Q-MANN의 성능을 검증했습니다.

*   **기존 MANN (8비트 고정 소수점 양자화)**:
    *   32비트 부동 소수점 대비 약 20배의 계산-에너지 이득을 얻었으나, 평균 에러율이 160% (최저)에서 200% (평균)까지 급증하여 학습 성능이 크게 저하되었습니다.
    *   조기 종료(ES) 적용 시 평균 에러율 증가 폭이 159%로 감소했고, ES와 메모리 컨트롤러 양자화 제어(MQ)를 함께 적용 시 120% 증가로 완화되었습니다.

*   **Q-MANN (8비트 고정 소수점 양자화)**:
    *   32비트 부동 소수점 대비 약 21배의 계산-에너지 이득을 얻었습니다.
    *   기존 MANN 대비 평균 에러율이 약 37% (최저)에서 41% (평균) 감소하여 상당한 성능 개선을 보였습니다. 이는 Q-MANN이 유사도 측정 시 고정 소수점 오버플로우가 거의 발생하지 않아 양자화 오류에 강인하기 때문입니다.
    *   ES와 MQ를 Q-MANN에 적용했을 때, 기존 MANN 대비 에러율을 최대 46%까지 추가로 감소시켰습니다.

*   **Q-MANN (이진 활성화 + 8비트 고정 소수점 매개변수)**:
    *   약 22배의 계산-에너지 이득을 달성했습니다.
    *   이진화로 인한 정보 손실에도 불구하고 고정 소수점 오버플로우가 크게 감소하여, 기존 8비트 고정 소수점 양자화보다 에러율 증가 폭이 낮았습니다 (기준선 대비 100~160% 증가).
    *   Q-MANN은 ES 및 MQ 적용 시 기존 MANN 대비 에러율을 최대 17% (최저)에서 30% (평균)까지 감소시켰습니다.

## 🧠 Insights & Discussion
*   이 연구는 MANN의 양자화에 있어 **콘텐츠 기반 주소 지정**이 핵심적인 취약점임을 명확히 밝혔습니다. 기존 MANN에서 사용되는 코사인 유사도는 학습 진행에 따라 유사도 분포가 무한히 확장되는 경향이 있어, 제한된 비트 너비의 고정 소수점 표현에서 치명적인 **오버플로우 오류**를 유발합니다. 이는 소프트맥스 함수를 통해 출력에 지수적으로 증폭되어 성능 저하로 이어집니다.
*   **Q-MANN의 핵심은 경계가 있는 Hamming 유사도를 도입**하여 유사도 분포를 좁게 유지하고 오버플로우를 효과적으로 방지하는 것입니다. 이로 인해 Q-MANN은 제한된 수치 표현 범위 내에서도 양자화 오류에 훨씬 더 강인한 학습 성능을 보였습니다.
*   **메모리 컨트롤러 양자화 제어(MQ)와 조기 종료(ES) 기법**은 양자화로 인한 학습 불안정성과 성능 변동성을 줄이는 데 중요한 역할을 했습니다.
*   Q-MANN의 유사도 측정 함수는 덧셈, 시프트, XNOR 게이트와 같은 간단한 연산만으로 구성되어, 저전력 및 실시간 처리가 요구되는 제한된 자원 환경에 매우 적합합니다.
*   본 연구는 기존 MANN이 코사인 유사도뿐만 아니라 내적(dot product)과 같은 유사도 측정 함수를 사용할 때도 양자화 오류에 취약하다는 것을 암시하며, Q-MANN의 접근 방식이 이러한 일반적인 문제에도 유효함을 시사합니다.

## 📌 TL;DR
*   **문제**: 메모리 증강 신경망(MANN)을 저자원 시스템에 배포하기 위한 양자화는 콘텐츠 기반 주소 지정에서 발생하는 고정 소수점 오버플로우 때문에 성능이 크게 저하되는 문제를 겪습니다.
*   **해결책**: 본 논문은 이러한 오버플로우를 방지하기 위해 경계가 있는 Hamming 유사도를 사용하는 Quantized MANN(Q-MANN)을 제안하고, 학습 안정화를 위한 메모리 컨트롤러 양자화 제어(MQ) 및 조기 종료(ES) 기법을 도입했습니다.
*   **성과**: Q-MANN은 8비트 양자화 환경에서 기존 MANN 대비 최대 46%의 에러율 감소와 최대 22배의 계산-에너지 이득을 달성하며, 양자화된 MANN의 효율적이고 견고한 학습 가능성을 입증했습니다.