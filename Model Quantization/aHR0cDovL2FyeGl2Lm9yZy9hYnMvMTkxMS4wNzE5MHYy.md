# Loss Aware Post-training Quantization
Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M. Bronstein, and Avi Mendelson

## 🧩 Problem to Solve
신경망 양자화는 딥러닝 모델을 리소스가 제한된 엣지 장치에 배포하는 데 필수적입니다. 현재의 후학습 양자화(Post-training Quantization, PTQ) 방법은 INT8 이상의 비트 폭에서는 합리적인 정확도를 제공하지만, INT4(또는 그 이하)의 낮은 비트 폭에서는 정확도 손실이 큽니다. 이는 주로 다음과 같은 두 가지 근본적인 문제점 때문입니다.
1.  **최적 메트릭 선택의 어려움:** 텐서 수준의 양자화 오류(예: 반올림 오류, 아웃라이어 클리핑)에 기반한 네트워크 성능의 최적 메트릭을 선택하기 어렵습니다.
2.  **레이어 간 의존성 및 비분리성:** 초기 레이어의 양자화 노이즈가 다음 레이어에 의해 증폭될 수 있어, 레이어 간 양자화 오류에 상호 의존성이 발생합니다. 이는 모든 네트워크 레이어에 걸쳐 양자화 파라미터를 공동으로 최적화해야 함을 의미하지만, 기존 PTQ 방법들은 각 레이어를 개별적으로 최적화합니다. 특히 공격적인 양자화(낮은 비트 폭) 시 손실 함수(loss function)의 지형(landscape)이 매우 비분리적이고 가파른 곡률을 보여, 양자화 파라미터 선택이 훨씬 더 어려워집니다.

## ✨ Key Contributions
*   **레이어 간 상호작용 분석:** 기존의 레이어별 양자화 방법이 네트워크 수준에서 최적의 성능을 달성하지 못하는 이유가 레이어 간의 강한 상호작용 때문임을 관찰하고 분석했습니다.
*   **다변수 최적화 문제로 재정의:** 네트워크 양자화를 레이어 양자화 스텝 크기($\Delta$)를 공동으로 최적화하여 신경망의 교차 엔트로피 손실을 최소화하는 다변수 최적화 문제로 간주합니다. 레이어별 양자화가 최적 지점 근처의 좁은 영역에서 2차 함수적으로 성능 저하가 발생한다는 점을 분석적 및 경험적으로 입증했습니다.
*   **새로운 PTQ 방법(LAPQ) 제안:** 레이어별 양자화와 다변수 2차 최적화를 결합한 Loss Aware Post-training Quantization(LAPQ) 방법을 제안했습니다. 이 방법은 현재의 최첨단 PTQ 방법을 두 가지 도전적인 작업(ImageNet 이미지 분류 및 NCF-1B 추천 시스템)과 여섯 가지 DNN 아키텍처에서 크게 능가함을 보여줍니다.

## 📎 Related Works
*   **양자화 접근 방식:** 크게 양자화 인식 학습(Quantization-Aware Training, QAT)과 후학습 양자화(Post-training Quantization, PTQ)로 나뉩니다.
    *   **QAT:** 2~4비트, 심지어 1비트 바이너리까지 베이스라인에 준하는 결과를 달성하지만, 방대한 레이블 데이터와 높은 계산 능력을 요구합니다. (예: [2,9,12,24,25] for low bit, [13,17,22] for binary)
    *   **PTQ:** 임베디드 하드웨어에서 널리 사용되며, 대부분 8비트 양자화에서 성능 저하 없이 구현 가능합니다. (예: Gong et al. [8]의 min-max (L$_{\infty}$ norm) 방식)
*   **클리핑 값 선택 개선:** Kullback-Leibler 발산 최소화 [19], 가중치 분포 가정 [1], 반복적인 MSE 최소화 [14] 등의 방법이 제안되었습니다.
*   **텐서 분포 변경:** 가중치 범위 균등화 [20] 또는 아웃라이어 채널 분할 [26]을 통해 양자화에 더 적합하도록 텐서 분포를 변경하는 접근 방식도 있습니다.
*   **양자화 바이어스 보정:** 양자화 과정에서 발생하는 바이어스(예: Finkelstein et al. [6]의 MobileNet 활성화 값 평균 이동)를 보정하는 연구가 진행되었으며, 이 연구에서는 Banner et al. [1]의 바이어스 보정 방법을 활용합니다.
*   **정교한 파라미터 할당:** 그룹별 [18], 채널별 [1,16], 픽셀별 [5], 필터별 [14] 양자화와 같은 접근 방식은 성능을 크게 향상시키지만, 특수 하드웨어 지원 및 추가 계산 리소스가 필요합니다. 본 연구는 레이어별 양자화에 중점을 둡니다.
*   **비균일 양자화:** 텐서 항목 값 클러스터링 [21] 및 비균일 양자화 [4]와 같은 방법은 더 나은 성능을 제공하지만, 효율적인 추론을 위해 하드웨어 지원이 필요합니다.
*   **손실 함수의 비분리성:** 기존 연구들은 최적화 과정에서 손실 함수가 비분리적일 수 있다는 점을 고려하지 않은 경우가 많았으며, Nagel et al. [20]이 인접 레이어 쌍을 함께 처리하여 부분적으로 해결했습니다.

## 🛠️ Methodology
제안하는 LAPQ (Loss Aware Post-training Quantization)는 복잡하고 비분리적인 손실 함수 지형을 극복하기 위한 3단계 최적화 과정을 따릅니다.

1.  **레이어별 최적화 (Layer-wise optimization):**
    *   각 레이어의 가중치와 활성화에 대해 양자화 오류의 $L_p$ norm을 최소화하는 클리핑 값($\Delta_p$)을 찾습니다.
    *   $e_p(\Delta) = \left( \|Q_{\Delta}(X) - X\|_p \right)^{1/p}$
    *   다양한 $p$ 값(예: $p=0.5, 1, 2, 3$)에 대해 $\Delta_p$를 계산하여 최적 값 $\Delta^*$ 근처의 후보들을 얻습니다. 이 과정은 최적의 초기 지점을 제공합니다.

2.  **2차 근사 (Quadratic approximation):**
    *   최적 양자화 스텝 $\Delta^*$ 근방에서 손실 함수 $L(\Delta)$가 2차 함수로 근사될 수 있다는 가설(Taylor 전개 기반: $\Delta L = L(\Delta) - L(\Delta^*) \approx \frac{1}{2} (\Delta^* - \Delta)^T H(\Delta^*) (\Delta^* - \Delta)$)을 활용합니다.
    *   1단계에서 얻은 $\Delta_p$ 데이터 포인트들을 사용하여 손실 $L(\Delta)$의 궤적을 만들고, 이 궤적에 2차 함수 $f(p)$를 피팅하여 최적의 양자화 스텝 $\Delta_{p^*}$를 추정합니다.
    *   이 $\Delta_{p^*}$는 다음 단계인 공동 최적화의 시작점으로 사용됩니다.

3.  **공동 최적화 (Joint optimization - Powell's method):**
    *   낮은 비트 폭 양자화에서 손실 함수의 가파른 곡률로 인해 $\Delta$ 값의 작은 오차도 성능 저하로 이어질 수 있습니다.
    *   따라서, 2단계에서 얻은 $\Delta_{p^*}$를 시작점으로 삼아, 파월(Powell)의 방법 [23]과 같은 경사 없는(gradient-free) 공동 최적화 알고리즘을 적용하여 손실 함수를 직접 최소화하고 최적의 $\Delta^*$를 찾습니다.
    *   이 반복적인 알고리즘은 검색 방향을 선형 조합하여 파라미터 세트를 최적화합니다.

## 📊 Results
*   **ImageNet 이미지 분류:**
    *   ResNet-18, ResNet-50, ResNet-101, Inception-V3 등 다양한 CNN 아키텍처에 대해 평가했습니다.
    *   512개의 무작위 이미지를 사용하여 양자화 파라미터를 보정했습니다. 첫 번째와 마지막 레이어는 양자화하지 않았습니다.
    *   MMSE (Mean Square Error)와 같은 기준선 방법뿐만 아니라 DUAL [14], ACIQ [1], KLD [19] 등 다른 레이어별 양자화 방법들과 비교했을 때, LAPQ는 대부분의 비트 폭 설정(예: W/A 32/4, 8/4, 8/3, 4/4)에서 월등히 뛰어난 성능을 보였습니다.
    *   특히 4비트 양자화에서도 수용 가능한 성능을 달성했으며, 때로는 FP32(32비트 부동소수점) 베이스라인에 근접했습니다.
*   **NCF-1B 추천 시스템:**
    *   MovieLens-1B 데이터셋에서 Neural Collaborative Filtering (NCF) [10] 모델에 대해 평가했습니다.
    *   5만 개의 무작위 사용자/아이템 쌍을 보정 세트로 사용했습니다.
    *   MMSE 방법은 8비트 양자화에서도 상당한 성능 저하를 보였으나, LAPQ는 FP32 결과에서 0.5% 미만의 손실로 베이스라인에 가까운 정확도를 달성했습니다.
*   **어블레이션 연구 (Ablation study):**
    *   **초기화의 중요성:** LAPQ의 레이어별 최적화와 2차 근사를 통한 초기화는 공동 최적화를 위한 훨씬 더 나은 시작점을 제공하여, 무작위 초기화나 단순 레이어별 MSE 최적화보다 최종 정확도를 크게 향상시켰습니다.
    *   **바이어스 보정의 효과:** 특히 MobileNet과 같은 경량 모델에서 바이어스 보정 [1]을 LAPQ와 결합했을 때 정확도가 크게 향상되는 것을 확인했습니다.

## 🧠 Insights & Discussion
*   **손실 지형의 복잡성 이해:** 낮은 정밀도 양자화 시 신경망의 손실 함수가 비분리적이고 가파른 곡률을 가진다는 중요한 통찰을 제공합니다. 이는 기존의 레이어별 양자화 방법이 효과적이지 않은 이유를 설명합니다. 양자화 오류가 커질수록 레이어 간의 상호작용이 강해져 손실 함수의 지형이 더욱 복잡해집니다.
*   **공동 최적화의 필요성:** 공격적인 양자화 환경에서는 각 레이어를 독립적으로 최적화하는 것이 아니라, 모든 양자화 파라미터를 공동으로 최적화하여 손실 함수를 직접 최소화하는 것이 필수적임을 입증했습니다.
*   **LAPQ의 장점:**
    *   기존의 최첨단 후학습 양자화 방법을 능가하는 성능을 제공합니다.
    *   채널별 또는 필터별 양자화와 같은 특수 하드웨어 지원이 필요하지 않고, 기존 하드웨어에서 쉽게 구현 가능한 레이어별 양자화를 수행합니다.
    *   일부 모델의 4비트 후학습 양자화에서 FP32 베이스라인에 근접한 성능을 달성한 최초의 방법 중 하나입니다.
*   **한계 및 미래 방향:** 이 연구는 후학습 양자화의 한계를 극복하는 데 기여했지만, 매우 낮은 비트 폭에서는 여전히 양자화 인식 학습(QAT) 수준에 도달하지 못할 수 있습니다. 보정 세트 크기는 일반화 성능과 실행 시간 간의 절충점을 반영하며, 이에 대한 추가 분석이 가능합니다.

## 📌 TL;DR
**문제:** 기존의 후학습 양자화(PTQ) 방법은 낮은 비트 폭(예: INT4)에서 손실 함수의 복잡하고 비분리적인 특성을 다루지 못해 정확도가 크게 떨어집니다.

**제안 방법:** LAPQ(Loss Aware Post-training Quantization)는 손실 함수 지형을 직접 고려하여 양자화 파라미터를 공동으로 최적화합니다. 이는 개별 레이어의 $L_p$ norm 양자화 오류 최소화를 통해 초기 후보군을 찾고, 손실 함수의 2차 근사를 통해 최적 지점 근처를 추정하며, 최종적으로 파월(Powell)의 방법과 같은 경사 없는 공동 최적화를 적용하여 전체 네트워크 손실을 최소화합니다.

**주요 결과:** LAPQ는 다양한 모델(ResNet, Inception, NCF-1B)과 데이터셋에서 기존 PTQ 방법들보다 훨씬 우수한 양자화 정확도를 달성하며, 특히 4비트 양자화에서 거의 FP32 수준의 성능을 보여주면서도 특수 하드웨어 지원을 요구하지 않습니다.