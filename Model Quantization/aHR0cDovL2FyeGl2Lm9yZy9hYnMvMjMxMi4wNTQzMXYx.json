{
  "url": "http://arxiv.org/abs/2312.05431v1",
  "title": "Efficient Quantization Strategies for Latent Diffusion Models",
  "authors": "Yuewei Yang, Xiaoliang Dai, Jialiang Wang, Peizhao Zhang, Hongbo Zhang",
  "year": 2023,
  "abstract": "Latent Diffusion Models (LDMs) capture the dynamic evolution of latent\nvariables over time, blending patterns and multimodality in a generative\nsystem. Despite the proficiency of LDM in various applications, such as\ntext-to-image generation, facilitated by robust text encoders and a variational\nautoencoder, the critical need to deploy large generative models on edge\ndevices compels a search for more compact yet effective alternatives. Post\nTraining Quantization (PTQ), a method to compress the operational size of deep\nlearning models, encounters challenges when applied to LDM due to temporal and\nstructural complexities. This study proposes a quantization strategy that\nefficiently quantize LDMs, leveraging Signal-to-Quantization-Noise Ratio (SQNR)\nas a pivotal metric for evaluation. By treating the quantization discrepancy as\nrelative noise and identifying sensitive part(s) of a model, we propose an\nefficient quantization approach encompassing both global and local strategies.\nThe global quantization process mitigates relative quantization noise by\ninitiating higher-precision quantization on sensitive blocks, while local\ntreatments address specific challenges in quantization-sensitive and\ntime-sensitive modules. The outcomes of our experiments reveal that the\nimplementation of both global and local treatments yields a highly efficient\nand effective Post Training Quantization (PTQ) of LDMs."
}