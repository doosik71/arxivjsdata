# Post-Training Quantization for Vision Transformer

Zhenhua Liu, Yunhe Wang, Kai Han, Siwei Ma, Wen Gao

## 🧩 Problem to Solve

최근 Vision Transformer(ViT)는 다양한 컴퓨터 비전 작업에서 뛰어난 성능을 보이고 있지만, CNN에 비해 복잡한 아키텍처로 인해 파라미터 수가 많고 연산 비용이 높아 모바일 또는 자원 제약이 있는 디바이스에 배포하기 어렵습니다. 기존 양자화 방법, 특히 학습 후 양자화(Post-Training Quantization, PTQ)는 주로 CNN이나 RNN을 위해 설계되었으며, ViT의 핵심 구성 요소인 어텐션 메커니즘의 특성을 충분히 고려하지 않아 성능 저하를 야기할 수 있습니다. 따라서 ViT 모델의 메모리 및 연산 비용을 줄이면서도 정확도를 최대한 유지할 수 있는 효과적인 학습 후 양자화 알고리즘이 필요합니다.

## ✨ Key Contributions

- **어텐션 메커니즘을 위한 랭킹 손실 도입:** 양자화 후에도 셀프 어텐션 결과의 상대적 순서를 유지하기 위해 기존 양자화 목적 함수에 랭킹 손실($L_{\text{ranking}}$)을 통합했습니다.
- **혼합 정밀도 양자화 체계 제안:** 각 어텐션 맵($A_l$)과 출력 특성($O_l$)의 핵 노름(nuclear norm)을 활용하여 레이어별 민감도를 분석하고, 이에 기반하여 각 트랜스포머 레이어에 최적의 비트 폭을 할당하는 혼합 정밀도 양자화 방식을 탐색했습니다.
- **유사성 인식 및 랭킹 인식 양자화:** 선형 연산에는 원본 및 양자화된 피처 맵 간의 유사성(Pearson 상관 계수 $\Gamma$)을 최대화하는 '유사성 인식 양자화'를, 셀프 어텐션 레이어에는 랭킹 손실을 포함하는 '랭킹 인식 양자화'를 도입했습니다.
- **편향 보정(Bias Correction) 방법 적용:** 양자화로 인해 발생하는 누적된 출력 편향 오차를 줄이기 위해 각 탐색 반복 후 편향 보정을 도입했습니다.
- **효과적인 최적화 전략:** 최적의 양자화 간격(quantization interval)을 찾기 위해 가중치($\Delta_W$)와 입력($\Delta_X$)의 양자화 간격을 교대로 탐색하는 효율적인 방법을 제안했습니다.
- **최첨단 성능 달성:** 여러 벤치마크 모델(ViT, DeiT, DETR) 및 데이터셋(ImageNet, CIFAR, COCO)에서 기존 학습 후 양자화 알고리즘보다 뛰어난 성능을 입증했습니다.

## 📎 Related Works

- **Vision Transformer:** ViT, DeiT와 같은 이미지 분류 모델뿐만 아니라, 객체 탐지(DETR), 의미론적 분할, 이미지 처리, 비디오 이해 등 다양한 CV 작업에 트랜스포머가 적용되었습니다. TNT(Transformer-iN-Transformer) 및 패치 슬리밍(patch slimming)과 같은 관련 모델 가속화 연구도 언급됩니다.
- **Compression of Transformer in NLP:** BERT와 같은 NLP 트랜스포머 모델의 압축을 위한 연구로, 8비트 양자화, 혼합 정밀도 양자화(mixed-precision quantization), 제품 양자화(product quantization, PQ) 및 TernaryBERT와 같은 저비트 양자화 기법이 제안되었습니다. 하지만 이러한 방법들은 CV 작업에 특화되지 않았으며 추가 학습 또는 미세 조정이 필요합니다.
- **Post-Training Quantization (PTQ):** 학습이나 미세 조정 없이 신경망 모델을 직접 양자화하는 효율적인 기술입니다. OMSE($L_2$ 거리 최적화), ACIQ(클리핑 범위 분석), OCS(Outlier Channel Splitting), Bit-Split and Stitching, AdaRound(가중치 반올림), Data-Free Quantization, ZeroQ(데이터 없이 통계 일치)와 같은 방법들이 있으나, 대부분 CNN에 중점을 두어 ViT의 고유한 구조를 고려하지 않았습니다.

## 🛠️ Methodology

본 논문은 ViT를 위한 혼합 정밀도 학습 후 양자화 기법을 제안하며, 핵심적으로 선형 레이어에는 유사성 인식 양자화를, 셀프 어텐션 레이어에는 랭킹 인식 양자화를 적용합니다.

1. **양자화 범위 및 함수:**
   - 균일 양자화 함수 $\Psi_{\Delta}(Y) = \text{Clip}(\text{Round}(\frac{Y}{\Delta}), -2^{b-1}, 2^{b-1}-1)$를 사용합니다. 여기서 $\Delta$는 양자화 간격, $b$는 비트 폭입니다.
   - 주로 MSA 및 MLP 모듈의 가중치($W_Q, W_K, W_V, W_O, W_1, W_2$)와 선형 레이어의 입력($X_l$)을 양자화합니다. Softmax 및 Layer Normalization은 양자화하지 않습니다.
2. **유사성 인식 양자화 (선형 연산):**
   - 양자화 간격 $\Delta_W, \Delta_X$를 최적화하여 원본 출력 $O_l$과 양자화된 출력 $\hat{O}_l$ 간의 유사성을 최대화합니다.
   - 유사성 측정에는 Pearson 상관 계수 $\Gamma(\hat{O}, O)$를 사용합니다.
   - 목표 함수: $\max_{\Delta_W^l, \Delta_X^l} \frac{1}{N} \sum_{i=1}^{N} \Gamma(O_i^l, \hat{O}_i^l)$.
3. **랭킹 인식 양자화 (셀프 어텐션):**
   - 셀프 어텐션($A_l = Q_l K_l^T$)에서 양자화로 인한 어텐션 맵의 상대적 순서 변화를 방지하기 위해 랭킹 손실($L_{\text{ranking}}$)을 도입합니다.
   - 랭킹 손실은 힌지 함수 $\Phi(p) = (\theta-p)^+$를 기반으로 한 쌍별 순위 기반 손실입니다.
   - 목표 함수: $\max_{\Delta_W^l, \Delta_X^l} \frac{1}{N} \sum_{i=1}^{N} \Gamma(O_i^l, \hat{O}_i^l) - \gamma \cdot L_{\text{ranking}}$.
4. **최적화 방법 (교대 탐색):**
   - 입력($\Delta_X$)을 고정하고 가중치($\Delta_W$)를 최적화한 후, 가중치를 고정하고 입력을 최적화하는 방식으로 $\Delta_W$와 $\Delta_X$를 교대로 탐색합니다.
   - 초기화는 가중치 또는 입력의 최댓값으로 설정하고, 탐색 공간은 $[ \alpha\Delta_l, \beta\Delta_l]$ 범위 내에서 후보 옵션을 선형적으로 나눕니다.
5. **편향 보정 (Bias Correction):**
   - 양자화로 인한 출력의 편향 오류를 줄이기 위해, 각 탐색 반복 후에 편향 보정을 적용합니다.
   - 예상되는 오류 $\text{E}[\tilde{O}]$를 계산하고 이를 레이어의 편향 파라미터에서 뺍니다.
6. **혼합 정밀도 양자화:**
   - 트랜스포머 레이어마다 민감도가 다르므로, 모든 레이어에 동일한 비트 폭을 할당하는 대신, 더 민감한 레이어에 더 많은 비트를 할당합니다.
   - 각 트랜스포머 레이어의 어텐션 맵과 출력 특성의 **핵 노름**을 사용하여 레이어 민감도를 추정합니다. 핵 노름은 행렬의 특이값(singular value)의 합으로, 데이터의 관련성을 나타냅니다.
   - $\Omega = \sum_{i=1}^{L} \Omega_i = \sum_{i=1}^{L} \sum_{j=1}^{m} \sigma_j(Y_i) \cdot \| \hat{Y}_i - Y_i \|_2^2$ 메트릭을 사용하여 후보 비트 폭 구성을 정렬하고, 주어진 모델 크기 목표에 대해 $\Omega$ 값이 최소인 구성을 선택합니다.

## 📊 Results

- **이미지 분류:**
  - **ViT-B, ViT-L:** CIFAR-10, CIFAR-100, ImageNet 데이터셋에서 기존 percentile-based 방법보다 2~4% 높은 정확도를 달성했습니다. 특히 ViT-L 8비트 모델은 CIFAR-10에서 심지어 풀-정밀도 모델보다 나은 성능을 보이기도 했습니다.
  - **DeiT-S, DeiT-B:** ImageNet에서 기존 percentile-based, EasyQuant, Bit-Split 대비 크게 향상된 성능을 보였습니다. DeiT-B 8비트 혼합 정밀도 모델은 81.29%의 Top-1 정확도를 달성하여 풀-정밀도 모델(81.8%)에 근접했습니다. 6비트에서도 기존 방법의 8비트 모델보다 뛰어난 성능을 보였습니다.
- **객체 탐지 (DETR on COCO2017):**
  - 제안된 방법은 6비트 양자화에서 기존 방법들(percentile-based, EasyQuant, Bit-Split)보다 1.1~2.6 mAP 더 높은 성능을 보였습니다. 8비트 혼합 정밀도 양자화에서는 41.7 mAP를 달성하여 풀-정밀도 모델(42.0 mAP)과 거의 동일한 성능을 보였습니다.
- **Ablation Study:**
  - ImageNet의 ViT-B 모델에 대한 실험 결과, 유사성 인식 양자화만 사용했을 때 75.42%의 Top-1 정확도를 보였습니다. 여기에 랭킹 인식 양자화 손실을 추가하면 0.52%, 편향 보정을 추가하면 0.39%의 성능 향상이 있었습니다.
  - 핵 노름 기반 혼합 정밀도 양자화(76.26%)는 단일 비트 폭 양자화(75.42%)보다 성능이 우수했으며, 여기에 랭킹 인식 및 편향 보정을 모두 적용했을 때 76.98%의 Top-1 정확도를 달성하여 풀-정밀도 모델(77.91%)에 필적하는 성능을 보였습니다. 이는 제안된 각 구성 요소들이 ViT 양자화에 유익함을 입증합니다.

## 🧠 Insights & Discussion

- 본 논문은 Vision Transformer에 특화된 학습 후 양자화 기법의 중요성과 효과를 명확히 보여줍니다. 특히, 어텐션 메커니즘의 독특한 특성(상대적 순서 보존)을 고려한 랭킹 손실의 도입은 ViT 양자화 시 발생하는 성능 저하를 효과적으로 완화하는 핵심 요소임을 입증했습니다. 또한, 레이어별 민감도를 핵 노름을 통해 파악하고 이를 기반으로 혼합 정밀도 양자화를 적용함으로써, 각 레이어의 중요도에 따라 비트 폭을 최적화하여 모델 용량을 효율적으로 압축하면서도 정확도 손실을 최소화했습니다.
- 제안된 방법은 큰 ViT 모델일수록 양자화로 인한 성능 저하가 적다는 흥미로운 결과도 보여줍니다. 이는 대규모 모델이 더 많은 파라미터를 가지고 있어 동일한 비트 폭으로 양자화될 때 더 높은 표현력을 유지할 수 있거나, 또는 어느 정도의 양자화 오류에 더 강인할 수 있음을 시사합니다. 이 연구는 ViT를 모바일 및 엣지 디바이스와 같은 자원 제약 환경에 배포하기 위한 실용적인 솔루션을 제공하며, 향후 더 낮은 비트 폭 양자화 연구의 기반이 될 수 있습니다.

## 📌 TL;DR

리소스 제약이 있는 환경에서 Vision Transformer를 배포하기 위해, 어텐션 메커니즘의 순서를 보존하는 **랭킹 손실**과 레이어별 민감도(핵 노름 기반)를 고려한 **혼합 정밀도 양자화**를 포함하는 효과적인 **학습 후 양자화(PTQ)** 기법을 제안합니다. 이 방법은 기존 PTQ 대비 ViT 및 DETR 모델에서 현저히 높은 정확도를 달성하며, 특히 8비트 양자화 시 풀-정밀도 모델과 필적하는 성능을 보여줍니다.
