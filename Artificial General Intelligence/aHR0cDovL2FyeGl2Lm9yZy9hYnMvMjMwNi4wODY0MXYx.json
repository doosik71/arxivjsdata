{
  "title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large\n  Language Models",
  "authors": "Lingxi Xie, Longhui Wei, Xiaopeng Zhang, Kaifeng Bi, Xiaotao Gu, Jianlong Chang, Qi Tian",
  "year": 2023,
  "url": "http://arxiv.org/abs/2306.08641v1",
  "abstract": "The AI community has been pursuing algorithms known as artificial general\nintelligence (AGI) that apply to any kind of real-world problem. Recently, chat\nsystems powered by large language models (LLMs) emerge and rapidly become a\npromising direction to achieve AGI in natural language processing (NLP), but\nthe path towards AGI in computer vision (CV) remains unclear. One may owe the\ndilemma to the fact that visual signals are more complex than language signals,\nyet we are interested in finding concrete reasons, as well as absorbing\nexperiences from GPT and LLMs to solve the problem. In this paper, we start\nwith a conceptual definition of AGI and briefly review how NLP solves a wide\nrange of tasks via a chat system. The analysis inspires us that unification is\nthe next important goal of CV. But, despite various efforts in this direction,\nCV is still far from a system like GPT that naturally integrates all tasks. We\npoint out that the essential weakness of CV lies in lacking a paradigm to learn\nfrom environments, yet NLP has accomplished the task in the text world. We then\nimagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale,\ninteractable environments, pre-trains it to predict future frames with respect\nto its action, and then fine-tunes it with instruction to accomplish various\ntasks. We expect substantial research and engineering efforts to push the idea\nforward and scale it up, for which we share our perspectives on future research\ndirections.",
  "citation": 12
}