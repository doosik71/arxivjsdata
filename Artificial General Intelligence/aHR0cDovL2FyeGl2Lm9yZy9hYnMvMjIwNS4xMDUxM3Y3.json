{
  "title": "Computable Artificial General Intelligence",
  "authors": "Michael Timothy Bennett",
  "year": 2022,
  "url": "http://arxiv.org/abs/2205.10513v7",
  "abstract": "Artificial general intelligence (AGI) may herald our extinction, according to\nAI safety research. Yet claims regarding AGI must rely upon mathematical\nformalisms -- theoretical agents we may analyse or attempt to build. AIXI\nappears to be the only such formalism supported by proof that its behaviour is\noptimal, a consequence of its use of compression as a proxy for intelligence.\nUnfortunately, AIXI is incomputable and claims regarding its behaviour highly\nsubjective. We argue that this is because AIXI formalises cognition as taking\nplace in isolation from the environment in which goals are pursued (Cartesian\ndualism). We propose an alternative, supported by proof and experiment, which\novercomes these problems. Integrating research from cognitive science with AI,\nwe formalise an enactive model of learning and reasoning to address the problem\nof subjectivity. This allows us to formulate a different proxy for\nintelligence, called weakness, which addresses the problem of incomputability.\nWe prove optimal behaviour is attained when weakness is maximised. This proof\nis supplemented by experimental results comparing weakness and description\nlength (the closest analogue to compression possible without reintroducing\nsubjectivity). Weakness outperforms description length, suggesting it is a\nbetter proxy. Furthermore we show that, if cognition is enactive, then\nminimisation of description length is neither necessary nor sufficient to\nattain optimal performance, undermining the notion that compression is closely\nrelated to intelligence. However, there remain open questions regarding the\nimplementation of scale-able AGI. In the short term, these results may be best\nutilised to improve the performance of existing systems. For example, our\nresults explain why Deepmind's Apperception Engine is able to generalise\neffectively, and how to replicate that performance by maximising weakness.",
  "citation": 11
}