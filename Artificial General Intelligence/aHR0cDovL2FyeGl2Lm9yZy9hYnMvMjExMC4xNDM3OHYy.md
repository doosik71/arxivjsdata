# Towards artificial general intelligence via a multimodal foundation model

Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, Hao Sun and Ji-Rong Wen

## 🧩 Problem to Solve

기존의 인공지능(AI) 연구는 대부분 단일 인지 능력(예: 이미지 분류, 언어 이해)에 초점을 맞춰 왔습니다. 이러한 한계를 극복하고 인간의 핵심 인지 활동을 모방하는 인공 일반 지능(AGI)을 향한 견고한 발걸음을 내딛기 위해서는 다양한 인지 작업을 처리할 수 있는 다중 모달 기반 모델이 필요합니다. 특히, 기존 다중 모달 모델들이 강력한 의미론적 상관관계(strong semantic correlation)를 가정하여 수집된 제한적인 데이터(예: 이미지-캡션 쌍)에 의존하고, 미세한 영역-단어 매칭을 위해 계산 비용이 많이 드는 객체 탐지기와 단일 타워 아키텍처를 사용하는 문제점을 해결해야 합니다.

## ✨ Key Contributions

- **약한 의미론적 상관관계 데이터 활용:** 인터넷에서 수집한 6억 5천만 개의 방대한 이미지-텍스트 쌍으로 구성된 약한 의미론적 상관관계 데이터셋(WSCD)을 활용하여 모델의 일반화 능력과 인지 능력을 향상시켰습니다.
- **효율적인 투-타워(Two-Tower) 아키텍처 및 MoCo 기반 대조 학습:** 계산 비용이 높은 객체 탐지기를 사용하지 않고, 이미지와 텍스트를 별도의 인코더로 처리하는 투-타워 아키텍처와 MoCo(Momentum Contrast) 기반의 교차 모달 대조 학습(cross-modal contrastive learning) 알고리즘을 제안하여 대규모 비지도 사전 학습을 가능하게 했습니다. 이를 통해 GPU 자원을 절약하면서도 대규모 음성 샘플을 효과적으로 활용합니다.
- **강력한 상상력 능력 입증:** 개발된 모델 해석 도구(신경망 시각화, 텍스트-투-이미지 생성)를 통해 BriVL이 추상적인 개념과 복잡한 문장에 대해 강력한 상상력과 상식 추론 능력을 가지고 있음을 시각적으로 증명했습니다.
- **다양한 다운스트림 작업에서의 우수한 성능:** 원격 감지 장면 분류, 뉴스 분류, 교차 모달 검색, 시각적 질문 응답(VQA) 등 광범위한 다운스트림 작업에서 CLIP과 같은 강력한 베이스라인을 능가하는 유망한 결과를 달성하여 교차 모달 이해 및 교차 도메인 학습/전이 능력을 입증했습니다.

## 📎 Related Works

- **단일 모달 사전 학습 모델:** GPT, BERT와 같은 NLP 모델과 BiT, ViT와 같은 CV 모델은 각 모달에서 큰 성공을 거두었으나, 단일 모달에만 이점을 제공합니다.
- **기존 다중 모달 기반 모델:** UNITER, OSCAR, M6(단일 타워 아키텍처) 및 CLIP, ALIGN(투-타워 아키텍처) 등이 있으며, 주로 이미지-캡션 쌍과 같은 강력한 의미론적 상관관계 데이터를 가정하고 사전 학습을 진행했습니다.
- **대조 학습(Contrastive Learning):** MoCo, SimCLR 등 단일 모달(이미지)에서 양성 샘플을 가깝게, 음성 샘플을 멀리 떨어뜨려 학습하는 자가 지도 학습(self-supervised learning) 방법론에서 영감을 받았습니다.

## 🛠️ Methodology

1. **대규모 약한 의미론적 상관관계 데이터셋(WSCD) 구축:**
   - 인터넷(뉴스, 백과사전, 소셜 미디어)에서 총 6억 5천만 개의 이미지-텍스트 쌍을 수집했습니다.
   - 이미지와 텍스트가 약하게 상관되어 있으며(예: 이미지 내용에 대한 상세한 설명 없이 "좋은 날이네요!"와 같은 제목), 실제 데이터 분포를 보존하기 위해 음란하거나 민감한 데이터만 필터링했습니다.
2. **투-타워(Two-Tower) 아키텍처 및 MSPP(Multi-Scale Patch Pooling) 모듈:**
   - **이미지 인코더:** EfficientNet-B7과 같은 CNN 백본, MSPP 모듈, 자기 주의(Self-Attention, SA) 블록, MLP(Multi-Layer Perceptron)로 구성됩니다. MSPP는 이미지의 여러 스케일에서 패치 특징을 추출하여 미세한 영역 특징을 효율적으로 포착하며, 계산 비용이 높은 객체 탐지기를 대체합니다.
   - **텍스트 인코더:** RoBERTa-Large와 같은 Transformer 인코더, SA 블록, MLP로 구성됩니다.
   - 두 인코더는 이미지와 텍스트를 동일한 의미 공간($\mathbb{R}^d$)에 임베딩합니다. 최종 임베딩 크기는 2,560입니다.
3. **MoCo 기반 교차 모달 대조 학습:**
   - **4-타워(Four-Tower) 아키텍처:** 기본 투-타워 외에, 두 개의 모멘텀 인코더($f_{\text{m}}^{(\text{i})}$ 및 $f_{\text{m}}^{(\text{t})}$)를 추가하여 동적으로 음성 샘플 큐($Q^{(\text{i})}$, $Q^{(\text{t})}$)를 유지합니다. 모멘텀 업데이트($\theta_{\text{m}} = m \cdot \theta_{\text{m}} + (1-m) \cdot \theta$)를 통해 부드럽게 업데이트되는 모멘텀 인코더로 음성 샘플을 인코딩합니다.
   - **InfoNCE 손실:** 이미지-텍스트 쌍 간의 교차 모달 대조 손실($L_{\text{i2t}}, L_{\text{t2i}}$)을 사용하여 양성 쌍의 유사도를 최대화하고 음성 쌍의 유사도를 최소화합니다.
     $$ L*{\text{i2t}} = -\frac{1}{N*{\text{b}}} \sum*{i} \log \frac{\exp(z*{\text{i}}^{(\text{i})} \cdot p*{\text{i}}^{(\text{t})} / \tau)}{\exp(z*{\text{i}}^{(\text{i})} \cdot p*{\text{i}}^{(\text{t})} / \tau) + \sum*{n^{(\text{t})}} \exp(z*{\text{i}}^{(\text{i})} \cdot n^{(\text{t})} / \tau)} $$
        $$ L*{\text{t2i}} = -\frac{1}{N*{\text{b}}} \sum*{i} \log \frac{\exp(z*{\text{i}}^{(\text{t})} \cdot p*{\text{i}}^{(\text{i})} / \tau)}{\exp(z*{\text{i}}^{(\text{t})} \cdot p*{\text{i}}^{(\text{i})} / \tau) + \sum*{n^{(\text{i})}} \exp(z*{\text{i}}^{(\text{t})} \cdot n^{(\text{i})} / \tau)} $$
        총 손실은 $L_{\text{total}} = L_{\text{i2t}} + L_{\text{t2i}}$입니다.
   - 이를 통해 적은 배치 크기로도 대규모 음성 샘플을 활용할 수 있어 GPU 자원을 절약합니다.
4. **모델 해석 도구:**
   - **신경망 시각화:** 임의의 노이즈 이미지($x^{(\text{i})}$)를 텍스트 임베딩($z^{(\text{t})}$)과 일치하도록 역전파를 통해 업데이트하여 모델이 텍스트에 대해 '상상하는' 이미지를 생성합니다. 손실 함수는 $L_{\text{vis}} = -\cos(z^{(\text{i})}, z^{(\text{t})})$입니다.
   - **텍스트-투-이미지 생성:** VQGAN과 같은 외부 이미지 생성기를 BriVL로 가이드하여 텍스트 프롬프트에 해당하는 고품질의 사실적인 이미지를 생성합니다.
     $$ L\_{\text{t2i}} = -\cos(z^{(\text{i})}, z^{(\text{t})}) $$

## 📊 Results

- **신경망 시각화:** "자연", "시간", "꿈"과 같은 추상적인 개념과 복잡한 문장에 대해서도 구체적이고 논리적인 이미지를 생성하여 강력한 상상력과 상식 이해 능력을 보여주었습니다. 특히, "mountains with forests"와 같이 유사한 프롬프트에 대해 미세한 시각적 차이를 반영하여 상상합니다.
- **텍스트-투-이미지 생성:** CLIP과 비교했을 때, BriVL은 더 사실적이고 전체적으로 일관된 이미지를 생성했습니다. "blazing sea", "cyberpunk-styled city" 등 실생활에 존재하지 않거나 드문 개념에 대해서도 일관성 있는 이미지를 생성하여 강력한 일반화 및 상상력을 입증했습니다. 이는 약한 의미론적 상관관계 데이터 사전 학습의 이점 때문입니다.
- **원격 감지 장면 분류 (UCM, AID):** 제로샷(zero-shot) 설정에서 CLIP을 포함한 강력한 베이스라인보다 높은 분류 정확도를 달성했습니다. 특히, "baseball field viewed from above"와 같은 원격 감지 텍스트에 대한 시각화는 BriVL이 카메라 원근 변환을 일반화하고 상식 추론 능력을 가지고 있음을 시사합니다.
- **뉴스 분류 (Toutiao News, THUCNews):** 단일 모달 작업인 뉴스 분류에서도 RoBERTa-large보다 우수한 성능을 보여, 다중 모달 학습이 단일 모달 인지 능력(상상력/연관 능력)을 향상시킬 수 있음을 입증했습니다.
- **교차 모달 검색 (AIC-ICC):** 이미지-투-텍스트 및 텍스트-투-이미지 검색 모두에서 "직접 학습(direct training)" 모델보다 사전 학습된 BriVL 모델이 훨씬 우수한 성능을 보였습니다. 이는 대규모 다중 모달 사전 학습의 유용성을 강조합니다.
- **시각적 질문 응답 (Visual7W):** 사전 학습된 BriVL은 "기차가 왜 흐릿한가요?"와 같은 질문에 "빠르게 움직이기 때문"이라고 답하는 등, 상식적인 추론 능력을 보여주었습니다. 이는 약한 의미론적 상관관계 데이터로 사전 학습함으로써 복잡한 연관성을 학습한 결과로 해석됩니다.

## 🧠 Insights & Discussion

- **AGI를 향한 중요성:** BriVL은 약한 의미론적 상관관계 데이터를 활용한 대규모 다중 모달 사전 학습을 통해 강력한 상상력과 상식 추론 능력을 보여주며, AGI 실현에 한 걸음 더 다가섰음을 시사합니다.
- **약한 의미론적 상관관계 데이터의 이점:** 기존의 강력한 상관관계 데이터는 객체와 단어 간의 직접적인 매칭을 학습하는 데 적합하지만, 약한 상관관계 데이터는 이미지와 텍스트 전반에 걸친 복잡한 인간의 감정과 생각을 융합하여 모델을 더 인지적이고 일반화할 수 있도록 만듭니다.
- **확장 가능성:** BriVL의 GPU 자원 절약형 사전 학습 프레임워크는 더 많은 모달리티(예: 비디오, 오디오)와 대규모 모델로 확장될 수 있어, AGI 연구를 가속화하고 신경 과학, 헬스케어 등 다양한 AI+ 분야에 폭넓게 적용될 잠재력을 가집니다.
- **한계 및 과제:** 사전 학습 데이터의 편향 학습, 악의적인 콘텐츠 생성 가능성 등 잠재적 위험과 윤리적 문제를 인식해야 합니다. 학술적으로는 더 깊이 있는 모델 해석 도구 개발, 다양한 모달리티를 포함하는 대규모 데이터셋 구축, 효과적인 미세 조정 기법 연구가 필요합니다.
- **미래 연구:** 다국어 데이터셋을 활용한 언어 번역 모델 개발, 비디오 및 오디오 모달리티 추가를 통한 더 지능적인 모델 구축 등이 가능합니다.

## 📌 TL;DR

BriVL은 AGI를 향한 발걸음으로, 인터넷에서 수집된 6억 5천만 개의 **약한 의미론적 상관관계 이미지-텍스트 데이터**로 사전 학습된 대규모 **다중 모달 기반 모델**입니다. 이 모델은 **효율적인 투-타워 아키텍처**와 **MoCo 기반 대조 학습**을 사용하여 미세한 영역-단어 매칭을 넘어 전반적인 이미지-텍스트 이해를 목표로 합니다. 결과적으로 BriVL은 **강력한 상상력과 상식 추론 능력**을 보여주며, **원격 감지, 뉴스 분류, 교차 모달 검색, VQA** 등 다양한 다운스트림 작업에서 **우수한 성능**을 달성했습니다. 이는 대규모 약한 상관관계 데이터 학습이 모델의 **인지적 능력과 일반화 능력**을 향상시켜 AGI에 더 가까워질 수 있음을 시사합니다.
