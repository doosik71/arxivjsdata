{
  "title": "Artificial General Intelligence, Existential Risk, and Human Risk\n  Perception",
  "authors": "David R. Mandel",
  "year": 2023,
  "url": "http://arxiv.org/abs/2311.08698v1",
  "abstract": "Artificial general intelligence (AGI) does not yet exist, but given the pace\nof technological development in artificial intelligence, it is projected to\nreach human-level intelligence within roughly the next two decades. After that,\nmany experts expect it to far surpass human intelligence and to do so rapidly.\nThe prospect of superintelligent AGI poses an existential risk to humans\nbecause there is no reliable method for ensuring that AGI goals stay aligned\nwith human goals. Drawing on publicly available forecaster and opinion data,\nthe author examines how experts and non-experts perceive risk from AGI. The\nfindings indicate that the perceived risk of a world catastrophe or extinction\nfrom AGI is greater than for other existential risks. The increase in perceived\nrisk over the last year is also steeper for AGI than for other existential\nthreats (e.g., nuclear war or human-caused climate change). That AGI is a\npressing existential risk is something on which experts and non-experts agree,\nbut the basis for such agreement currently remains obscure.",
  "citation": 1
}