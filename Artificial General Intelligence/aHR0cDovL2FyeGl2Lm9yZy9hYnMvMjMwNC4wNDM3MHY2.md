# OpenAGI: When LLM Meets Domain Experts

Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang

## 🧩 Problem to Solve

인간 지능(HI)은 기본적인 기술들을 결합하여 복잡한 작업을 해결하는 데 탁월하며, 이러한 능력은 인공 일반 지능(AGI)을 향한 AI 에이전트에게 필수적입니다. 대규모 언어 모델(LLM)은 학습 및 추론 능력을 바탕으로 외부 모델, 도구, 플러그인 또는 API를 활용하여 복잡한 문제를 해결할 가능성을 보입니다. 그러나 현재 연구는 다음과 같은 주요 문제에 직면해 있습니다:

- **확장성(Extensibility):** WebGPT, ToolFormer와 같은 기존 시스템은 고정된 수의 모델을 사용하므로 기능 확장이 어렵습니다.
- **비선형 작업 계획(Nonlinear Task Planning):** 대부분의 연구가 선형적(각 하위 작업이 다음 작업 시작 전에 완료되어야 함)으로만 작업을 계획하여 복잡한 다중 모달 입력 작업을 해결하기에 부적합합니다.
- **정량적 평가(Quantitative Evaluation):** HuggingGPT와 같이 주로 질적 결과만 제공하는 경우가 많아 LLM의 계획 능력이 최적인지 객관적으로 평가하기 어렵습니다.

## ✨ Key Contributions

- **OpenAGI 플랫폼 소개:** 복잡한 다단계 실제 작업을 해결하기 위한 오픈 소스 AGI 연구 및 개발 플랫폼인 OpenAGI를 제안합니다. 이 플랫폼은 LLM 기반의 (개방형 도메인) 모델 합성을 지원하며, Hugging Face, GitHub 및 LangChain의 다양한 모델과 데이터셋을 활용하여 LLM의 전반적인 계획 및 문제 해결 능력을 정량화하는 것을 목표로 합니다.
- **LLM+RLTF 접근 방식 제안:** LLM을 컨트롤러로 사용하여 다양한 외부 전문가 모델을 선택, 합성 및 실행하여 복잡한 작업을 해결하는 LLM+RLTF (Reinforcement Learning from Task Feedback) 접근 방식을 제안합니다. 작업 실행 결과에서 얻은 피드백은 LLM의 계획 전략을 개선하여 전반적인 성능과 문제 해결 능력을 향상시키는 데 사용됩니다.
- **다양한 LLM 평가 및 분석:** 오픈 소스 및 클로즈드 소스 LLM을 OpenAGI 파이프라인과 다양한 학습 방식(zero-shot, few-shot, fine-tuning, RLTF)에서 평가합니다. 이 연구는 RLTF와 같은 적절한 학습 방식과 결합될 경우, 더 작은 규모의 LLM도 훨씬 큰 매개변수 규모를 가진 경쟁 모델보다 뛰어난 성능을 보일 수 있음을 시사합니다.

## 📎 Related Works

- **대규모 언어 모델(LLM) 및 AI 에이전트:**
  - GPT, LLaMA, T5 시리즈와 같은 LLM은 자연어 이해, 생성, 추론에서 뛰어난 능력을 보였습니다.
  - **Augmented Language Models (ALMs):** LLM의 한계를 보완하기 위해 외부 자원 활용 및 추론 능력을 강화한 모델들입니다.
    - **ToolFormer [40]:** LLM이 외부 API 도구에 접근하도록 API 태그를 텍스트 시퀀스 내에 삽입합니다.
    - **Visual ChatGPT [51]:** ChatGPT와 Visual Foundation Models (VFMs)을 결합하여 시각적 콘텐츠를 생성합니다.
    - **HuggingGPT [42]:** ChatGPT와 Hugging Face 허브의 태스크별 모델을 통합하여 AI 작업을 해결합니다.
    - **AutoGPT [15]:** 다중 목표를 설정하고 작업을 반복하여 달성하는 자동화된 에이전트입니다.
- **인간 피드백 기반 강화 학습(RLHF):** LLM을 인간 가치에 맞추기 위해 인간 피드백을 수집하여 LLM을 미세 조정하는 기술입니다 (예: InstructGPT [27]). 그러나 RLTF는 인간 개입 없이 태스크 피드백을 활용하여 학습 방향을 안내함으로써, 수동 레이블링 의존성을 줄입니다.

## 🛠️ Methodology

OpenAGI 플랫폼은 **벤치마크 작업(benchmark tasks)**과 **개방형 작업(open-ended tasks)**이라는 두 가지 접근 방식을 통해 다양한 요구사항을 충족합니다.

### 벤치마크 작업

LLM의 복잡한 다단계 작업 계획 능력을 정량적으로 평가하기 위한 도구를 제공합니다.

1. **도메인 전문가 모델 세트:**
   - **언어 관련 모델:** FinBert (감성 분석), BART (텍스트 요약), T5 (기계 번역), DistilRoberta (마스크 채우기), DistilBERT (질문 답변) 등.
   - **비전 관련 모델:** ViT (이미지 분류), DETR (객체 탐지), Colorizer (색상화), Swin2SR (이미지 초해상도), Restormer (이미지 노이즈 제거 및 디블러링) 등.
   - **비전-언어 모델:** GIT (시각적 질문 답변), Vision Encoder Decoder (이미지 캡셔닝), Stable Diffusion (텍스트-이미지 생성) 등.
   - **데이터셋:** ImageNet-1K, COCO, CNN/Daily Mail, SST2, TextVQA, SQuAD 등의 원본 데이터셋을 활용합니다.
2. **다단계 작업 및 데이터셋 구성:**
   - **데이터 증강 방법:** Gaussian Blur, Gaussian Noise, Grayscale, Low Resolution, Translation, Word Mask 등의 증강 기법을 적용하여 복잡한 다단계 작업을 생성합니다.
   - **작업 분류:** 이미지 입력-이미지 출력, 이미지 입력-텍스트 출력, 텍스트 입력-이미지 출력, 텍스트 입력-텍스트 출력, 이미지-텍스트 쌍 입력-텍스트 출력, 텍스트-텍스트 쌍 입력-텍스트 출력 등 6가지 주요 카테고리로 총 185개의 다단계 작업(117개 선형, 68개 비선형)이 개발되었습니다.
3. **평가 지표:**
   - **CLIP Score [16]:** 텍스트-이미지 생성 기반 작업에 사용됩니다.
   - **BERT Score [56]:** 텍스트 출력 작업에 사용됩니다.
   - **ViT Score:** 이미지 출력 작업에 사용됩니다.

### 개방형 작업

높은 수준의 창의적이고 상상적인 문제 해결 능력을 요구하며 탐색적 연구를 장려합니다.

- **모델 확장성:** LangChain을 통합하여 Google Search, Wikipedia, Wolfram Alpha 등 추가적인 외부 전문가 모델을 제공합니다.
- **비선형 계획:** LLM이 온라인 검색 등을 통해 정보를 수집하고, 이를 바탕으로 그림, 시, 음악 등을 생성하는 비선형(트리 구조) 계획 능력을 시연합니다.

### 태스크 피드백 기반 강화 학습 (RLTF)

LLM의 계획 전략을 개선하기 위해 강화 학습(RL) 기법을 사용합니다.

- **메커니즘:** LLM이 고안한 솔루션의 실행 후 얻은 작업 성능 피드백을 활용하여 LLM의 계획 전략을 정교화합니다.
- **알고리즘:** REINFORCE [50] 알고리즘을 사용하며, 보상 신호 $R$이 미분 불가능하므로 다음과 같은 정책 경사도 방법을 통해 LLM의 매개변수 $\Phi$를 업데이트합니다:
  $$ \nabla*{\Phi} J(\Phi) \approx \frac{1}{|T*{\text{train}}|} \sum*{t \in T*{\text{train}}} \nabla*{\Phi} \log P(s*{\text{train}}|\Phi) \cdot (R-b) $$
    여기서 $b$는 이전 보상 신호의 이동 평균인 기준 함수(baseline function)입니다.

### 계획 솔루션 파서

GPT-3.5를 기반으로 한 파서를 사용하여 LLM의 원본 출력을 실행 가능한 다단계 작업 계획 솔루션으로 변환합니다.

### 제약 조건 생성 (Constrained Generation)

LLM이 유효하고 실행 가능한 계획을 생성하도록 빔 탐색(beam search)에 제약 조건을 적용합니다. 이는 모델 이름만 생성하도록 하고, 연속적인 모델들의 입출력 모달리티가 일치하도록 보장합니다.

## 📊 Results

- **LLM 성능 비교:**
  - **클로즈드 소스 LLM:** GPT-4는 zero-shot 및 few-shot 시나리오 모두에서 가장 높은 점수를 기록하며 우수한 성능을 보였습니다.
  - **오픈 소스 LLM:** LLaMA-2-13B가 가장 좋은 결과를 보였으며, Fine-tuning 및 RLTF와 같은 튜닝 방식이 Flan-T5-Large, Vicuna-7B, LLaMA-2-13B의 성능을 크게 향상시켰습니다. 모든 오픈 소스 모델은 RLTF 접근 방식에서 최고의 성능을 달성했습니다.
- **RLTF의 효과:** RLTF 튜닝을 통해 LLaMA-2-13B의 성능이 GPT-3.5에 근접하여, 작은 규모의 LLM도 적절한 학습 방식과 결합될 경우 대규모 모델에 필적하는 잠재력을 가질 수 있음을 입증했습니다.
- **프롬프트의 영향:**
  - **클로즈드 소스 LLM:** 상세한 모델 관련 정보(Prompt-1, Prompt-2)가 제공될 때 성능이 향상되는 경향을 보였습니다.
  - **오픈 소스 LLM:** 상대적으로 약한 오픈 소스 LLM은 상세한 프롬프트의 모호한 세부 정보에 의해 혼란을 겪을 수 있음을 시사했습니다.
- **비선형 계획 사례 연구:** GPT-3.5는 주어진 이미지와 질문에 대해 비선형적이고 합리적인 계획을 성공적으로 생성한 반면, Flan-T5-Large와 Vicuna-7B는 이해 및 계획 생성에서 어려움을 겪었습니다.

## 🧠 Insights & Discussion

- **AGI를 향한 유망한 접근:** LLM과 도메인별 전문가 모델의 통합은 인간의 일반 지능과 전문화된 지능의 조화를 모방하여 AGI를 향한 유망한 접근 방식을 제공합니다. OpenAGI는 복잡한 다단계 작업을 해결하는 LLM의 계획 및 문제 해결 능력을 정량화하는 데 기여합니다.
- **RLTF의 중요성:** RLTF는 외부 텍스트 데이터에만 의존하는 LLM의 한계를 극복하고, 실제 태스크 피드백을 통해 모델이 자체적으로 전략을 개선하여 더욱 강력하고 적응성 있는 AI 에이전트 개발에 중요한 역할을 합니다.
- **작은 LLM의 잠재력:** RLTF와 같은 효과적인 학습 스키마를 통해 작은 규모의 오픈 소스 LLM도 대형 클로즈드 소스 모델에 필적하는 성능을 낼 수 있음을 보여주며, 이는 효율적인 AGI 개발의 가능성을 시사합니다.
- **한계 및 향후 과제:** 도메인 전문가 모델의 OOD(Out-of-Distribution) 일반화 능력 한계와 최적의 작업 계획을 식별하는 어려움이 있습니다 (RLTF가 이 문제 해결에 기여할 수 있음). 또한 AI 시스템의 오용을 방지하기 위한 신뢰할 수 있는 에이전트 개발이 중요합니다.
- **미래 연구 방향:**
  - **Human-in-the-loop 에이전트:** LLM이 적절한 모델이 없을 때 인간 전문가에게 답변을 요청하여 인간-AI 협업을 강화합니다.
  - **신뢰할 수 있는 에이전트(Trustworthy agents):** 작업 해결 과정에서 안전 및 윤리적 기준을 보장합니다.
  - **자체 개선 에이전트(Self-improving agents):** OpenAGI가 작업을 독립적으로 생성, 탐색하고, 자체 성찰 및 개선을 통해 지능형 에이전트의 자율성을 강화합니다.

## 📌 TL;DR

OpenAGI는 LLM이 다양한 도메인 전문가 모델을 합성하여 복잡한 다단계 실제 작업을 해결하도록 돕는 오픈 소스 AGI 연구 플랫폼입니다. 이 플랫폼은 벤치마크 및 개방형 작업을 제공하며, 태스크 피드백 기반 강화 학습(RLTF) 메커니즘을 통해 LLM의 계획 전략을 개선합니다. 실험 결과, RLTF가 적용된 소규모 LLM도 대규모 모델에 필적하는 성능을 달성할 수 있음을 보여주며, AGI 개발의 확장성, 비선형 계획, 정량적 평가 문제를 해결하는 데 기여합니다.
