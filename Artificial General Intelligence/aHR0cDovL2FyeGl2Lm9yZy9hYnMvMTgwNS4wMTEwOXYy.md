# AGI Safety Literature Review

Tom Everitt, Gary Lea, Marcus Hutter

## 🧩 Problem to Solve

본 논문은 인공일반지능(AGI) 개발이 가져올 수 있는 막대한 잠재적 이점과 함께 제기되는 심각한 안전 문제들을 해결하고자 한다. AGI의 통제 불능이 초래할 수 있는 재앙적 결과를 예방하기 위해, AGI가 개발되기 전부터 안전 문제를 연구하고 해결책을 모색하는 것이 핵심 연구 과제이다. 현재까지 제한된 지식으로 AGI를 이해하고, 개발 시점을 예측하며, 발생 가능한 문제와 그 해결책을 체계적으로 정리하는 것이 목표이다.

## ✨ Key Contributions

- AGI 안전 분야의 최신 연구 문헌을 포괄적으로 수집 및 정리하여 접근성을 높였다.
- AGI와 관련된 주요 안전 문제점들을 체계적으로 식별하고 분류했다.
- 이러한 안전 문제들을 해결하거나 완화하기 위한 다양한 설계 아이디어와 연구 동향을 조사했다.
- AGI에 대한 현재의 이해, 개발 시점 예측, 그리고 AGI가 생성된 후 발생할 수 있는 시나리오들을 다루었다.
- AGI 안전에 관한 현재의 공공 정책 현황을 검토하고 정책적 권고 사항들을 제시했다.
- 다양한 AGI 안전 연구 아젠다(MIRI, FLI, ANU, CHAI, FHI, DeepMind, OpenAI 등) 간의 문제점 연결고리를 시각적으로 제시하여 분야 전반의 이해를 도왔다.

## 📎 Related Works

- **Nick Bostrom의 『초지능(Superintelligence: Paths, Dangers, Strategies)』 (2014)**: AGI의 위험성에 대한 중요한 기반을 다진 저서.
- **Sotala와 Yampolskiy (2014)**: AGI 안전 문헌에 대한 이전의 광범위한 조사.
- **Legg와 Hutter (2007b)의 지능 정의**: 알고리즘 정보 이론과 AIXI 이론에 기반한 공식적인 지능 정의.
- **Omohundro (2007, 2008)와 Bostrom (2012)**: AGI의 수렴적 도구적 목표(convergent instrumental goals) 개념.
- **Kurzweil (2005)의 『특이점이 온다(The Singularity Is Near)』**: AGI 출현 시기에 대한 예측과 특이점 개념.
- **MIRI, OpenAI, DeepMind, FHI, CHAI 등 주요 AI 연구 기관의 안전 연구 아젠다 및 문제점 컬렉션**: 이 분야의 다양한 핵심 문제와 해결책 접근 방식들을 제시한다.

## 🛠️ Methodology

이 논문은 AGI 안전 분야의 기존 연구들을 체계적으로 검토하는 문헌 연구(literature review) 접근 방식을 사용한다.

1. **AGI 이해 모델 탐색**:

   - 지능의 정의(Legg-Hutter), 직교성 가설(orthogonality thesis), 수렴적 도구적 목표(convergent instrumental goals)와 같은 AGI의 개념적 모델을 탐색한다.
   - AIXI 프레임워크 및 논리적 비전지성(logical non-omniscience)과 같은 formal한 접근 방식을 검토한다.

2. **AGI 개발 예측 분석**:

   - 전문가 설문조사 및 기술 동향 분석을 통해 AGI 출현 시기에 대한 다양한 예측을 조사한다.
   - 기술적 특이점(technological singularity) 개념과 이에 대한 반론들을 다룬다.

3. **AGI 안전 문제 식별 및 분류**:

   - MIRI, FLI, DeepMind, OpenAI 등 주요 연구 기관의 발표된 연구 아젠다와 문제점 목록을 종합하여 AGI 안전 문제를 크게 7가지 범주(가치 명세, 신뢰성, 교정 가능성, 보안, 안전한 학습, 이해 가능성, 사회적 영향)와 덜 명확한 문제들(하위 에이전트, 악성 신념 분포, 물리적 의사결정 등)로 분류한다.

4. **안전한 AGI 설계 아이디어 조사**:

   - 식별된 문제들을 해결하기 위한 구체적인 설계 아이디어 및 기술적 접근 방식을 제시한다. 여기에는 강화 학습에서의 보상 함수 정렬, 자가 수정 제어, 의사결정 이론, 적대적 공격 방어, 설명 가능한 AI, 안전한 탐색, AGI 격리(boxing), 메타 인지 등 다양한 방법론이 포함된다.

5. **공공 정책 및 거버넌스 검토**:
   - AGI 안전에 대한 정책적 권고 사항들을 검토하고, 국제적 협력의 필요성 및 규제에 대한 논의를 분석한다.
   - 다양한 국가 및 기관(IEEE, ACM, OECD, ISO/IEC 등)의 현재 정책 현황과 동향을 살펴본다.

## 📊 Results

- **AGI 출현 예측**: AI 전문가들의 설문조사에 따르면, 인간 수준의 AGI는 수십 년 내(중앙값 2040년-2061년)에 출현할 가능성이 높다고 예측된다. 지능 폭발(intelligence explosion) 가능성 또한 중요한 논의 대상이다.
- **AGI 안전 문제 클러스터 식별**: 가치 명세(value specification), 신뢰성(reliability), 교정 가능성(corrigibility), 보안(security), 안전한 학습(safe learning), 이해 가능성(intelligibility), 사회적 영향(societal consequences) 등 7가지 주요 문제 범주가 확인되었다. 또한 하위 에이전트, 악성 신념 분포, 메타 인지 등 덜 명확하지만 중요한 문제들도 제기되었다.
- **안전한 AGI 설계 아이디어 다양성**: 역강화 학습(Inverse Reinforcement Learning, IRL), 협력적 역강화 학습(Cooperative Inverse Reinforcement Learning, CIRL), 승인 지향 에이전트(approval-directed agents), 기능적 의사결정 이론(functional decision theory), 중단 가능성(corrigibility) 설계, 적대적 예제 방어(ReluPlex), 설명 가능한 AI 기술, 안전한 탐색 및 재난 감지기 등 다양한 기술적 해결책들이 모색되고 있다.
- **공공 정책 현황**: AGI에 대한 공공 정책은 초기 단계이며 단편적이다. IEEE, ACM, OECD 등 전문 기관과 기업, 학술 단체들이 가이드라인과 권고 사항을 제시하고 있지만, 국가 간 AI 경쟁 심화(AI arms race)와 같은 부정적 역학 관계가 글로벌 정책 조율을 저해할 수 있음이 확인되었다. 외재적(extrinsic) 규제와 내재적(intrinsic) 동기 부여 방식 간의 효과성에 대한 논의도 진행 중이다.

## 🧠 Insights & Discussion

- **선제적 안전 연구의 중요성**: AGI의 잠재적 재앙과 '지능 폭발' 가능성을 고려할 때, AGI가 실제로 개발되기 전에 안전 문제를 연구하고 완화하는 것이 절대적으로 중요하다.
- **제어의 복잡성**: 인간보다 훨씬 지능적인 기계를 제어하는 것은 깊은 철학적, 기술적 도전이며, 단순히 '유익한' 목표를 설계하는 것만으로는 충분하지 않다. 시스템이 스마트해진다고 해서 유익한 목표가 자동으로 생성되지는 않는다.
- **즉각적인 이점**: AGI 안전 연구는 현재의 AI 시스템(예: 강건함, 설명 가능성, 안전한 학습)에도 적용될 수 있는 많은 단기적 이점을 제공하며, 이는 연구의 가치를 더욱 높인다.
- **학제 간 접근의 필요성**: AGI 안전은 컴퓨터 과학뿐만 아니라 철학, 경제학, 인지 과학, 공공 정책 등 다양한 분야의 통찰력을 요구하는 복합적인 문제이다.
- **지속적인 도전 과제**: 지능의 정확한 측정, 인간 가치의 정의, 장기적인 목표 정렬 및 교정 가능성 보장 등 많은 근본적인 문제들이 여전히 해결되지 않은 채 남아있다.
- **정책적 시급성**: 'AI 군비 경쟁'과 같은 국제적 역학 관계는 안전한 AGI 개발을 보장하기 위한 건전한 공공 정책 수립 및 국제 협력의 시급성을 강조한다.

## 📌 TL;DR

- **문제**: 인공일반지능(AGI)은 인류에게 큰 잠재력을 제공하지만, 통제되지 않을 경우 심각한 안전 문제와 재앙적 위험을 초래할 수 있다. 본 논문은 AGI가 개발되기 전에 이러한 위험을 식별하고 해결책을 모색하기 위해 AGI 안전 문헌을 포괄적으로 검토한다.
- **방법**: AGI의 개념적 모델(지능 정의, 직교성 등)을 이해하고, AGI 출현 시점 및 특이점 가능성에 대한 예측들을 분석한다. MIRI, DeepMind 등 주요 연구 기관의 아젠다를 기반으로 AGI 안전 문제(가치 명세, 신뢰성, 교정 가능성, 보안, 안전한 학습, 이해 가능성 등)를 분류하고, 각 문제에 대한 기술적 해결책(예: 역강화 학습, 기능적 의사결정 이론, 설명 가능한 AI) 및 공공 정책 동향을 조사한다.
- **발견**: AI 전문가들은 수십 년 내 AGI 출현을 예측하며, 지능 폭발 가능성이 높다고 본다. AGI가 인간 가치와 정렬되지 않거나, 자가 수정 과정에서 문제가 발생하거나, 통제 불능이 되는 등 다양한 안전 문제가 식별되었다. 이를 해결하기 위한 역강화 학습, 교정 가능 에이전트 설계, 적대적 공격 방어 등의 기술적 접근법들이 활발히 연구 중이다. 공공 정책은 아직 초기 단계이나, 국가 간 AI 경쟁이 심화되면서 안전 연구와 정책 수립의 시급성이 강조되고 있다.
