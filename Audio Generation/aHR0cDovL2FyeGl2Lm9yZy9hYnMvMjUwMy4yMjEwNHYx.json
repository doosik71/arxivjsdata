{
  "title": "M2D2: Exploring General-purpose Audio-Language Representations Beyond\n  CLAP",
  "authors": "Daisuke Niizumi, Daiki Takeuchi, Masahiro Yasuda, Binh Thien Nguyen, Yasunori Ohishi, Noboru Harada",
  "year": 2025,
  "url": "http://arxiv.org/abs/2503.22104v1",
  "abstract": "Contrastive language-audio pre-training (CLAP) has addressed audio-language\ntasks such as audio-text retrieval by aligning audio and text in a common\nfeature space. While CLAP addresses general audio-language tasks, its audio\nfeatures do not generalize well in audio tasks. In contrast, self-supervised\nlearning (SSL) models learn general-purpose audio features that perform well in\ndiverse audio tasks. We pursue representation learning that can be widely used\nin audio applications and hypothesize that a method that learns both general\naudio features and CLAP features should achieve our goal, which we call a\ngeneral-purpose audio-language representation. To implement our hypothesis, we\npropose M2D2, a second-generation masked modeling duo (M2D) that combines an\nSSL M2D and CLAP. M2D2 learns two types of features using two modalities (audio\nand text) in a two-stage training process. It also utilizes advanced LLM-based\nsentence embeddings in CLAP training for powerful semantic supervision. In the\nfirst stage, M2D2 learns generalizable audio features from M2D and CLAP, where\nCLAP aligns the features with the fine LLM-based semantic embeddings. In the\nsecond stage, it learns CLAP features using the audio features learned from the\nLLM-based embeddings. Through these pre-training stages, M2D2 should enhance\ngeneralizability and performance in its audio and CLAP features. Experiments\nvalidated that M2D2 achieves effective general-purpose audio-language\nrepresentation, highlighted with SOTA fine-tuning mAP of 49.0 for AudioSet,\nSOTA performance in music tasks, and top-level performance in audio-language\ntasks."
}