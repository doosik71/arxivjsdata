{
  "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual\n  Representation and Generation",
  "authors": "Kun Su, Xiulong Liu, Eli Shlizerman",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.19132v1",
  "abstract": "Video encompasses both visual and auditory data, creating a perceptually rich\nexperience where these two modalities complement each other. As such, videos\nare a valuable type of media for the investigation of the interplay between\naudio and visual elements. Previous studies of audio-visual modalities\nprimarily focused on either audio-visual representation learning or generative\nmodeling of a modality conditioned on the other, creating a disconnect between\nthese two branches. A unified framework that learns representation and\ngenerates modalities has not been developed yet. In this work, we introduce a\nnovel framework called Vision to Audio and Beyond (VAB) to bridge the gap\nbetween audio-visual representation learning and vision-to-audio generation.\nThe key approach of VAB is that rather than working with raw video frames and\naudio data, VAB performs representation learning and generative modeling within\nlatent spaces. In particular, VAB uses a pre-trained audio tokenizer and an\nimage encoder to obtain audio tokens and visual features, respectively. It then\nperforms the pre-training task of visual-conditioned masked audio token\nprediction. This training strategy enables the model to engage in contextual\nlearning and simultaneous video-to-audio generation. After the pre-training\nphase, VAB employs the iterative-decoding approach to rapidly generate audio\ntokens conditioned on visual features. Since VAB is a unified model, its\nbackbone can be fine-tuned for various audio-visual downstream tasks. Our\nexperiments showcase the efficiency of VAB in producing high-quality audio from\nvideo, and its capability to acquire semantic audio-visual features, leading to\ncompetitive results in audio-visual retrieval and classification."
}