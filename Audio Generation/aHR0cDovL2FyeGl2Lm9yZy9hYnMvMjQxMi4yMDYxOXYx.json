{
  "title": "Audiopedia: Audio QA with Knowledge",
  "authors": "Abhirama Subramanyam Penamakuri, Kiran Chhatre, Akshat Jain",
  "year": 2024,
  "url": "http://arxiv.org/abs/2412.20619v1",
  "abstract": "In this paper, we introduce Audiopedia, a novel task called Audio Question\nAnswering with Knowledge, which requires both audio comprehension and external\nknowledge reasoning. Unlike traditional Audio Question Answering (AQA)\nbenchmarks that focus on simple queries answerable from audio alone, Audiopedia\ntargets knowledge-intensive questions. We define three sub-tasks: (i) Single\nAudio Question Answering (s-AQA), where questions are answered based on a\nsingle audio sample, (ii) Multi-Audio Question Answering (m-AQA), which\nrequires reasoning over multiple audio samples, and (iii) Retrieval-Augmented\nAudio Question Answering (r-AQA), which involves retrieving relevant audio to\nanswer the question. We benchmark large audio language models (LALMs) on these\nsub-tasks and observe suboptimal performance. To address this, we propose a\ngeneric framework that can be adapted to any LALM, equipping them with\nknowledge reasoning capabilities. Our framework has two components: (i) Audio\nEntity Linking (AEL) and (ii) Knowledge-Augmented Audio Large Multimodal Model\n(KA2LM), which together improve performance on knowledge-intensive AQA tasks.\nTo our knowledge, this is the first work to address advanced audio\nunderstanding via knowledge-intensive tasks like Audiopedia."
}