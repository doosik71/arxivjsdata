# A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific Approaches

Keerthi Devireddy

## 🧩 Problem to Solve

이 논문은 딥러닝 모델, 특히 CNN이 이미지 인식 등 다양한 분야에서 뛰어난 성능을 보이지만, 그 의사결정 과정이 불투명한 '블랙박스'라는 문제를 다룹니다. 이는 모델이 _왜_ 특정 예측을 하는지 이해하기 어렵게 만들며, 헬스케어, 자율 시스템, 금융, 법률 및 윤리적 AI와 같은 고위험 분야에서 AI에 대한 신뢰와 투명성을 저해합니다. 설명 가능한 AI(XAI)는 이러한 복잡한 모델에 대한 해석 가능성을 제공하여, 사용자와 규제 기관이 AI 기반 결정을 신뢰할 수 있도록 돕는 것을 목표로 합니다.

## ✨ Key Contributions

- 모델 불가지론적(model-agnostic) 방법(LIME, SHAP)과 모델 특정적(model-specific) 방법(Grad-CAM, Guided Backpropagation)을 체계적으로 비교 분석했습니다.
- 다양한 종(7가지)에 대해 ResNet50 모델을 사용하여 이미지 분류 예측을 설명하는 각 XAI 방법의 효과를 평가했습니다.
- 각 방법의 강점, 약점, 그리고 적용 분야를 명확히 제시하며, 블랙박스 모델의 의사결정 과정에 대한 고유한 통찰력을 제공함을 입증했습니다.
- 단일 XAI 방법이 모든 시나리오에 적합하지 않으며, 포괄적인 모델 이해를 위해서는 여러 XAI 기술을 결합하는 하이브리드 접근 방식이 가장 효과적임을 강조했습니다.

## 📎 Related Works

- **LIME (Local Interpretable Model-Agnostic Explanations)**: 복잡한 모델의 개별 예측을 간단한 대체 모델로 근사하여 설명하는 방법론 (Ribeiro et al., 2016).
- **SHAP (Shapley Additive Explanations)**: 협력 게임 이론에서 파생된 Shapley 값을 사용하여 특징의 기여도를 공정하게 할당하는 통합된 설명 방법 (Lundberg & Lee, 2017).
- **Grad-CAM (Gradient-weighted Class Activation Mapping)**: CNN의 특징 맵에 대한 클래스 점수의 기울기를 사용하여 이미지 내에서 클래스를 식별하는 데 가장 영향력 있는 영역을 시각화하는 방법 (Selvaraju et al., 2017).
- **Guided Backpropagation**: 표준 역전파를 수정하여 음의 기울기를 억제하고 양의, 클래스 관련 특징만을 전파시켜 고해상도 시각화를 생성하는 방법 (Springenberg et al., 2015).
- **ResNet50**: 이미지 분류 작업에서 널리 채택된 강력한 딥러닝 아키텍처 (He et al., 2016).

## 🛠️ Methodology

이 연구는 ResNet50 모델을 사용하여 7가지 다양한 종(사모예드, 몰티즈, 아메리카 울새, 거위, 코요테, 이집트 고양이, 무당벌레)에 대한 이미지 분류 예측을 해석하기 위해 네 가지 XAI 기술(LIME, SHAP, Grad-CAM, Guided Backpropagation)을 적용하고 비교 분석했습니다.

- **모델**: ResNet50
  - 견고한 아키텍처와 이미지 분류 태스크에서의 광범위한 채택을 이유로 선택되었습니다. 깊은 잔여 연결(residual connections)은 그래디언트 흐름을 향상시켜 복잡한 시각 입력에서 효과적인 특징 추출을 가능하게 합니다.
- **데이터셋**: 질감, 모양 및 색상 패턴의 다양성을 고려하여 7가지 종의 이미지를 사용했습니다. 이는 XAI 방법이 다양한 시각적 속성에 얼마나 잘 적응하는지 평가하기 위함입니다.
- **XAI 방법**:
  - **모델 불가지론적 방법**:
    - **LIME**: 입력 데이터에 변형을 가하고, 모델의 예측 변화를 관찰하여 관심 인스턴스 주변의 동작을 모방하는 간단한 선형 모델을 훈련합니다.
      $$ \arg \min*{g \in G} L(f,g,\pi*{x}) + \Omega(g) $$
            여기서 $L(f,g,\pi_{x})$는 $g$가 $f$를 모방하도록 하는 손실 함수이며, $\pi_{x}$는 $x$에 가까운 교란 인스턴스에 더 높은 가중치를 부여하고, $\Omega(g)$는 $g$의 해석 가능성(예: 희소성 제약)을 규제합니다.
    - **SHAP**: 특징의 다른 연합(coalition)에 대한 기여도를 기반으로 특징 중요도를 할당합니다.
      $$ \varphi*{i} = \sum*{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} (f(S \cup \{i\})-f(S)) $$
            여기서 $S$는 특징의 부분 집합, $N$은 모든 특징의 집합, $f(S)$는 $S$에 있는 특징만 고려했을 때의 모델 출력입니다.
  - **모델 특정적 방법**:
    - **Grad-CAM**: CNN의 합성곱 계층 특징 맵에 대한 클래스 점수의 기울기를 계산하여 이미지에서 가장 영향력 있는 영역을 시각화합니다.
      $$ L*{c} = \text{ReLU} \left( \sum*{k} \alpha*{c}^{k} A^{k} \right) $$
            여기서 $A^{k}$는 합성곱 계층 $k$의 활성화 맵이며, $\alpha*{c}^{k}$는 중요도 가중치로, $\alpha_{c}^{k} = \frac{1}{Z} \sum_{i} \sum_{j} \frac{\partial y_{c}}{\partial A_{ij}^{k}}$ 입니다.
    - **Guided Backpropagation**: 표준 역전파를 수정하여 음의 기울기를 억제하고, 오직 양의, 클래스 관련 특징만 전파되도록 합니다.
      $$ R*{l} = \max(0,R*{l+1})\cdot \max\left(0, \frac{\partial y}{\partial x}\right) $$
            여기서 $R_l$은 계층 $l$에서의 관련성입니다.

## 📊 Results

- **일반적인 관찰**: 각 XAI 방법은 동일한 이미지를 다르게 해석하며, ResNet50이 시각 데이터를 어떻게 분류하는지에 대한 고유한 통찰력을 제공합니다. LIME과 SHAP은 중요도를 광범위하게 할당하는 반면, Grad-CAM과 Guided Backpropagation은 특정 특징 활성화에 중점을 둡니다.
- **종별 비교**:
  - **개(사모예드, 몰티즈)**: LIME & SHAP은 전체 몸의 윤곽을 강조하여 질감과 모양의 중요성을 나타냈습니다. Grad-CAM은 얼굴, 특히 눈과 코에 집중했습니다. Guided Backpropagation은 털 패턴의 미세한 질감을 강조했습니다.
  - **야생 동물(코요테, 이집트 고양이)**: SHAP과 LIME은 동물의 전체 몸을 강조했지만 배경 요소도 포함하여 잠재적인 모델 편향을 시사했습니다. Grad-CAM은 몸통과 얼굴 영역에 초점을 맞추어 클래스 식별 능력을 보였습니다. Guided Backpropagation은 털 질감과 몸 윤곽선을 명확히 보여주는 뛰어난 가장자리 감지 능력을 보였습니다.
  - **새(아메리카 울새, 거위)**: Grad-CAM은 머리와 부리에 강하게 초점을 맞춰 이 영역들이 분류에 크게 기여함을 확인했습니다. SHAP과 LIME은 중요도를 전체 몸에 더 넓게 분산시켜 전체적인 특징 기여도를 잘 나타냈습니다. Guided Backpropagation은 깃털 패턴을 명확하게 강조하여 CNN의 미세한 질감 의존성을 보여주었습니다.
  - **곤충(무당벌레)**: LIME & SHAP은 검은 점이 있는 붉은 등껍질을 주로 강조하여 색상과 패턴이 중요함을 확인했습니다. Grad-CAM은 상반부에 집중하여 더듬이와 머리 구조가 주요 단서임을 시사했습니다. Guided Backpropagation은 등껍질 가장자리를 정확하게 윤곽선으로 나타내어 모델이 전체 윤곽선을 명확하게 인식함을 확인했습니다.
- **모델 불가지론적 vs. 모델 특정적 방법 간의 상충 관계**:
  - **일반화 vs. 모델 제약**: 모델 불가지론적 방법(LIME, SHAP)은 일반화 가능한 설명을 제공하지만 계산 비용이 높을 수 있습니다. 모델 특정적 방법(Grad-CAM, Guided Backpropagation)은 효율적이지만 특정 아키텍처에 제약됩니다.
  - **특징 기여도 차이**: SHAP은 일관되게 전역적 중요도를 제공하는 반면, LIME은 지역적인 교란 기반 설명에 중점을 둡니다. Grad-CAM은 활성화가 높은 영역을 강조하고, Guided Backpropagation은 가장자리 감지 및 질감 초점을 향상시킵니다.
  - **계산 효율성**: SHAP은 계산 집약적이지만 신뢰도가 높습니다. Grad-CAM과 Guided Backpropagation은 실시간 시나리오에 적합한 더 빠른 해석을 제공합니다.

## 🧠 Insights & Discussion

이 비교 분석은 단일 XAI 방법이 보편적으로 우월하지 않음을 보여줍니다. 대신, 각 방법의 효과는 당면한 문제, 모델 아키텍처 및 계산 제약 조건에 따라 달라집니다. LIME과 SHAP은 다양한 모델에 걸쳐 광범위한 특징 기여도를 제공하는 반면, Grad-CAM과 Guided Backpropagation은 공간적 활성화 통찰력을 제공하는 딥러닝 중심의 설명을 제공합니다.

가장 중요한 통찰은 포괄적이고 신뢰할 수 있는 AI 모델 해석을 위해서는 **하이브리드 접근 방식(여러 방법을 결합)**이 가장 효과적이라는 것입니다. 예를 들어, SHAP을 사용하여 전역적 해석 가능성을 얻고 Grad-CAM을 사용하여 클래스 식별적 지역화를 수행하면 전체적이고 세밀한 통찰력을 모두 확보할 수 있습니다.

**한계 및 미래 방향**:

- 계산 비용이 높은 방법(예: SHAP)의 확장성 및 효율성 개선.
- CNN 이외의 트랜스포머, 그래프 신경망 등 비합성곱 아키텍처로 모델 특정적 해석 가능성 기술 확장.
- 이러한 설명이 인간의 직관 및 도메인 전문 지식과 얼마나 잘 일치하는지 평가하여 헬스케어, 자율 시스템과 같은 고위험 애플리케이션에서의 실용적인 유용성 보장.

XAI가 계속 발전함에 따라, 계산 효율성과 실제 사용 가능성을 해결하면서 여러 해석 가능성 접근 방식을 통합하는 것이 투명하고 신뢰할 수 있는 AI 시스템을 구축하는 데 중요할 것입니다.

## 📌 TL;DR

블랙박스 AI 모델의 불투명성을 해결하기 위해, 이 논문은 모델 불가지론적(LIME, SHAP) 및 모델 특정적(Grad-CAM, Guided Backpropagation) XAI 방법을 ResNet50 이미지 분류 모델에 적용하여 비교했습니다. 연구 결과, 각 방법이 모델의 의사결정에 대한 고유한 통찰력(광범위 vs. 정밀, 거시적 vs. 미세)을 제공하며 상충 관계가 존재함을 발견했습니다. 결론적으로, 포괄적인 모델 해석을 위해서는 다양한 XAI 기술을 결합하는 하이브리드 접근 방식이 가장 효과적입니다.
