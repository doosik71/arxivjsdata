# Expanding Explainability: Towards Social Transparency in AI systems

UPOL EHSAN, Q. VERA LIAO, MICHAEL MULLER, MARK O. RIEDL, JUSTIN D. WEISZ

## 🧩 Problem to Solve

인공지능(AI) 시스템이 중요한 의사결정을 중재하는 역할이 커지면서, 최종 사용자가 정보에 입각한 책임감 있는 행동을 취하기 위한 설명 가능성(Explainability)이 중요해지고 있습니다. 기존의 설명 가능한 AI (XAI) 접근 방식은 주로 알고리즘 중심적이었고 기술적 투명성에 초점을 맞추어 왔습니다. 그러나 실제 인간 상호작용에서의 설명은 사회적으로 상황에 따라 달라지며, AI 시스템 역시 사회-조직적 맥락에 깊이 내재되어 있습니다.

이러한 알고리즘 중심의 관점은 AI 시스템과 설명이 놓인 사회-조직적 맥락을 간과하는 "인식론적 사각지대(epistemic blind spot)"를 만들어냅니다. 이로 인해 XAI는 사람들이 AI 시스템을 이해하고 신뢰를 구축하며 올바른 결정을 내리는 데 필요한 완전하고 총체적인 설명(holistic explanation)을 제공하지 못하고 있습니다.

## ✨ Key Contributions

- XAI의 "인식론적 사각지대"인 사회-조직적 맥락 부족 문제를 Critical Technical Practice (CTP) 관점에서 비판적으로 조명합니다.
- AI 시스템의 '사회적 투명성(Social Transparency, ST)' 개념을 탐구하고, 과거 사용자 상호작용에 대한 '무엇(What)', '왜(Why)', '누가(Who)', '언제(When)' 정보를 반영하는 4가지 디자인 요소를 포함하는 시나리오 기반의 추측성 디자인을 개발합니다.
- 형성적 연구를 통해 AI 중재 의사결정을 둘러싼 기술적(AI), 의사결정, 조직적이라는 세 가지 수준의 맥락과 ST가 이러한 맥락을 가시화하여 미치는 잠재적 영향을 실증적으로 도출하고 개념적 프레임워크를 제시합니다.
- AI 시스템에 ST를 도입할 때 발생할 수 있는 잠재적 도전 과제, 위험, 긴장 관계 및 디자인 통찰력을 공유하여 XAI의 디자인 공간을 확장합니다.

## 📎 Related Works

이 연구는 다음의 선행 연구들을 바탕으로 합니다:

- **Explainable AI (XAI):** 알고리즘의 동작이나 결정을 이해하기 쉽게 만드는 것을 목표로 하며, 모델 투명성, 사후 설명(post-hoc explanations), 사용자 중심 XAI 접근 방식에 대한 연구를 포함합니다. 그러나 알고리즘 중심의 한계와 실제 사용 환경에서의 효과 부족, 사회 과학적 통찰력의 미반영이라는 비판을 받았습니다.
- **Sociotechnical approaches to AI:** AI 시스템의 사회적 상황성을 인식하고 기술 중심적 관점의 위험성을 지적하며, Critical Technical Practice (CTP), Reflective Design, Value Sensitive Algorithm Design 등 인간 중심적이고 사회-조직적 맥락에 민감한 AI 개발을 강조하는 연구들입니다.
- **Social transparency and related concepts:** 인간-인간 상호작용에서 타인의 활동을 투명하게 만드는 개념을 다룹니다.
  - **Social Translucence:** 디지털 시스템에서 타인의 존재와 활동에 대한 "사회적 단서"를 가시화하여 효과적인 온라인 협업을 돕는 개념입니다.
  - **Group Awareness:** 분산된 팀을 지원하는 그룹웨어에서 '누가', '무엇을', '어디서', '어떻게', '언제', '왜'에 대한 지식을 포함하는 그룹 인식 프레임워크입니다.
  - **Social Transparency (ST):** 네트워크 정보 교환에서 정체성 투명성, 내용 투명성, 상호작용 투명성을 통해 사람들이 사회적 추론을 하도록 돕는 개념입니다.
  - **Knowledge Sharing and Acquisition:** 지식 공유 및 획득을 위한 저장소 모델과 전문성 공유 모델, 조직의 메타 지식(누가 무엇을 아는지) 획득을 통한 '주변 인식(ambient awareness)'과 '전이 기억 시스템(Transactive Memory Systems, TMS)' 개발을 다룹니다.
  - **Cognitive heuristics:** 다른 사람의 선택, 상호작용 등을 가시화하여 밴드왜건 효과, 권위, 유사성 등의 사회적 휴리스틱이 의사결정에 미치는 영향을 연구합니다.

## 🛠️ Methodology

1. **시나리오 기반 디자인(Scenario-Based Design, SBD) 탐색:**

   - AI 중재 의사결정 시나리오(사이버 보안, 채용, 헬스케어, 영업 등)를 시작으로, 8개 기술 기업의 AI 사용자 및 실무자 21명과 4회의 워크숍을 진행했습니다.
   - 참여자들과 함께 다른 사용자들과 AI 시스템 간의 상호작용에 대해 어떤 정보를 보고 싶은지 시각적 목업을 만들며 브레인스토밍했습니다.
   - 이를 통해 '누가(Who) 무엇을(What) 언제(When) 왜(Why)' 했는지에 대한 4가지 핵심 정보(4W)를 ST의 구성 요소로 도출했습니다. 'Why'는 결정 뒤에 숨겨진 뉘앙스를 포착하기 위해 자유 형식의 댓글 기능으로 설계되었습니다.
   - 영업 시나리오(AI 기반 가격 추천 도구)를 최종 시나리오로 선정하고 4W 요소를 포함한 시각적 목업(Figure 1)을 개발했습니다.

2. **형성적 사용자 연구:**

   - AI 시스템을 사용하거나 개발, 설계하는 이해관계자 29명(판매자 8명, 비판매자 21명)을 6개 회사에서 모집하여 온라인 반구조화 인터뷰를 진행했습니다.
   - **인터뷰 절차:**
     - **1단계:** 참여자들의 AI 시스템 사용 경험 및 설명 가능성 요구 사항, 사회-조직적 맥락에 대해 논의했습니다.
     - **2단계:** 시나리오 기반 디자인을 상세히 탐구했습니다. AI의 가격 추천과 기술적 설명을 본 후, 참여자들에게 초기 가격과 자신감을 측정했습니다. 그 다음 ST 요약 정보와 4W 블록을 순차적으로 공개하며 생각 과정을 듣고, ST 기능 추가에 대한 반응과 최종 가격, 자신감을 측정했습니다. 4W의 중요도 순위와 그 이유를 물었습니다.
     - **3단계:** ST 개념에 대한 토론 후, 참여자 자신의 도메인에서의 ST 적용 가능성 시나리오를 브레인스토밍하고 4W 중요도를 재평가했습니다.
     - **4단계:** ST 도입의 잠재적 부정적 결과 및 AI 시스템의 설명 가능성에 대한 반성적 논의를 진행했습니다.

3. **질적 분석:**
   - 총 29시간 분량의 인터뷰 데이터를 전사하고, 주제 분석(thematic analysis)과 근거 이론(grounded theory)을 결합하여 분석했습니다.
   - 두 명의 저자가 독립적으로 비디오 및 전사본을 오픈 코딩하고, 코드 클러스터링을 통해 주제(theme)를 도출했습니다. 반복적인 논의를 통해 코드와 주제를 다듬고 합의에 이르렀습니다.

## 📊 Results

- **기술적 투명성의 한계:** 참가자들은 AI 알고리즘이 모든 중요한 맥락적 요인을 고려할 수 없으며, "실제 삶은 숫자 이상이며, 특히 관계를 생각할 때 더욱 그렇다"고 지적했습니다. AI의 추천에만 의존하는 것은 복잡한 의사결정을 지원하기에 불충분했습니다.
- **ST 도입의 효과:**
  - ST 정보를 확인한 후, 29명 중 26명이 제안 가격을 낮추었고(평균 $110.7에서 $73.8로), 24명이 결정에 대한 자신감(10점 만점에 평균 6.4점에서 8.3점으로)을 높였습니다. 이는 ST 정보가 참가자들이 더 신중하게 가격을 설정하고 결정에 대해 더 확신을 갖도록 도왔음을 시사합니다.
- **ST가 가시화하는 세 가지 수준의 맥락:**
  1. **기술적(AI) 맥락:**
     - AI의 과거 의사결정 궤적과 사용자 상호작용을 통해 AI의 실제 성능을 추적하고 AI에 대한 신뢰를 조정합니다.
     - 인간적 요소를 AI에 주입하여 사회적 기반의 인식과 휴리스틱을 유도하고 '전이적 신뢰(transitive trust)'를 형성합니다.
  2. **의사결정 맥락:**
     - 과거 의사결정의 지역적 맥락과 '크루 지식(crew knowledge, 비공식적, 상황적 지식)'에 대한 현장 접근을 제공합니다.
     - **실천 가능한 통찰력:** AI의 기능 공간에 포착되지 않는 추가 변수, 유사한 결정 및 결과와의 유추적 추론을 통해 의사결정을 개선하고 자신감을 높이며 후속 조치를 지원합니다.
     - **사회적 검증:** 의사결정 과정에서 개인적 취약성(vulnerability)을 줄이고, AI에 대한 이의 제기(contestability)를 용이하게 합니다.
  3. **조직적 맥락:**
     - 조직적 메타 지식('누가 무엇을 아는지') 및 관행을 가시화합니다.
     - **조직 규범 및 가치 이해:** 의사결정을 개선하고 직무 기대치를 설정하며, 새로운 직원에게 기업 문화를 학습시키는 데 도움을 줍니다.
     - **책임성 및 감사 가능성:** 과거 의사결정을 추적 가능하게 하고, 책임 부여 및 평가를 용이하게 합니다. (단, 감시의 위험도 존재).
     - **전문성 위치 파악:** 동료 간 지원 시스템을 촉진하고, '기관 기억(institutional memory)' 및 '전이 기억 시스템(Transactive Memory System, TMS)' 형성을 지원하여 '집단적 마음(collective mind)'을 구축합니다.
- **4W 디자인 특징:**
  - **무엇(What):** 과거 사용자의 AI 추천 수락/거부 여부 및 판매 성공/실패와 같은 결과 정보. AI 성능 추적 및 기계 이의 제기에 중요하며, 모든 ST 정보의 '스냅샷'으로 간주됩니다. (전반적인 중요도 1위)
  - **왜(Why):** 이전 사용자가 결정을 내린 이유에 대한 자유 형식 댓글. AI 성능 이해, 실천 가능한 통찰력, 조직 규범/가치 이해, 의사결정에 대한 사회적 검증을 제공합니다. (전반적인 중요도 2위)
  - **누가(Who):** 이전 사용자의 이름, 직위, 프로필 사진. 전문성 위치 파악, 전이적 신뢰, 사회적 검증에 기여합니다. (전반적인 중요도 3위)
  - **언제(When):** 의사결정 시점을 나타내는 타임스탬프. 정보의 관련성 판단, 의사결정 맥락 강화에 중요합니다. (전반적인 중요도 4위)
  - 4W의 상대적 중요도 순위는 영업 시나리오와 참가자 개인의 도메인 모두에서 일관되게 유지되었습니다.
- **다른 도메인으로의 전이 가능성:**
  - **사이버 보안:** 동료들의 AI 추천 사용 방식에 대한 인식 부족 문제 해결, 티켓팅 시스템에서 ST를 통한 의사결정 개선 및 사회적 검증 제공, 조직 관행에 대한 통찰력 제공, 글로벌/지역적 맥락 보강에 유용합니다.
  - **헬스케어 (방사선과 및 종양학):** 동료 검토 및 교차 훈련 기회 촉진, '종양 위원회(tumor boards)'와 유사하게 AI 추천에 대한 동료 피드백 및 검토 제공, 다중 이해관계자 문제 해결을 위한 개인화된 치료 계획 수립에 유용합니다.

## 🧠 Insights & Discussion

- **총체적 설명 가능성(Holistic Explainability):** 이 연구는 AI 설명 가능성에 대한 기존의 알고리즘 중심적 사고방식의 "터널 시야(tunnel-visioned)"를 확장하여, 사회-조직적 맥락을 포함한 "총체적" 설명을 가능하게 합니다. ST는 AI를 넘어서는 "주변 시야(peripheral vision)"를 제공하여 왜-질문(why-questions)에 대한 다각적인 답변을 가능하게 하고, 비주요 이해관계자에게도 의사결정을 설명할 수 있도록 돕습니다.
- **인간-AI 앙상블(Human-AI assemblage)의 구체화:** ST는 의사결정 과정에서 인간적 요소를 전면에 내세워, 인간-AI 앙상블을 명시적으로 구체화합니다. 조직적 메타 지식은 전이 기억 시스템(TMS)의 형성을 촉진하고, 이는 장기적으로 AI를 포함하는 '집단적 마음'으로 이어질 수 있습니다. 그러나 정보 공유에 대한 비판적 고려가 필요하며, 이해관계자의 가치에 민감한 사회기술적 접근 방식이 중요합니다.
- **사회적 상황에 기반한 XAI를 향하여:**
  - **실질적 과제:** 방대한 정보를 사용자 워크플로우에 효과적으로 통합하는 문제(정보 과부하, 관련성 필터링), ST 정보의 품질 및 적용 가능성 검증, 모델 업데이트 시 ST 정보의 차이점 플래그 지정 등이 있습니다.
  - **기술 혁신:** AI가 사회적 데이터(예: 인간의 합리화)를 학습하여 성능과 설명을 개선하는 흥미로운 연구 분야를 제시합니다. 이는 AI가 추가 기능과 지역화된 규칙을 학습하고, 인간이 설명하는 방식을 모방하여 보다 인간이 이해하기 쉬운 설명을 생성하는 데 기여할 수 있습니다.
- **제한 사항:** 본 연구는 시나리오 기반 디자인의 형성적 단계로, 통찰력은 평가적이라기보다는 형성적(formative)으로 해석되어야 합니다. 향후 ST의 디자인 공간 확장, 통찰력의 전이 가능성 심층 분석, 사용자 신뢰에 대한 장기적 영향 연구 등이 필요합니다.

## 📌 TL;DR

이 논문은 기존 XAI의 알고리즘 중심적 한계를 지적하며, AI 시스템의 설명 가능성을 사회-조직적 맥락까지 확장하는 '사회적 투명성(Social Transparency, ST)' 개념을 제안합니다. '누가', '무엇을', '언제', '왜'라는 4W 디자인 요소를 포함하는 시나리오 기반 디자인을 통해 29명의 AI 이해관계자 인터뷰를 진행한 결과, ST가 AI 시스템에 대한 신뢰 조정, 의사결정 개선, 조직적 집단 행동 촉진 등 총체적 설명 가능성을 가능하게 함을 발견했습니다. 이는 기술적(AI), 의사결정, 조직적 세 가지 수준에서 맥락적 이해를 제공하며, ST 도입 시 개인 정보 보호, 편향, 정보 과부하, 기여 유인 부족 등의 도전 과제도 함께 논의하여 인간 중심 XAI의 디자인 공간을 넓히는 데 기여합니다.
