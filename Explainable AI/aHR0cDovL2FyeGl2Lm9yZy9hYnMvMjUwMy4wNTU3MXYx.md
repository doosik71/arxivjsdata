# Compliance of AI Systems
Julius Schöning, Niklas Kruse

## 🧩 Problem to Solve
이 논문은 인공지능(AI) 시스템이 다양한 분야에 통합됨에 따라, 다가오는 법규(특히 유럽연합의 AI Act)를 준수하는 강력한 개념이 필요하다는 문제의식을 다룹니다. 분산된 특성과 제한된 컴퓨팅 자원으로 인해 복잡한 규제 준수 메커니즘을 구현하기 어려운 엣지 디바이스에서의 AI 배포와 관련된 고유한 문제점들을 특히 강조합니다. AI 시스템의 신뢰성, 투명성, 설명 가능성을 보장하기 위한 초석으로서 데이터셋 규정 준수의 중요성이 간과되는 문제도 제기합니다.

## ✨ Key Contributions
*   AI 시스템 개발, 배포 및 운영 시 법적 규정 준수를 위한 최초의 **모범 사례(best practices)**를 제안합니다.
*   설명 가능한 AI(XAI)에 대한 과학적 통찰력과 규제 요구 사항을 통합하는 **플랫폼 기반 접근 방식**을 도입하여, 초기 단계부터 신뢰할 수 있고 규정을 준수하는 AI 시스템 개발을 지원합니다.
*   개발자가 개발 과정에서 초기 법률 평가를 수행할 수 있도록 지원하여, 법적 문제 발생 위험과 비용이 많이 드는 재개발 필요성을 최소화합니다.
*   데이터셋 준수의 중요성을 강조하며, 이는 AI 시스템의 신뢰성, 투명성, 설명 가능성을 보장하는 데 필수적임을 지적합니다.
*   AI 시스템의 라이프사이클(데이터 및 아이디어, 데이터 선택, 데이터 정제 및 변환, AI 아키텍처 선택, AI 아키텍처 훈련, 임베디드 하드웨어 배포, 평가) 각 단계에서 필요한 규정 준수 사항을 식별합니다.

## 📎 Related Works
*   **인공지능 응용 분야:** 농업 [1, 2], 폐쇄 루프 제어 시스템 [3, 4], 시각 분석 [5, 6], 자율 주행 [7, 8] 등 다양한 분야에서 AI 솔루션의 광범위한 활용.
*   **AI의 상업적 확산 및 사회적 영향:** AI 시스템이 연구 환경을 넘어 경제적 맥락으로 확장되고 있으며 [9], 신뢰성 부족 [14] 등 사회적 요소가 중요해짐 [12, 13].
*   **설명 가능한 AI (XAI):** AI 시스템의 작동 방식을 설명하여 신뢰성을 높이고 오류를 이해하는 방법 탐구 [16, 22]. XAI는 시스템의 라이프사이클에 따라 Ex-Ante, Ex-Nunc, Ex-Post 방법론으로 분류됨 [16, 21].
*   **규제 프레임워크:** 유럽연합의 AI Act [15]와 같은 새로운 법규의 등장.
*   **데이터셋 준수:** 이미지 데이터에 대한 자동화된 데이터 기록 제어 구현 사례 [24].
*   **법률 평가 도구:** 농업 AI 시스템의 규정 준수를 위한 앱 기반 법률 검증 연구 [25].

## 🛠️ Methodology
논문은 AI 적용의 6단계 파이프라인 [17]을 기반으로 규정 준수를 보장하는 접근 방식을 제시하며, 이를 위한 플랫폼 기반 솔루션을 제안합니다.

1.  **AI 개발 파이프라인 분석:**
    *   **1단계: 데이터 및 애플리케이션 아이디어**
    *   **2단계: 데이터 선택**
    *   **3단계: 데이터 정제 및 변환** (편향 방지 및 일반화 보장)
    *   **4단계: AI 아키텍처 선택**
    *   **5단계: AI 아키텍처 훈련**
    *   **6단계: 임베디드 하드웨어 배포 및 평가** (운영 설계 도메인 명확화, 적대적 공격 방지)

2.  **XAI와 법률 준수 통합:**
    *   **Ex-Ante XAI:** AI 시스템 훈련 전 데이터셋 구성 및 특성 분포를 조사하여 모델 동작 예측. AI Act 10조 II항에 따라 법적 적합성과 데이터셋 구성의 연관성을 파악하는 데 중요.
    *   **Ex-Nunc XAI:** 모델 아키텍처 및 훈련 결과를 분석하여 의사결정 프로세스 추론.
    *   **Ex-Post XAI:** 모델의 의사결정 및 추론 과정을 운영 단계에서 분석. XAI는 신뢰성 확보에 필수적이며, 시스템의 동기를 이해하는 데 기여.

3.  **제안된 플랫폼 기반 접근 방식:**
    *   **초기 법률 평가 지원:** 개발자가 AI 시스템 개념화 단계부터 관련 규정, 규범, 표준을 고려하도록 안내. 이를 위해 법률 전문가의 개입 없이도 초기 법률 평가를 수행할 수 있는 "전문가 시스템(expert system)"을 통합. (LLM 기반 솔루션 대신 전문가 시스템을 사용하는 이유: 법률 상황의 급변화, 독일 전문직 법률 제한, AI Act 6조 범위 문제).
    *   **자동화된 데이터셋 평가:** AI 시스템 데이터베이스의 합법성을 자동으로 확인하는 방법을 포함. 특히 이미지 데이터의 경우 저작권 및 초상권 침해 가능성을 확인.
    *   **이해관계자별 신뢰성 증명:** 규제 기관(법적 준수 및 의도된 동기 증명), 시스템 사용자(이해하기 쉬운 시각적 증거를 통한 시스템 이해도 증명) 등 다양한 이해관계자의 요구에 맞춰 신뢰성 증명 절차 제공.
    *   **두 부분 접근법:**
        *   기술적 요구 사항과 법적 요구 사항 결합: 개발 비용을 줄이면서도 AI 시스템에 의한 법률 위반 가능성을 줄임.
        *   수신자의 기대에 따라 신뢰성을 추가 설명: 신뢰성의 다양한 개념적 구성 요소를 기반으로 설명을 확장.

## 📊 Results
이 논문은 직접적인 실험 결과를 제시하기보다는, AI 시스템의 규정 준수 및 신뢰성 확보를 위한 체계적인 분석과 이에 기반한 "모범 사례" 및 "플랫폼 구조"를 제안하는 데 중점을 둡니다.

*   **주요 도전 과제 식별:** 엣지 디바이스의 제한된 자원, 분산된 특성으로 인한 복잡한 규제 준수 메커니즘 구현의 어려움, 그리고 개발자들이 법률을 이해하는 데 겪는 어려움이 AI 개발 및 배포 과정에서 발생하는 주요 문제로 파악되었습니다.
*   **조기 법률 평가의 중요성 강조:** AI 개발 초기 단계에서 법적 평가가 이루어지지 않으면, 나중에 법적 문제가 발견될 경우 전체 개발 프로세스를 다시 시작해야 하는 막대한 비용과 시간 낭비가 발생할 수 있음을 지적했습니다.
*   **플랫폼의 잠재적 이점 제시:** 제안된 플랫폼 기반 접근 방식은 개발자들이 개념화 단계에서부터 법적 위험을 인지하고 해결할 수 있도록 도와 재개발 위험을 최소화하며, 법률 자문 비용을 크게 증가시키지 않으면서도 규제 요구 사항을 충족할 수 있게 합니다.
*   **정성적 예시 (작업자 할당 AI 시스템):** 특정 AI 시스템(예: 생산 현장에서 작업자에게 작업을 할당하는 AI)이 고위험 시스템으로 분류될 수 있으며, 데이터셋 편향(성별 편향 데이터 등)으로 인해 작업자의 건강이나 안전에 부정적인 영향을 미칠 수 있음을 예시로 들어, 데이터셋 준수의 중요성을 구체적으로 설명합니다.
*   **신뢰성 증명의 다면성:** 규제 당국에 대한 법적 준수 증명과 사용자에게 제공되는 이해하기 쉬운 설명 등, AI 시스템의 신뢰성을 다양한 이해관계자의 관점에서 증명해야 함을 보여줍니다.

## 🧠 Insights & Discussion
*   **조기 법률 평가의 중요성:** AI 시스템 개발 과정에서 법적 고려 사항을 조기에 통합하는 것이 비용과 시간 낭비를 줄이는 핵심입니다. 기존에는 개발 완료 후 법률 검토가 이루어져 문제가 발견될 경우 재개발이 필요했지만, 제안된 플랫폼은 이를 초기 단계로 앞당겨 비효율성을 해소합니다.
*   **데이터셋 준수:** AI 시스템의 근본적인 신뢰성과 합법성은 훈련 데이터셋의 품질과 준수 여부에 크게 좌우됩니다. 데이터셋에 편향이나 법적 문제가 내재되어 있다면, 아무리 정교한 AI 모델이라도 신뢰할 수 없게 되므로, 데이터셋의 자동화된 법적 검증이 필수적입니다.
*   **전문가 시스템 대 LLM:** 법률 자문 플랫폼에서 LLM(대규모 언어 모델) 대신 전문가 시스템을 선호하는 결정은 흥미로운 통찰입니다. 이는 법률 영역의 빠른 변화, 독일 법률 전문가 규정(RDG)에 따른 법률 자문의 제한, 그리고 AI 시스템 자체가 "고위험 AI 시스템"으로 분류될 경우 발생할 수 있는 신뢰성 문제(AI Act 6조)를 회피하기 위함입니다. 이는 실용성과 법적 안정성을 고려한 선택으로 보입니다.
*   **다양한 이해관계자의 요구 충족:** AI 시스템의 신뢰성은 단순히 기술적 또는 법적 준수를 넘어섭니다. 규제 기관, 개발자, 최종 사용자 등 각기 다른 이해관계자들이 AI 시스템에 대해 기대하는 신뢰의 기준이 다르므로, 이들의 요구에 맞춰 설명 가능성(XAI)을 제공하고 신뢰를 증명하는 것이 중요합니다.
*   **제한 사항:** 제안된 플랫폼은 개발자의 초기 법률 평가를 돕지만, 전문적인 법률 전문가의 최종 평가를 완전히 대체할 수는 없습니다. 또한, 이 시스템의 출력은 개별 사례에 대한 조언이 아닌 "일반적으로 적용 가능한" 정보여야 합니다.

## 📌 TL;DR
AI 시스템의 급속한 확산에 따라 법규 준수, 특히 EU AI Act 준수가 시급한 문제로 대두되고 있습니다. 이 논문은 데이터셋 규정 준수와 엣지 디바이스의 제한된 자원으로 인한 문제를 포함한 AI 규정 준수 과제를 다루며, 개발 초기 단계부터 신뢰성과 법적 적합성을 보장하는 **플랫폼 기반 접근 방식**을 제안합니다. 이 플랫폼은 AI 개발 파이프라인에 설명 가능한 AI(XAI) 방법론과 법률 평가를 통합하여, 개발자가 법적 위험을 조기에 인지하고 재개발 비용을 최소화할 수 있도록 돕습니다. 법률 자문에 LLM 대신 **전문가 시스템**을 활용하여 법적 변화에 대한 민첩성과 법적 규제 준수 가능성을 높이는 것이 핵심 특징입니다.