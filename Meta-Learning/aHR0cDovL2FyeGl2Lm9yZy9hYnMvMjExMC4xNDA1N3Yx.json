{
  "title": "Meta-learning with an Adaptive Task Scheduler",
  "authors": "Huaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, Mehrdad Mahdavi, Defu Lian, Chelsea Finn",
  "year": 2021,
  "url": "http://arxiv.org/abs/2110.14057v1",
  "abstract": "To benefit the learning of a new task, meta-learning has been proposed to\ntransfer a well-generalized meta-model learned from various meta-training\ntasks. Existing meta-learning algorithms randomly sample meta-training tasks\nwith a uniform probability, under the assumption that tasks are of equal\nimportance. However, it is likely that tasks are detrimental with noise or\nimbalanced given a limited number of meta-training tasks. To prevent the\nmeta-model from being corrupted by such detrimental tasks or dominated by tasks\nin the majority, in this paper, we propose an adaptive task scheduler (ATS) for\nthe meta-training process. In ATS, for the first time, we design a neural\nscheduler to decide which meta-training tasks to use next by predicting the\nprobability being sampled for each candidate task, and train the scheduler to\noptimize the generalization capacity of the meta-model to unseen tasks. We\nidentify two meta-model-related factors as the input of the neural scheduler,\nwhich characterize the difficulty of a candidate task to the meta-model.\nTheoretically, we show that a scheduler taking the two factors into account\nimproves the meta-training loss and also the optimization landscape. Under the\nsetting of meta-learning with noise and limited budgets, ATS improves the\nperformance on both miniImageNet and a real-world drug discovery benchmark by\nup to 13% and 18%, respectively, compared to state-of-the-art task schedulers."
}