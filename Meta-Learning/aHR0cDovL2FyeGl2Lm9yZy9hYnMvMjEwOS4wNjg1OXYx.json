{
  "title": "One-Class Meta-Learning: Towards Generalizable Few-Shot Open-Set\n  Classification",
  "authors": "Jedrzej Kozerawski, Matthew Turk",
  "year": 2021,
  "url": "http://arxiv.org/abs/2109.06859v1",
  "abstract": "Real-world classification tasks are frequently required to work in an\nopen-set setting. This is especially challenging for few-shot learning problems\ndue to the small sample size for each known category, which prevents existing\nopen-set methods from working effectively; however, most multiclass few-shot\nmethods are limited to closed-set scenarios. In this work, we address the\nproblem of few-shot open-set classification by first proposing methods for\nfew-shot one-class classification and then extending them to few-shot\nmulticlass open-set classification. We introduce two independent few-shot\none-class classification methods: Meta Binary Cross-Entropy (Meta-BCE), which\nlearns a separate feature representation for one-class classification, and\nOne-Class Meta-Learning (OCML), which learns to generate one-class classifiers\ngiven standard multiclass feature representation. Both methods can augment any\nexisting few-shot learning method without requiring retraining to work in a\nfew-shot multiclass open-set setting without degrading its closed-set\nperformance. We demonstrate the benefits and drawbacks of both methods in\ndifferent problem settings and evaluate them on three standard benchmark\ndatasets, miniImageNet, tieredImageNet, and Caltech-UCSD-Birds-200-2011, where\nthey surpass the state-of-the-art methods in the few-shot multiclass open-set\nand few-shot one-class tasks."
}