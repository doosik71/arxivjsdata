{
  "title": "Bootstrapped Meta-Learning",
  "authors": "Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, Satinder Singh",
  "year": 2021,
  "url": "http://arxiv.org/abs/2109.04504v2",
  "abstract": "Meta-learning empowers artificial intelligence to increase its efficiency by\nlearning how to learn. Unlocking this potential involves overcoming a\nchallenging meta-optimisation problem. We propose an algorithm that tackles\nthis problem by letting the meta-learner teach itself. The algorithm first\nbootstraps a target from the meta-learner, then optimises the meta-learner by\nminimising the distance to that target under a chosen (pseudo-)metric. Focusing\non meta-learning with gradients, we establish conditions that guarantee\nperformance improvements and show that the metric can control\nmeta-optimisation. Meanwhile, the bootstrapping mechanism can extend the\neffective meta-learning horizon without requiring backpropagation through all\nupdates. We achieve a new state-of-the art for model-free agents on the Atari\nALE benchmark and demonstrate that it yields both performance and efficiency\ngains in multi-task meta-learning. Finally, we explore how bootstrapping opens\nup new possibilities and find that it can meta-learn efficient exploration in\nan epsilon-greedy Q-learning agent, without backpropagating through the update\nrule."
}