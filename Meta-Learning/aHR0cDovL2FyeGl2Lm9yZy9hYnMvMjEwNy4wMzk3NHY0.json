{
  "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision",
  "authors": "Vitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, Sergey Levine",
  "year": 2021,
  "url": "http://arxiv.org/abs/2107.03974v4",
  "abstract": "Meta-reinforcement learning (RL) methods can meta-train policies that adapt\nto new tasks with orders of magnitude less data than standard RL, but\nmeta-training itself is costly and time-consuming. If we can meta-train on\noffline data, then we can reuse the same static dataset, labeled once with\nrewards for different tasks, to meta-train policies that adapt to a variety of\nnew tasks at meta-test time. Although this capability would make meta-RL a\npractical tool for real-world use, offline meta-RL presents additional\nchallenges beyond online meta-RL or standard offline RL settings. Meta-RL\nlearns an exploration strategy that collects data for adapting, and also\nmeta-trains a policy that quickly adapts to data from a new task. Since this\npolicy was meta-trained on a fixed, offline dataset, it might behave\nunpredictably when adapting to data collected by the learned exploration\nstrategy, which differs systematically from the offline data and thus induces\ndistributional shift. We propose a hybrid offline meta-RL algorithm, which uses\noffline data with rewards to meta-train an adaptive policy, and then collects\nadditional unsupervised online data, without any reward labels to bridge this\ndistribution shift. By not requiring reward labels for online collection, this\ndata can be much cheaper to collect. We compare our method to prior work on\noffline meta-RL on simulated robot locomotion and manipulation tasks and find\nthat using additional unsupervised online data collection leads to a dramatic\nimprovement in the adaptive capabilities of the meta-trained policies, matching\nthe performance of fully online meta-RL on a range of challenging domains that\nrequire generalization to new tasks."
}