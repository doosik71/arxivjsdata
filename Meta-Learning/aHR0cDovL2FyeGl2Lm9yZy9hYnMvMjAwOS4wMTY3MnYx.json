{
  "title": "Yet Meta Learning Can Adapt Fast, It Can Also Break Easily",
  "authors": "Han Xu, Yaxin Li, Xiaorui Liu, Hui Liu, Jiliang Tang",
  "year": 2020,
  "url": "http://arxiv.org/abs/2009.01672v1",
  "abstract": "Meta learning algorithms have been widely applied in many tasks for efficient\nlearning, such as few-shot image classification and fast reinforcement\nlearning. During meta training, the meta learner develops a common learning\nstrategy, or experience, from a variety of learning tasks. Therefore, during\nmeta test, the meta learner can use the learned strategy to quickly adapt to\nnew tasks even with a few training samples. However, there is still a dark side\nabout meta learning in terms of reliability and robustness. In particular, is\nmeta learning vulnerable to adversarial attacks? In other words, would a\nwell-trained meta learner utilize its learned experience to build wrong or\nlikely useless knowledge, if an adversary unnoticeably manipulates the given\ntraining set? Without the understanding of this problem, it is extremely risky\nto apply meta learning in safety-critical applications. Thus, in this paper, we\nperform the initial study about adversarial attacks on meta learning under the\nfew-shot classification problem. In particular, we formally define key elements\nof adversarial attacks unique to meta learning and propose the first attacking\nalgorithm against meta learning under various settings. We evaluate the\neffectiveness of the proposed attacking strategy as well as the robustness of\nseveral representative meta learning algorithms. Experimental results\ndemonstrate that the proposed attacking strategy can easily break the meta\nlearner and meta learning is vulnerable to adversarial attacks. The\nimplementation of the proposed framework will be released upon the acceptance\nof this paper."
}