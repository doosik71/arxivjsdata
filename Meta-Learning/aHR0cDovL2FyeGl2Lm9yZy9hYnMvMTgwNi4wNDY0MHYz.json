{
  "title": "Unsupervised Meta-Learning for Reinforcement Learning",
  "authors": "Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine",
  "year": 2018,
  "url": "http://arxiv.org/abs/1806.04640v3",
  "abstract": "Meta-learning algorithms use past experience to learn to quickly solve new\ntasks. In the context of reinforcement learning, meta-learning algorithms\nacquire reinforcement learning procedures to solve new problems more\nefficiently by utilizing experience from prior tasks. The performance of\nmeta-learning algorithms depends on the tasks available for meta-training: in\nthe same way that supervised learning generalizes best to test points drawn\nfrom the same distribution as the training points, meta-learning methods\ngeneralize best to tasks from the same distribution as the meta-training tasks.\nIn effect, meta-reinforcement learning offloads the design burden from\nalgorithm design to task design. If we can automate the process of task design\nas well, we can devise a meta-learning algorithm that is truly automated. In\nthis work, we take a step in this direction, proposing a family of unsupervised\nmeta-learning algorithms for reinforcement learning. We motivate and describe a\ngeneral recipe for unsupervised meta-reinforcement learning, and present an\ninstantiation of this approach. Our conceptual and theoretical contributions\nconsist of formulating the unsupervised meta-reinforcement learning problem and\ndescribing how task proposals based on mutual information can be used to train\noptimal meta-learners. Our experimental results indicate that unsupervised\nmeta-reinforcement learning effectively acquires accelerated reinforcement\nlearning procedures without the need for manual task design and these\nprocedures exceed the performance of learning from scratch."
}