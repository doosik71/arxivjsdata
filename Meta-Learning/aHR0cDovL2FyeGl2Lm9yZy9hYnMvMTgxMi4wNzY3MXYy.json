{
  "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for\n  Model-Based RL",
  "authors": "Anusha Nagabandi, Chelsea Finn, Sergey Levine",
  "year": 2018,
  "url": "http://arxiv.org/abs/1812.07671v2",
  "abstract": "Humans and animals can learn complex predictive models that allow them to\naccurately and reliably reason about real-world phenomena, and they can adapt\nsuch models extremely quickly in the face of unexpected changes. Deep neural\nnetwork models allow us to represent very complex functions, but lack this\ncapacity for rapid online adaptation. The goal in this paper is to develop a\nmethod for continual online learning from an incoming stream of data, using\ndeep neural network models. We formulate an online learning procedure that uses\nstochastic gradient descent to update model parameters, and an expectation\nmaximization algorithm with a Chinese restaurant process prior to develop and\nmaintain a mixture of models to handle non-stationary task distributions. This\nallows for all models to be adapted as necessary, with new models instantiated\nfor task changes and old models recalled when previously seen tasks are\nencountered again. Furthermore, we observe that meta-learning can be used to\nmeta-train a model such that this direct online adaptation with SGD is\neffective, which is otherwise not the case for large function approximators. In\nthis work, we apply our meta-learning for online learning (MOLe) approach to\nmodel-based reinforcement learning, where adapting the predictive model is\ncritical for control; we demonstrate that MOLe outperforms alternative prior\nmethods, and enables effective continuous adaptation in non-stationary task\ndistributions such as varying terrains, motor failures, and unexpected\ndisturbances."
}