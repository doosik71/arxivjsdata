{
  "title": "Transfer Meta-Learning: Information-Theoretic Bounds and Information\n  Meta-Risk Minimization",
  "authors": "Sharu Theresa Jose, Osvaldo Simeone, Giuseppe Durisi",
  "year": 2020,
  "url": "http://arxiv.org/abs/2011.02872v2",
  "abstract": "Meta-learning automatically infers an inductive bias by observing data from a\nnumber of related tasks. The inductive bias is encoded by hyperparameters that\ndetermine aspects of the model class or training algorithm, such as\ninitialization or learning rate. Meta-learning assumes that the learning tasks\nbelong to a task environment, and that tasks are drawn from the same task\nenvironment both during meta-training and meta-testing. This, however, may not\nhold true in practice. In this paper, we introduce the problem of transfer\nmeta-learning, in which tasks are drawn from a target task environment during\nmeta-testing that may differ from the source task environment observed during\nmeta-training. Novel information-theoretic upper bounds are obtained on the\ntransfer meta-generalization gap, which measures the difference between the\nmeta-training loss, available at the meta-learner, and the average loss on\nmeta-test data from a new, randomly selected, task in the target task\nenvironment. The first bound, on the average transfer meta-generalization gap,\ncaptures the meta-environment shift between source and target task environments\nvia the KL divergence between source and target data distributions. The second,\nPAC-Bayesian bound, and the third, single-draw bound, account for this shift\nvia the log-likelihood ratio between source and target task distributions.\nFurthermore, two transfer meta-learning solutions are introduced. For the\nfirst, termed Empirical Meta-Risk Minimization (EMRM), we derive bounds on the\naverage optimality gap. The second, referred to as Information Meta-Risk\nMinimization (IMRM), is obtained by minimizing the PAC-Bayesian bound. IMRM is\nshown via experiments to potentially outperform EMRM."
}