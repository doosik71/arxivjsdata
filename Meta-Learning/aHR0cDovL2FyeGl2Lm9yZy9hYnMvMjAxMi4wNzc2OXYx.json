{
  "title": "Variable-Shot Adaptation for Online Meta-Learning",
  "authors": "Tianhe Yu, Xinyang Geng, Chelsea Finn, Sergey Levine",
  "year": 2020,
  "url": "http://arxiv.org/abs/2012.07769v1",
  "abstract": "Few-shot meta-learning methods consider the problem of learning new tasks\nfrom a small, fixed number of examples, by meta-learning across static data\nfrom a set of previous tasks. However, in many real world settings, it is more\nnatural to view the problem as one of minimizing the total amount of\nsupervision --- both the number of examples needed to learn a new task and the\namount of data needed for meta-learning. Such a formulation can be studied in a\nsequential learning setting, where tasks are presented in sequence. When\nstudying meta-learning in this online setting, a critical question arises: can\nmeta-learning improve over the sample complexity and regret of standard\nempirical risk minimization methods, when considering both meta-training and\nadaptation together? The answer is particularly non-obvious for meta-learning\nalgorithms with complex bi-level optimizations that may demand large amounts of\nmeta-training data. To answer this question, we extend previous meta-learning\nalgorithms to handle the variable-shot settings that naturally arise in\nsequential learning: from many-shot learning at the start, to zero-shot\nlearning towards the end. On sequential learning problems, we find that\nmeta-learning solves the full task set with fewer overall labels and achieves\ngreater cumulative performance, compared to standard supervised methods. These\nresults suggest that meta-learning is an important ingredient for building\nlearning systems that continuously learn and improve over a sequence of\nproblems."
}