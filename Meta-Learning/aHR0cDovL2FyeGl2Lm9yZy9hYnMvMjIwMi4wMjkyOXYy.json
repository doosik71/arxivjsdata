{
  "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
  "authors": "Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, Junshan Zhang",
  "year": 2022,
  "url": "http://arxiv.org/abs/2202.02929v2",
  "abstract": "Existing offline reinforcement learning (RL) methods face a few major\nchallenges, particularly the distributional shift between the learned policy\nand the behavior policy. Offline Meta-RL is emerging as a promising approach to\naddress these challenges, aiming to learn an informative meta-policy from a\ncollection of tasks. Nevertheless, as shown in our empirical studies, offline\nMeta-RL could be outperformed by offline single-task RL methods on tasks with\ngood quality of datasets, indicating that a right balance has to be delicately\ncalibrated between \"exploring\" the out-of-distribution state-actions by\nfollowing the meta-policy and \"exploiting\" the offline dataset by staying close\nto the behavior policy. Motivated by such empirical analysis, we explore\nmodel-based offline Meta-RL with regularized Policy Optimization (MerPO), which\nlearns a meta-model for efficient task structure inference and an informative\nmeta-policy for safe exploration of out-of-distribution state-actions. In\nparticular, we devise a new meta-Regularized model-based Actor-Critic (RAC)\nmethod for within-task policy optimization, as a key building block of MerPO,\nusing conservative policy evaluation and regularized policy improvement; and\nthe intrinsic tradeoff therein is achieved via striking the right balance\nbetween two regularizers, one based on the behavior policy and the other on the\nmeta-policy. We theoretically show that the learnt policy offers guaranteed\nimprovement over both the behavior policy and the meta-policy, thus ensuring\nthe performance improvement on new tasks via offline Meta-RL. Experiments\ncorroborate the superior performance of MerPO over existing offline Meta-RL\nmethods."
}