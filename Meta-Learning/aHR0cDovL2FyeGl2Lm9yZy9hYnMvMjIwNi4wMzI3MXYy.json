{
  "title": "On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning",
  "authors": "Zhao Mandi, Pieter Abbeel, Stephen James",
  "year": 2022,
  "url": "http://arxiv.org/abs/2206.03271v2",
  "abstract": "Intelligent agents should have the ability to leverage knowledge from\npreviously learned tasks in order to learn new ones quickly and efficiently.\nMeta-learning approaches have emerged as a popular solution to achieve this.\nHowever, meta-reinforcement learning (meta-RL) algorithms have thus far been\nrestricted to simple environments with narrow task distributions. Moreover, the\nparadigm of pretraining followed by fine-tuning to adapt to new tasks has\nemerged as a simple yet effective solution in supervised and self-supervised\nlearning. This calls into question the benefits of meta-learning approaches\nalso in reinforcement learning, which typically come at the cost of high\ncomplexity. We hence investigate meta-RL approaches in a variety of\nvision-based benchmarks, including Procgen, RLBench, and Atari, where\nevaluations are made on completely novel tasks. Our findings show that when\nmeta-learning approaches are evaluated on different tasks (rather than\ndifferent variations of the same task), multi-task pretraining with fine-tuning\non new tasks performs equally as well, or better, than meta-pretraining with\nmeta test-time adaptation. This is encouraging for future research, as\nmulti-task pretraining tends to be simpler and computationally cheaper than\nmeta-RL. From these findings, we advocate for evaluating future meta-RL methods\non more challenging tasks and including multi-task pretraining with fine-tuning\nas a simple, yet strong baseline."
}