{
  "title": "Incremental Meta-Learning via Indirect Discriminant Alignment",
  "authors": "Qing Liu, Orchid Majumder, Alessandro Achille, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto",
  "year": 2020,
  "url": "http://arxiv.org/abs/2002.04162v2",
  "abstract": "Majority of the modern meta-learning methods for few-shot classification\ntasks operate in two phases: a meta-training phase where the meta-learner\nlearns a generic representation by solving multiple few-shot tasks sampled from\na large dataset and a testing phase, where the meta-learner leverages its\nlearnt internal representation for a specific few-shot task involving classes\nwhich were not seen during the meta-training phase. To the best of our\nknowledge, all such meta-learning methods use a single base dataset for\nmeta-training to sample tasks from and do not adapt the algorithm after\nmeta-training. This strategy may not scale to real-world use-cases where the\nmeta-learner does not potentially have access to the full meta-training dataset\nfrom the very beginning and we need to update the meta-learner in an\nincremental fashion when additional training data becomes available. Through\nour experimental setup, we develop a notion of incremental learning during the\nmeta-training phase of meta-learning and propose a method which can be used\nwith multiple existing metric-based meta-learning algorithms. Experimental\nresults on benchmark dataset show that our approach performs favorably at test\ntime as compared to training a model with the full meta-training set and incurs\nnegligible amount of catastrophic forgetting"
}