{
  "url": "http://arxiv.org/abs/1711.10761v1",
  "title": "Transfer Learning with Binary Neural Networks",
  "authors": "Sam Leroux, Steven Bohez, Tim Verbelen, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt",
  "year": 2017,
  "abstract": "Previous work has shown that it is possible to train deep neural networks\nwith low precision weights and activations. In the extreme case it is even\npossible to constrain the network to binary values. The costly floating point\nmultiplications are then reduced to fast logical operations. High end smart\nphones such as Google's Pixel 2 and Apple's iPhone X are already equipped with\nspecialised hardware for image processing and it is very likely that other\nfuture consumer hardware will also have dedicated accelerators for deep neural\nnetworks. Binary neural networks are attractive in this case because the\nlogical operations are very fast and efficient when implemented in hardware. We\npropose a transfer learning based architecture where we first train a binary\nnetwork on Imagenet and then retrain part of the network for different tasks\nwhile keeping most of the network fixed. The fixed binary part could be\nimplemented in a hardware accelerator while the last layers of the network are\nevaluated in software. We show that a single binary neural network trained on\nthe Imagenet dataset can indeed be used as a feature extractor for other\ndatasets."
}