# ADAPTIVE LEARNING WITH BINARY NEURONS

Juan-Manuel Torres-Moreno, Mirta B. Gordon

## 🧩 Problem to Solve

이 논문은 패턴 분류(pattern classification) 작업을 위한 효율적인 신경망 학습 알고리즘을 제안합니다. 기존 다층 퍼셉트론(multilayer perceptron)은 비선형 문제 해결에 필요하지만, 최적의 네트워크 구조(유닛 수, 층 수)를 사전에 알 수 없어 많은 시행착오가 필요하며, 종종 과적합(overfitting) 문제로 이어진다는 한계가 있습니다. 특히 백프로파게이션(Backpropagation)과 같은 알고리즘은 분류 문제를 회귀(regression) 문제로 변환하여 출력 유닛의 제곱 오차(squared training error)를 최소화하는데, 이는 클래스 경계(class boundaries)보다는 전체 입력 공간에 걸쳐 함수를 근사하려 하여 비효율적일 수 있습니다. 따라서, 자동적으로 최적의 네트워크 구조를 찾으면서도 작고 일반화 성능이 뛰어난 신경망을 구축하는 것이 주요 문제입니다.

## ✨ Key Contributions

* **NetLines (Neural Encoder Through Linear Separations) 알고리즘 제안:** 이진 유닛으로 구성된 단일 은닉층(single hidden layer)과 이진 출력 유닛을 가진 피드포워드 신경망을 생성하는 새로운 증분 학습(incremental learning) 알고리즘을 소개합니다.
* **작고 효율적인 네트워크 생성:** NetLines는 알려진 벤치마크 문제에서 기존 최고 성능 알고리즘보다 더 작고 우수한 일반화 성능을 가진 네트워크를 생성함을 보여줍니다.
* **수렴 정리(Convergence Theorem) 증명:** 유한한 수의 은닉 유닛으로 이진 및 실수 값 입력 패턴 모두에 대한 해가 존재함을 보장합니다.
* **과적합에 대한 새로운 관점:** NetLines에서 오류 없이 학습(error-free training)을 통해 생성된 네트워크가 일반적으로 더 큰 경향이 있음에도 불구하고, 과적합이 일반화 성능을 저하시키지 않으며, 조기 종료(early stopping)가 일반화 성능을 향상시키지 않음을 실험적으로 입증합니다. 이는 분류 문제에서 편향(bias)과 분산(variance)의 비선형적 조합에 대한 최근 이론과 일치합니다.
* **다중 클래스 문제 확장:** 임의의 이진 분류기와 결합 가능한 계층적 네트워크 트리(Tree of Networks, TON) 구조를 제안하여 다중 클래스 문제 해결 방법을 제시합니다.

## 📎 Related Works

* **증분 학습/성장 알고리즘 (Incremental/Growth Algorithms):**
  * **Tiling 알고리즘 [29]:** 은닉층을 순차적으로 추가하여 학습 오류를 줄이지만, 매우 큰 네트워크를 생성할 수 있습니다.
  * **Upstart 알고리즘 [11]:** 입력층과 기존 은닉 유닛 사이에 '자식(daughter)' 은닉 유닛 쌍을 추가하는 트리형 구조입니다.
  * **Parity Machine 학습 (Tiling-like):** Offset [28] 및 MonoPlane [38]은 각 추가 유닛이 이전 유닛의 오류를 수정하도록 훈련됩니다.
  * **Sequential Learning [27]:** 각 유닛이 '순수한 반공간(pure half-space)' 제약 조건을 유지하며 학습하도록 합니다.
  * **Cascade-Correlation [9]:** 여러 실수 값 뉴런 풀 중에서 유닛을 선택하여 목표와 학습 오류 간의 상관 관계를 학습시킵니다.
  * **결정 트리 (Decision Trees) [5]:** CART, Neural-Trees [36], Modified Neural Tree Network [10]와 같이 입력 공간을 이분법적으로 분할하는 트리형 신경망 구조를 생성합니다.
* **퍼셉트론 학습 알고리즘 (Perceptron Learning Algorithms):**
  * **Pocket 알고리즘 [15]:** 자연스러운 정지 조건이 없어 사용자의 인내심에 의존합니다.
  * **Minimerror [20, 33]:** 본 논문에서 사용된 알고리즘으로, 패턴의 안정도(stability)에 기반한 비용 함수를 최소화합니다.
  * **다른 알고리즘:** MNTN [10] 및 ITRULE [18] 등도 유사한 비용 함수를 사용하지만, Minimerror처럼 온도를 통한 학습 기여 패턴 제어 기능은 없습니다.

## 🛠️ Methodology

NetLines 알고리즘은 성장 휴리스틱(growth heuristic)과 각 유닛을 훈련시키는 특정 학습 알고리즘(Minimerror)을 결합합니다.

1. **NetLines 성장 휴리스틱:**
    * 초기 $h=0$ (은닉 유닛 수). 모든 패턴에 대해 원본 목표($\tau_{\mu}^{h+1} = \tau_{\mu}$)를 설정합니다.
    * **반복 (Repeat) 과정:**
        1. **은닉 유닛 훈련:**
            * $h$를 1 증가시키고 새로운 은닉 유닛을 입력에 연결합니다.
            * 훈련 세트 $\left\{\vec{\xi}_{\mu}, \tau_{\mu}^{h}\right\}$로 $h$번째 은닉 유닛을 훈련시킵니다.
            * 학습 후, 각 패턴 $\mu$에 대한 은닉 유닛의 상태 $\sigma_{\mu}^{h} = \text{sign}(\vec{w}_h \cdot \vec{\xi}_{\mu})$를 결정합니다.
            * 만약 $h=1$이고 모든 $\sigma_{\mu}^{1} = \tau_{\mu}^{1}$ (훈련 세트가 선형 분리 가능)이면 학습을 중단합니다.
            * 그렇지 않으면, 다음 은닉 유닛의 목표를 $\tau_{\mu}^{h+1} = \sigma_{\mu}^{h} \tau_{\mu}$로 설정합니다. (이전 유닛이 패턴을 올바르게 분류했다면 +1, 그렇지 않다면 -1)
        2. **출력 유닛 훈련:**
            * 출력 유닛을 $h$개의 훈련된 은닉 유닛에 연결합니다.
            * 은닉 유닛의 출력(내부 표현, IR) $\left\{\vec{\sigma}_{\mu}^{(h)}, \tau_{\mu}\right\}$를 사용하여 출력 유닛을 원본 목표 $\tau_{\mu}$에 대해 훈련합니다.
            * 학습 후, 네트워크의 출력 $\zeta_{\mu}^{(h)} = \text{sign}(\vec{W}^{(h)} \cdot \vec{\sigma}_{\mu}^{(h)})$를 얻습니다.
            * 다음 은닉 유닛의 목표를 $\tau_{\mu}^{h+1} = \zeta_{\mu} \tau_{\mu}$로 설정합니다. (출력 유닛이 패턴을 올바르게 분류했다면 +1, 그렇지 않다면 -1)
            * 훈련 오류 $e$의 수를 계산합니다.
    * **종료 조건 (Until):** $h$가 최대 은닉 유닛 수 ($H_{\text{max}}$)에 도달하거나 훈련 오류 $e$가 허용 가능한 최대치 ($E_{\text{max}}$)보다 작거나 같아질 때까지 반복합니다.
2. **Minimerror 퍼셉트론 학습 알고리즘:**
    * 패턴의 안정도(stability) $\gamma_{\mu} = \frac{\tau_{\mu} \vec{w} \cdot \vec{\xi}_{\mu}}{||\vec{w}||}$를 통해 정의되는 비용 함수 $E = \frac{1}{2} \sum_{\mu=1}^{P} [1 - \tanh(\frac{\gamma_{\mu}}{2T})]$를 최소화합니다. 여기서 $\gamma_{\mu}$는 패턴이 분리 초평면(hyperplane)으로부터 얼마나 떨어져 있는지와 올바르게 분류되었는지 여부를 나타냅니다.
    * 확률적 경사 하강법(gradient descent)과 결정론적 어닐링(deterministic annealing)을 결합하여 가중치를 업데이트합니다.
    * 잘못 분류된 패턴($\gamma_{\mu} \le 0$)에 대해서는 높은 온도 $T^{-}$를, 올바르게 분류된 패턴($\gamma_{\mu} > 0$)에 대해서는 낮은 온도 $T^{+}$를 적용하며, 비율 $\theta=T^{-}/T^{+}$를 사용합니다. 이를 통해 학습에 기여하는 패턴의 위치를 정밀하게 제어합니다.
    * 가중치 $\vec{w}$는 각 업데이트 후 $\sqrt{N+1}$로 정규화됩니다.
3. **데이터 표준화 (Data Standardization):**
    * 모든 입력 패턴은 각 컴포넌트별로 평균 0, 분산 1이 되도록 선형 변환 ($\tilde{\xi}_{\mu i} = \frac{\xi_{\mu i} - \langle \xi_i \rangle}{\Delta_i}$)을 통해 표준화됩니다.
    * 이를 통해 Minimerror의 파라미터($\epsilon=0.02, \delta T^{-1}=10^{-3}, \theta=6$)를 문제에 따라 조정할 필요 없이 고정된 값으로 사용할 수 있습니다.
4. **다중 클래스 문제 확장 (Tree of Networks, TON):**
    * 클래스 순서 $(c_1, c_2, \dots, c_C)$를 정하고, 첫 번째 네트워크는 $c_1$ 클래스를 나머지 클래스와 분리하도록 훈련됩니다.
    * $c_1$ 패턴을 제거한 후, 두 번째 네트워크는 $c_2$를 나머지 클래스와 분리하는 식으로 $C-1$개의 네트워크를 계층적으로 구성합니다.
    * 패턴의 예측된 클래스는 순서대로 검사했을 때 첫 번째로 +1을 출력하는 네트워크에 해당하는 클래스입니다.
    * 서로 다른 클래스 순서로 훈련된 여러 TON의 투표(vote)를 통해 일반화 성능을 향상시킵니다.

## 📊 Results

NetLines는 다양한 벤치마크 문제에서 뛰어난 성능을 보였습니다.

* **Parity Function (이진 입력):** $N=2$부터 $N=11$까지 모든 경우에 대해 최적의 네트워크 구조($H=N$ 은닉 뉴런)를 찾았으며, 이는 NetLines의 구조 학습 능력을 입증합니다.
* **Sonar Signals (실수 값 입력):** 은닉 유닛 없이도 (즉, 단일 퍼셉트론으로) 훈련 및 테스트 세트 모두 선형 분리 가능함을 보여주며 문제를 해결했습니다.
* **Monk's Problem (이진 입력):**
  * Monk1, Monk2 문제에서는 $\epsilon_g = 0$으로 완벽한 일반화 성능을 달성했습니다.
  * 노이즈가 있는 Monk3 문제에서는 $\epsilon_g = 0.0277$ (12개 패턴 오분류)로, 백프로파게이션, Cascade-Correlation 등 최상위 신경망 방법들과 동등하거나 우수한 성능을 보였고, MonoPlane보다 뛰어났습니다. 네트워크 크기(58개 가중치)는 중간 수준이었습니다.
* **Two or More Lumps (이진 입력):**
  * MonoPlane 및 Upstart와 유사한 일반화 성능을 보였습니다.
  * **조기 종료의 영향:** $H=2$로 은닉 유닛 수를 제한하는 조기 종료를 적용했을 때, 훈련 오류는 감소했지만 일반화 오류($\epsilon_g$)는 개선되지 않았습니다. 이는 NetLines의 경우 오류 없는 학습으로 인한 과적합이 일반화 성능 저하로 이어지지 않음을 시사합니다. NetLines는 MonoPlane보다 적은 가중치를 사용했습니다.
* **Wisconsin Breast Cancer Database (실수 값 입력):**
  * Rprop, Cascade-Correlation 등 다른 알고리즘에 비해 더 낮은 $\epsilon_g$와 더 적은 파라미터를 사용하여 최상위 성능을 달성했습니다.
* **Diabetes Diagnosis (실수 값 입력):**
  * 훨씬 적은 파라미터로 이 벤치마크에서 발표된 최고 성능을 기록했습니다.
* **다중 클래스 문제 (Breiman's Waveform, Fisher's Iris):**
  * **TON 투표 전략의 유효성:** 단일 네트워크의 단순한 승자 독식(WTA) 방식보다 제안된 TON(Tree of Networks)의 투표 전략이 일반화 성능을 크게 향상시켰습니다 (특히 Iris 데이터베이스에서 뚜렷함).
  * Breiman's Waveform의 경우, 노이즈로 인해 분류 오류의 이론적 하한이 $\approx14\%$임에도 불구하고 NetLines는 훈련 오류 없이 학습했으며, 조기 종료는 일반화 성능을 개선하지 못했습니다.

## 🧠 Insights & Discussion

* **자동 적응 및 효율성:** NetLines는 특정 작업의 복잡성에 자동으로 적응하는 신경망을 생성하며, 개별 퍼셉트론 훈련으로 문제를 단순화하여 학습이 빠릅니다. 최적의 아키텍처를 찾기 위한 사전 시행착오가 필요 없다는 장점이 있습니다.
* **수렴 보장:** 이진 및 실수 값 입력 모두에 대해 유한한 수의 은닉 유닛으로 해가 존재함이 이론적으로 보장됩니다.
* **정보 압축:** 이진 은닉 유닛은 입력 패턴의 충실한 인코딩(faithful encoding)을 구성하며, 이는 입력 공간을 $H$차원의 이산 은닉 공간으로 매핑하는 정보 압축으로 볼 수 있습니다. 각 은닉 뉴런은 입력 공간에서 클래스 간의 선형 경계를 정의합니다.
* **과적합에 대한 통찰:** NetLines는 훈련 오류가 0이 될 때까지 은닉 뉴런을 추가함에도 불구하고, 생성된 네트워크가 일반적으로 작습니다. 더 중요한 것은, NetLines에서는 훈련 오류가 0에 도달할 때까지 학습하더라도 과적합이 일반화 성능을 저하시키지 않으며, 조기 종료가 일반화 성능을 개선하지 않는다는 점입니다. 이는 회귀 문제와 달리 분류 문제에서 편향과 분산의 상호작용이 비선형적이라는 최근 이론과 부합합니다.
* **안정적인 분류기:** NetLines는 작은 분산(variance)을 가진 안정적인 분류기이므로, 부스팅(boosting)이나 배깅(bagging)과 같은 비용이 많이 드는 절차가 성능을 크게 향상시키지 못할 것으로 예상됩니다.
* **제한 사항:** 새로운 패턴이 추가되거나 클래스 사전 확률(class priors)이 변경될 때 네트워크를 재훈련하는 것이 간단하지 않다는 점과, 후방 확률(posterior probabilities)을 추정하려면 순수 네트워크 출력보다 더 많은 정보가 필요하다는 점이 한계로 언급됩니다.

## 📌 TL;DR

**문제:** 기존 신경망 학습 알고리즘은 최적 구조 탐색에 시행착오가 많고, 과적합 위험이 있어 작고 일반화 성능이 뛰어난 신경망을 효율적으로 구축하기 어려웠습니다.
**방법:** NetLines는 증분 학습 방식의 피드포워드 신경망 알고리즘으로, `Minimerror`라는 효율적인 퍼셉트론 학습 알고리즘을 사용하여 이진 은닉 유닛과 출력 유닛을 순차적으로 추가하며 훈련합니다. 다중 클래스 문제는 계층적인 네트워크 트리(TON)와 투표를 통해 해결합니다.
**주요 발견:** NetLines는 벤치마크에서 기존 알고리즘 대비 더 작고 우수한 일반화 성능을 가진 네트워크를 생성합니다. 특히, 오류 없이 학습하거나 네트워크 크기를 줄이기 위한 조기 종료를 적용하더라도 일반화 성능이 저하되거나 개선되지 않는다는 점은 분류 문제에서의 과적합에 대한 일반적인 통념에 도전하는 중요한 통찰입니다.
