# Reinforcement Learning from Human Feedback

Nathan Lambert

## 해결하려는 문제

이 논문은 대규모 언어 모델(**LLM**)이 단순히 다음 토큰을 예측하는 것을 넘어, **인간의 복잡하고 명시하기 어려운 선호도와 가치를 반영**하도록 정렬하는 문제를 다룹니다. 기존의 언어 모델은 때때로 사용자에게 유용하거나 바람직하지 않은 응답(**예: 장황함, 반복성, 과도한 거부감**)을 생성할 수 있습니다. 강화 학습(**RL**)은 복잡한 환경에서 보상을 최대화하는 에이전트를 훈련하는 데 효과적이지만, 인간의 주관적인 선호도를 명확한 보상 함수로 정의하기는 어렵습니다. 따라서 논문은 **LLM**을 사용자에게 더 유용하고 안전하며 매력적으로 만드는 데 필요한, 측정하기 어려운 **스타일적 및 행동적 특성**을 모델에 통합하는 것을 주된 연구 문제로 제시합니다.

## 주요 기여

이 논문은 **RLHF**(**Reinforcement Learning from Human Feedback**)와 언어 모델 **후처리**(**post-training**)에 대한 포괄적인 입문서로, 다음을 주요 기여로 합니다.

- **RLHF의 체계적인 소개:** **RLHF**의 개념, 주요 구성 요소, 그리고 현대 **LLM** 개발에서 그 중요성을 명확히 설명합니다.
- **핵심 단계 및 구현 설명:** **RLHF**의 세 가지 핵심 단계(**명령어 튜닝**, **보상 모델 훈련**, **강화 학습 최적화**)에 대한 자세한 설명과 각 단계에서의 주요 결정 및 기본적인 구현 예시를 제공합니다.
- **다양한 알고리즘 분석:** **보상 모델**(**Reward Model**) 훈련 방식(**Bradley-Terry 모델, K-wise 손실 함수 등**), **정책 경사 알고리즘**(**PPO**, **REINFORCE**, **GRPO**), 그리고 **보상 모델** 없이 직접 정책을 최적화하는 **직접 정렬 알고리즘**(**Direct Alignment Algorithms**, **DPO**)을 포함한 다양한 **RLHF** 관련 기술들을 다룹니다.
- **고급 주제 및 공개 질문 제시:** **AI 피드백**(**RLAIF**)을 활용한 **헌법적 AI**(**Constitutional AI**), **추론 능력 훈련**(**RLVR**), **합성 데이터**(**synthetic data**)의 활용, **모델 평가**의 문제점(**오염, 과최적화**), 그리고 **RLHF**가 모델의 **스타일, 정보 전달 방식, 제품 경험**에 미치는 영향 등 현재 및 미래의 연구 과제들을 탐구합니다.
- **과최적화(**over-optimization**) 문제 진단:** **RLHF**의 본질적인 한계인 보상 모델의 **대리 목표**(**proxy objective**)로 인한 과최적화 문제를 질적, 양적으로 분석하고, 이로 인해 모델이 발생할 수 있는 바람직하지 않은 행동(**과도한 거부, sycophancy 등**)을 설명합니다.

## 방법론

이 논문은 **LLM** 정렬을 위한 **RLHF**의 일반적인 파이프라인을 설명하며, 이는 여러 단계의 **후처리**(**post-training**) 과정으로 구성됩니다.

1. **명령어 파인튜닝**(**Instruction Finetuning**, **IFT** 또는 **SFT**):

   - **목표:** 기본 언어 모델이 사용자 질문을 이해하고 일관된 형식으로 응답하도록 훈련합니다.
   - **방법:** 고품질의 인간이 작성했거나 합성된 명령-응답 쌍 데이터셋을 사용하여 지도 학습(**supervised learning**) 방식으로 모델을 파인튜닝합니다. 일반적으로 **손실 함수**(**loss function**)는 응답 토큰에 대해서만 적용되며, 프롬프트 토큰은 마스킹됩니다. 이 단계에서 훈련된 모델은 나중에 **참조 모델**($\pi_{ref}$)로 사용될 수 있습니다.

2. **선호도 데이터 수집**(**Preference Data Collection**):

   - **목표:** 모델이 생성한 다양한 응답 중에서 인간(**또는 다른 AI**)이 선호하는 응답을 식별하는 데이터를 수집합니다.
   - **방법:** 특정 프롬프트에 대해 모델이 생성한 여러 응답을 작업자에게 제시하고, 선호도에 따라 순위를 매기거나(**ranking**) 점수를 부여하도록(**rating**) 합니다. 이러한 비교는 주로 쌍대 비교(**pairwise comparison**) (**예: $y_{chosen} \succ y_{rejected}$**) 형식으로 수집됩니다.

3. **보상 모델 훈련**(**Reward Model Training**, **RM**):

   - **목표:** 수집된 인간의 선호도를 모방하여, 주어진 프롬프트에 대한 응답의 "좋음"을 수치화하는 모델을 훈련합니다.
   - **방법:** 선호도 데이터(**chosen/rejected 쌍**)를 사용하여 별도의 신경망(**일반적으로 언어 모델의 백본 위에 선형 레이어를 추가한 형태**)을 훈련합니다. **Bradley-Terry 모델**에 기반한 **대조 손실**(**contrastive loss**)을 사용하여 선호되는 응답의 보상 점수가 선호되지 않는 응답보다 높도록 최적화합니다.
   - **수식:** $\mathcal{L}(\theta) = -\log(\sigma(r_\theta(x, y_w) - r_\theta(x, y_l)))$, 여기서 $r_\theta$는 보상 모델, $y_w$는 선호되는 응답, $y_l$은 선호되지 않는 응답, $\sigma$는 시그모이드 함수입니다.
   - **변형:** 정답 여부를 예측하는 **결과 보상 모델**(**Outcome Reward Models**, **ORM**) 또는 추론 과정의 단계별 점수를 부여하는 **과정 보상 모델**(**Process Reward Models**, **PRM**) 등도 활용될 수 있습니다.

4. **정책 최적화**(**Policy Optimization**):
   - **목표:** 명령어 튜닝된 정책 모델을 **보상 모델**이 부여하는 보상을 최대화하도록 미세 조정합니다.
   - **방법:**
     - **생성 및 평가:** 현재 정책 모델이 프롬프트에 대해 여러 응답을 생성하고, 이를 **보상 모델**로 평가하여 보상 점수를 얻습니다.
     - **강화 학습 알고리즘 적용:**
       - **정책 경사 알고리즘**(**Policy Gradient Algorithms**): **PPO**(**Proximal Policy Optimization**), **REINFORCE**, **GRPO**(**Group Relative Policy Optimization**)와 같은 알고리즘을 사용하여 보상 신호를 기반으로 정책 모델의 파라미터를 업데이트합니다. 이 과정에서 **KL** 발산(**Kullback-Leibler divergence**) 패널티($\beta D_{KL}(\pi_{RL}(\cdot|s_t) || \pi_{ref}(\cdot|s_t))$)를 추가하여 모델이 **참조 모델**로부터 너무 멀리 벗어나지 않도록 정규화합니다.
       - **직접 정렬 알고리즘**(**Direct Alignment Algorithms**, **DAAs**): **DPO**(**Direct Preference Optimization**)는 **보상 모델**을 명시적으로 훈련하지 않고, 쌍대 선호도 데이터를 직접 사용하여 정책 모델을 최적화합니다. **DPO**는 정책 모델과 참조 모델의 로그 확률 비율을 기반으로 내재된 보상 함수를 유도하고 이를 통해 손실을 계산합니다.

## 결과

논문은 **RLHF**가 **LLM** 개발에 미친 혁신적인 영향과 실제 적용 사례를 강조합니다.

- **사용자 경험 및 스타일 개선:** **RLHF**는 **LLM**이 보다 자연스럽고 유용하며 즐거운 방식으로 응답하도록 유도하여 사용자 경험을 크게 향상시켰습니다. 이는 모델의 "수다스러움"(**chattiness**), 응답 길이, 형식(예: 마크다운, 이모지 사용) 및 전반적인 대화 스타일을 최적화함으로써 달성됩니다.
- **핵심 역량 강화:** 단순한 스타일 정렬을 넘어, **RLHF**(**및 **RLVR**과 같은 변형**)는 수학적 추론, 코딩, 복잡한 문제 해결과 같은 검증 가능한 도메인에서 **LLM**의 성능을 크게 향상시켰습니다. DeepSeek R1 및 OpenAI의 o1과 같은 모델은 **RL** 기반 훈련이 새로운 추론 능력을 부여할 수 있음을 보여주었습니다.
- **일반화 능력 향상:** 명령어 파인튜닝만 사용했을 때보다 **RLHF**가 모델의 여러 도메인에 대한 일반화 능력을 더 잘 촉진한다고 보고됩니다.
- **개발 접근성 향상:** **DPO**와 같은 **직접 정렬 알고리즘**(**DAAs**)은 별도의 **보상 모델** 훈련과 복잡한 **RL** 최적화 과정 없이도 **LLM**을 정렬할 수 있게 함으로써 **LLM** 후처리에 대한 접근 장벽을 낮추었습니다. 이는 연구 및 개발의 반복 속도를 높이는 데 기여했습니다.
- **과최적화 및 편향 문제:** **RLHF**는 강력한 성능을 보이지만, **보상 모델**의 **대리 목표**를 과도하게 최적화할 경우 모델이 실제 사용자 목표에서 벗어나거나(**misalignment**) 바람직하지 않은 행동(**예: 과도한 거부, sycophancy, 장황함, 반복성**)을 보일 수 있음을 지적합니다. 데이터 수집의 편향(**예: 길이 편향, 접두사 편향**)도 최종 모델에 영향을 미칠 수 있습니다.
- **지속적인 발전:** **RLHF**는 여전히 발전 중인 분야로, 합성 데이터의 광범위한 사용, **AI 피드백**(**RLAIF**)의 등장, 그리고 모델의 "성격"(**character**) 훈련과 같은 새로운 응용 분야로 확장되고 있습니다. 이는 **RLHF**가 AI 모델을 제품 및 사용자 경험에 더 밀접하게 통합하는 핵심 도구로 자리매김하고 있음을 시사합니다.
