# Reinforcement learning
Sarod Yatawatta

## 해결해야 할 문제
이 논문은 천문학 분야에서 천체 관측 및 과학적 지식 발전을 위한 **지루하고 반복적인 계획**, **스케줄링**, **데이터 수집** 및 **데이터 후처리** 작업을 다룹니다. 이러한 작업의 많은 부분이 **전문 천문학자의 지침과 실행**에 의존하고 있어 비효율적일 수 있습니다. 따라서 이 연구는 **인공지능**(**AI**) **에이전트**가 이러한 작업을 수행하도록 가르칠 수 있는 **강화 학습**(**RL**) 메커니즘을 탐구하여 천문학 운영의 효율성을 높이는 것을 목표로 합니다.

## 주요 기여
*   **현대 심층 강화 학습**(**Deep RL**)의 **최신 개요**를 제공하고, 특히 **천문학 분야에서의 활용 가능성**에 중점을 둡니다.
*   **RL 이론의 기본 개념**(**상태**, **행동**, **보상**, **마르코프 결정 과정**(**MDP**), **Q 함수**, **가치 함수**, **정책** 등)을 **간결하게 설명**하여 새로운 사용자가 RL 기술을 신속하게 적용할 수 있도록 돕습니다.
*   **모델-프리**(**Model-Free**) **RL 알고리즘**(**`DDPG`**, **`TD3`**, **`SAC`** 등)과 **모델-기반**(**Model-Based**) **RL 알고리즘**(**`PETS`** 등)을 상세히 소개합니다.
*   **기존 지식**을 **RL 에이전트 훈련에 통합**하는 새로운 메커니즘인 **힌트 보조 강화 학습**(**Hint Assisted RL**)을 제안하고 그 효과를 입증합니다.
*   **쌍족 보행자**(**Bipedal Walker**) 과제와 **천문학적 보정**(**Calibration**) 문제에 대한 **RL 알고리즘의 실제 적용 사례**를 제시하여 그 성능과 잠재력을 시연합니다.
*   **강화 학습**을 **천문학에 적용할 때의 실제적인 고려 사항**과 **잠재적 응용 분야**(**계획 및 제어**, **자원 할당**, **하이퍼파라미터 튜닝**, **새로운 과학 발견** 등)를 제시합니다.

## 방법론
이 논문은 **심층 신경망**(**DNN**)을 사용하여 **Q 함수**, **가치 함수**, **정책**을 표현하는 **심층 강화 학습**(**Deep RL**) 접근 방식을 채택합니다.

*   **강화 학습 기본 요소**:
    *   **에이전트**는 **환경**과 상호 작용하며, **관측**(**observation**)을 받고 **행동**(**action**)을 수행하며 **보상**(**reward**)을 받습니다.
    *   **마르코프 결정 과정**(**MDP**)으로 모델링되며, 다음 상태가 현재 상태와 행동에만 의존하는 **마르코프 속성**을 가정합니다.
    *   **Q 함수** $Q(s,a)$는 특정 상태 $s$에서 행동 $a$를 취했을 때의 예상 누적 보상을 나타냅니다.
    *   **정책** $\pi(s)$는 주어진 상태에서 취할 행동을 결정하는 매핑입니다.
    *   **벨만 방정식** $Q(s,a) = r(s,a) + \gamma \max_{a'} Q(s',a')$을 기반으로 Q-값을 업데이트합니다.

*   **일반적인 훈련 기법**:
    *   **경험 재생**(**Experience Replay**): 과거 상호 작용 `(s, a, r, s')` 튜플을 `리플레이 버퍼`(**replay buffer**) `D`에 저장하고, 이를 재사용하여 **데이터 부족 문제**를 완화하고 **오프-폴리시**(**off-policy**) 학습을 가능하게 합니다.
    *   **탐색 대 활용**(**Exploitation vs. Exploration**): `ε-탐욕적 행동 선택`(**ε-greedy action selection**)을 사용하여 탐색(랜덤 행동)과 활용(최대 보상 행동)의 균형을 맞춥니다.
    *   **훈련 안정성**: 학습률 조정 및 이중 Q-함수 사용 등으로 안정성을 높입니다.

*   **모델-프리 심층 강화 학습 알고리즘**:
    *   **이산 행동 RL**:
        *   `Q-learning`: `Q-테이블`을 DNN으로 일반화하여 `Q_θ(s,a)`를 학습합니다.
        *   `Double Q-learning`: 두 개의 Q-함수 $Q_1$, $Q_2$를 사용하여 Q-값의 **과대평가**(**overestimation**)를 방지합니다.
    *   **연속 행동 RL**(**액터-크리틱**(**Actor-Critic**) **방법**): **액터**(**actor**)가 정책을 구현하고 **크리틱**(**critic**)이 Q-함수를 평가합니다.
        *   `DDPG`(**Deep Deterministic Policy Gradient**): **확정적 정책**(**deterministic policy**)을 사용하며, **타겟 네트워크**(**target networks**)와 `Polyak averaging`을 통해 학습 안정성을 높입니다. 행동에 노이즈를 추가하여 탐색을 유도합니다.
        *   `TD3`(**Twin Delayed DDPG**): `DDPG`의 과대평가 문제를 해결하기 위해 두 개의 Q-함수를 사용하고, 정책 업데이트를 지연시키며, 행동에 `클리핑된 노이즈`(**clipped noise**)를 적용하여 정책을 부드럽게 합니다.
        *   `SAC`(**Soft Actor-Critic**): **확률적 정책**(**stochastic policy**)을 사용하며, **엔트로피 정규화**(**entropy regularization**)를 통해 정책의 **무작위성**(**randomness**)을 보상에 포함시켜 탐색을 장려합니다. `재매개변수화 트릭`(**re-parameterization trick**)을 사용하여 정책을 미분 가능하게 만듭니다.

*   **모델-기반 강화 학습**:
    *   **환경의 동적 모델 학습**: 환경의 `역학 모델`(**dynamics model**) `p(s'|s,a)`을 학습하기 위해 **확률적 DNN**(**probabilistic DNN**) 앙상블을 사용합니다. 이는 **불확실성**(**aleatoric and epistemic uncertainty**)을 줄이는 데 도움이 됩니다.
    *   `PETS`(**Probabilistic Ensemble with Trajectory Sampling**): 학습된 모델을 사용하여 미래 궤적을 시뮬레이션하고 `교차 엔트로피 방법`(**CEM**)을 통해 최적의 행동 시퀀스를 결정하여 **데이터 생성 비용**이 높은 환경에서 효율성을 높입니다.

*   **힌트 보조 강화 학습**(**Hint Assisted RL**):
    *   **기존 지식 통합**: 외부에서 제공되는 **힌트**(**hint**) $h$를 행동 $a$에 대한 제약으로 사용하여 정책 학습을 개선합니다.
    *   **`ADMM` 기반 최적화**: **증강 라그랑지안**(**augmented Lagrangian**) $J_h(\phi)$를 사용하여 정책 손실 함수에 제약 $c(a,h)$를 추가합니다.
    $$J\_h(\phi) = J(\phi) + \frac{\rho}{2} [c(a\_\phi,h) - \delta]\_+^2 + \lambda [c(a\_\phi,h) - \delta]\_+$$
    여기서 $[x]\_+ = x$ if $x > 0$ and $[x]\_+ = 0$ otherwise. 이는 힌트가 부정확할 수 있는 가능성을 $\delta$로 표현합니다.

## 결과
*   **쌍족 보행자 과제**:
    *   **평탄한 지면 환경**: `SAC` 알고리즘이 `TD3`보다 더 높은 누적 보상을 달성하며 목표 보상에 도달하는 데 더 효과적이었습니다.
    *   **장애물 지면 환경**(**어려운 버전**): `SAC`와 `TD3` 모두에서 **힌트 보조 강화 학습**의 효과가 두드러졌습니다. 힌트(쉬운 환경에서 훈련된 에이전트로부터 얻음)를 사용한 에이전트가 힌트 없이 훈련된 에이전트보다 **더 높은 누적 보상**을 얻었습니다. 특히 `SAC`에 힌트를 적용했을 때, 거의 목표 보상에 도달하는 뛰어난 성능을 보였습니다. 이는 힌트가 **비정상적인 환경**(**non-stationary environment**)에서도 학습 성능을 향상시킬 수 있음을 시사합니다.
*   **천문학 보정 문제**:
    *   데이터 모델링에서 최적의 **기저 함수 집합**(**basis function set**)을 선택하고 **계산 예산**을 최적화하는 문제에 `SAC` 알고리즘과 **힌트**를 적용하는 개념적 프레임워크를 제시했습니다.
    *   시뮬레이션 결과, 에피소드가 진행됨에 따라 **평균 보상이 지속적으로 증가**하여 에이전트가 주어진 문제를 성공적으로 학습하고 있음을 보여주었습니다. 이는 강화 학습이 복잡한 천문학적 최적화 문제에 적용될 수 있음을 입증합니다.

이러한 결과는 **강화 학습**(**RL**)이 **천문학 분야의 다양한 자동화 및 최적화 문제**에 효과적으로 적용될 수 있으며, 특히 **기존의 도메인 지식을 힌트 형태로 통합**할 경우 학습 효율성과 성능을 크게 향상시킬 수 있음을 보여줍니다.