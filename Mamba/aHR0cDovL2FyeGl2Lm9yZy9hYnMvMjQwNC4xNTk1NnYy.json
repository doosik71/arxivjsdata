{
  "title": "A Survey on Visual Mamba",
  "authors": "Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, Zi Ye",
  "year": 2024,
  "url": "http://arxiv.org/abs/2404.15956v2",
  "abstract": "State space models (SSMs) with selection mechanisms and hardware-aware\narchitectures, namely Mamba, have recently demonstrated significant promise in\nlong-sequence modeling. Since the self-attention mechanism in transformers has\nquadratic complexity with image size and increasing computational demands, the\nresearchers are now exploring how to adapt Mamba for computer vision tasks.\nThis paper is the first comprehensive survey aiming to provide an in-depth\nanalysis of Mamba models in the field of computer vision. It begins by\nexploring the foundational concepts contributing to Mamba's success, including\nthe state space model framework, selection mechanisms, and hardware-aware\ndesign. Next, we review these vision mamba models by categorizing them into\nfoundational ones and enhancing them with techniques such as convolution,\nrecurrence, and attention to improve their sophistication. We further delve\ninto the widespread applications of Mamba in vision tasks, which include their\nuse as a backbone in various levels of vision processing. This encompasses\ngeneral visual tasks, Medical visual tasks (e.g., 2D / 3D segmentation,\nclassification, and image registration, etc.), and Remote Sensing visual tasks.\nWe specially introduce general visual tasks from two levels: High/Mid-level\nvision (e.g., Object detection, Segmentation, Video classification, etc.) and\nLow-level vision (e.g., Image super-resolution, Image restoration, Visual\ngeneration, etc.). We hope this endeavor will spark additional interest within\nthe community to address current challenges and further apply Mamba models in\ncomputer vision."
}