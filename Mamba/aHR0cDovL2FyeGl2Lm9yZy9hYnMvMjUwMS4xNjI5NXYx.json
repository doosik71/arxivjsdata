{
  "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with\n  Modality-Aware Sparsity",
  "authors": "Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, Lili Yu",
  "year": 2025,
  "url": "http://arxiv.org/abs/2501.16295v1",
  "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers for sequential modeling, but their inability to leverage\nmodality-specific features limits their performance in multi-modal pretraining.\nHere, we propose Mixture-of-Mamba, a novel SSM architecture that introduces\nmodality-aware sparsity through modality-specific parameterization of the Mamba\nblock. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996;\n2024), we extend the benefits of modality-aware sparsity to SSMs while\npreserving their computational efficiency. We evaluate Mixture-of-Mamba across\nthree multi-modal pretraining settings: Transfusion (interleaved text and\ncontinuous image tokens with diffusion loss), Chameleon (interleaved text and\ndiscrete image tokens), and an extended three-modality framework incorporating\nspeech. Mixture-of-Mamba consistently reaches the same loss values at earlier\ntraining steps with significantly reduced computational costs. In the\nTransfusion setting, Mixture-of-Mamba achieves equivalent image loss using only\n34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting,\nMixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at\nthe 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the\nthree-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the\n1.4B scale. Our ablation study highlights the synergistic effects of decoupling\nprojection components, where joint decoupling yields greater gains than\nindividual modifications. These results establish modality-aware sparsity as a\nversatile and effective design principle, extending its impact from\nTransformers to SSMs and setting new benchmarks in multi-modal pretraining. Our\ncode can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba"
}