{
  "title": "Selective Visual Prompting in Vision Mamba",
  "authors": "Yifeng Yao, Zichen Liu, Zhenyu Cui, Yuxin Peng, Jiahuan Zhou",
  "year": 2024,
  "url": "http://arxiv.org/abs/2412.08947v1",
  "abstract": "Pre-trained Vision Mamba (Vim) models have demonstrated exceptional\nperformance across various computer vision tasks in a computationally efficient\nmanner, attributed to their unique design of selective state space models. To\nfurther extend their applicability to diverse downstream vision tasks, Vim\nmodels can be adapted using the efficient fine-tuning technique known as visual\nprompting. However, existing visual prompting methods are predominantly\ntailored for Vision Transformer (ViT)-based models that leverage global\nattention, neglecting the distinctive sequential token-wise compression and\npropagation characteristics of Vim. Specifically, existing prompt tokens\nprefixed to the sequence are insufficient to effectively activate the input and\nforget gates across the entire sequence, hindering the extraction and\npropagation of discriminative information. To address this limitation, we\nintroduce a novel Selective Visual Prompting (SVP) method specifically for the\nefficient fine-tuning of Vim. To prevent the loss of discriminative information\nduring state space propagation, SVP employs lightweight selective prompters for\ntoken-wise prompt generation, ensuring adaptive activation of the update and\nforget gates within Mamba blocks to promote discriminative information\npropagation. Moreover, considering that Vim propagates both shared cross-layer\ninformation and specific inner-layer information, we further refine SVP with a\ndual-path structure: Cross-Prompting and Inner-Prompting. Cross-Prompting\nutilizes shared parameters across layers, while Inner-Prompting employs\ndistinct parameters, promoting the propagation of both shared and specific\ninformation, respectively. Extensive experimental results on various\nlarge-scale benchmarks demonstrate that our proposed SVP significantly\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-SVP."
}