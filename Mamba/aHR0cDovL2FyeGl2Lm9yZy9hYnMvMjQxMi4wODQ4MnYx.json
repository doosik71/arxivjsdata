{
  "title": "SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp\n  Segmentation",
  "authors": "Tapas Kumar Dutta, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha",
  "year": 2024,
  "url": "http://arxiv.org/abs/2412.08482v1",
  "abstract": "Polyp segmentation in colonoscopy is crucial for detecting colorectal cancer.\nHowever, it is challenging due to variations in the structure, color, and size\nof polyps, as well as the lack of clear boundaries with surrounding tissues.\nTraditional segmentation models based on Convolutional Neural Networks (CNNs)\nstruggle to capture detailed patterns and global context, limiting their\nperformance. Vision Transformer (ViT)-based models address some of these issues\nbut have difficulties in capturing local context and lack strong zero-shot\ngeneralization. To this end, we propose the Mamba-guided Segment Anything Model\n(SAM-Mamba) for efficient polyp segmentation. Our approach introduces a\nMamba-Prior module in the encoder to bridge the gap between the general\npre-trained representation of SAM and polyp-relevant trivial clues. It injects\nsalient cues of polyp images into the SAM image encoder as a domain prior while\ncapturing global dependencies at various scales, leading to more accurate\nsegmentation results. Extensive experiments on five benchmark datasets show\nthat SAM-Mamba outperforms traditional CNN, ViT, and Adapter-based models in\nboth quantitative and qualitative measures. Additionally, SAM-Mamba\ndemonstrates excellent adaptability to unseen datasets, making it highly\nsuitable for real-time clinical use."
}