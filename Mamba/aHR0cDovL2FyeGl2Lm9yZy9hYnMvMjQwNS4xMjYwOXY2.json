{
  "title": "Mamba in Speech: Towards an Alternative to Self-Attention",
  "authors": "Xiangyu Zhang, Qiquan Zhang, Hexin Liu, Tianyi Xiao, Xinyuan Qian, Beena Ahmed, Eliathamby Ambikairajah, Haizhou Li, Julien Epps",
  "year": 2024,
  "url": "http://arxiv.org/abs/2405.12609v6",
  "abstract": "Transformer and its derivatives have achieved success in diverse tasks across\ncomputer vision, natural language processing, and speech processing. To reduce\nthe complexity of computations within the multi-head self-attention mechanism\nin Transformer, Selective State Space Models (i.e., Mamba) were proposed as an\nalternative. Mamba exhibited its effectiveness in natural language processing\nand computer vision tasks, but its superiority has rarely been investigated in\nspeech signal processing. This paper explores solutions for applying Mamba to\nspeech processing by discussing two typical speech processing tasks: speech\nrecognition, which requires semantic and sequential information, and speech\nenhancement, which focuses primarily on sequential patterns. The experimental\nresults confirm that bidirectional Mamba (BiMamba) consistently outperforms\nvanilla Mamba, highlighting the advantages of a bidirectional design for speech\nprocessing. Moreover, experiments demonstrate the effectiveness of BiMamba as\nan alternative to the self-attention module in the Transformer model and its\nderivates, particularly for the semantic-aware task. The crucial technologies\nfor transferring Mamba to speech are then summarized in ablation studies and\nthe discussion section, offering insights for extending this research to a\nbroader scope of tasks."
}