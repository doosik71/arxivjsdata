{
  "title": "Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting",
  "authors": "Aobo Liang, Xingguo Jiang, Yan Sun, Xiaohou Shi, Ke Li",
  "year": 2024,
  "url": "http://arxiv.org/abs/2404.15772v3",
  "abstract": "Long-term time series forecasting (LTSF) provides longer insights into future\ntrends and patterns. Over the past few years, deep learning models especially\nTransformers have achieved advanced performance in LTSF tasks. However, LTSF\nfaces inherent challenges such as long-term dependencies capturing and sparse\nsemantic characteristics. Recently, a new state space model (SSM) named Mamba\nis proposed. With the selective capability on input data and the hardware-aware\nparallel computing algorithm, Mamba has shown great potential in balancing\npredicting performance and computational efficiency compared to Transformers.\nTo enhance Mamba's ability to preserve historical information in a longer\nrange, we design a novel Mamba+ block by adding a forget gate inside Mamba to\nselectively combine the new features with the historical features in a\ncomplementary manner. Furthermore, we apply Mamba+ both forward and backward\nand propose Bi-Mamba+, aiming to promote the model's ability to capture\ninteractions among time series elements. Additionally, multivariate time series\ndata in different scenarios may exhibit varying emphasis on intra- or\ninter-series dependencies. Therefore, we propose a series-relation-aware\ndecider that controls the utilization of channel-independent or channel-mixing\ntokenization strategy for specific datasets. Extensive experiments on 8\nreal-world datasets show that our model achieves more accurate predictions\ncompared with state-of-the-art methods."
}