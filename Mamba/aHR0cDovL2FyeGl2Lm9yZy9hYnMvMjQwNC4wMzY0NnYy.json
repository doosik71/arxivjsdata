{
  "title": "Locating and Editing Factual Associations in Mamba",
  "authors": "Arnab Sen Sharma, David Atkinson, David Bau",
  "year": 2024,
  "url": "http://arxiv.org/abs/2404.03646v2",
  "abstract": "We investigate the mechanisms of factual recall in the Mamba state space\nmodel. Our work is inspired by previous findings in autoregressive transformer\nlanguage models suggesting that their knowledge recall is localized to\nparticular modules at specific token locations; we therefore ask whether\nfactual recall in Mamba can be similarly localized. To investigate this, we\nconduct four lines of experiments on Mamba. First, we apply causal tracing or\ninterchange interventions to localize key components inside Mamba that are\nresponsible for recalling facts, revealing that specific components within\nmiddle layers show strong causal effects at the last token of the subject,\nwhile the causal effect of intervening on later layers is most pronounced at\nthe last token of the prompt, matching previous findings on autoregressive\ntransformers. Second, we show that rank-one model editing methods can\nsuccessfully insert facts at specific locations, again resembling findings on\ntransformer LMs. Third, we examine the linearity of Mamba's representations of\nfactual relations. Finally we adapt attention-knockout techniques to Mamba in\norder to dissect information flow during factual recall. We compare Mamba\ndirectly to a similar-sized autoregressive transformer LM and conclude that\ndespite significant differences in architectural approach, when it comes to\nfactual recall, the two architectures share many similarities."
}