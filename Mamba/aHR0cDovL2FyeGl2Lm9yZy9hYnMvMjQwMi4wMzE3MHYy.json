{
  "title": "Is Mamba Capable of In-Context Learning?",
  "authors": "Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank Hutter",
  "year": 2024,
  "url": "http://arxiv.org/abs/2402.03170v2",
  "abstract": "State of the art foundation models such as GPT-4 perform surprisingly well at\nin-context learning (ICL), a variant of meta-learning concerning the learned\nability to solve tasks during a neural network forward pass, exploiting\ncontextual information provided as input to the model. This useful ability\nemerges as a side product of the foundation model's massive pretraining. While\ntransformer models are currently the state of the art in ICL, this work\nprovides empirical evidence that Mamba, a newly proposed state space model\nwhich scales better than transformers w.r.t. the input sequence length, has\nsimilar ICL capabilities. We evaluated Mamba on tasks involving simple function\napproximation as well as more complex natural language processing problems. Our\nresults demonstrate that, across both categories of tasks, Mamba closely\nmatches the performance of transformer models for ICL. Further analysis reveals\nthat, like transformers, Mamba appears to solve ICL problems by incrementally\noptimizing its internal representations. Overall, our work suggests that Mamba\ncan be an efficient alternative to transformers for ICL tasks involving long\ninput sequences. This is an exciting finding in meta-learning and may enable\ngeneralizations of in-context learned AutoML algorithms (like TabPFN or\nOptformer) to long input sequences."
}