{
  "title": "Understanding Input Selectivity in Mamba: Impact on Approximation Power,\n  Memorization, and Associative Recall Capacity",
  "authors": "Ningyuan Huang, Miguel Sarabia, Abhinav Moudgil, Pau Rodriguez, Luca Zappella, Federico Danieli",
  "year": 2025,
  "url": "http://arxiv.org/abs/2506.11891v1",
  "abstract": "State-Space Models (SSMs), and particularly Mamba, have recently emerged as a\npromising alternative to Transformers. Mamba introduces input selectivity to\nits SSM layer (S6) and incorporates convolution and gating into its block\ndefinition. While these modifications do improve Mamba's performance over its\nSSM predecessors, it remains largely unclear how Mamba leverages the additional\nfunctionalities provided by input selectivity, and how these interact with the\nother operations in the Mamba architecture. In this work, we demystify the role\nof input selectivity in Mamba, investigating its impact on function\napproximation power, long-term memorization, and associative recall\ncapabilities. In particular: (i) we prove that the S6 layer of Mamba can\nrepresent projections onto Haar wavelets, providing an edge over its Diagonal\nSSM (S4D) predecessor in approximating discontinuous functions commonly arising\nin practice; (ii) we show how the S6 layer can dynamically counteract memory\ndecay; (iii) we provide analytical solutions to the MQAR associative recall\ntask using the Mamba architecture with different mixers -- Mamba, Mamba-2, and\nS4D. We demonstrate the tightness of our theoretical constructions with\nempirical results on concrete tasks. Our findings offer a mechanistic\nunderstanding of Mamba and reveal opportunities for improvement."
}