{
  "title": "A Hybrid Transformer-Mamba Network for Single Image Deraining",
  "authors": "Shangquan Sun, Wenqi Ren, Juxiang Zhou, Jianhou Gan, Rui Wang, Xiaochun Cao",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.00410v1",
  "abstract": "Existing deraining Transformers employ self-attention mechanisms with\nfixed-range windows or along channel dimensions, limiting the exploitation of\nnon-local receptive fields. In response to this issue, we introduce a novel\ndual-branch hybrid Transformer-Mamba network, denoted as TransMamba, aimed at\neffectively capturing long-range rain-related dependencies. Based on the prior\nof distinct spectral-domain features of rain degradation and background, we\ndesign a spectral-banded Transformer blocks on the first branch. Self-attention\nis executed within the combination of the spectral-domain channel dimension to\nimprove the ability of modeling long-range dependencies. To enhance\nfrequency-specific information, we present a spectral enhanced feed-forward\nmodule that aggregates features in the spectral domain. In the second branch,\nMamba layers are equipped with cascaded bidirectional state space model modules\nto additionally capture the modeling of both local and global information. At\neach stage of both the encoder and decoder, we perform channel-wise\nconcatenation of dual-branch features and achieve feature fusion through\nchannel reduction, enabling more effective integration of the multi-scale\ninformation from the Transformer and Mamba branches. To better reconstruct\ninnate signal-level relations within clean images, we also develop a spectral\ncoherence loss. Extensive experiments on diverse datasets and real-world images\ndemonstrate the superiority of our method compared against the state-of-the-art\napproaches."
}