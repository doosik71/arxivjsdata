{
  "title": "MambaOut: Do We Really Need Mamba for Vision?",
  "authors": "Weihao Yu, Xinchao Wang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2405.07992v3",
  "abstract": "Mamba, an architecture with RNN-like token mixer of state space model (SSM),\nwas recently introduced to address the quadratic complexity of the attention\nmechanism and subsequently applied to vision tasks. Nevertheless, the\nperformance of Mamba for vision is often underwhelming when compared with\nconvolutional and attention-based models. In this paper, we delve into the\nessence of Mamba, and conceptually conclude that Mamba is ideally suited for\ntasks with long-sequence and autoregressive characteristics. For vision tasks,\nas image classification does not align with either characteristic, we\nhypothesize that Mamba is not necessary for this task; Detection and\nsegmentation tasks are also not autoregressive, yet they adhere to the\nlong-sequence characteristic, so we believe it is still worthwhile to explore\nMamba's potential for these tasks. To empirically verify our hypotheses, we\nconstruct a series of models named MambaOut through stacking Mamba blocks while\nremoving their core token mixer, SSM. Experimental results strongly support our\nhypotheses. Specifically, our MambaOut model surpasses all visual Mamba models\non ImageNet image classification, indicating that Mamba is indeed unnecessary\nfor this task. As for detection and segmentation, MambaOut cannot match the\nperformance of state-of-the-art visual Mamba models, demonstrating the\npotential of Mamba for long-sequence visual tasks. The code is available at\nhttps://github.com/yuweihao/MambaOut"
}