{
  "title": "Mamba for Scalable and Efficient Personalized Recommendations",
  "authors": "Andrew Starnes, Clayton Webster",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.17165v1",
  "abstract": "In this effort, we propose using the Mamba for handling tabular data in\npersonalized recommendation systems. We present the \\textit{FT-Mamba} (Feature\nTokenizer\\,$+$\\,Mamba), a novel hybrid model that replaces Transformer layers\nwith Mamba layers within the FT-Transformer architecture, for handling tabular\ndata in personalized recommendation systems. The \\textit{Mamba model} offers an\nefficient alternative to Transformers, reducing computational complexity from\nquadratic to linear by enhancing the capabilities of State Space Models (SSMs).\nFT-Mamba is designed to improve the scalability and efficiency of\nrecommendation systems while maintaining performance. We evaluate FT-Mamba in\ncomparison to a traditional Transformer-based model within a Two-Tower\narchitecture on three datasets: Spotify music recommendation, H\\&M fashion\nrecommendation, and vaccine messaging recommendation. Each model is trained on\n160,000 user-action pairs, and performance is measured using precision (P),\nrecall (R), Mean Reciprocal Rank (MRR), and Hit Ratio (HR) at several\ntruncation values. Our results demonstrate that FT-Mamba outperforms the\nTransformer-based model in terms of computational efficiency while maintaining\nor exceeding performance across key recommendation metrics. By leveraging Mamba\nlayers, FT-Mamba provides a scalable and effective solution for large-scale\npersonalized recommendation systems, showcasing the potential of the Mamba\narchitecture to enhance both efficiency and accuracy."
}