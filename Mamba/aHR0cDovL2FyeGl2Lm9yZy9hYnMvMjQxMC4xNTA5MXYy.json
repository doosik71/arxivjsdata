{
  "title": "Spatial-Mamba: Effective Visual State Space Models via Structure-aware\n  State Fusion",
  "authors": "Chaodong Xiao, Minghan Li, Zhengqiang Zhang, Deyu Meng, Lei Zhang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2410.15091v2",
  "abstract": "Selective state space models (SSMs), such as Mamba, highly excel at capturing\nlong-range dependencies in 1D sequential data, while their applications to 2D\nvision tasks still face challenges. Current visual SSMs often convert images\ninto 1D sequences and employ various scanning patterns to incorporate local\nspatial dependencies. However, these methods are limited in effectively\ncapturing the complex image spatial structures and the increased computational\ncost caused by the lengthened scanning paths. To address these limitations, we\npropose Spatial-Mamba, a novel approach that establishes neighborhood\nconnectivity directly in the state space. Instead of relying solely on\nsequential state transitions, we introduce a structure-aware state fusion\nequation, which leverages dilated convolutions to capture image spatial\nstructural dependencies, significantly enhancing the flow of visual contextual\ninformation. Spatial-Mamba proceeds in three stages: initial state computation\nin a unidirectional scan, spatial context acquisition through structure-aware\nstate fusion, and final state computation using the observation equation. Our\ntheoretical analysis shows that Spatial-Mamba unifies the original Mamba and\nlinear attention under the same matrix multiplication framework, providing a\ndeeper understanding of our method. Experimental results demonstrate that\nSpatial-Mamba, even with a single scan, attains or surpasses the\nstate-of-the-art SSM-based models in image classification, detection and\nsegmentation. Source codes and trained models can be found at\nhttps://github.com/EdwardChasel/Spatial-Mamba.",
  "citation": 21
}