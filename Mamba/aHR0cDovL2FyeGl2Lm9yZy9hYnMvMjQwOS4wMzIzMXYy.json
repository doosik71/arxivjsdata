{
  "title": "State-space models are accurate and efficient neural operators for\n  dynamical systems",
  "authors": "Zheyuan Hu, Nazanin Ahmadi Daryakenari, Qianli Shen, Kenji Kawaguchi, George Em Karniadakis",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.03231v2",
  "abstract": "Physics-informed machine learning (PIML) has emerged as a promising\nalternative to classical methods for predicting dynamical systems, offering\nfaster and more generalizable solutions. However, existing models, including\nrecurrent neural networks (RNNs), transformers, and neural operators, face\nchallenges such as long-time integration, long-range dependencies, chaotic\ndynamics, and extrapolation, to name a few. To this end, this paper introduces\nstate-space models implemented in Mamba for accurate and efficient dynamical\nsystem operator learning. Mamba addresses the limitations of existing\narchitectures by dynamically capturing long-range dependencies and enhancing\ncomputational efficiency through reparameterization techniques. To extensively\ntest Mamba and compare against another 11 baselines, we introduce several\nstrict extrapolation testbeds that go beyond the standard interpolation\nbenchmarks. We demonstrate Mamba's superior performance in both interpolation\nand challenging extrapolation tasks. Mamba consistently ranks among the top\nmodels while maintaining the lowest computational cost and exceptional\nextrapolation capabilities. Moreover, we demonstrate the good performance of\nMamba for a real-world application in quantitative systems pharmacology for\nassessing the efficacy of drugs in tumor growth under limited data scenarios.\nTaken together, our findings highlight Mamba's potential as a powerful tool for\nadvancing scientific machine learning in dynamical systems modeling. (The code\nwill be available at\nhttps://github.com/zheyuanhu01/State_Space_Model_Neural_Operator upon\nacceptance.)"
}