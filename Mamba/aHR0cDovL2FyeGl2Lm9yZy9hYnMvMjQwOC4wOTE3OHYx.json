{
  "title": "MambaTrack: A Simple Baseline for Multiple Object Tracking with State\n  Space Model",
  "authors": "Changcheng Xiao, Qiong Cao, Zhigang Luo, Long Lan",
  "year": 2024,
  "url": "http://arxiv.org/abs/2408.09178v1",
  "abstract": "Tracking by detection has been the prevailing paradigm in the field of\nMulti-object Tracking (MOT). These methods typically rely on the Kalman Filter\nto estimate the future locations of objects, assuming linear object motion.\nHowever, they fall short when tracking objects exhibiting nonlinear and diverse\nmotion in scenarios like dancing and sports. In addition, there has been\nlimited focus on utilizing learning-based motion predictors in MOT. To address\nthese challenges, we resort to exploring data-driven motion prediction methods.\nInspired by the great expectation of state space models (SSMs), such as Mamba,\nin long-term sequence modeling with near-linear complexity, we introduce a\nMamba-based motion model named Mamba moTion Predictor (MTP). MTP is designed to\nmodel the complex motion patterns of objects like dancers and athletes.\nSpecifically, MTP takes the spatial-temporal location dynamics of objects as\ninput, captures the motion pattern using a bi-Mamba encoding layer, and\npredicts the next motion. In real-world scenarios, objects may be missed due to\nocclusion or motion blur, leading to premature termination of their\ntrajectories. To tackle this challenge, we further expand the application of\nMTP. We employ it in an autoregressive way to compensate for missing\nobservations by utilizing its own predictions as inputs, thereby contributing\nto more consistent trajectories. Our proposed tracker, MambaTrack, demonstrates\nadvanced performance on benchmarks such as Dancetrack and SportsMOT, which are\ncharacterized by complex motion and severe occlusion."
}