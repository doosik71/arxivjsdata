{
  "title": "An Empirical Study of Mamba-based Pedestrian Attribute Recognition",
  "authors": "Xiao Wang, Weizhe Kong, Jiandong Jin, Shiao Wang, Ruichong Gao, Qingchuan Ma, Chenglong Li, Jin Tang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.10374v2",
  "abstract": "Current strong pedestrian attribute recognition models are developed based on\nTransformer networks, which are computationally heavy. Recently proposed models\nwith linear complexity (e.g., Mamba) have garnered significant attention and\nhave achieved a good balance between accuracy and computational cost across a\nvariety of visual tasks. Relevant review articles also suggest that while these\nmodels can perform well on some pedestrian attribute recognition datasets, they\nare generally weaker than the corresponding Transformer models. To further tap\ninto the potential of the novel Mamba architecture for PAR tasks, this paper\ndesigns and adapts Mamba into two typical PAR frameworks, i.e., the text-image\nfusion approach and pure vision Mamba multi-label recognition framework. It is\nfound that interacting with attribute tags as additional input does not always\nlead to an improvement, specifically, Vim can be enhanced, but VMamba cannot.\nThis paper further designs various hybrid Mamba-Transformer variants and\nconducts thorough experimental validations. These experimental results indicate\nthat simply enhancing Mamba with a Transformer does not always lead to\nperformance improvements but yields better results under certain settings. We\nhope this empirical study can further inspire research in Mamba for PAR, and\neven extend into the domain of multi-label recognition, through the design of\nthese network structures and comprehensive experimentation. The source code of\nthis work will be released at \\url{https://github.com/Event-AHU/OpenPAR}"
}