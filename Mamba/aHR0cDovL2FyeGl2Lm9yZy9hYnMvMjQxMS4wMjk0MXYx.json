{
  "title": "A Mamba Foundation Model for Time Series Forecasting",
  "authors": "Haoyu Ma, Yushu Chen, Wenlai Zhao, Jinzhe Yang, Yingsheng Ji, Xinghua Xu, Xiaozhu Liu, Hao Jing, Shengzhuo Liu, Guangwen Yang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2411.02941v1",
  "abstract": "Time series foundation models have demonstrated strong performance in\nzero-shot learning, making them well-suited for predicting rapidly evolving\npatterns in real-world applications where relevant training data are scarce.\nHowever, most of these models rely on the Transformer architecture, which\nincurs quadratic complexity as input length increases. To address this, we\nintroduce TSMamba, a linear-complexity foundation model for time series\nforecasting built on the Mamba architecture. The model captures temporal\ndependencies through both forward and backward Mamba encoders, achieving high\nprediction accuracy. To reduce reliance on large datasets and lower training\ncosts, TSMamba employs a two-stage transfer learning process that leverages\npretrained Mamba LLMs, allowing effective time series modeling with a moderate\ntraining set. In the first stage, the forward and backward backbones are\noptimized via patch-wise autoregressive prediction; in the second stage, the\nmodel trains a prediction head and refines other components for long-term\nforecasting. While the backbone assumes channel independence to manage varying\nchannel numbers across datasets, a channel-wise compressed attention module is\nintroduced to capture cross-channel dependencies during fine-tuning on specific\nmultivariate datasets. Experiments show that TSMamba's zero-shot performance is\ncomparable to state-of-the-art time series foundation models, despite using\nsignificantly less training data. It also achieves competitive or superior\nfull-shot performance compared to task-specific prediction models. The code\nwill be made publicly available."
}