{
  "title": "TransMamba: Fast Universal Architecture Adaption from Transformers to\n  Mamba",
  "authors": "Xiuwei Chen, Sihao Lin, Xiao Dong, Zisheng Chen, Meng Cao, Jianhua Han, Hang Xu, Xiaodan Liang",
  "year": 2025,
  "url": "http://arxiv.org/abs/2502.15130v1",
  "abstract": "Transformers have been favored in both uni-modal and multi-modal foundation\nmodels for their flexible scalability in attention modules. Consequently, a\nnumber of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, are\npublicly available. Recent research has introduced subquadratic architectures\nlike Mamba, which enables global awareness with linear complexity.\nNevertheless, training specialized subquadratic architectures from scratch for\ncertain tasks is both resource-intensive and time-consuming. As a motivator, we\nexplore cross-architecture training to transfer the ready knowledge in existing\nTransformer models to alternative architecture Mamba, termed TransMamba. Our\napproach employs a two-stage strategy to expedite training new Mamba models,\nensuring effectiveness in across uni-modal and cross-modal tasks. Concerning\narchitecture disparities, we project the intermediate features into an aligned\nlatent space before transferring knowledge. On top of that, a Weight Subcloning\nand Adaptive Bidirectional distillation method (WSAB) is introduced for\nknowledge transfer without limitations on varying layer counts. For cross-modal\nlearning, we propose a cross-Mamba module that integrates language awareness\ninto Mamba's visual features, enhancing the cross-modal interaction\ncapabilities of Mamba architecture. Despite using less than 75% of the training\ndata typically required for training from scratch, TransMamba boasts\nsubstantially stronger performance across various network architectures and\ndownstream tasks, including image classification, visual question answering,\nand text-video retrieval. The code will be publicly available."
}