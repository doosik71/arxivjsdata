# Mamba: Linear-Time Sequence Modeling with Selective State Spaces

Albert Gu, Tri Dao

## 🧩 Problem to Solve

최근 딥러닝 애플리케이션의 기반이 되는 파운데이션 모델은 거의 대부분 트랜스포머(Transformer) 아키텍처와 핵심 어텐션(Attention) 모듈에 의존하고 있습니다. 그러나 트랜스포머는 긴 시퀀스에서 연산 비효율성(시퀀스 길이에 대한 2차 스케일링)과 유한한 컨텍스트 윈도우(KV 캐시 필요)라는 근본적인 단점을 가지고 있습니다.

이를 해결하기 위해 선형 어텐션(linear attention), 게이트형 컨볼루션(gated convolution), 순환 모델(recurrent models), 구조화된 상태 공간 모델(Structured State Space Models, SSMs) 등 다양한 준-2차(sub-quadratic) 시간 복잡도를 가진 아키텍처들이 개발되었으나, 언어와 같은 중요한 모달리티에서는 트랜스포머만큼의 성능을 달성하지 못했습니다.

본 연구는 기존 모델들의 주요 약점이 **콘텐츠 기반 추론(content-based reasoning) 능력 부족**에 있음을 규명하고, 특히 시불변(Linear Time-Invariant, LTI) SSMs가 이산적이고 정보 밀도가 높은 데이터(텍스트, DNA 등)를 모델링하는 데 효과적이지 못하다는 문제를 해결하고자 합니다.

## ✨ Key Contributions

- **선택적 상태 공간 모델(Selective State Space Models, S6) 도입**: SSM 파라미터($\Delta$, $\mathbf{B}$, $\mathbf{C}$)를 입력에 따라 변하는 함수로 만들어, 모델이 현재 토큰에 따라 시퀀스 길이를 따라 정보를 선택적으로 전파하거나 잊어버리도록 함으로써 이산 모달리티의 약점을 해결했습니다.
- **하드웨어 인지형 병렬 알고리즘 개발**: 입력 의존적인 파라미터 변화로 인해 효율적인 컨볼루션 사용이 불가능해지자, 재귀 모드(recurrent mode)에서 스캔(scan) 연산을 사용하는 하드웨어 인지형 병렬 알고리즘을 설계하여 GPU 메모리 계층 구조를 효율적으로 활용하고 성능 병목 현상을 극복했습니다.
- **간소화된 엔드-투-엔드 신경망 아키텍처(Mamba) 제안**: 어텐션이나 심지어 MLP 블록 없이도 선택적 SSM을 통합한 단순하고 동질적인 아키텍처인 Mamba를 제시했습니다.
- **선형 시간 스케일링 및 빠른 추론**: Mamba는 시퀀스 길이에 대해 선형적으로 스케일링되며, 트랜스포머보다 5배 높은 처리량으로 빠른 추론 속도를 제공합니다.
- **다양한 모달리티에서 SOTA 성능 달성**: 언어, 오디오, 유전체학 등 여러 모달리티에서 기존 최첨단 모델들을 능가하는 성능을 보이며, 백만(million) 길이 시퀀스에서도 성능 향상을 입증했습니다.
- **언어 모델링 성능 향상**: Mamba-3B 모델은 동일 크기의 트랜스포머를 능가하고, 2배 큰 트랜스포머 모델과 유사한 사전 학습 및 다운스트림 평가 성능을 달성했습니다.

## 📎 Related Works

- **트랜스포머(Transformers)**: Vaswani et al. (2017)의 셀프 어텐션 기반 모델.
- **구조화된 상태 공간 모델(Structured State Space Models, SSMs/S4)**: Gu, Goel, and Ré (2022); Gu, Johnson, Goel, et al. (2021) 등. RNN과 CNN의 조합으로, 선형 또는 준선형 스케일링을 가짐.
- **효율적인 어텐션 변형**: 선형 어텐션(Katharopoulos et al. 2020), FlashAttention (Dao, Fu, Ermon, et al. 2022).
- **기존 SSM 아키텍처**:
  - H3 (Dao, Fu, Saab, et al. 2023): S4와 두 개의 게이트 연결을 결합.
  - Hyena (Poli et al. 2023): H3와 유사하지만 S4 대신 MLP로 파라미터화된 전역 컨볼루션 사용.
  - RetNet (Y. Sun et al. 2023): 추가 게이트와 간소화된 SSM을 사용.
  - RWKV (B. Peng et al. 2023): 선형 어텐션 근사를 기반으로 한 언어 모델링용 RNN.
- **선택적 SSM과 밀접한 모델**: S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), SRU (Lei et al. 2017).
- **게이팅(Gating) 메커니즘**: LSTM (Hochreiter and Schmidhuber 1997), GRU (J. Chung et al. 2014) 등.
- **하이퍼네트워크(Hypernetworks)**, **데이터 의존성(Data-dependence)**.

## 🛠️ Methodology

Mamba는 세 가지 주요 개선 사항을 통해 기존 SSM의 한계를 극복합니다.

1. **선택 메커니즘 (Selection Mechanism)**
   - **문제 인식**: 기존 LTI SSM은 시불변 다이내믹스(constant dynamics)를 가져 입력 콘텐츠에 따라 정보를 선택적으로 처리하거나 무시할 수 없다는 한계가 있었습니다. 이는 '선택적 복사(Selective Copying)'나 '귀납적 헤드(Induction Heads)'와 같은 콘텐츠 기반 추론이 필요한 합성 태스크에서 실패하는 원인이 됩니다.
   - **구현**: SSM의 이산화(discretization)에 사용되는 파라미터 $\Delta$, $\mathbf{B}$, $\mathbf{C}$를 입력 $\mathbf{x}$의 함수로 만듭니다.
     - $\Delta_t = \tau_{\Delta}(\text{Parameter} + s_{\Delta}(\mathbf{x}_t))$
     - $\mathbf{B}_t = s_{\mathbf{B}}(\mathbf{x}_t)$
     - $\mathbf{C}_t = s_{\mathbf{C}}(\mathbf{x}_t)$
     - 여기서 $s_{\mathbf{B}}, s_{\mathbf{C}}$는 선형 투영(Linear projection), $s_{\Delta}$는 입력 $\mathbf{x}$를 1차원으로 투영 후 브로드캐스트하는 함수이며, $\tau_{\Delta}$는 softplus 활성화 함수를 사용합니다.
   - **효과**: 모델이 입력 $\mathbf{x}_t$에 따라 정보를 필터링하거나, 숨겨진 상태 $h_t$를 업데이트하는 방식을 조절하여 관련 정보를 무기한 기억할 수 있게 됩니다. 이는 RNN의 게이팅 메커니즘과 깊은 연관이 있습니다 (Theorem 1).
2. **하드웨어 인지형 알고리즘 (Hardware-aware Algorithm: Selective Scan)**
   - **도전**: 파라미터가 입력 의존적으로 변하면 LTI 속성이 사라져 효율적인 컨볼루션 계산 방식(3)을 사용할 수 없게 되고, 재귀 계산 방식(2)은 $\mathcal{O}(B L D N)$ 크기의 잠재 상태 $h$를 직접 물질화해야 하므로 메모리 오버헤드가 큽니다.
   - **핵심 아이디어**: GPU 메모리 계층 구조를 활용하여 잠재 상태 $h$를 GPU HBM(고대역폭 메모리)이 아닌 더 빠른 SRAM(Static Random-Access Memory)에서만 물질화하도록 합니다.
   - **구현**:
     - **커널 퓨전(Kernel Fusion)**: 이산화 단계, 스캔(scan) 연산, $\mathbf{C}$와의 곱셈을 하나의 GPU 커널로 융합합니다. 이를 통해 HBM과 SRAM 간의 I/O를 $\mathcal{O}(N)$ 배 감소시켜 20-40배의 속도 향상을 달성합니다.
     - **병렬 스캔(Parallel Scan)**: 재귀 연산의 순차적 특성을 병렬 스캔 알고리즘으로 해결하여 병렬화를 가능하게 합니다.
     - **재계산(Recomputation)**: 역전파를 위해 필요한 중간 상태를 저장하지 않고, 역전파 시점에 필요할 때 SRAM에 다시 계산하여 HBM 메모리 사용량을 최적화합니다. 결과적으로 FlashAttention을 사용한 최적화된 트랜스포머와 유사한 메모리 요구사항을 가집니다.
3. **간소화된 SSM 아키텍처 (Mamba Architecture)**
   - **구조**: 기존 H3 아키텍처(SSM 블록과 MLP 블록이 교차)와 트랜스포머의 MLP 블록 디자인을 결합하여, 어텐션이나 별도의 MLP 블록 없이 단일하고 동질적인 Mamba 블록을 반복적으로 쌓는 구조입니다.
   - **블록 구성**: 입력 선형 투영 $\rightarrow$ 시퀀스 변환(Selective SSM) $\rightarrow$ 활성화 함수(SiLU/Swish) $\rightarrow$ 출력 선형 투영.
   - **확장 계수**: 모델 차원 $D$를 확장 계수 $E$ (기본값 2)만큼 확장합니다. MHA(Multi-Head Attention)와 MLP 블록을 포함하는 트랜스포머 레이어와 유사한 파라미터 수를 맞추기 위해 2개의 Mamba 블록을 사용합니다.
   - **정규화 및 잔차 연결**: 표준 LayerNorm 및 잔차 연결을 포함합니다.

## 📊 Results

- **합성 태스크**:
  - **Selective Copying**: LTI SSM과 게이트 아키텍처는 부분적으로만 개선된 반면, S6(선택적 SSM)는 이 태스크를 완벽하게 해결했으며, Mamba 아키텍처와 결합 시 99.8%의 정확도를 달성했습니다.
  - **Induction Heads**: Mamba는 훈련 길이($2^8=256$)의 4000배가 넘는 백만(1M) 길이 시퀀스까지 완벽하게 외삽(extrapolate)하여 문제 해결 능력을 보였습니다. 이는 다른 어떤 모델도 2배 이상 외삽하지 못한 것과 대조적입니다.
- **언어 모델링**:
  - **스케일링 법칙**: Mamba는 The Pile 데이터셋에서 $125\text{M}$부터 $1.3\text{B}$ 파라미터까지 모델 크기를 키웠을 때, 트랜스포머++(LLaMa/PaLM 레시피 기반)와 견줄만한 성능을 달성한 최초의 선형 시간 모델입니다. 특히 시퀀스 길이가 길어질수록 성능 격차가 벌어졌습니다.
  - **다운스트림 평가**: Mamba-3B 모델은 동일 크기의 Pythia 모델을 모든 제로샷 평가 태스크에서 능가했으며, 심지어 2배 큰 Pythia-7B 모델보다도 우수한 성능을 보였습니다 (예: 상식 추론에서 4점 향상).
- **DNA 모델링**:
  - **스케일링 법칙 (모델 크기)**: Mamba는 HG38(인간 유전체) 데이터셋에서 모델 크기 증가에 따라 퍼플렉시티가 원활하게 감소하며, HyenaDNA 및 트랜스포머++보다 우수하게 스케일링됩니다.
  - **스케일링 법칙 (컨텍스트 길이)**: Mamba는 최대 1M 길이의 시퀀스까지 긴 컨텍스트를 활용하여 퍼플렉시티를 향상시키는 유일한 모델이었습니다. HyenaDNA는 컨텍스트 길이가 길어질수록 성능이 저하되었습니다.
  - **종 분류 (유인원)**: Mamba는 1M 시퀀스 길이에서 81.31%의 정확도를 달성하여, HyenaDNA (54.87%)를 크게 능가했습니다.
- **오디오 모델링 및 생성**:
  - **장기 컨텍스트 사전 학습**: YouTubeMix 데이터셋에서 Mamba는 기존 SaShiMi보다 지속적으로 우수한 성능(BPB)을 보였으며, 백만 길이 시퀀스(분 단위 오디오)까지 성능이 향상되었습니다.
  - **음성 생성 (SC09)**: 소형 Mamba 모델(6.1M)은 최첨단 GAN 및 Diffusion 기반 모델을 능가했으며, 파라미터 수가 더 큰 Mamba 모델(24.3M)은 충실도(fidelity) 측정치에서 더욱 극적인 개선을 보였습니다.
- **속도 및 메모리 벤치마크**:
  - **훈련 속도**: 최적화된 SSM 스캔 연산은 PyTorch 표준 스캔보다 40배 빠르며, 32K 이상 시퀀스 길이에서 FlashAttention-2보다 최대 7배 빠릅니다.
  - **추론 처리량**: 재귀 모델로서 Mamba는 KV 캐시가 필요 없어 트랜스포머보다 5배 높은 추론 처리량을 달성합니다 (Mamba-6.9B는 트랜스포머-1.3B보다 높은 처리량).
  - **메모리**: Mamba의 메모리 사용량은 FlashAttention을 사용한 최적화된 트랜스포머와 유사합니다.

## 🧠 Insights & Discussion

- **선택성의 중요성**: 본 연구는 시퀀스 모델의 효율성과 효과성 트레이드오프가 '상태를 얼마나 잘 압축하는가'에 달려있으며, 이를 위해서는 '선택성(selectivity)'이라는 컨텍스트 인지적 능력이 핵심임을 강조합니다. Mamba의 선택 메커니즘은 입력 의존적인 방식으로 정보를 필터링하고, 상태를 재설정하며, 가변적인 시퀀스 간격을 처리함으로써 기존 LTI SSM이 다루기 어려웠던 이산 데이터를 효과적으로 모델링할 수 있게 합니다.
- **연속-이산 스펙트럼**: SSM은 원래 연속 시간 데이터(오디오, 비디오)에 강력한 귀납적 편향(inductive bias)을 가졌습니다. 선택 메커니즘은 텍스트나 DNA와 같은 이산 모달리티에서의 약점을 보완하지만, 반대로 LTI SSM이 뛰어난 연속 데이터에서는 성능을 저해할 수 있습니다. 오디오 파형에 대한 실험에서 이러한 트레이드오프가 확인되었습니다.
- **의미**: Mamba는 효율성(선형 스케일링, 빠른 추론)과 효과성(트랜스포머 수준의 성능, 콘텐츠 기반 추론) 사이의 간극을 성공적으로 메웠습니다. 이는 특히 유전체학, 오디오, 비디오와 같은 긴 컨텍스트를 요구하는 새로운 모달리티에서 파운데이션 모델을 구축하는 데 중요한 역할을 할 수 있습니다. Mamba는 범용 시퀀스 모델 백본의 강력한 후보입니다.
- **제한 및 향후 연구**:
  - 현재 평가는 주로 3B 파라미터 이하의 소규모 모델에 국한되어 있으며, Llama와 같은 대규모 LLM(7B 이상)에서의 성능 검증이 필요합니다.
  - 파라미터 스케일링에 따른 추가적인 엔지니어링 및 모델 조정이 필요할 수 있습니다.
  - 사전 학습된 Mamba 모델이 트랜스포머 기반 LLM과 유사한 미세 조정, 프롬프팅, 인컨텍스트 학습, RLHF 등 다양한 기능과 상호작용 방식을 가질 수 있는지 탐색해야 합니다.

## 📌 TL;DR

Mamba는 트랜스포머의 비효율성과 기존 시불변(LTI) SSM의 콘텐츠 기반 추론 한계를 해결하기 위해 **선택적 상태 공간 모델(Selective State Space Models, S6)**을 도입합니다. 이 모델은 SSM 파라미터를 입력 의존적으로 만들어 정보를 선택적으로 전파/망각하며, **하드웨어 인지형 병렬 스캔 알고리즘**을 통해 효율적인 계산을 가능하게 합니다. 어텐션 없는 **간소화된 아키텍처**를 사용하는 Mamba는 시퀀스 길이에 대해 **선형적으로 스케일링**되고 **5배 빠른 추론** 속도를 제공합니다. 언어, DNA, 오디오 등 다양한 모달리티에서 **최첨단 성능**을 달성하며, 특히 언어 모델링에서 Mamba-3B는 동일 크기 트랜스포머를 능가하고 2배 큰 트랜스포머와 비견되는 품질을 보입니다. Mamba는 범용 시퀀스 모델 백본으로서 효율성과 성능을 겸비한 강력한 대안을 제시합니다.
