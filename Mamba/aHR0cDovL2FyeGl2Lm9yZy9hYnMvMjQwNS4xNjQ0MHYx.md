# MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting
Xiuding Cai, Yaoyao Zhu, Xueyao Wang, Yu Yao

## 🧩 Problem to Solve
Transformer 기반 모델은 장기 시계열 예측(LTSF) 분야에서 지배적이었지만, 시퀀스 길이에 따른 2차 계산 복잡도와 순열 불변 편향(permutation invariant bias) 문제에 직면해 있습니다. 최근 선택적 상태 공간 모델(SSMs)을 기반으로 하는 Mamba 모델은 선형 복잡도와 높은 처리량을 제공하며 Transformer의 경쟁자로 떠올랐습니다. 하지만, Mamba를 기존 Transformer 기반 LTSF 모델(PatchTST, iTransformer)에 단순히 적용했을 때, 효율성은 개선되었으나 예측 성능 면에서는 Transformer에 비해 이점을 보이지 못했습니다. 이 연구는 Mamba가 LTSF에 적합하다는 직관과 달리 초기 성능이 저조했던 이유를 분석하고, Mamba의 잠재력을 LTSF에 최적화하여 활용하는 것을 목표로 합니다.

## ✨ Key Contributions
*   **MambaTS 제안:** 개선된 선택적 SSM 기반의 새로운 시계열 예측 모델 MambaTS를 소개합니다. VST를 통해 모든 변수의 과거 정보를 효율적으로 통합하여 전역적인 회고 시퀀스를 생성합니다.
*   **Temporal Mamba Block (TMB) 개발:** 바닐라 Mamba의 인과적 컨볼루션이 LTSF에서 불필요함을 밝히고, 이를 제거한 TMB를 제안합니다. 또한, 모델 과적합을 완화하기 위해 TMB의 선택적 파라미터에 드롭아웃 메커니즘을 통합합니다.
*   **변수 순열 훈련 (VPT) 전략 구현:** 변수 순서의 불확실성이 모델에 미치는 영향을 제거하고 모델의 로컬 컨텍스트 상호작용 능력을 향상시키기 위해 VPT 전략을 도입합니다.
*   **변수 인식 시간에 따른 스캔 (VAST) 제안:** 훈련 중 다른 변수 간의 관계를 동적으로 발견하고, 추론 시 비대칭 외판원 문제(ATSP) 솔버를 활용하여 최적의 변수 스캔 순서를 결정합니다.
*   8개 공개 데이터셋에 대한 광범위한 실험을 통해 MambaTS가 대부분의 LTSF 태스크에서 최첨단(SOTA) 성능을 달성함을 입증합니다.

## 📎 Related Works
*   **장기 시계열 예측(LTSF):** RNN, TCN, Transformer 등 딥러닝 기반 방법론으로 발전해왔습니다. Transformer는 자기 어텐션 메커니즘으로 장기 의존성을 잘 포착했으나, 2차 복잡도와 긴 룩백 윈도우에서 성능 개선이 없는 문제점이 지적되었습니다(예: DLinear). Patch-based 기법(PatchTST)과 변수 독립적(iTransformer) 접근 방식도 연구되었습니다.
*   **상태 공간 모델(SSMs):** S4, H3, Gated State Space, RWKV 등 효율적인 SSM 변형들이 제안되었습니다. Mamba는 데이터 의존적인 선택 메커니즘을 도입하여 긴 시퀀스를 선형 시간으로 모델링할 수 있게 했으며, Transformer에 비해 경쟁력 있는 성능을 보였습니다.
*   **Mamba의 확장:** Mamba는 이미지, 포인트 클라우드, 테이블, 그래프 등 비시퀀스 데이터에도 성공적으로 확장되었습니다. 스캔 순서 민감도를 해결하기 위해 양방향/다방향 스캔 연구가 있었으나, 시계열 문제에서 *변수 스캔 순서* 문제를 다룬 연구는 제한적이었습니다. 본 연구는 VAST 전략을 통해 이 문제를 최초로 탐구합니다.

## 🛠️ Methodology
MambaTS는 다음과 같은 네 가지 주요 개선 사항을 통해 Mamba를 LTSF에 최적화합니다.

1.  **패치 및 토큰화:** PatchTST와 유사하게 각 변수를 패치로 분할하고 선형 매핑을 통해 $D$ 차원 토큰으로 변환하여 메모리 사용량을 줄입니다.
2.  **시간에 따른 변수 스캔 (VST):** $K$개의 변수에서 얻은 $K \times M$개의 토큰을 각 타임스텝에서 변수들의 토큰을 교대로 구성하여 시간 축을 따라 확장합니다. 이는 전역적인 과거 룩백 윈도우를 형성하고 Mamba의 선형 복잡도 및 선택적 장점을 활용합니다.
3.  **시간적 맘바 블록 (TMB):**
    *   기존 Mamba Block의 인과적 컨볼루션 레이어($\text{Conv}(\text{Linear}(x_t))$)를 제거합니다. VST는 지역적으로 인과적이지 않은 시퀀스를 생성하므로 인과적 컨볼루션이 불필요하다고 판단했습니다.
    *   Mamba의 선택적 파라미터($\Delta, B, C$)에 드롭아웃 메커니즘을 도입합니다. 이는 Mamba의 과도한 정보 통합이 과적합으로 이어질 수 있다는 발견에 기반하여 과적합을 방지합니다.
    *   변형된 계산식: $h_t = \text{SSM}(\text{Dropout}(\text{Linear}(x_t))) + \sigma(\text{Linear}(x_t))$
4.  **변수 순열 훈련 (VPT):**
    *   훈련 중 각 타임스텝에서 $K$개의 변수 순서를 일관되게 섞습니다. 디코딩 후에는 원래 순서로 되돌립니다.
    *   이는 정의되지 않은 변수 순서의 영향을 완화하고 모델의 로컬 컨텍스트 상호작용 능력을 향상시킵니다.
5.  **변수 인식 시간에 따른 스캔 (VAST):**
    *   **훈련 단계:** $K \times K$ 크기의 방향성 그래프 인접 행렬 $P$를 유지하며, $p_{i,j}$는 노드 $i$에서 노드 $j$로의 비용을 나타냅니다. 매 순열 후 계산된 훈련 손실 $l^{(t)}$를 이용하여 $P$를 지수 이동 평균으로 업데이트합니다.
        $$p^{(t)}_{v_k, v_{k+1}} = \beta p^{(t-1)}_{v_k, v_{k+1}} + (1-\beta) \bar{l}^{(t)}$$
        여기서 $\bar{l}^{(t)}$는 중앙값 처리된 손실입니다.
    *   **추론 단계:** $P$ 행렬을 바탕으로 모든 노드를 방문하는 최단 경로를 찾는 비대칭 외판원 문제(ATSP)를 해결하여 최적의 변수 스캔 순서를 결정합니다. NP-hard 문제이므로 효율성과 성능의 균형을 위해 휴리스틱 기반의 모의 담금질(Simulated Annealing) 알고리즘을 사용합니다.

## 📊 Results
*   **최첨단 성능:** MambaTS는 ETTh2, ETTm2, Weather, Electricity, Traffic, Solar, Covid-19, PEMS 등 8개 공개 데이터셋에서 다양한 예측 길이에 걸쳐 대부분의 경우 SOTA 성능을 달성했습니다 (가장 낮은 MSE 및 MAE).
*   **기존 모델 대비 강점:**
    *   DLinear나 PatchTST와 같은 변수 독립적 모델은 변수 수가 적은 데이터셋에서 강점을 보였으나, Traffic($K=862$)이나 Covid-19($K=948$)와 같이 변수 수가 많은 복잡한 데이터셋에서는 성능이 저하되었습니다. MambaTS는 이 두 가지 유형의 데이터셋 모두에서 강건한 성능을 보였습니다.
    *   iTransformer는 복잡한 데이터셋에서 뛰어났지만 변수가 적은 데이터셋에서는 성능이 상대적으로 떨어지는 경향을 보였습니다.
*   **어블레이션 연구 (Ablation Studies):**
    *   **구성 요소 어블레이션:** VST, TMB, VAST는 각각 독립적으로 성능 향상에 기여했으며, 이들을 조합했을 때 최적의 성능을 달성했습니다. TMB 단독은 큰 성능 향상을 보였고, VST와 TMB의 조합은 TMB 단독보다는 약간 낮았는데, 이는 VST가 모든 변수를 포함하면서 TMB가 인과적 컨볼루션을 제거함에 따라 변수 순서에 더 민감해졌기 때문이며, VAST가 이 문제를 해결했습니다.
    *   **드롭아웃 어블레이션:** TMB의 드롭아웃은 과적합을 방지하고 검증 손실을 낮추는 데 도움이 되었으며, 0.2~0.3의 드롭아웃 비율에서 최적의 성능을 보였습니다.
    *   **VAST 어블레이션:** VPT 적용 후 "Random (100x)" 스캔이 "W/o VPT"보다 훨씬 우수한 성능을 보였고, VAST의 모의 담금질(SA) 기반 ATSP 솔버가 다른 휴리스틱 솔버(Greedy, Local Search, Lin & Kernighan)보다 일관적으로 뛰어난 성능을 보였습니다.
*   **룩백 윈도우 증가 효과:** MambaTS는 룩백 윈도우 길이가 길어질수록 일관되게 성능 이점을 얻었습니다. 이는 Transformer 기반 모델들이 긴 입력에서 이점을 얻지 못하거나 불연속적인 이득을 보이는 것과 대조적입니다.
*   **효율성 분석:** MambaTS는 $O(K \cdot \frac{L}{P})$의 계산 복잡도를 가지며, 이는 변수 수($K$)와 패치화된 시퀀스 길이($L/P$)에 선형적으로 비례하여 효율적인 장기 시계열 예측을 가능하게 합니다.

## 🧠 Insights & Discussion
*   MambaTS는 Mamba 모델이 LTSF에 적합하도록 성공적으로 개선했음을 보여줍니다. 기존 Mamba의 효율성은 유지하면서, LTSF 특유의 데이터 구조와 예측 요구사항에 맞춘 최적화를 통해 성능 문제를 해결했습니다.
*   **VST**는 다변량 시계열 데이터를 Mamba가 처리하기에 적합한 전역적인 시퀀스 형태로 효율적으로 변환합니다.
*   **TMB**에서 인과적 컨볼루션을 제거한 것은 VST로 인해 시퀀스의 지역적 인과성이 깨지기 때문에 합리적인 접근입니다. 드롭아웃은 과도한 정보 통합으로 인한 과적합을 효과적으로 완화했습니다.
*   **VPT와 VAST**는 다변량 시계열 예측에서 중요한 변수 간의 관계와 스캔 순서 문제를 혁신적으로 해결했습니다. 특히 VAST는 학습된 거리 행렬을 통해 최적의 순서를 찾는 아이디어가 SSM 기반 모델에 대한 새로운 시사점을 제공합니다.
*   MambaTS가 긴 룩백 윈도우를 효과적으로 활용할 수 있다는 점은 복잡한 장기 의존성 포착 능력에 대한 확신을 줍니다. 이는 2차 복잡도와 긴 시퀀스 처리의 어려움에 직면했던 Transformer 기반 모델에 대한 강력한 대안이 될 수 있습니다.
*   선형 계산 복잡도는 대규모 시계열 데이터 및 초장기 예측 시나리오에서 MambaTS의 실용성과 확장성을 크게 향상시킵니다.

## 📌 TL;DR
*   **문제:** Transformer는 LTSF에서 2차 복잡도와 순열 불변 편향 문제가 있으며, Mamba는 효율적이나 LTSF 성능은 아쉬웠습니다.
*   **제안 방법:** MambaTS는 Mamba를 LTSF에 맞게 개선했습니다. **VST**로 모든 변수의 정보를 통합하고, **TMB**에서 인과적 컨볼루션을 제거하고 드롭아웃을 추가하여 과적합을 방지합니다. 또한 **VPT**로 변수 순서 민감도를 완화하며, **VAST**로 변수 간 관계를 학습하고 ATSP 솔버로 최적의 스캔 순서를 찾아냅니다.
*   **주요 결과:** MambaTS는 8개 공개 데이터셋에서 선형 복잡도로 기존 SOTA 모델을 뛰어넘는 성능을 달성했으며, 긴 룩백 윈도우를 효과적으로 활용하고 과적합을 줄였습니다.