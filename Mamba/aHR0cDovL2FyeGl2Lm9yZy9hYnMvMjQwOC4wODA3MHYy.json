{
  "title": "MambaMIM: Pre-training Mamba with State Space Token Interpolation and\n  its Application to Medical Image Segmentation",
  "authors": "Fenghe Tang, Bingkun Nian, Yingtai Li, Zihang Jiang, Jie Yang, Wei Liu, S. Kevin Zhou",
  "year": 2024,
  "url": "http://arxiv.org/abs/2408.08070v2",
  "abstract": "Recently, the state space model Mamba has demonstrated efficient\nlong-sequence modeling capabilities, particularly for addressing long-sequence\nvisual tasks in 3D medical imaging. However, existing generative\nself-supervised learning methods have not yet fully unleashed Mamba's potential\nfor handling long-range dependencies because they overlook the inherent causal\nproperties of state space sequences in masked modeling. To address this\nchallenge, we propose a general-purpose pre-training framework called MambaMIM,\na masked image modeling method based on a novel TOKen-Interpolation strategy\n(TOKI) for the selective structure state space sequence, which learns causal\nrelationships of state space within the masked sequence. Further, MambaMIM\nintroduces a bottom-up 3D hybrid masking strategy to maintain a masking\nconsistency across different architectures and can be used on any single or\nhybrid Mamba architecture to enhance its multi-scale and long-range\nrepresentation capability. We pre-train MambaMIM on a large-scale dataset of\n6.8K CT scans and evaluate its performance across eight public medical\nsegmentation benchmarks. Extensive downstream experiments reveal the\nfeasibility and advancement of using Mamba for medical image pre-training. In\nparticular, when we apply the MambaMIM to a customized architecture that\nhybridizes MedNeXt and Vision Mamba, we consistently obtain the\nstate-of-the-art segmentation performance. The code is available at:\nhttps://github.com/FengheTan9/MambaMIM."
}