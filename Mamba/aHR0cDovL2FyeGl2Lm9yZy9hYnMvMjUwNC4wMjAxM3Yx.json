{
  "title": "Attention Mamba: Time Series Modeling with Adaptive Pooling Acceleration\n  and Receptive Field Enhancements",
  "authors": "Sijie Xiong, Shuqing Liu, Cheng Tang, Fumiya Okubo, Haoling Xiong, Atsushi Shimada",
  "year": 2025,
  "url": "http://arxiv.org/abs/2504.02013v1",
  "abstract": "\"This work has been submitted to the lEEE for possible publication. Copyright\nmay be transferred without noticeafter which this version may no longer be\naccessible.\" Time series modeling serves as the cornerstone of real-world\napplications, such as weather forecasting and transportation management.\nRecently, Mamba has become a promising model that combines near-linear\ncomputational complexity with high prediction accuracy in time series modeling,\nwhile facing challenges such as insufficient modeling of nonlinear dependencies\nin attention and restricted receptive fields caused by convolutions. To\novercome these limitations, this paper introduces an innovative framework,\nAttention Mamba, featuring a novel Adaptive Pooling block that accelerates\nattention computation and incorporates global information, effectively\novercoming the constraints of limited receptive fields. Furthermore, Attention\nMamba integrates a bidirectional Mamba block, efficiently capturing long-short\nfeatures and transforming inputs into the Value representations for attention\nmechanisms. Extensive experiments conducted on diverse datasets underscore the\neffectiveness of Attention Mamba in extracting nonlinear dependencies and\nenhancing receptive fields, establishing superior performance among leading\ncounterparts. Our codes will be available on GitHub."
}