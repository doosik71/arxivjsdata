{
  "title": "Mamba-ST: State Space Model for Efficient Style Transfer",
  "authors": "Filippo Botti, Alex Ergasti, Leonardo Rossi, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.10385v1",
  "abstract": "The goal of style transfer is, given a content image and a style source,\ngenerating a new image preserving the content but with the artistic\nrepresentation of the style source. Most of the state-of-the-art architectures\nuse transformers or diffusion-based models to perform this task, despite the\nheavy computational burden that they require. In particular, transformers use\nself- and cross-attention layers which have large memory footprint, while\ndiffusion models require high inference time. To overcome the above, this paper\nexplores a novel design of Mamba, an emergent State-Space Model (SSM), called\nMamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation\nto simulate the behavior of cross-attention layers, which are able to combine\ntwo separate embeddings into a single output, but drastically reducing memory\nusage and time complexity. We modified the Mamba's inner equations so to accept\ninputs from, and combine, two separate data streams. To the best of our\nknowledge, this is the first attempt to adapt the equations of SSMs to a vision\ntask like style transfer without requiring any other module like\ncross-attention or custom normalization layers. An extensive set of experiments\ndemonstrates the superiority and efficiency of our method in performing style\ntransfer compared to transformers and diffusion models. Results show improved\nquality in terms of both ArtFID and FID metrics. Code is available at\nhttps://github.com/FilippoBotti/MambaST."
}