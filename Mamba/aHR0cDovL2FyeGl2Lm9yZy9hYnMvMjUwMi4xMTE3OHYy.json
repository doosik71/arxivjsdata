{
  "title": "DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage\n  Object Detection",
  "authors": "A. Enes Doruk, Hasan F. Ates",
  "year": 2025,
  "url": "http://arxiv.org/abs/2502.11178v2",
  "abstract": "Recent 2D CNN-based domain adaptation approaches struggle with long-range\ndependencies due to limited receptive fields, making it difficult to adapt to\ntarget domains with significant spatial distribution changes. While\ntransformer-based domain adaptation methods better capture distant\nrelationships through self-attention mechanisms that facilitate more effective\ncross-domain feature alignment, their quadratic computational complexity makes\npractical deployment challenging for object detection tasks across diverse\ndomains. Inspired by the global modeling and linear computation complexity of\nthe Mamba architecture, we present the first domain-adaptive Mamba-based\none-stage object detection model, termed DA-Mamba. Specifically, we combine\nMamba's efficient state-space modeling with attention mechanisms to address\ndomain-specific spatial and channel-wise variations. Our design leverages\ndomain-adaptive spatial and channel-wise scanning within the Mamba block to\nextract highly transferable representations for efficient sequential\nprocessing, while cross-attention modules generate long-range, mixed-domain\nspatial features to enable robust soft alignment across domains. Besides,\nmotivated by the observation that hybrid architectures introduce feature noise\nin domain adaptation tasks, we propose an entropy-based knowledge distillation\nframework with margin ReLU, which adaptively refines multi-level\nrepresentations by suppressing irrelevant activations and aligning uncertainty\nacross source and target domains. Finally, to prevent overfitting caused by the\nmixed-up features generated through cross-attention mechanisms, we propose\nentropy-driven gating attention with random perturbations that simultaneously\nrefine target features and enhance model generalization."
}