{
  "title": "Is Mamba Effective for Time Series Forecasting?",
  "authors": "Zihan Wang, Fanheng Kong, Shi Feng, Ming Wang, Xiaocui Yang, Han Zhao, Daling Wang, Yifei Zhang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.11144v3",
  "abstract": "In the realm of time series forecasting (TSF), it is imperative for models to\nadeptly discern and distill hidden patterns within historical time series data\nto forecast future states. Transformer-based models exhibit formidable efficacy\nin TSF, primarily attributed to their advantage in apprehending these patterns.\nHowever, the quadratic complexity of the Transformer leads to low computational\nefficiency and high costs, which somewhat hinders the deployment of the TSF\nmodel in real-world scenarios. Recently, Mamba, a selective state space model,\nhas gained traction due to its ability to process dependencies in sequences\nwhile maintaining near-linear complexity. For TSF tasks, these characteristics\nenable Mamba to comprehend hidden patterns as the Transformer and reduce\ncomputational overhead compared to the Transformer. Therefore, we propose a\nMamba-based model named Simple-Mamba (S-Mamba) for TSF. Specifically, we\ntokenize the time points of each variate autonomously via a linear layer. A\nbidirectional Mamba layer is utilized to extract inter-variate correlations and\na Feed-Forward Network is set to learn temporal dependencies. Finally, the\ngeneration of forecast outcomes through a linear mapping layer. Experiments on\nthirteen public datasets prove that S-Mamba maintains low computational\noverhead and achieves leading performance. Furthermore, we conduct extensive\nexperiments to explore Mamba's potential in TSF tasks. Our code is available at\nhttps://github.com/wzhwzhwzh0921/S-D-Mamba.",
  "citation": 148
}