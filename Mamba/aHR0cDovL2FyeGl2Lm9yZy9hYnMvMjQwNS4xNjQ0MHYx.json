{
  "url": "http://arxiv.org/abs/2405.16440v1",
  "title": "MambaTS: Improved Selective State Space Models for Long-term Time Series\n  Forecasting",
  "authors": "Xiuding Cai, Yaoyao Zhu, Xueyao Wang, Yu Yao",
  "year": 2024,
  "abstract": "In recent years, Transformers have become the de-facto architecture for\nlong-term sequence forecasting (LTSF), but faces challenges such as quadratic\ncomplexity and permutation invariant bias. A recent model, Mamba, based on\nselective state space models (SSMs), has emerged as a competitive alternative\nto Transformer, offering comparable performance with higher throughput and\nlinear complexity related to sequence length. In this study, we analyze the\nlimitations of current Mamba in LTSF and propose four targeted improvements,\nleading to MambaTS. We first introduce variable scan along time to arrange the\nhistorical information of all the variables together. We suggest that causal\nconvolution in Mamba is not necessary for LTSF and propose the Temporal Mamba\nBlock (TMB). We further incorporate a dropout mechanism for selective\nparameters of TMB to mitigate model overfitting. Moreover, we tackle the issue\nof variable scan order sensitivity by introducing variable permutation\ntraining. We further propose variable-aware scan along time to dynamically\ndiscover variable relationships during training and decode the optimal variable\nscan order by solving the shortest path visiting all nodes problem during\ninference. Extensive experiments conducted on eight public datasets demonstrate\nthat MambaTS achieves new state-of-the-art performance.",
  "citation": 26
}