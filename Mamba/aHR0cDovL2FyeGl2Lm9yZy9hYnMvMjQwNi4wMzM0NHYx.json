{
  "title": "Audio Mamba: Bidirectional State Space Model for Audio Representation\n  Learning",
  "authors": "Mehmet Hamza Erol, Arda Senocak, Jiu Feng, Joon Son Chung",
  "year": 2024,
  "url": "http://arxiv.org/abs/2406.03344v1",
  "abstract": "Transformers have rapidly become the preferred choice for audio\nclassification, surpassing methods based on CNNs. However, Audio Spectrogram\nTransformers (ASTs) exhibit quadratic scaling due to self-attention. The\nremoval of this quadratic self-attention cost presents an appealing direction.\nRecently, state space models (SSMs), such as Mamba, have demonstrated potential\nin language and vision tasks in this regard. In this study, we explore whether\nreliance on self-attention is necessary for audio classification tasks. By\nintroducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based\nmodel for audio classification, we aim to address this question. We evaluate\nAuM on various audio datasets - comprising six different benchmarks - where it\nachieves comparable or better performance compared to well-established AST\nmodel.",
  "citation": 47
}