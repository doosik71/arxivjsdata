{
  "title": "LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image\n  Segmentation",
  "authors": "Weibin Liao, Yinghao Zhu, Xinyuan Wang, Chengwei Pan, Yasha Wang, Liantao Ma",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.05246v2",
  "abstract": "UNet and its variants have been widely used in medical image segmentation.\nHowever, these models, especially those based on Transformer architectures,\npose challenges due to their large number of parameters and computational\nloads, making them unsuitable for mobile health applications. Recently, State\nSpace Models (SSMs), exemplified by Mamba, have emerged as competitive\nalternatives to CNN and Transformer architectures. Building upon this, we\nemploy Mamba as a lightweight substitute for CNN and Transformer within UNet,\naiming at tackling challenges stemming from computational resource limitations\nin real medical settings. To this end, we introduce the Lightweight Mamba UNet\n(LightM-UNet) that integrates Mamba and UNet in a lightweight framework.\nSpecifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure\nMamba fashion to extract deep semantic features and model long-range spatial\ndependencies, with linear computational complexity. Extensive experiments\nconducted on two real-world 2D/3D datasets demonstrate that LightM-UNet\nsurpasses existing state-of-the-art literature. Notably, when compared to the\nrenowned nnU-Net, LightM-UNet achieves superior segmentation performance while\ndrastically reducing parameter and computation costs by 116x and 21x,\nrespectively. This highlights the potential of Mamba in facilitating model\nlightweighting. Our code implementation is publicly available at\nhttps://github.com/MrBlankness/LightM-UNet."
}