{
  "title": "Vision Mamba for Classification of Breast Ultrasound Images",
  "authors": "Ali Nasiri-Sarvi, Mahdi S. Hosseini, Hassan Rivaz",
  "year": 2024,
  "url": "http://arxiv.org/abs/2407.03552v2",
  "abstract": "Mamba-based models, VMamba and Vim, are a recent family of vision encoders\nthat offer promising performance improvements in many computer vision tasks.\nThis paper compares Mamba-based models with traditional Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) using the breast ultrasound BUSI\ndataset and Breast Ultrasound B dataset. Our evaluation, which includes\nmultiple runs of experiments and statistical significance analysis,\ndemonstrates that some of the Mamba-based architectures often outperform CNN\nand ViT models with statistically significant results. For example, in the B\ndataset, the best Mamba-based models have a 1.98\\% average AUC and a 5.0\\%\naverage Accuracy improvement compared to the best non-Mamba-based model in this\nstudy. These Mamba-based models effectively capture long-range dependencies\nwhile maintaining some inductive biases, making them suitable for applications\nwith limited data. The code is available at\n\\url{https://github.com/anasiri/BU-Mamba}",
  "citation": 12
}