{
  "url": "http://arxiv.org/abs/2405.11831v2",
  "title": "SSAMBA: Self-Supervised Audio Representation Learning with Mamba State\n  Space Model",
  "authors": "Siavash Shams, Sukru Samet Dindar, Xilin Jiang, Nima Mesgarani",
  "year": 2024,
  "abstract": "Transformers have revolutionized deep learning across various tasks,\nincluding audio representation learning, due to their powerful modeling\ncapabilities. However, they often suffer from quadratic complexity in both GPU\nmemory usage and computational inference time, affecting their efficiency.\nRecently, state space models (SSMs) like Mamba have emerged as a promising\nalternative, offering a more efficient approach by avoiding these complexities.\nGiven these advantages, we explore the potential of SSM-based models in audio\ntasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the\nfirst self-supervised, attention-free, and SSM-based model for audio\nrepresentation learning. SSAMBA leverages the bidirectional Mamba to capture\ncomplex audio patterns effectively. We incorporate a self-supervised\npretraining framework that optimizes both discriminative and generative\nobjectives, enabling the model to learn robust audio representations from\nlarge-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as\naudio classification, keyword spotting, and speaker identification. Our results\ndemonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram\nTransformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7%\nfaster in batch inference speed and 95.4% more memory-efficient than SSAST for\nthe tiny model size with an input token size of 22k. These efficiency gains,\ncombined with superior performance, underscore the effectiveness of SSAMBA's\narchitectural innovation, making it a compelling choice for a wide range of\naudio processing applications."
}