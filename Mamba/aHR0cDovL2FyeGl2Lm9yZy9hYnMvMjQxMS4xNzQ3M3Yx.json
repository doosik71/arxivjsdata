{
  "title": "TinyViM: Frequency Decoupling for Tiny Hybrid Vision Mamba",
  "authors": "Xiaowen Ma, Zhenliang Ni, Xinghao Chen",
  "year": 2024,
  "url": "http://arxiv.org/abs/2411.17473v1",
  "abstract": "Mamba has shown great potential for computer vision due to its linear\ncomplexity in modeling the global context with respect to the input length.\nHowever, existing lightweight Mamba-based backbones cannot demonstrate\nperformance that matches Convolution or Transformer-based methods. We observe\nthat simply modifying the scanning path in the image domain is not conducive to\nfully exploiting the potential of vision Mamba. In this paper, we first perform\ncomprehensive spectral and quantitative analyses, and verify that the Mamba\nblock mainly models low-frequency information under Convolution-Mamba hybrid\narchitecture. Based on the analyses, we introduce a novel Laplace mixer to\ndecouple the features in terms of frequency and input only the low-frequency\ncomponents into the Mamba block. In addition, considering the redundancy of the\nfeatures and the different requirements for high-frequency details and\nlow-frequency global information at different stages, we introduce a frequency\nramp inception, i.e., gradually reduce the input dimensions of the\nhigh-frequency branches, so as to efficiently trade-off the high-frequency and\nlow-frequency components at different layers. By integrating mobile-friendly\nconvolution and efficient Laplace mixer, we build a series of tiny hybrid\nvision Mamba called TinyViM. The proposed TinyViM achieves impressive\nperformance on several downstream tasks including image classification,\nsemantic segmentation, object detection and instance segmentation. In\nparticular, TinyViM outperforms Convolution, Transformer and Mamba-based models\nwith similar scales, and the throughput is about 2-3 times higher than that of\nother Mamba-based models. Code is available at\nhttps://github.com/xwmaxwma/TinyViM."
}