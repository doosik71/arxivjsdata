{
  "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan",
  "authors": "Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, Chang Xu",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.09338v1",
  "abstract": "Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba."
}