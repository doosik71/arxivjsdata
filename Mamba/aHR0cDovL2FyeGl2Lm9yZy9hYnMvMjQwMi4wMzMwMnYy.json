{
  "title": "Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining",
  "authors": "Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, Shanshan Wang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2402.03302v2",
  "abstract": "Accurate medical image segmentation demands the integration of multi-scale\ninformation, spanning from local features to global dependencies. However, it\nis challenging for existing methods to model long-range global information,\nwhere convolutional neural networks (CNNs) are constrained by their local\nreceptive fields, and vision transformers (ViTs) suffer from high quadratic\ncomplexity of their attention mechanism. Recently, Mamba-based models have\ngained great attention for their impressive ability in long sequence modeling.\nSeveral studies have demonstrated that these models can outperform popular\nvision models in various tasks, offering higher accuracy, lower memory\nconsumption, and less computational burden. However, existing Mamba-based\nmodels are mostly trained from scratch and do not explore the power of\npretraining, which has been proven to be quite effective for data-efficient\nmedical image analysis. This paper introduces a novel Mamba-based model,\nSwin-UMamba, designed specifically for medical image segmentation tasks,\nleveraging the advantages of ImageNet-based pretraining. Our experimental\nresults reveal the vital role of ImageNet-based training in enhancing the\nperformance of Mamba-based models. Swin-UMamba demonstrates superior\nperformance with a large margin compared to CNNs, ViTs, and latest Mamba-based\nmodels. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba\noutperforms its closest counterpart U-Mamba_Enc by an average score of 2.72%.",
  "citation": 298
}