{
  "title": "Mamba-in-Mamba: Centralized Mamba-Cross-Scan in Tokenized Mamba Model\n  for Hyperspectral Image Classification",
  "authors": "Weilian Zhou, Sei-Ichiro Kamata, Haipeng Wang, Man-Sing Wong,  Huiying,  Hou",
  "year": 2024,
  "url": "http://arxiv.org/abs/2405.12003v4",
  "abstract": "Hyperspectral image (HSI) classification is pivotal in the remote sensing\n(RS) field, particularly with the advancement of deep learning techniques.\nSequential models, adapted from the natural language processing (NLP) field\nsuch as Recurrent Neural Networks (RNNs) and Transformers, have been tailored\nto this task, offering a unique viewpoint. However, several challenges persist\n1) RNNs struggle with centric feature aggregation and are sensitive to\ninterfering pixels, 2) Transformers require significant computational resources\nand often underperform with limited HSI training samples, and 3) Current\nscanning methods for converting images into sequence-data are simplistic and\ninefficient. In response, this study introduces the innovative Mamba-in-Mamba\n(MiM) architecture for HSI classification, the first attempt of deploying State\nSpace Model (SSM) in this task. The MiM model includes 1) A novel centralized\nMamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2)\nA Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask\n(GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) for\nenhanced feature generation and concentration, and 3) A Weighted MCS Fusion\n(WMF) module coupled with a Multi-Scale Loss Design to improve decoding\nefficiency. Experimental results from three public HSI datasets with fixed and\ndisjoint training-testing samples demonstrate that our method outperforms\nexisting baselines and state-of-the-art approaches, highlighting its efficacy\nand potential in HSI applications."
}