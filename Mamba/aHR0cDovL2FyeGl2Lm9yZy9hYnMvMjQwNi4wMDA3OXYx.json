{
  "title": "Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence\n  Modeling",
  "authors": "Sili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo, Hechang Chen, Lichao Sun, Bo Yang",
  "year": 2024,
  "url": "http://arxiv.org/abs/2406.00079v1",
  "abstract": "Recent works have shown the remarkable superiority of transformer models in\nreinforcement learning (RL), where the decision-making problem is formulated as\nsequential generation. Transformer-based agents could emerge with\nself-improvement in online environments by providing task contexts, such as\nmultiple trajectories, called in-context RL. However, due to the quadratic\ncomputation complexity of attention in transformers, current in-context RL\nmethods suffer from huge computational costs as the task horizon increases. In\ncontrast, the Mamba model is renowned for its efficient ability to process\nlong-term dependencies, which provides an opportunity for in-context RL to\nsolve tasks that require long-term memory. To this end, we first implement\nDecision Mamba (DM) by replacing the backbone of Decision Transformer (DT).\nThen, we propose a Decision Mamba-Hybrid (DM-H) with the merits of transformers\nand Mamba in high-quality prediction and long-term memory. Specifically, DM-H\nfirst generates high-value sub-goals from long-term memory through the Mamba\nmodel. Then, we use sub-goals to prompt the transformer, establishing\nhigh-quality predictions. Experimental results demonstrate that DM-H achieves\nstate-of-the-art in long and short-term tasks, such as D4RL, Grid World, and\nTmaze benchmarks. Regarding efficiency, the online testing of DM-H in the\nlong-term task is 28$\\times$ times faster than the transformer-based baselines.",
  "citation": 14
}