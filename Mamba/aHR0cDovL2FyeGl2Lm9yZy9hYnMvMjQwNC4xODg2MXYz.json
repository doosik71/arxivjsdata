{
  "url": "http://arxiv.org/abs/2404.18861v3",
  "title": "Visual Mamba: A Survey and New Outlooks",
  "authors": "Rui Xu, Shu Yang, Yihui Wang, Yu Cai, Bo Du, Hao Chen",
  "year": 2024,
  "abstract": "Mamba, a recent selective structured state space model, excels in long\nsequence modeling, which is vital in the large model era. Long sequence\nmodeling poses significant challenges, including capturing long-range\ndependencies within the data and handling the computational demands caused by\ntheir extensive length. Mamba addresses these challenges by overcoming the\nlocal perception limitations of convolutional neural networks and the quadratic\ncomputational complexity of Transformers. Given its advantages over these\nmainstream foundation architectures, Mamba exhibits great potential to be a\nvisual foundation architecture. Since January 2024, Mamba has been actively\napplied to diverse computer vision tasks, yielding numerous contributions. To\nhelp keep pace with the rapid advancements, this paper reviews visual Mamba\napproaches, analyzing over 200 papers. This paper begins by delineating the\nformulation of the original Mamba model. Subsequently, it delves into\nrepresentative backbone networks, and applications categorized using different\nmodalities, including image, video, point cloud, and multi-modal data.\nParticularly, we identify scanning techniques as critical for adapting Mamba to\nvision tasks, and decouple these scanning techniques to clarify their\nfunctionality and enhance their flexibility across various applications.\nFinally, we discuss the challenges and future directions, providing insights\ninto new outlooks in this fast evolving area. A comprehensive list of visual\nMamba models reviewed in this work is available at\nhttps://github.com/Ruixxxx/Awesome-Vision-Mamba-Models."
}