{
  "title": "Sparse Mamba: Introducing Controllability, Observability, And Stability\n  To Structural State Space Models",
  "authors": "Emadeldeen Hamdan, Hongyi Pan, Ahmet Enis Cetin",
  "year": 2024,
  "url": "http://arxiv.org/abs/2409.00563v3",
  "abstract": "Structured state space models' (SSMs) development in recent studies, such as\nMamba and Mamba2, outperformed and solved the computational inefficiency of\ntransformers and large language models at small to medium scale. In this work,\nwe introduce the concept of controllability and observability to the original\nMamba SSM's architecture in our Sparse-Mamba (S-Mamba) for natural language\nprocessing (NLP) applications. Moreover, we reinforce stability on the $nxn$\n$A$ matrix on Mmaba2. The Mamba SSMs architecture drops the need for attention\nlayers or multilayer perception blocks in transformers. However, current Mamba\nmodels lack reinforcement of controllability in state-space equations for\ncomputing the $A$, $B$, $C$, and $D$ matrices at each time step, leading to\nincreased complexity and computational costs. Furthermore, the $A$ matrix in\nMamba2 is not always stable. We demonstrate a reduction of parameters compared\nto the first published Mamba and Mamba2. We showcase an improvement in\nperplexity by 5\\% and a decrease in training time by 3\\% after reinforcing\ncontrollability and observability on the original Mamba architecture in our\nproposed S-Mamba. We further enforce stability on the $A$ matrix in Mamba2 to\nimprove the loss and perplexity of the model. The controllable and stable $n\n\\times n$ state matrix $A$ is sparse, and it has only $n$ free parameters. Our\nnovel approach will ensure controllable/observable and stable SSMs, which will\nbe the gate key for Mamba3."
}