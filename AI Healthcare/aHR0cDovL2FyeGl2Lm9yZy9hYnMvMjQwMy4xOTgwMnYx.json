{
  "title": "Developing Healthcare Language Model Embedding Spaces",
  "authors": "Niall Taylor, Dan Schofield, Andrey Kormilitzin, Dan W Joyce, Alejo Nevado-Holgado",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.19802v1",
  "abstract": "Pre-trained Large Language Models (LLMs) often struggle on out-of-domain\ndatasets like healthcare focused text. We explore specialized pre-training to\nadapt smaller LLMs to different healthcare datasets. Three methods are\nassessed: traditional masked language modeling, Deep Contrastive Learning for\nUnsupervised Textual Representations (DeCLUTR), and a novel pre-training\nobjective utilizing metadata categories from the healthcare settings. These\nschemes are evaluated on downstream document classification tasks for each\ndataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification\ntasks, delivering strong performance from limited labeled data and with fewer\nmodel parameter updates required. While metadata-based pre-training does not\nfurther improve classifications across the datasets, it yields interesting\nembedding cluster separability. All domain adapted LLMs outperform their\npublicly available general base LLM, validating the importance of\ndomain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational\nbudgets, an essential capability for responsible and sustainable deployment in\nlocal healthcare settings. We provide pre-training guidelines for specialized\nhealthcare LLMs, motivate continued inquiry into contrastive objectives, and\ndemonstrates adaptation techniques to align small LLMs with privacy-sensitive\nmedical tasks.",
  "citation": 5
}