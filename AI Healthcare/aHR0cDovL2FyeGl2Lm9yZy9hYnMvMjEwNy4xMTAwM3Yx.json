{
  "title": "Model Selection for Offline Reinforcement Learning: Practical\n  Considerations for Healthcare Settings",
  "authors": "Shengpu Tang, Jenna Wiens",
  "year": 2021,
  "url": "http://arxiv.org/abs/2107.11003v1",
  "abstract": "Reinforcement learning (RL) can be used to learn treatment policies and aid\ndecision making in healthcare. However, given the need for generalization over\ncomplex state/action spaces, the incorporation of function approximators (e.g.,\ndeep neural networks) requires model selection to reduce overfitting and\nimprove policy performance at deployment. Yet a standard validation pipeline\nfor model selection requires running a learned policy in the actual\nenvironment, which is often infeasible in a healthcare setting. In this work,\nwe investigate a model selection pipeline for offline RL that relies on\noff-policy evaluation (OPE) as a proxy for validation performance. We present\nan in-depth analysis of popular OPE methods, highlighting the additional\nhyperparameters and computational requirements (fitting/inference of auxiliary\nmodels) when used to rank a set of candidate policies. We compare the utility\nof different OPE methods as part of the model selection pipeline in the context\nof learning to treat patients with sepsis. Among all the OPE methods we\nconsidered, fitted Q evaluation (FQE) consistently leads to the best validation\nranking, but at a high computational cost. To balance this trade-off between\naccuracy of ranking and computational efficiency, we propose a simple two-stage\napproach to accelerate model selection by avoiding potentially unnecessary\ncomputation. Our work serves as a practical guide for offline RL model\nselection and can help RL practitioners select policies using real-world\ndatasets. To facilitate reproducibility and future extensions, the code\naccompanying this paper is available online at\nhttps://github.com/MLD3/OfflineRL_ModelSelection."
}