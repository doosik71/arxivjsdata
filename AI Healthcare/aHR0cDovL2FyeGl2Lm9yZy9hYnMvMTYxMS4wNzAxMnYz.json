{
  "url": "http://arxiv.org/abs/1611.07012v3",
  "title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning",
  "authors": "Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F. Stewart, Jimeng Sun",
  "year": 2016,
  "abstract": "Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts.",
  "citation": 923
}