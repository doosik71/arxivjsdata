{
  "title": "The Security of Deep Learning Defences for Medical Imaging",
  "authors": "Moshe Levy, Guy Amit, Yuval Elovici, Yisroel Mirsky",
  "year": 2022,
  "url": "http://arxiv.org/abs/2201.08661v1",
  "abstract": "Deep learning has shown great promise in the domain of medical image\nanalysis. Medical professionals and healthcare providers have been adopting the\ntechnology to speed up and enhance their work. These systems use deep neural\nnetworks (DNN) which are vulnerable to adversarial samples; images with\nimperceivable changes that can alter the model's prediction. Researchers have\nproposed defences which either make a DNN more robust or detect the adversarial\nsamples before they do harm. However, none of these works consider an informed\nattacker which can adapt to the defence mechanism. We show that an informed\nattacker can evade five of the current state of the art defences while\nsuccessfully fooling the victim's deep learning model, rendering these defences\nuseless. We then suggest better alternatives for securing healthcare DNNs from\nsuch attacks: (1) harden the system's security and (2) use digital signatures.",
  "citation": 2
}