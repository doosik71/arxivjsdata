{
  "title": "MIMIC-IF: Interpretability and Fairness Evaluation of Deep Learning\n  Models on MIMIC-IV Dataset",
  "authors": "Chuizheng Meng, Loc Trinh, Nan Xu, Yan Liu",
  "year": 2021,
  "url": "http://arxiv.org/abs/2102.06761v1",
  "abstract": "The recent release of large-scale healthcare datasets has greatly propelled\nthe research of data-driven deep learning models for healthcare applications.\nHowever, due to the nature of such deep black-boxed models, concerns about\ninterpretability, fairness, and biases in healthcare scenarios where human\nlives are at stake call for a careful and thorough examinations of both\ndatasets and models. In this work, we focus on MIMIC-IV (Medical Information\nMart for Intensive Care, version IV), the largest publicly available healthcare\ndataset, and conduct comprehensive analyses of dataset representation bias as\nwell as interpretability and prediction fairness of deep learning models for\nin-hospital mortality prediction. In terms of interpretabilty, we observe that\n(1) the best performing interpretability method successfully identifies\ncritical features for mortality prediction on various prediction models; (2)\ndemographic features are important for prediction. In terms of fairness, we\nobserve that (1) there exists disparate treatment in prescribing mechanical\nventilation among patient groups across ethnicity, gender and age; (2) all of\nthe studied mortality predictors are generally fair while the IMV-LSTM\n(Interpretable Multi-Variable Long Short-Term Memory) model provides the most\naccurate and unbiased predictions across all protected groups. We further draw\nconcrete connections between interpretability methods and fairness metrics by\nshowing how feature importance from interpretability methods can be beneficial\nin quantifying potential disparities in mortality predictors."
}