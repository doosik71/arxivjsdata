{
  "title": "Transformers in Healthcare: A Survey",
  "authors": "Subhash Nerella, Sabyasachi Bandyopadhyay, Jiaqing Zhang, Miguel Contreras, Scott Siegel, Aysegul Bumin, Brandon Silva, Jessica Sena, Benjamin Shickel, Azra Bihorac, Kia Khezeli, Parisa Rashidi",
  "year": 2023,
  "url": "http://arxiv.org/abs/2307.00067v1",
  "abstract": "With Artificial Intelligence (AI) increasingly permeating various aspects of\nsociety, including healthcare, the adoption of the Transformers neural network\narchitecture is rapidly changing many applications. Transformer is a type of\ndeep learning architecture initially developed to solve general-purpose Natural\nLanguage Processing (NLP) tasks and has subsequently been adapted in many\nfields, including healthcare. In this survey paper, we provide an overview of\nhow this architecture has been adopted to analyze various forms of data,\nincluding medical imaging, structured and unstructured Electronic Health\nRecords (EHR), social media, physiological signals, and biomolecular sequences.\nThose models could help in clinical diagnosis, report generation, data\nreconstruction, and drug/protein synthesis. We identified relevant studies\nusing the Preferred Reporting Items for Systematic Reviews and Meta-Analyses\n(PRISMA) guidelines. We also discuss the benefits and limitations of using\ntransformers in healthcare and examine issues such as computational cost, model\ninterpretability, fairness, alignment with human values, ethical implications,\nand environmental impact."
}