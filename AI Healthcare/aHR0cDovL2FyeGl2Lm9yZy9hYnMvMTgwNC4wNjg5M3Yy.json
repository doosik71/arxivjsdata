{
  "title": "A Study on Overfitting in Deep Reinforcement Learning",
  "authors": "Chiyuan Zhang, Oriol Vinyals, Remi Munos, Samy Bengio",
  "year": 2018,
  "url": "http://arxiv.org/abs/1804.06893v2",
  "abstract": "Recent years have witnessed significant progresses in deep Reinforcement\nLearning (RL). Empowered with large scale neural networks, carefully designed\narchitectures, novel training algorithms and massively parallel computing\ndevices, researchers are able to attack many challenging RL problems. However,\nin machine learning, more training power comes with a potential risk of more\noverfitting. As deep RL techniques are being applied to critical problems such\nas healthcare and finance, it is important to understand the generalization\nbehaviors of the trained agents. In this paper, we conduct a systematic study\nof standard RL agents and find that they could overfit in various ways.\nMoreover, overfitting could happen \"robustly\": commonly used techniques in RL\nthat add stochasticity do not necessarily prevent or detect overfitting. In\nparticular, the same agents and learning algorithms could have drastically\ndifferent test performance, even when all of them achieve optimal rewards\nduring training. The observations call for more principled and careful\nevaluation protocols in RL. We conclude with a general discussion on\noverfitting in RL and a study of the generalization behaviors from the\nperspective of inductive bias."
}