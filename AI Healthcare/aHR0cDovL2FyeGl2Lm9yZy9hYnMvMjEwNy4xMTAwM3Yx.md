# Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings

Shengpu Tang, Jenna Wiens

## 🧩 Problem to Solve

헬스케어 분야에서 강화 학습 (RL)을 사용하여 치료 정책을 학습하고 의사 결정을 돕는 것은 중요하지만, 복잡한 상태/행동 공간에 대한 일반화를 위해 함수 근사기(예: 심층 신경망)를 사용할 경우 과적합 방지 및 정책 성능 향상을 위한 모델 선택이 필수적입니다. 그러나 표준 검증 파이프라인은 학습된 정책을 실제 환경에서 실행해야 하는데, 이는 헬스케어 환경에서 비실용적이거나 위험할 수 있습니다. 오프라인 RL은 잘 정립된 훈련-검증 프레임워크가 부족합니다. 오프폴리시 평가(OPE)가 검증 성능의 대안으로 사용될 수 있지만, 다양한 OPE 방법의 유용성과 이들이 추가 하이퍼파라미터 및 계산 요구 사항을 수반한다는 점 때문에 실제 적용에 어려움이 있습니다.

## ✨ Key Contributions

- 후보 정책의 검증 성능 추정을 위한 네 가지 인기 있는 OPE 방법(WIS, AM, FQE, WDR)을 분석하고, 이들의 추가 하이퍼파라미터, 보조 모델, 계산 요구 사항을 강조했습니다.
- 패혈증 환자 치료를 위한 RL 정책 학습 문제에서 모델 선택을 위한 OPE 검증 성능 사용의 효과를 실증적으로 평가하고 실제 고려 사항을 다루었습니다.
- 계산 효율성과 정책 성능의 균형을 맞추기 위해 OPE 추정기를 효과적으로 결합하는 간단한 2단계 선택 접근 방식을 제안했습니다.

## 📎 Related Works

- Prasad et al. (2017)의 기계 환기 이탈 문제, Komorowski et al. (2018)의 패혈증 치료, Futoma et al. (2020a,b)의 ICU 저혈압 관리 등 임상 치료 분야에서의 강화 학습 (RL) 적용 연구.
- Mnih et al. (2015), Silver et al. (2017)과 같이 환경과 상호작용하는 온라인 RL의 성공 사례.
- 의료 환경에서 주로 접하는 오프라인 데이터의 중요성과 오프라인 RL 기술의 필요성 (Levine et al., 2020).
- OPE를 사용한 하이퍼파라미터 선택에 관한 연구 (Paine et al., 2020; Fu et al., 2021).
- 오프라인 RL 모델 선택 및 하이퍼파라미터 튜닝에 대한 기타 연구들 (Fard and Pineau, 2010; Farahmand and Szepesvári, 2011; Irpan et al., 2019; Lee et al., 2020; Kuzborskij et al., 2021; Xie and Jiang, 2021).

## 🛠️ Methodology

- **모델 선택 프레임워크**: 관측 데이터셋 $D_{obs}$를 훈련 $D_{train}$과 검증 $D_{val}$로 분할합니다. $D_{train}$으로 후보 정책 세트 $\{\pi_k\}_{k=1}^K$를 학습하고, $D_{val}$에 OPE를 적용하여 각 후보 정책의 가치를 추정하여 최적의 정책을 선택합니다.

- **탐구된 OPE 방법**:

  - **가중치 기반 중요도 샘플링 (WIS, Weighted Importance Sampling)**:
    - **설명**: 평가 정책과 행동 정책 간의 차이를 보정하기 위해 에피소드에 중요도 가중치 $\rho_{1:t}$를 부여하여 정책 가치를 추정합니다.
    - **하이퍼파라미터**: 정책 완화 파라미터 $\epsilon$ (예: $\tilde{\pi}(a|s) = 1_{a=\pi(s)}(1-\epsilon) + 1_{a \neq \pi(s)}(\frac{\epsilon}{|A|-1})$).
    - **보조 모델**: 행동 정책 $\hat{\pi}_b(a|s)$를 데이터로부터 추정합니다.
    - **계산 비용**: 모델 학습 및 추론에 각각 $O(N)$의 복잡도를 가지며, 후보 정책 수 $K$에 대해 상각 가능합니다.
  - **근사 모델 (AM, Approximate Model)**:
    - **설명**: 데이터로부터 환경 모델($\hat{p}(s'|s,a)$, $\hat{r}(s,a)$ 등)을 학습하고, 학습된 모델을 사용하여 정책 가치를 평가합니다 (주로 몬테카를로 롤아웃을 통해).
    - **하이퍼파라미터**: 롤아웃 길이 $H$.
    - **보조 모델**: 전이 역학 $\hat{p}(s'|s,a)$ 및 보상 함수 $\hat{r}(s,a)$를 학습합니다.
    - **계산 비용**: 모델 학습에 $O(N)$, $K$개의 후보 정책에 대한 추론에 $O(HmK)$의 복잡도를 가집니다.
  - **적합 Q 평가 (FQE, Fitted Q Evaluation)**:
    - **설명**: 정책 $\pi$의 Q-함수 $\hat{Q}_{\pi}^{FQE}(s,a)$를 벨만 방정식 반복과 지도 학습을 통해 관측 데이터로부터 직접 학습합니다.
    - **하이퍼파라미터**: FQE 반복 횟수 $H$.
    - **보조 모델**: 각 후보 정책 $\pi_k$에 대해 $H$개의 Q-네트워크 $\{\tilde{Q}_h\}_{h=1}^H$ 시퀀스를 학습합니다.
    - **계산 비용**: $K$개의 정책 각각에 대해 $H$번의 학습이 필요하므로 학습에 $O(HNK)$, 추론에 $O(mK)$의 복잡도를 가집니다. 가장 계산 비용이 높습니다.
  - **가중치 이중 강건 추정기 (WDR, Weighted Doubly Robust)**:
    - **설명**: 중요도 샘플링과 가치 함수 추정치를 결합한 하이브리드 방법입니다.
    - **하이퍼파라미터**: $\epsilon$과 $H$.
    - **보조 모델**: WIS, AM, 또는 FQE의 모델에 의존합니다.
    - **계산 비용**: WDR-AM은 $O(|A|^H HNK)$로 계산 비용이 매우 높아 비실용적입니다. WDR-FQE는 추론에 $O(NK)$의 복잡도를 가집니다.

- **2단계 선택 접근 방식**:
  1. **1단계**: 계산 효율적인 OPE (예: WIS)를 사용하여 전체 후보 정책 중 유망한 하위 집합 ($\alpha$ 비율)을 식별합니다.
  2. **2단계**: 더 정확하지만 비용이 많이 드는 OPE (예: FQE)를 사용하여 가지치기된 하위 집합에서 최종 정책을 선택합니다.
  - **하이퍼파라미터**: 하위 집합 크기 $\alpha$. 이 값은 정확도와 계산 비용 사이의 균형을 조절합니다.

## 📊 Results

- **OPE 방법 비교**:

  - FQE는 가장 우수한 순위 품질(Spearman $\rho = 0.89$)과 가장 낮은 후회도(regret)를 보였으며, 참 정책 가치에 가장 가까운 추정치를 제공했습니다 (Figure 2).
  - WIS는 높은 분산을 보였으나, 정책 가치 순위에서 상대적으로 높은 $\rho = 0.74$를 기록하며 낮은 후회도를 달성했습니다.
  - AM은 정책 가치를 일관되게 과소평가했지만, 전반적인 순위는 합리적인 수준($\rho = 0.66$)이었습니다.
  - WDR-FQE는 WIS와 FQE보다 낮은 순위 품질($\rho = 0.64$)을 보였으며, 때로는 매우 높은 후회도를 나타냈습니다. 이는 하이브리드 방법이 항상 구성 요소를 능가하지 않을 수 있음을 시사합니다.
  - FQI 가치 추정치와 RMS-TDE는 모델 선택을 위한 검증 지표로 부적합했습니다 ($\rho < 0.3$, 높은 후회도).

- **2단계 선택 접근 방식**:

  - 단순 평균(점수 또는 순위) 방식은 WDR-FQE보다 우수했지만, 전체 후보 정책에 대해 두 OPE를 모두 계산해야 하므로 계산 비용이 높았습니다.
  - 제안된 2단계 접근 방식은 FQE와 유사하거나 더 낮은 후회도를 달성하면서 (특히 $\alpha=24$ 또는 $48$에서 최적), 상당한 계산 비용 절감 효과를 보였습니다 (Figure 3).

- **민감도 분석**:
  - **OPE 하이퍼파라미터**: AM은 롤아웃 길이 $H$ 값에 가장 강건했지만, 다른 OPE 방법보다 후회도가 높았습니다. WIS와 FQE는 $\epsilon$ 또는 $H$ 값에 민감했지만, 경험적 규칙에 따른 적절한 값에서 낮은 후회도를 보였습니다 (Figure 4).
  - **검증 데이터 샘플 크기**: 모든 OPE 방법은 더 큰 검증 데이터셋에서 낮은 후회도를 보였습니다. 2단계 접근 방식은 합리적인 크기의 데이터셋(1,000 에피소드 이상)에서 FQE와 유사한 성능을 유지했습니다 (Figure 5, 왼쪽).
  - **행동 정책 및 탐색 정도**: 무작위(탐색적인) 행동 정책이 일반적으로 더 낮은 후회도를 보였습니다. 탐색이 적은($\epsilon$-greedy) 정책 또는 혼합 정책 환경에서도 WIS는 큰 영향을 받지 않았고, 2단계 선택 방식은 계산 비용을 줄이면서 FQE의 후회도 성능을 유지하거나 능가했습니다 (Figure 5, 오른쪽).

## 🧠 Insights & Discussion

- **함의**: 시뮬레이터 사용이 어려운 헬스케어 분야에서 오프라인 RL 모델 선택을 위한 OPE는 유망한 해결책입니다. FQE가 일반적으로 가장 정확한 순위를 제공하며, 제안된 2단계 접근 방식은 정확도와 계산 비용 사이에서 좋은 균형을 이룹니다. 이 연구는 실제 데이터셋을 사용하여 정책을 선택하는 RL 실무자들에게 실용적인 가이드를 제공하며, 재현성을 높이고 공정한 비교를 가능하게 합니다.
- **한계**:
  - 모의 RL 작업(패혈증 시뮬레이터)에서 실험 수행: 실제 데이터의 모든 미묘한 차이(이산 행동, 짧은 기간, 희소 보상, 노이즈 없는 관측, 결측치 없음)를 포착하지 못할 수 있습니다.
  - 제한적인 OPE 방법 비교: 헬스케어 작업에 일반적으로 사용되는 4가지 OPE 방법만 고려했습니다.
  - 검증 데이터 사용: 보조 모델 학습 및 OPE 추정치 계산에 동일한 검증 데이터를 사용하여 OPE의 이론적 보장에 필요한 독립성 가정을 위반할 가능성이 있습니다.
  - 고급 OPE "트릭" 미적용: 중요도 가중치 절단이나 정교한 AM 모델 등을 구현하지 않았습니다.
  - 바닐라 RL 알고리즘 사용: 분포 변화를 직접적으로 다루지 않는 FQI를 사용했으며, 이 프레임워크는 분포 변화를 고려하는 OPE 알고리즘으로 확장될 수 있습니다.
- **향후 연구**: 더 많은 헬스케어 RL 문제 탐구, 다른 OPE 방법(예: 주변화된 중요도 샘플링) 조사, OPE에서 분포 변화를 처리하도록 프레임워크 확장.

## 📌 TL;DR

**문제**: 헬스케어 분야 오프라인 강화 학습에서 모델 선택은 실제 환경에서의 검증이 어렵고 위험하기 때문에 도전적입니다. 대안으로 사용되는 오프폴리시 평가(OPE)는 자체적인 하이퍼파라미터와 높은 계산 비용 문제로 실용적 적용에 제약이 있습니다.

**방법**: OPE를 검증 성능의 대안으로 활용하는 오프라인 RL 모델 선택 프레임워크를 제안하고, WIS, AM, FQE, WDR의 네 가지 주요 OPE 방법을 분석했습니다. 또한, 계산 효율성과 정확도 균형을 위한 2단계 선택 접근 방식(효율적인 OPE로 후보군 가지치기 후, 정확한 OPE로 최종 선택)을 제안했습니다.

**결과**: FQE가 가장 정확한 정책 순위를 제공하지만 계산 비용이 높습니다. 제안된 2단계 접근 방식(예: WIS + FQE 조합)은 정확도와 계산 효율성 면에서 최적의 균형을 이루며, 실제 헬스케어 환경에서도 다른 전략들보다 일관되게 우수한 성능을 보였습니다.
