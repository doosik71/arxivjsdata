{
  "title": "Efficient Representation Learning for Healthcare with\n  Cross-Architectural Self-Supervision",
  "authors": "Pranav Singh, Jacopo Cirrone",
  "year": 2023,
  "url": "http://arxiv.org/abs/2308.10064v1",
  "abstract": "In healthcare and biomedical applications, extreme computational requirements\npose a significant barrier to adopting representation learning. Representation\nlearning can enhance the performance of deep learning architectures by learning\nuseful priors from limited medical data. However, state-of-the-art\nself-supervised techniques suffer from reduced performance when using smaller\nbatch sizes or shorter pretraining epochs, which are more practical in clinical\nsettings. We present Cross Architectural - Self Supervision (CASS) in response\nto this challenge. This novel siamese self-supervised learning approach\nsynergistically leverages Transformer and Convolutional Neural Networks (CNN)\nfor efficient learning. Our empirical evaluation demonstrates that CASS-trained\nCNNs and Transformers outperform existing self-supervised learning methods\nacross four diverse healthcare datasets. With only 1% labeled data for\nfinetuning, CASS achieves a 3.8% average improvement; with 10% labeled data, it\ngains 5.9%; and with 100% labeled data, it reaches a remarkable 10.13%\nenhancement. Notably, CASS reduces pretraining time by 69% compared to\nstate-of-the-art methods, making it more amenable to clinical implementation.\nWe also demonstrate that CASS is considerably more robust to variations in\nbatch size and pretraining epochs, making it a suitable candidate for machine\nlearning in healthcare applications."
}