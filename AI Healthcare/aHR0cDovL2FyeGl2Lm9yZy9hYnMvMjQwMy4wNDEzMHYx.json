{
  "title": "An Explainable AI Framework for Artificial Intelligence of Medical\n  Things",
  "authors": "Al Amin, Kamrul Hasan, Saleh Zein-Sabatto, Deo Chimba, Imtiaz Ahmed, Tariqul Islam",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.04130v1",
  "abstract": "The healthcare industry has been revolutionized by the convergence of\nArtificial Intelligence of Medical Things (AIoMT), allowing advanced\ndata-driven solutions to improve healthcare systems. With the increasing\ncomplexity of Artificial Intelligence (AI) models, the need for Explainable\nArtificial Intelligence (XAI) techniques become paramount, particularly in the\nmedical domain, where transparent and interpretable decision-making becomes\ncrucial. Therefore, in this work, we leverage a custom XAI framework,\nincorporating techniques such as Local Interpretable Model-Agnostic\nExplanations (LIME), SHapley Additive exPlanations (SHAP), and\nGradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for\nthe domain of AIoMT. The proposed framework enhances the effectiveness of\nstrategic healthcare methods and aims to instill trust and promote\nunderstanding in AI-driven medical applications. Moreover, we utilize a\nmajority voting technique that aggregates predictions from multiple\nconvolutional neural networks (CNNs) and leverages their collective\nintelligence to make robust and accurate decisions in the healthcare system.\nBuilding upon this decision-making process, we apply the XAI framework to brain\ntumor detection as a use case demonstrating accurate and transparent diagnosis.\nEvaluation results underscore the exceptional performance of the XAI framework,\nachieving high precision, recall, and F1 scores with a training accuracy of 99%\nand a validation accuracy of 98%. Combining advanced XAI techniques with\nensemble-based deep-learning (DL) methodologies allows for precise and reliable\nbrain tumor diagnoses as an application of AIoMT.",
  "citation": 19
}