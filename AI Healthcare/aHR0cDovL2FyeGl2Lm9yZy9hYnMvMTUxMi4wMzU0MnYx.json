{
  "url": "http://arxiv.org/abs/1512.03542v1",
  "title": "Distilling Knowledge from Deep Networks with Applications to Healthcare\n  Domain",
  "authors": "Zhengping Che, Sanjay Purushotham, Robinder Khemani, Yan Liu",
  "year": 2015,
  "abstract": "Exponential growth in Electronic Healthcare Records (EHR) has resulted in new\nopportunities and urgent needs for discovery of meaningful data-driven\nrepresentations and patterns of diseases in Computational Phenotyping research.\nDeep Learning models have shown superior performance for robust prediction in\ncomputational phenotyping tasks, but suffer from the issue of model\ninterpretability which is crucial for clinicians involved in decision-making.\nIn this paper, we introduce a novel knowledge-distillation approach called\nInterpretable Mimic Learning, to learn interpretable phenotype features for\nmaking robust prediction while mimicking the performance of deep learning\nmodels. Our framework uses Gradient Boosting Trees to learn interpretable\nfeatures from deep learning models such as Stacked Denoising Autoencoder and\nLong Short-Term Memory. Exhaustive experiments on a real-world clinical\ntime-series dataset show that our method obtains similar or better performance\nthan the deep learning models, and it provides interpretable phenotypes for\nclinical decision making.",
  "citation": 184
}