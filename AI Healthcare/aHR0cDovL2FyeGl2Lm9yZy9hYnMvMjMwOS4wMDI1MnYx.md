# Self-Attentive Transformer를 활용한 설명 가능한 의료 영상 진단: 헬스케어 분야 설명 가능한 AI(XAI)에 대한 고찰

Tin Lai

## 🧩 Problem to Solve

최근 인공지능(AI)은 의료 영상 분석 및 진단에서 상당한 발전을 이루었지만, 딥러닝 모델, 특히 Vision Transformer(ViT)는 종종 "블랙 박스"로 간주되어 의사 결정 과정을 이해하기 어렵다는 문제가 있습니다. 의료 진단과 같이 정확하고 신뢰할 수 있는 결과가 필수적인 중요한 애플리케이션에서는 이러한 모델의 불투명성이 신뢰성과 안전성에 대한 우려를 제기합니다. 따라서 AI 시스템의 의사 결정 과정을 전문가와 비전문가 모두에게 투명하고 이해하기 쉽게 만들어야 하는 것이 핵심적인 연구 문제입니다.

## ✨ Key Contributions

이 논문은 다음과 같은 주요 기여를 합니다:

- 의료 영상 분석을 위한 Vision Transformer(ViT)의 최근 발전 동향을 요약합니다.
- ViT 모델의 의사 결정 과정을 이해하기 위한 다양한 해석적 접근 방식(XAI)을 포괄적으로 검토합니다.
- 의료 진단 애플리케이션에서 AI 시스템의 투명성과 신뢰성을 높이는 데 XAI 기술의 중요성을 강조합니다.
- Grad-CAM, Saliency Maps, CAVs, DeepLift, LRP, Guided Backpropagation 등 주요 XAI 방법론을 소개하고 그 작동 원리를 설명합니다.
- ViT 기반 의료 영상 모델의 실제 적용 사례(COVID-19 진단, 암 분류 등)와 XAI를 통한 해석 가능성을 제시합니다.
- 해석 가능한 AI 모델의 현재 한계와 미래 연구 방향(통합 프레임워크, 불확실성 정량화 등)을 논의합니다.

## 📎 Related Works

이 논문은 인공지능이 의료 영상 분야에 적용된 다양한 선행 연구들을 참조하며, 특히 다음 분야에 중점을 둡니다:

- **의료 영상 AI 적용:** 피부, 흉부, 뇌, 간 등 다양한 신체 부위의 의료 영상 진단(예: 피부 병변, 폐렴, 뇌종양, 간암 진단)에 딥러닝 알고리즘이 활용된 사례들을 언급합니다.
- **초기 ViT 및 하이브리드 모델:** Vision Transformer(ViT)가 컴퓨터 비전 분야에서 성공을 거둔 이후, 의료 영상 분류에 ViT를 처음 도입한 TransMed와 같은 선구적인 연구들을 포함하여, ViT와 CNN을 결합한 하이브리드 아키텍처(예: Gene-Transformer, GasHis-Transformer)들을 다룹니다.
- **설명 가능한 AI(XAI) 일반론:** AI의 "블랙 박스" 문제를 해결하기 위한 XAI 분야의 개념과 중요성을 설명하며, LIME(Local Interpretable Model-Agnostic Explanations) 및 Grad-CAM과 같은 모델 불가지론적(model-agnostic) 또는 모델 특정(model-specific) 설명 기법들을 인용합니다.

## 🛠️ Methodology

이 논문은 ViT의 기본 구조를 설명한 후, ViT의 의사 결정 과정을 해석하기 위한 다양한 XAI 방법론들을 체계적으로 검토합니다.

**1. Vision Transformer(ViT) 기본 구조:**

- **패치 임베딩 (Patch Embedding):** 입력 영상 $x \in \mathbb{R}^{H \times W \times 3}$을 고정된 크기의 시퀀셜 패치 $x_p \in \mathbb{R}^{N \times (P^2 d)}$로 분할하고 선형적으로 임베딩합니다. 여기서 $(H, W)$는 영상의 높이와 너비, $(P, P)$는 패치 해상도, $d$는 출력 채널, $N = HW/P^2$는 영상 토큰의 수입니다.
- **트랜스포머 인코더 (Transformer Encoder):** 임베딩된 패치 토큰은 여러 개의 트랜스포머 인코더 블록에 입력됩니다. 각 인코더는 주로 두 가지 서브 레이어로 구성됩니다:
  - **멀티 헤드 셀프 어텐션 (Multi-Head Self-Attention, MSA):** 토큰들을 쿼리(Q), 키(K), 값(V) 벡터로 변환하고, 각 토큰 쌍 간의 유사도 점수를 계산하여 다른 토큰에 대한 주의(attention) 정도를 결정합니다. 수식은 다음과 같습니다:
    $$x'_{\ell} = \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{Q \cdot K^\top}{\sqrt{d}}\right) \cdot V$$
    여기서 $x'_{\ell}$은 $\ell$-번째 레이어의 MSA 출력입니다.
  - **MLP 블록 (MLP Block):** MSA의 출력은 레이어 정규화(Layer Normalisation, LN) 후 두 개의 완전 연결 레이어와 GELU 활성화 함수로 구성된 MLP 블록에 입력됩니다.
    $$x_{\ell} = \text{MLP}(\text{LN}(x'_{\ell})) + x'_{\ell}$$
    여기서 $x_{\ell}$은 $\ell$-번째 인코더 블록의 최종 출력입니다.

**2. 설명 가능성(Explainability) 방법론 (XAI):**

- **Gradient-weighted Class Activation Mapping (Grad-CAM):** 신경망의 결정에 가장 큰 영향을 미친 영상 영역을 시각화하는 기울기 기반 기법입니다. 마지막 컨볼루션 레이어로 전파된 기울기를 활용하여 각 뉴런에 중요도 값을 부여합니다.
- **Saliency Maps:** 영상 내 개별 픽셀이 최종 분류에 기여하는 정도를 보여주는 기울기 기반 시각화 기법입니다. 손실 함수의 입력 픽셀에 대한 기울기를 역전파하여 계산합니다.
- **Concept Activation Vectors (CAVs):** 사용자 정의 개념을 기반으로 신경망에 대한 전역적(global) 설명을 제공하는 기법입니다. 특정 개념과 관련된 인스턴스와 무관한 인스턴스로 이진 분류기를 훈련하여 CAV를 도출합니다.
- **Deep Learning Important FeaTures (DeepLift):** 뉴런 활성화의 차이를 참조 동작과 비교하여 기여도 점수를 결정하는 설명 기법입니다. 입력 특징이 출력 예측에 얼마나 기여하는지 정량화합니다.
- **Layer-wise Relevance Propagation (LRP):** 모델의 예측을 역방향으로 전파하여 뉴런이 받은 관련성(relevance)이 하위 레이어에 균등하게 분배되도록 하는 기법입니다.
- **Guided Backpropagation:** ReLU와 디컨볼루션을 결합한 설명 방법으로, 높은 레이어에서 오는 안내 신호(guidance signal)를 통해 음의 기울기 역전파를 방지하여 시각화된 활성화를 감소시키는 뉴런을 배제합니다.

## 📊 Results

이 논문은 다양한 의료 영상 애플리케이션에서 ViT와 XAI의 결합이 어떻게 활용되는지를 보여주는 기존 연구들을 요약합니다.

- **COVID-19 진단:** ViT 기반 모델은 흉부 X선(CXR) 및 CT 영상에서 COVID-19를 진단하는 데 우수한 성능을 보였으며, Saliency Map과 같은 XAI 기법을 사용하여 모델의 예측에 기여하는 핵심 영역을 시각화했습니다 (예: xViTCOS 모델).
- **암 분류:**
  - **유방암:** ViT 기반 모델이 유방 초음파 영상에서 양성, 악성, 정상 범주를 분류하는 데 CNN보다 우수한 성능을 보였습니다.
  - **폐암:** Gene-Transformer와 같은 하이브리드 Transformer-CNN 아키텍처가 폐암 아형 분류에서 CNN 기준선보다 뛰어났습니다.
  - **위암:** Multi-scale GasHis-Transformer가 위암 조직 병리 영상 진단에서 강력한 일반화 능력을 입증했습니다.
  - **뇌종양:** TransMIL은 전체 슬라이드 영상(Whole Slide Image, WSI) 기반의 뇌종양 분류에서 형태학적 및 공간적 정보를 활용하여 기존 CNN 기반 방법보다 우수한 성능과 빠른 수렴을 보였습니다. GTN(Graph Transformer Network)은 그래프 기반 WSI 표현을 활용하여 폐암 진단에 효과적이었고 GraphCAM으로 주요 영역을 식별했습니다.
- **ViT 해석 가능성 평가:** Table 1에서는 다양한 트랜스포머 모델을 위한 상호 운용성(interoperability) 접근 방식들을 Pixel Accuracy, mAP, mF1, mIoU와 같은 지표로 요약합니다. Transformer Attribution, Generic Attribution, Token-wise Approximation과 같은 방법들이 Raw Attention이나 Rollout 같은 순진한(naïve) 방법보다 더 높은 성능을 보이며, 특히 Class-specific한 설명(예: Grad-CAM, Transformer Attribution)이 중요한 의료 진단에서 그 가치를 입증합니다.
- **의미 있는 시각화:** Cross-attention 맵은 U-Net Transformer 아키텍처에서 영상 분할 작업 시 다양한 레이어에서 모델이 어떤 영역에 집중하는지를 보여주며, 하위 레이어에서는 넓은 영역, 상위 레이어에서는 특정 고해상도 영역에 초점을 맞추는 경향을 시각화합니다. 또한, AB-MIL과 같은 방법은 인스턴스 확률 유도를 통해 특징 증류(feature distillation)에 활용되어 종양 영역 검출 시 높은 확률 영역을 밝은 색으로 표시함으로써 긍정적인 검출 영역과 일치함을 보여줍니다.

## 🧠 Insights & Discussion

- **신뢰와 투명성의 중요성:** 딥러닝 모델의 복잡성으로 인한 '블랙 박스' 특성은 의료 진단과 같은 중요 분야에서 사용자(의료진, 환자)의 신뢰를 저해할 수 있습니다. XAI는 모델의 의사 결정 과정을 설명함으로써 이러한 격차를 해소하고, 의료 전문가들이 AI 시스템의 진단 권고를 이해하고 검증하여 정보에 입각한 결정을 내릴 수 있도록 돕습니다.
- **편향 식별 및 책임성:** 해석 가능한 AI 모델은 잠재적 편향을 식별하고 수정하는 데 기여하여 공정하고 편향 없는 의사 결정을 가능하게 합니다. 이는 모델의 책임성을 확보하고, 다양한 인구 집단에 대한 공정한 결과를 보장하는 데 필수적입니다.
- **ViT의 장단점:** ViT는 데이터 내의 장거리 관계를 포착하는 데 효과적이지만, 국부적 특징이 중요한 특정 작업에서는 어려움을 겪을 수 있습니다. 또한, ViT는 일반적으로 많은 양의 훈련 데이터를 필요로 하는데, 이는 의료 분야에서 대규모 주석 데이터셋을 수집하기 어렵다는 점에서 한계가 됩니다.
- **계산 복잡성:** ViT의 셀프 어텐션 메커니즘은 특히 고해상도 영상에 적용될 때 제곱 시간 복잡도를 가지므로 계산 비용이 많이 듭니다. 이는 고성능 컴퓨팅 자원에 대한 접근성이 낮은 소규모 클리닉에는 실질적인 어려움이 될 수 있습니다.
- **미래 방향:**
  - **통합 프레임워크:** 여러 해석 기법을 결합한 통합 프레임워크 개발을 통해 모델 결정에 대한 보다 포괄적인 이해를 제공할 수 있습니다.
  - **불확실성 정량화:** 해석 가능한 모델에 불확실성 추정을 통합하면 모델의 신뢰성과 견고성을 높일 수 있습니다. 이는 특히 환자 결과에 중대한 영향을 미칠 수 있는 중요한 의료 의사 결정 시나리오에서 중요합니다 (예: 불확실성 지도 활용).
  - **지상 예측(Grounded Predictions):** 데이터셋 내의 불확실성을 해결하는 접근 방식은 모델이 보다 근거 있는 예측을 생성하도록 유도할 수 있습니다.

## 📌 TL;DR

이 논문은 의료 분야에서 Vision Transformer(ViT)의 "블랙 박스" 문제를 해결하기 위한 설명 가능한 AI(XAI) 기법들을 검토합니다. ViT의 기본 작동 방식과 Grad-CAM, Saliency Maps 등 주요 XAI 방법론을 설명하며, 이들이 COVID-19 진단 및 암 분류와 같은 의료 영상 애플리케이션에서 ViT의 의사 결정 과정을 어떻게 시각화하고 해석하는지 제시합니다. 궁극적으로 XAI는 의료 AI 시스템의 투명성, 신뢰성, 책임성을 높여 환자 치료와 의료 관행을 개선하는 데 기여하며, 대규모 데이터 및 계산 복잡성과 같은 ViT의 한계를 극복하고 불확실성 정량화와 같은 미래 연구를 통해 더욱 발전할 수 있습니다.
