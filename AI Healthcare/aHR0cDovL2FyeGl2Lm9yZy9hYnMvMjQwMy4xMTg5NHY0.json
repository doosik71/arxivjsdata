{
  "title": "From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?",
  "authors": "Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, Giorgos Papanastasiou",
  "year": 2024,
  "url": "http://arxiv.org/abs/2403.11894v4",
  "abstract": "Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare."
}