{
  "title": "Explainable Deep Learning in Healthcare: A Methodological Survey from an\n  Attribution View",
  "authors": "Di Jin, Elena Sergeeva, Wei-Hung Weng, Geeticka Chauhan, Peter Szolovits",
  "year": 2021,
  "url": "http://arxiv.org/abs/2112.02625v1",
  "abstract": "The increasing availability of large collections of electronic health record\n(EHR) data and unprecedented technical advances in deep learning (DL) have\nsparked a surge of research interest in developing DL based clinical decision\nsupport systems for diagnosis, prognosis, and treatment. Despite the\nrecognition of the value of deep learning in healthcare, impediments to further\nadoption in real healthcare settings remain due to the black-box nature of DL.\nTherefore, there is an emerging need for interpretable DL, which allows end\nusers to evaluate the model decision making to know whether to accept or reject\npredictions and recommendations before an action is taken. In this review, we\nfocus on the interpretability of the DL models in healthcare. We start by\nintroducing the methods for interpretability in depth and comprehensively as a\nmethodological reference for future researchers or clinical practitioners in\nthis field. Besides the methods' details, we also include a discussion of\nadvantages and disadvantages of these methods and which scenarios each of them\nis suitable for, so that interested readers can know how to compare and choose\namong them for use. Moreover, we discuss how these methods, originally\ndeveloped for solving general-domain problems, have been adapted and applied to\nhealthcare problems and how they can help physicians better understand these\ndata-driven technologies. Overall, we hope this survey can help researchers and\npractitioners in both artificial intelligence (AI) and clinical fields\nunderstand what methods we have for enhancing the interpretability of their DL\nmodels and choose the optimal one accordingly.",
  "citation": 84
}