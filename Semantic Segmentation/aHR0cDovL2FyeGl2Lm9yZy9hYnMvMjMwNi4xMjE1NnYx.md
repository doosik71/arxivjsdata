# Fast Segment Anything

Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, Jinqiao Wang

## 🧩 Problem to Solve

최근 제안된 Segment Anything Model (SAM)은 컴퓨터 비전 분야에 큰 영향을 미쳤으나, 고해상도 입력에 대한 Transformer 아키텍처의 막대한 계산 비용으로 인해 산업 시나리오에서 광범위하게 적용되기 어렵다는 문제가 있습니다. 본 논문은 SAM과 유사한 성능을 유지하면서도 실시간으로 작동하는 훨씬 빠른 대안을 제공하여 이러한 계산 비용 문제를 해결하고자 합니다.

## ✨ Key Contributions

* **실시간 CNN 기반 솔루션 제안**: Segment Anything 작업을 위한 새로운 실시간 CNN 기반 솔루션인 FastSAM을 소개합니다. 이 솔루션은 경쟁력 있는 성능을 유지하면서 계산 요구 사항을 크게 줄입니다.
* **CNN 탐지기 적용 연구**: Segment Anything 작업에 CNN 탐지기를 적용한 첫 번째 연구로, 복잡한 비전 작업에서 경량 CNN 모델의 잠재력에 대한 통찰력을 제공합니다.
* **광범위한 비교 평가**: 제안된 방법과 SAM을 다양한 벤치마크에서 비교 평가하여 Segment Anything 도메인에서의 강점과 약점에 대한 통찰력을 제공합니다.
* **획기적인 속도 향상**: SAM 대비 최대 50배 빠른 런타임 속도를 달성하며, comparable한 성능을 유지합니다.
* **효율적인 데이터 활용**: SAM 저자들이 공개한 SA-1B 데이터셋의 1/50(2%)만을 사용하여 기존 인스턴스 분할 방법론을 직접 훈련시켜 효율성을 입증합니다.

## 📎 Related Works

* **Segment Anything Model (SAM) [19]**: 본 연구의 주요 비교 대상이자 동기를 부여한 기초 비전 모델입니다.
* **YOLOv8-seg [16]**: FastSAM의 기반이 되는 객체 탐지 및 인스턴스 분할 모델입니다.
* **YOLACT [4]**: YOLOv8-seg의 인스턴스 분할 브랜치에 적용된 원리입니다.
* **CLIP [31]**: 텍스트 프롬프트 기반 선택에 사용되는 텍스트 임베딩 모델입니다.
* **SA-1B dataset**: SAM에서 공개한 대규모 이미지 분할 데이터셋입니다.
* **이전 객체 제안 방법**: EdgeBox [38], Geodesic [21], Selective Search [34], MCG [2], DeepMask [29], OLN [17], RPN [32], UniDetector [36] 등.
* **벤치마크 데이터셋**: BSDS500 [1, 28] (엣지 검출), MS COCO [13, 25] 및 LVIS [13] (객체 제안 및 인스턴스 분할), MVTec AD [3] (이상 감지), ReDWeb-S [27] (주목 객체 분할), 건물 추출 데이터셋 [14].
* **ViTDet [23]**: 일부 평가에서 비교 대상으로 사용된 모델입니다.

## 🛠️ Methodology

FastSAM은 "Segment Anything" 작업을 두 가지 순차적인 단계로 재구성합니다:

1. **모든 인스턴스 분할 (All-instance Segmentation, AIS)**:
    * **모델 아키텍처**: YOLOv8-seg [16]를 기반으로 하며, 이는 YOLOv5 [15]에서 발전하여 C2f 모듈 및 분리된 Anchor-Free 헤드 구조를 포함합니다.
    * **인스턴스 분할**: YOLACT [4] 원리를 적용합니다. 백본 네트워크와 FPN [24]을 통해 이미지에서 특징을 추출합니다.
    * **병렬 처리**: 탐지 브랜치(카테고리 및 바운딩 박스 출력)와 분할 브랜치(k개의 프로토타입 및 k개의 마스크 계수 출력)가 병렬로 작동합니다. 마스크 계수와 프로토타입의 곱셈 및 합산을 통해 최종 인스턴스 분할 결과를 얻습니다.
    * **훈련**: SAM 저자들이 공개한 방대한 SA-1B 데이터셋의 2% (1/50)만을 사용하여 훈련합니다.
2. **프롬프트 기반 선택 (Prompt-guided Selection, PGS)**:
    * AIS 단계에서 얻은 모든 객체 또는 영역 분할 결과물에서 사용자의 프롬프트에 따라 특정 객체를 식별하고 선택합니다.
    * **포인트 프롬프트**: 선택된 점들이 위치한 마스크를 찾습니다. 전경/배경 점을 사용하여 관련 없는 마스크를 필터링하고, 여러 마스크에 걸쳐 있는 경우 병합합니다. 형태학적 연산을 사용하여 마스크 병합 성능을 향상합니다.
    * **박스 프롬프트**: 선택된 박스와 각 마스크의 바운딩 박스 간의 IoU(Intersection over Union)를 계산하여 가장 높은 IoU 점수를 가진 마스크를 선택합니다.
    * **텍스트 프롬프트**: CLIP [31] 모델을 사용하여 텍스트 임베딩을 추출하고, 이를 각 마스크의 내부 특징과 유사성 측정 지표를 통해 매칭하여 가장 유사한 마스크를 선택합니다.

## 📊 Results

* **런타임 효율성**: 단일 NVIDIA GeForce RTX 3090 GPU에서 SAM보다 50배 빠른 (SAM-H 32x32 대비) 속도를 달성했습니다 (FastSAM: 40ms/이미지, SAM-H: 2099ms/이미지). FastSAM의 실행 속도는 프롬프트 수에 따라 변하지 않아 "Everything" 모드에서 더 효율적입니다.
* **제로샷 엣지 검출 (BSDS500)**: SAM과 유사한 성능을 보였습니다. 파라미터 수가 SAM(0.6G)보다 훨씬 적음(68M)에도 불구하고 일반적으로 좋은 엣지 맵을 생성하며, SAM과 마찬가지로 실제보다 더 많은 엣지를 예측하는 경향이 있습니다.
* **제로샷 객체 제안 생성 (COCO, LVIS)**:
  * **바운딩 박스 제안**: COCO AR@1000에서 SAM-H E32보다 1.2점 높은 63.7을 기록했으며, LVIS v1 bbox AR@1000에서는 SAM-H E64보다 5% 이상 뛰어난 57.1을 달성했습니다.
  * **마스크 제안**: 마스크 AR@1000은 SAM보다 상대적으로 낮게 나타났습니다 (LVIS에서 FastSAM 49.7 vs SAM 59.3). 이는 주로 작은 객체에 대한 마스크 품질이 충분히 정교하지 않기 때문입니다.
* **제로샷 인스턴스 분할**: ViTDet 박스를 프롬프트로 사용하여 인스턴스 분할을 수행했을 때, SAM보다 낮은 AP를 기록했습니다 (COCO 37.9 vs 46.5, LVIS 34.5 vs 44.7).
* **제로샷 텍스트 프롬프트 기반 객체 현지화**: 텍스트 프롬프트에 따라 객체를 잘 분할할 수 있음을 정성적 결과로 보여주었습니다. 다만, 각 마스크 영역을 CLIP 특징 추출기에 입력해야 하므로 실행 속도가 만족스럽지 못합니다.
* **실제 애플리케이션**:
  * **이상 감지**: "everything" 모드에서 SAM과 유사하게 거의 모든 영역을 분할하지만, 정밀도는 낮습니다. 포인트/박스 프롬프트를 사용하면 정확한 결함 영역을 분할합니다.
  * **주목 객체 분할**: "everything" 모드에서 SAM과 미미한 차이를 보이며, 관련 없는 배경 객체를 더 적게 분할합니다. 포인트 프롬프트 사용 시 SAM-point와 거의 동일한 결과를 얻습니다.
  * **건물 추출**: 규칙적인 형태의 객체를 잘 분할하지만, SAM에 비해 그림자 관련 영역은 적게 분할합니다. 포인트/박스 프롬프트를 통한 관심 영역 선택이 가능하며, 노이즈에 대한 어느 정도의 저항력을 보여줍니다.

## 🧠 Insights & Discussion

* **산업 적용 가능성**: FastSAM은 SAM 대비 50배 이상 빠른 속도를 바탕으로 도로 장애물 감지, 비디오 인스턴스 추적, 이미지 조작 등 다양한 산업 애플리케이션에 적합한 실시간 솔루션입니다.
* **효율성-정확도 균형**: 이 연구는 대규모 트랜스포머 대신 CNN 구조에 인간의 사전 지식(human priors)을 도입함으로써, 특정 시각 작업에 최적화된 더 작고 효율적인 모델이 우수한 효율성-정확도 균형을 달성할 수 있음을 보여줍니다. 이는 모델 압축의 실현 가능성을 제시합니다.
* **한계점**:
  * **마스크 품질**: SAM에 비해 마스크 생성 성능이 낮고, 특히 작은 객체의 경우 마스크 품질이 떨어집니다.
  * **신뢰도 점수**: 낮은 품질의 작은 마스크가 높은 신뢰도 점수를 가질 수 있는데, 이는 신뢰도 점수가 YOLOv8의 바운딩 박스 점수에 기반하며 마스크 품질과 강하게 연관되지 않기 때문입니다.
  * **YOLACT의 한계**: 작은 객체의 마스크가 정사각형에 가까운 경향이 있고, 큰 객체의 마스크 경계에 일부 아티팩트가 발생할 수 있습니다. 이는 YOLACT 방법의 약점입니다.
  * **데이터 활용**: SA-1B 데이터셋의 1/50만 사용했으므로, 더 많은 훈련 데이터를 활용하면 성능 향상이 가능할 것입니다.
* **향후 연구**: 마스크 품질 지표 예측, 마스크 프로토타입 역량 강화 또는 마스크 생성기 재구성 등 약점 개선을 위한 후속 연구의 가능성을 제시합니다.

## 📌 TL;DR

FastSAM은 강력하지만 느린 SAM의 실시간 산업 적용 문제를 해결하기 위해, "Segment Anything" 작업을 CNN 기반의 YOLOv8-seg를 활용한 **모든 인스턴스 분할**과 **프롬프트 기반 선택** 두 단계로 재구성했습니다. SA-1B 데이터셋의 2%만으로 훈련하여 SAM과 비교 가능한 성능을 유지하면서도 **50배 빠른 속도**를 달성했습니다. 특히 박스 제안 생성에서 강점을 보이지만, 작은 객체의 마스크 품질 및 인스턴스 분할 AP는 SAM보다 약간 낮습니다. 이는 CNN 기반 아키텍처와 사전 지식을 통해 효율적인 비전 모델을 구축할 수 있음을 보여줍니다.
