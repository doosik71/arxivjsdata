{
  "url": "http://arxiv.org/abs/2106.02638v3",
  "title": "Associating Objects with Transformers for Video Object Segmentation",
  "authors": "Zongxin Yang, Yunchao Wei, Yi Yang",
  "year": 2021,
  "abstract": "This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computing resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\nuniformly. In detail, AOT employs an identification mechanism to associate\nmultiple targets into the same high-dimensional embedding space. Thus, we can\nsimultaneously process multiple objects' matching and segmentation decoding as\nefficiently as processing a single object. For sufficiently modeling\nmulti-object association, a Long Short-Term Transformer is designed for\nconstructing hierarchical matching and propagation. We conduct extensive\nexperiments on both multi-object and single-object benchmarks to examine AOT\nvariant networks with different complexities. Particularly, our R50-AOT-L\noutperforms all the state-of-the-art competitors on three popular benchmarks,\ni.e., YouTube-VOS (84.1% J&F), DAVIS 2017 (84.9%), and DAVIS 2016 (91.1%),\nwhile keeping more than $3\\times$ faster multi-object run-time. Meanwhile, our\nAOT-T can maintain real-time multi-object speed on the above benchmarks. Based\non AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.",
  "citation": 424
}