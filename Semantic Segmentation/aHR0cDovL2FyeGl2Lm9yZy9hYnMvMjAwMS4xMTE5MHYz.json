{
  "url": "http://arxiv.org/abs/2001.11190v3",
  "title": "2018 Robotic Scene Segmentation Challenge",
  "authors": "Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamohammadi, Imanol Luengo, Felix Fuentes, Evangello Flouty, Ahmed Mohammed, Marius Pedersen, Avinash Kori, Varghese Alex, Ganapathy Krishnamurthi, David Rauber, Robert Mendel, Christoph Palm, Sophia Bano, Guinther Saibro, Chi-Sheng Shih, Hsun-An Chiang, Juntang Zhuang, Junlin Yang, Vladimir Iglovikov, Anton Dobrenkii, Madhu Reddiboina, Anubhav Reddy, Xingtong Liu, Cong Gao, Mathias Unberath, Myeonghyeon Kim, Chanho Kim, Chaewon Kim, Hyejin Kim, Gyeongmin Lee, Ihsan Ullah, Miguel Luna, Sang Hyun Park, Mahdi Azizian, Danail Stoyanov, Lena Maier-Hein, Stefanie Speidel",
  "year": 2020,
  "abstract": "In 2015 we began a sub-challenge at the EndoVis workshop at MICCAI in Munich\nusing endoscope images of ex-vivo tissue with automatically generated\nannotations from robot forward kinematics and instrument CAD models. However,\nthe limited background variation and simple motion rendered the dataset\nuninformative in learning about which techniques would be suitable for\nsegmentation in real surgery. In 2017, at the same workshop in Quebec we\nintroduced the robotic instrument segmentation dataset with 10 teams\nparticipating in the challenge to perform binary, articulating parts and type\nsegmentation of da Vinci instruments. This challenge included realistic\ninstrument motion and more complex porcine tissue as background and was widely\naddressed with modifications on U-Nets and other popular CNN architectures. In\n2018 we added to the complexity by introducing a set of anatomical objects and\nmedical devices to the segmented classes. To avoid over-complicating the\nchallenge, we continued with porcine data which is dramatically simpler than\nhuman tissue due to the lack of fatty tissue occluding many organs.",
  "citation": 69
}