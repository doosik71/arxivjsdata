{
  "url": "http://arxiv.org/abs/2406.16085v2",
  "title": "A Simple Framework for Open-Vocabulary Zero-Shot Segmentation",
  "authors": "Thomas Stegm√ºller, Tim Lebailly, Nikola Dukic, Behzad Bozorgtabar, Tinne Tuytelaars, Jean-Philippe Thiran",
  "year": 2024,
  "abstract": "Zero-shot classification capabilities naturally arise in models trained\nwithin a vision-language contrastive framework. Despite their classification\nprowess, these models struggle in dense tasks like zero-shot open-vocabulary\nsegmentation. This deficiency is often attributed to the absence of\nlocalization cues in captions and the intertwined nature of the learning\nprocess, which encompasses both image representation learning and\ncross-modality alignment. To tackle these issues, we propose SimZSS, a Simple\nframework for open-vocabulary Zero-Shot Segmentation. The method is founded on\ntwo key principles: i) leveraging frozen vision-only models that exhibit\nspatial awareness while exclusively aligning the text encoder and ii)\nexploiting the discrete nature of text and linguistic knowledge to pinpoint\nlocal concepts within captions. By capitalizing on the quality of the visual\nrepresentations, our method requires only image-caption pairs datasets and\nadapts to both small curated and large-scale noisy datasets. When trained on\nCOCO Captions across 8 GPUs, SimZSS achieves state-of-the-art results on 7 out\nof 8 benchmark datasets in less than 15 minutes."
}