{
  "url": "http://arxiv.org/abs/2002.06736v1",
  "title": "Directional Deep Embedding and Appearance Learning for Fast Video Object\n  Segmentation",
  "authors": "Yingjie Yin, De Xu, Xingang Wang, Lei Zhang",
  "year": 2020,
  "abstract": "Most recent semi-supervised video object segmentation (VOS) methods rely on\nfine-tuning deep convolutional neural networks online using the given mask of\nthe first frame or predicted masks of subsequent frames. However, the online\nfine-tuning process is usually time-consuming, limiting the practical use of\nsuch methods. We propose a directional deep embedding and appearance learning\n(DDEAL) method, which is free of the online fine-tuning process, for fast VOS.\nFirst, a global directional matching module, which can be efficiently\nimplemented by parallel convolutional operations, is proposed to learn a\nsemantic pixel-wise embedding as an internal guidance. Second, an effective\ndirectional appearance model based statistics is proposed to represent the\ntarget and background on a spherical embedding space for VOS. Equipped with the\nglobal directional matching module and the directional appearance model\nlearning module, DDEAL learns static cues from the labeled first frame and\ndynamically updates cues of the subsequent frames for object segmentation. Our\nmethod exhibits state-of-the-art VOS performance without using online\nfine-tuning. Specifically, it achieves a J & F mean score of 74.8% on DAVIS\n2017 dataset and an overall score G of 71.3% on the large-scale YouTube-VOS\ndataset, while retaining a speed of 25 fps with a single NVIDIA TITAN Xp GPU.\nFurthermore, our faster version runs 31 fps with only a little accuracy loss.\nOur code and trained networks are available at\nhttps://github.com/YingjieYin/Directional-Deep-Embedding-and-Appearance-Learning-for-Fast-Video-Object-Segmentation.",
  "citation": 18
}