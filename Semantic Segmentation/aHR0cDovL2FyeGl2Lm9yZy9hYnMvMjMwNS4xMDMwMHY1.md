# One-Prompt to Segment All Medical Images

Junde Wu, Jiayuan Zhu, Yueming Jin, Min Xu

## 🧩 Problem to Solve

의료 영상 분할은 다양한 영상 종류와 타겟 라벨로 인해 파운데이션 모델(Foundation Models)을 적용하기 어려운 분야입니다. 기존 접근 방식에는 다음과 같은 문제점이 있습니다:

* **대화형 분할 모델(예: SAM)의 한계:** 추론 시 각 샘플에 대해 사용자 프롬프트(예: 포인트, 바운딩 박스)가 필요하여 시간 소모적이며 자동화 파이프라인 구축에 적합하지 않습니다.
* **Few/One-Shot 학습 모델의 한계:** 새로운 작업에 대해 레이블이 지정된 샘플(지원 예제)이 필요하며, 이는 비용이 많이 들고 실제 임상 환경에서 충분한 데이터를 확보하기 어렵습니다.
이 논문은 이 모든 한계를 극복하고 보이지 않는(unseen) 의료 영상 분할 작업을 단일 프롬프트만으로 처리할 수 있는 보편적인 의료 영상 분할 패러다임을 제안하는 것을 목표로 합니다.

## ✨ Key Contributions

* **'One-Prompt Segmentation' 패러다임 도입:** 단 하나의 프롬프트된 샘플만으로 이전에 보지 못한(unseen) 의료 영상 분할 작업을 수행할 수 있는 강력하고 저비용의 보편적인 분할 패러다임을 제안합니다.
* **독자적인 One-Prompt Former 모델 제안:** 프롬프트된 템플릿 피처를 쿼리 이미지 분할 과정에 효율적으로 통합하기 위한 One-Prompt Former 모듈을 디코더로 설계했습니다. 이는 다중 피처 스케일에서 템플릿과 쿼리 피처를 융합합니다.
* **다양한 임상 실습을 위한 4가지 프롬프트 유형 설정:** Click, BBox 외에 자유롭게 그릴 수 있는 Doodle과 세그멘테이션 마스크 자체를 프롬프트로 사용하는 SegLab을 도입하여 다양한 의료 타겟 프롬프트에 대응합니다.
* **대규모 의료 영상 데이터셋 구축 및 어노테이션:** 78개의 오픈 소스 데이터셋을 수집하고, 그중 64개 데이터셋을 모델 학습에 사용했습니다. 또한, 3,000개 이상의 샘플에 대해 임상의들이 직접 프롬프트를 어노테이션했습니다.

## 📎 Related Works

이 논문은 다음과 같은 기존 연구들과 비교 및 관련성을 언급합니다:

* **대화형 분할 모델(Interactive Segmentation Models):** Segment Anything Model (SAM) [27] 및 이를 의료 영상 분할에 적용하기 위한 미세 조정(fine-tuning) 연구들 (SAM-U [14], VMN [61], iSegFormer [34], MedSAM [37], MSA [58], SAM-Med2D [13]). 이러한 모델들은 각 이미지에 대한 사용자 프롬프트를 필요로 합니다.
* **Few/One-Shot 학습 모델(Few/One-shot Learning Models):** PANet [51], ALPNet [41], SENet [47], UniverSeg [8], DAT [59], ProbOne [15], HyperSegNas [42], LT-Net [53]. 이 모델들은 새로운 작업을 위해 1개 또는 몇 개의 레이블된 지원 샘플을 필요로 합니다.
* **완전 지도 학습 모델(Fully-supervised Models):** TransUNet [12], Swin-UNetr [18], nnUNet [24], MedSegDiff [57], TransAttUNet [9], UNETR [19]. 이 모델들은 특정 작업에 맞춰 훈련되며 제로샷 일반화 능력이 부족합니다.

## 🛠️ Methodology

One-Prompt 모델은 이미지 인코더와 일련의 One-Prompt Former(디코더)로 구성됩니다 (Fig. 2 (a)).

1. **입력:** 쿼리 이미지 $x_q$, 템플릿 이미지 $x_t$, 템플릿 이미지의 프롬프트 $p_t$를 입력으로 받습니다.
2. **인코더:** $x_q$와 $x_t$는 동일한 인코더(CNN 또는 ViT 기반)를 통과하여 피처 $f_q$와 $f_t$를 추출합니다.
3. **프롬프트(Prompts):**
    * Click, BBox, Doodle, SegLab의 네 가지 프롬프트 유형을 지원합니다.
    * Click과 Doodle은 전경(foreground)과 배경(background)을, BBox는 좌상단과 우하단 코너 포인트를 나타냅니다. 이들은 위치 인코딩(positional encoding)과 학습 가능한 임베딩으로 변환됩니다.
    * SegLab은 사전 학습된 오토인코더를 통해 임베딩으로 변환됩니다.
    * 모든 프롬프트는 두 개의 임베딩 $p_1$과 $p_2$로 표현됩니다.
4. **One-Prompt Former (디코더):**
    * 다운샘플링된 쿼리 피처를 디코딩하며, 프롬프트 임베딩, 다중 스케일 템플릿 피처, 다중 스케일 쿼리 피처를 통합합니다.
    * 각 One-Prompt Former 블록은 두 개의 병렬 브랜치(쿼리 브랜치, 템플릿 브랜치)와 Cross Attention으로 구성됩니다 (Fig. 2 (b)).
    * **쿼리 브랜치:** skip-connected 쿼리 임베딩 $e_q^l$를 쿼리로 사용하고, 이전 블록의 출력을 키(key)와 값(value)으로 사용하여 Cross Attention을 수행합니다. 이후 대칭적으로 템플릿 피처를 통합합니다.
    * **템플릿 브랜치:** 제안된 Prompt-Parser를 사용하여 프롬프트 $p$를 $e_q^l$ 및 $e_t^l$와 통합한 후, Cross Attention을 통해 쿼리 피처를 통합합니다.
    * 마지막으로 Cross Attention이 두 브랜치를 통합하여 프롬프트된 템플릿 분할 정보를 쿼리 영역으로 전달합니다.
5. **Prompt-Parser:**
    * 프롬프트, 쿼리 및 템플릿 피처를 효과적으로 혼합하기 위해 설계되었습니다 (Fig. 2 (c)).
    * **Prompting Step:** $e_t^l, p_1, p_2$를 스택($[e_t^l; p_1; p_2]$)하고 MLP 레이어를 적용하여 마스크 $M$을 생성합니다. $M = w[e_t^l; p_1; p_2](e_q^l)^T$.
    * **Masking Step:** $M$에 Gaussian Masking 연산($e_G = Max(e_{tq}^l * k_G[Conv(M)], e_{tq}^l)$)을 적용하여 프롬프트된 영역을 활성화하고 불확실성을 포함한 공간을 확대합니다.
6. **학습 및 손실:**
    * 총 78개의 데이터셋 중 64개 데이터셋으로 학습하고 14개로 테스트합니다.
    * 각 학습 데이터셋은 프롬프트된 템플릿, 학습, 검증 스플릿으로 나뉩니다.
    * 훈련 중에는 쿼리 이미지와 동일한 데이터셋에서 프롬프트된 템플릿을 무작위로 선택합니다.
    * 최종 손실은 교차 엔트로피 손실(cross-entropy loss)과 다이스 손실(dice loss)의 합입니다.
    * 추론 시에는 템플릿 스플릿에서 무작위로 프롬프트된 템플릿을 선택하며, 분산 완화를 위해 50회 실행하여 앙상블합니다.

## 📊 Results

* **One-Prompt 전이 능력 (v.s. Few/One-shot Models):**
  * 14개의 보지 못한 테스트 데이터셋에서 One-Prompt 모델은 다양한 프롬프트 유형(Click, BBox, Doodle, SegLab)에 걸쳐 기존 Few/One-shot 모델들을 일관되게 능가했습니다 (Fig. 3, Fig. 5).
  * 특히, 모든 모델에 세그멘테이션 레이블을 '프롬프트'로 제공하는 공정한 비교(SegLab)에서 One-Prompt 모델은 경쟁 모델들보다 평균 11.2% 더 높은 성능을 보였습니다.
* **대화형 분할 능력 (v.s. Interactive Segmentation Models):**
  * 쿼리 이미지 자체가 프롬프트된 템플릿으로 제공될 때 (대화형 분할 설정), One-Prompt 모델은 기존 대화형 분할 모델(SAM, MedSAM 등)들을 모든 14개 데이터셋에서 크게 능가했습니다 (Fig. 4). 이는 One-Prompt 모델이 이 특정 설정에 맞춰 의도적으로 훈련되지 않았음에도 불구하고 우수한 성능을 보임을 나타냅니다.
* **제로샷 분할 능력 (v.s. Fully-supervised & SAM-based Models):**
  * 템플릿 이미지에 일반 격자 포인트로 프롬프트를 주어 모든 salient 타겟을 분할하는 'segment everything' 설정에서, One-Prompt 모델은 기존 완전 지도 학습 모델 및 SAM 기반 모델들을 11개 보지 못한 데이터셋에서 모두 능가했습니다 (Table 1).
  * 평균 64.0%의 Dice 점수를 달성하여, 차순위 모델보다 10.7%p 높은 성능을 보였습니다.
* **프롬프트-파서 (Prompt-Parser) 어블레이션:**
  * 제안된 Stack MLP + Gaussian Masking 조합이 가장 높은 성능을 달성했습니다 (Fig. 6).
* **추론 시 다른 템플릿 제공에 따른 분산:**
  * 다양한 템플릿을 사용하여 100회 반복 실험한 결과, 모델의 분산은 일관되게 13% 미만으로 유지되어 견고한 제로샷 일반화 능력을 보여줍니다. (Fig. 7).
  * 세분화된 SegLab 프롬프트 사용 시 분산이 가장 작았습니다.
* **프롬프트 품질 및 유형의 영향:**
  * 프롬프트 품질이 높을수록 모델 성능이 점진적으로 향상되며, SegLab이 가장 높은 성능을 보였습니다. BBox와 Doodle은 Click보다 우수했습니다 (Fig. 8). 이는 사용자 프롬프트와 모델 성능 간의 트레이드오프를 시사합니다.
* **모델 효율성:**
  * One-Prompt 모델은 기존 Few/One-shot 모델보다 사용자 비용 시간(User-Cost Time)을 크게 절감합니다 (이미지당 평균 27.47초에서 2.28초로).
  * 또한, TransUNet과 비교하여 성능 감소가 3.23%에 불과하며, 단 한 번의 훈련으로 모든 하위 작업을 처리하여 파라미터, 훈련 시간, 사용자 어노테이션 시간을 크게 절약합니다 (Table 2).

## 🧠 Insights & Discussion

* **혁신적인 패러다임:** One-Prompt Segmentation은 의료 영상 분할 분야에서 파운데이션 모델의 활용성을 혁신적으로 확장합니다. 단 하나의 프롬프트만으로 새로운 작업에 대한 강력한 제로샷 일반화 능력을 제공하여 기존 방법론의 한계를 극복합니다.
* **실용성 증대:** 다양한 프롬프트 유형(Click, BBox, Doodle, SegLab) 지원은 임상의들이 가장 편리하고 효과적인 방식으로 모델과 상호작용할 수 있게 하여, 복잡한 훈련이나 미세 조정 없이도 시스템을 사용할 수 있도록 합니다. 이는 임상 실습에서의 자동화 파이프라인 구축 및 활용 가능성을 높입니다.
* **강력한 일반화 능력:** 대규모의 다양하고 임상의가 어노테이션한 데이터셋으로 훈련함으로써, One-Prompt 모델은 이전에 보지 못한 다양한 의료 영상 분할 작업에서 탁월한 성능을 입증했습니다.
* **효율성:** 기존의 완전 지도 학습이나 Few/One-shot 학습 방식과 비교하여 훈련 및 사용자 어노테이션 비용을 대폭 절감하면서도 경쟁 우위의 성능을 유지합니다.
* **한계 및 향후 과제:** 프롬프트 품질이 모델 성능에 큰 영향을 미치므로, 높은 품질의 프롬프트를 자동으로 생성하거나 최소한의 프롬프트로도 높은 성능을 유지하는 방법에 대한 연구가 필요할 수 있습니다. 또한, 특정 템플릿이 쿼리 이미지와 크게 다를 경우 발생하는 성능 분산을 줄이는 것이 중요합니다.

## 📌 TL;DR

이 논문은 보이지 않는 의료 영상 분할 작업을 위해 **단 하나의 프롬프트된 샘플**만으로도 강력한 제로샷 일반화 능력을 발휘하는 **'One-Prompt Segmentation'**이라는 새로운 패러다임을 제안합니다. 제안된 **One-Prompt Model**은 독자적인 **One-Prompt Former** 모듈과 임상 환경에 최적화된 4가지 프롬프트 유형을 활용하여 쿼리 이미지와 템플릿의 정보를 효과적으로 융합합니다. 64개의 방대한 의료 데이터셋과 3천 개 이상의 임상의 어노테이션 프롬프트로 훈련된 이 모델은 14개의 보지 못한 데이터셋에서 기존 Few/One-shot 및 대화형 분할 모델들을 크게 능가하며, 의료 영상 분할의 효율성과 실용성을 혁신적으로 향상시켰습니다.
