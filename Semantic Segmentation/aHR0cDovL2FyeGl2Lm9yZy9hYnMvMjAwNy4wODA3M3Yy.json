{
  "title": "Unseen Object Instance Segmentation for Robotic Environments",
  "authors": "Christopher Xie, Yu Xiang, Arsalan Mousavian, Dieter Fox",
  "year": 2020,
  "url": "http://arxiv.org/abs/2007.08073v2",
  "abstract": "In order to function in unstructured environments, robots need the ability to\nrecognize unseen objects. We take a step in this direction by tackling the\nproblem of segmenting unseen object instances in tabletop environments.\nHowever, the type of large-scale real-world dataset required for this task\ntypically does not exist for most robotic settings, which motivates the use of\nsynthetic data. Our proposed method, UOIS-Net, separately leverages synthetic\nRGB and synthetic depth for unseen object instance segmentation. UOIS-Net is\ncomprised of two stages: first, it operates only on depth to produce object\ninstance center votes in 2D or 3D and assembles them into rough initial masks.\nSecondly, these initial masks are refined using RGB. Surprisingly, our\nframework is able to learn from synthetic RGB-D data where the RGB is\nnon-photorealistic. To train our method, we introduce a large-scale synthetic\ndataset of random objects on tabletops. We show that our method can produce\nsharp and accurate segmentation masks, outperforming state-of-the-art methods\non unseen object instance segmentation. We also show that our method can\nsegment unseen objects for robot grasping."
}