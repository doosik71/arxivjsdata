# 비디오 객체 분할을 위한 Transductive 접근법

Yizhuo Zhang, Zhirong Wu, Houwen Peng, Stephen Lin

## 🧩 Problem to Solve

기존의 준지도(semi-supervised) 비디오 객체 분할(VOS) 방법들은 대부분 광학 흐름(optical flow), 인스턴스 분할(instance segmentation) 등 다른 도메인에서 훈련된 추가 모듈의 정보를 활용합니다. 이로 인해 방법론 간의 공정한 비교가 어렵고, 복잡성이 증가하며, 특정 객체 범주에 대한 일반화 능력이 제한될 수 있습니다. 이 논문은 이러한 외부 모듈, 추가 데이터셋, 또는 특수 아키텍처 디자인 없이, 첫 프레임의 마스크만 주어진 상황에서 목표 객체를 비디오 시퀀스에서 효과적으로 분리하는 간단하면서도 강력한 전도적(transductive) 접근 방식을 제안하여 이 문제를 해결하고자 합니다.

## ✨ Key Contributions

* **간단하고 강력한 전도적 방법론 제안**: 광학 흐름, 인스턴스 분할과 같은 추가 모듈이나 데이터셋, 전용 아키텍처 디자인 없이 비디오 객체 분할을 수행하는 전도적 추론(transductive inference) 기반의 방법을 제시합니다.
* **홀리스틱한 레이블 전파**: 픽셀 레이블을 임베딩 공간에서의 특징 유사도를 기반으로 전파하는 방식을 사용하며, 기존 전파 방식과 달리 장기적인 객체 외형을 고려하여 시간적 정보를 전체적으로 확산시킵니다.
* **고효율 및 빠른 속도**: 적은 추가 계산 오버헤드로 약 37 FPS의 빠른 추론 속도를 달성합니다.
* **경쟁력 있는 성능**: 바닐라 ResNet50 백본을 사용한 단일 모델로 DAVIS 2017 검증 세트에서 72.3%, 테스트 세트에서 63.1%의 준수한 성능을 보여주며, Youtube-VOS에서도 경쟁력 있는 결과를 달성합니다.
* **새로운 연구를 위한 강력한 기준점 제시**: 단순하면서도 높은 성능과 효율성을 겸비하여 향후 연구의 견고한 기준점(baseline)이 될 수 있음을 강조합니다.

## 📎 Related Works

* **단일 프레임 모델 (Single frame models)**: OSVOS [7], Lucid [25], OnAVOS [42] 등과 같이 첫 프레임에 대해 모델을 미세 조정하고 개별 테스트 프레임에 대해 추론하는 방식. 공간적 연속성을 강조하지만, 시간당 수십 초의 미세 조정 시간이 소요되어 비효율적입니다.
* **전파 기반 모델 (Propagation-based models)**: VideoMatch [22, 9]처럼 이미지 픽셀을 특징 공간에 임베딩하고 픽셀 유사도를 활용하여 레이블을 전파하는 방법. 후속 연구 [36, 45, 34, 41]에서는 이전 프레임을 참조로 추가하여 성능을 향상시켰으나, 지역적이고 희소한 전파 방식은 드리프트 문제에 취약합니다.
* **장거리 시공간 모델 (Long-range spatio-temporal models)**: 순환 신경망(RNN) [45, 21] 또는 그래픽 모델(MRF) [17, 38] 기반으로 조밀한 장거리 시공간 볼륨에 대해 최적화하려는 시도. RNN은 이전 프레임의 추정 오류에 민감하며, MRF는 계산 비용이 높습니다.
* **다른 비전 문제와의 연관성 (Relation to other vision problems)**: OSVOS-S [31] (인스턴스 분할), PReMVOS [30], DyeNet [29] (객체 재식별), CNN-MRF [3], MaskTrack [34], MaskRNN [21] (광학 흐름 추정) 등 다른 작업 모듈을 통합하여 성능을 향상시키려 했으나, 전이 학습 의존성 및 새로운 객체 범주나 폐색 상황에서의 한계가 있습니다.
* **가장 관련성 높은 연구 (Most relevant works)**: Space-time memory network (STM) [33]은 조밀한 장기 정보 활용이라는 점에서 유사하지만, 본 연구는 고전적인 준지도 학습에서 파생된 전도적 프레임워크를 기반으로 하며, 더 간단하고 효율적입니다.

## 🛠️ Methodology

본 논문의 핵심은 비디오 시퀀스 내의 레이블 없는 구조를 완전히 활용하는 전도적 추론(transductive inference) 접근 방식입니다.

1. **전도적 추론 프레임워크 ($Q(\hat{y})$)**:
    * 주어진 L개의 레이블된 데이터와 N-L개의 레이블 없는 데이터에 대해 레이블 없는 데이터의 레이블 $\hat{y}_{i}$를 추론하는 문제입니다.
    * 에너지 함수는 다음과 같이 정의됩니다:
        $$Q(\hat{y}) = \sum_{i,j} w_{ij} \left\| \frac{\hat{y}_i}{\sqrt{d_i}} - \frac{\hat{y}_j}{\sqrt{d_j}} \right\|^2 + \mu \sum_{i=1}^l \|\hat{y}_i - y_i\|^2$$
        여기서 $w_{ij}$는 데이터 포인트 $x_i, x_j$ 간의 유사도, $d_i = \sum_j w_{ij}$는 픽셀 $i$의 차수입니다. 첫 번째 항은 유사한 포인트가 동일한 레이블을 갖도록 하는 평활화 제약(smoothness constraint)이며, 두 번째 항은 초기 레이블된 관측값에 대한 적합성 제약(fitting constraint)입니다.
    * 이 문제는 반복 알고리즘으로 해결됩니다:
        $$\hat{y}^{(k+1)} = \alpha S \hat{y}^{(k)} + (1-\alpha)y^{(0)}$$
        여기서 $\alpha = \mu/(\mu+1)$, $S = D^{-1/2} W D^{-1/2}$는 정규화된 유사도 행렬이며, $y^{(0)}$는 초기 레이블 관측값입니다.

2. **온라인 비디오 객체 분할 적용**:
    * 비디오 프레임이 순차적으로 스트리밍되므로, 현재 프레임 $t$의 추론이 미래 프레임에 의존하지 않도록 온라인 방식으로 작동합니다.
    * 현재 프레임 $t+1$의 예측은 이전 프레임 $1$부터 $t$까지의 예측에 기반합니다:
        $$\hat{y}^{(t+1)} = S_{1:t \to t+1} \hat{y}^{(t)}$$
        첫 프레임 외에는 레이블이 주어지지 않으므로, $y^{(0)}$ 항은 생략됩니다.

3. **레이블 전파**:
    * **유사도 측정 ($w_{ij}$)**: 외형(appearance) 항과 공간(spatial) 항을 결합하여 전역적 고수준 의미론과 지역적 저수준 공간 연속성을 모두 고려합니다.
        $$w_{ij} = \exp(f_i^{\text{T}}f_j) \cdot \exp\left(-\frac{||loc(i)-loc(j)||^2}{\sigma^2}\right)$$
        여기서 $f_i, f_j$는 CNN을 통한 픽셀 $p_i, p_j$의 특징 임베딩이고, $loc(i)$는 픽셀 $i$의 공간 위치입니다.
    * **프레임 샘플링**: 모든 이전 프레임에 대한 유사도 행렬 계산은 비실용적이므로, 효율성을 위해 총 9개의 프레임을 샘플링합니다. 대상 프레임 직전의 4개 연속 프레임(지역적 움직임 모델링)과 나머지 36개 프레임에서 희소하게 샘플링된 5개 프레임(장기 상호작용 모델링)을 사용합니다.
    * **간단한 움직임 사전 (Simple motion prior)**: 시간적으로 멀리 떨어진 픽셀은 공간적 의존성이 약하다는 가정을 바탕으로, 지역적으로 샘플링된 프레임에는 작은 $\sigma=8$을, 멀리 떨어진 참조 프레임에는 큰 $\sigma=21$을 사용하여 공간 항을 조절합니다.

4. **외형 임베딩 학습**:
    * 2D CNN (ResNet-50)을 사용하여 외형 임베딩을 학습합니다. 임베딩은 움직임, 스케일, 변형으로 인한 단기 및 장기 변화를 포착하도록 설계됩니다.
    * 학습은 표준 교차 엔트로피 손실 함수를 사용합니다:
        $$L = -\sum_i \log P(\hat{y}_i=y_i|x_i)$$
    * **구현 세부 사항**: ResNet-50의 세 번째 및 네 번째 잔여 블록의 컨볼루션 보폭을 1로 설정하여 고해상도 출력을 유지하고, 256차원 임베딩을 위해 추가 1x1 컨볼루션 레이어를 사용합니다. ImageNet 사전 학습 가중치를 사용하여 DAVIS 2017 및 Youtube-VOS 데이터셋에서 미세 조정을 수행합니다.

## 📊 Results

* **DAVIS 2017 유효성 검사 세트**: J&F 측정 기준 72.3%의 전반적인 점수를 달성했습니다. 이는 STM [33]보다 약간 우수하며, 다른 전파 기반 방법들을 4%(평균 J) ~ 3%(평균 J&F) 상회합니다. 미세 조정 기반 방법인 DyeNet 및 CNN-MRF보다도 2% 높은 성능을 보이며 훨씬 단순하고 빠릅니다.
* **DAVIS 2017 테스트 세트**: 63.1%의 전반적인 점수를 기록했습니다. 테스트 세트의 높은 폐색 빈도로 인해 재식별 모듈을 사용하는 방법들에 비해 다소 낮지만, FEELVOS와 같은 다른 비-미세조정 기반 방법들보다는 우수합니다.
* **Youtube-VOS 유효성 검사 세트**: 67.8%의 전반적인 점수로, STM [33] (대규모 사전 학습 활용)을 제외한 모든 이전 연구를 능가합니다. DAVIS로 훈련된 모델도 Youtube-VOS에서 67.4%의 뛰어난 일반화 능력을 보였습니다.
* **속도 분석**: 단일 Titan Xp GPU에서 37 FPS의 빠른 속도로 실행됩니다. 이는 기존 방법들보다 훨씬 빠르면서도 최신 기술에 필적하는 성능을 제공하여 속도와 성능의 균형을 잘 맞춥니다.
* **어블레이션 연구 (Ablation Study)**: 조밀한 장거리 의존성(dense long-range dependencies)이 추적 성능을 크게 향상시키며, 공간 항(spatial term)이 객체 경계를 부드럽게 합니다. 간단한 움직임 사전은 약 1%의 성능 향상을 가져옵니다.

## 🧠 Insights & Discussion

* **미탐색된 시공간 구조의 활용**: 본 연구는 비디오 객체 분할에서 레이블 없는 시공간 볼륨 내의 풍부한 구조를 활용할 수 있음을 입증했습니다. 전도적 추론을 통해 이러한 구조를 효과적으로 발견하고 활용할 수 있습니다.
* **단순성과 효율성의 중요성**: 추가 모듈, 데이터셋, 복잡한 아키텍처 없이도 경쟁력 있는 성능을 달성함으로써, 모델의 단순성과 추론 효율성이 실제 응용에 있어 중요한 이점임을 보여줍니다.
* **시간적 안정성**: 탐지 기반 방법(예: PReMVOS)이 갑작스러운 객체 손실이나 신원 변경에 취약한 반면, 제안된 방법은 시간적으로 일관된 예측을 생성하여 비디오 분석의 안정성을 높입니다.
* **광학 흐름과의 관계**: 모델은 대상 프레임의 픽셀과 과거 프레임의 픽셀을 연결하는 "소프트(soft)" 메커니즘을 학습합니다. 이는 광학 흐름과 유사하지만, 본 모델은 배경보다 객체에 더 의미 있는 흐름을 생성하며, 명시적인 광학 흐름 평활화 제약은 오히려 VOS 성능을 저하시켰습니다. 이는 모델이 VOS에 최적화된 방식으로 시공간 정보를 학습했음을 시사합니다.
* **제한 사항**: DAVIS 2017 테스트 세트와 같이 동일 범주 내의 객체 간의 심한 폐색이 빈번한 시나리오에서는 재식별 모듈을 사용하는 방법보다 성능 격차가 발생할 수 있습니다.

## 📌 TL;DR

이 논문은 추가 모듈이나 데이터셋 없이도 비디오 객체 분할을 수행하는 **간단하고 효율적인 전도적(transductive) 접근법**을 제시합니다. 첫 프레임의 마스크를 기반으로, 시간적으로 조밀하고 희소한 샘플링 전략과 공간적 평활화 항을 포함하는 **전체적인 레이블 전파 방식**을 통해 장기적인 객체 외형 변화를 효과적으로 포착합니다. ResNet-50 백본을 사용하여 DAVIS 2017에서 72.3%의 높은 성능과 37 FPS의 빠른 속도를 달성하며, **단순하면서도 강력한 VOS 모델의 새로운 기준점**을 제시합니다.
