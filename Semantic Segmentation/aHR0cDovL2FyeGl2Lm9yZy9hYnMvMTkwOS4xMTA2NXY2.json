{
  "url": "http://arxiv.org/abs/1909.11065v6",
  "title": "Segmentation Transformer: Object-Contextual Representations for Semantic\n  Segmentation",
  "authors": "Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang",
  "year": 2019,
  "abstract": "In this paper, we address the semantic segmentation problem with a focus on\nthe context aggregation strategy. Our motivation is that the label of a pixel\nis the category of the object that the pixel belongs to. We present a simple\nyet effective approach, object-contextual representations, characterizing a\npixel by exploiting the representation of the corresponding object class.\nFirst, we learn object regions under the supervision of ground-truth\nsegmentation. Second, we compute the object region representation by\naggregating the representations of the pixels lying in the object region. Last,\n% the representation similarity we compute the relation between each pixel and\neach object region and augment the representation of each pixel with the\nobject-contextual representation which is a weighted aggregation of all the\nobject region representations according to their relations with the pixel. We\nempirically demonstrate that the proposed approach achieves competitive\nperformance on various challenging semantic segmentation benchmarks:\nCityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K,\nLIP, PASCAL-Context, and COCO-Stuff. Our submission \"HRNet + OCR + SegFix\"\nachieves 1-st place on the Cityscapes leaderboard by the time of submission.\nCode is available at: https://git.io/openseg and https://git.io/HRNet.OCR. We\nrephrase the object-contextual representation scheme using the Transformer\nencoder-decoder framework. The details are presented in~Section3.3.",
  "citation": 9
}