{
  "title": "Moving Object Segmentation: All You Need Is SAM (and Flow)",
  "authors": "Junyu Xie, Charig Yang, Weidi Xie, Andrew Zisserman",
  "year": 2024,
  "url": "http://arxiv.org/abs/2404.12389v2",
  "abstract": "The objective of this paper is motion segmentation -- discovering and\nsegmenting the moving objects in a video. This is a much studied area with\nnumerous careful, and sometimes complex, approaches and training schemes\nincluding: self-supervised learning, learning from synthetic datasets,\nobject-centric representations, amodal representations, and many more. Our\ninterest in this paper is to determine if the Segment Anything model (SAM) can\ncontribute to this task. We investigate two models for combining SAM with\noptical flow that harness the segmentation power of SAM with the ability of\nflow to discover and group moving objects. In the first model, we adapt SAM to\ntake optical flow, rather than RGB, as an input. In the second, SAM takes RGB\nas an input, and flow is used as a segmentation prompt. These surprisingly\nsimple methods, without any further modifications, outperform all previous\napproaches by a considerable margin in both single and multi-object benchmarks.\nWe also extend these frame-level segmentations to sequence-level segmentations\nthat maintain object identity. Again, this simple model achieves outstanding\nperformance across multiple moving object segmentation benchmarks."
}