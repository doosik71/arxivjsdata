{
  "url": "http://arxiv.org/abs/2104.11692v1",
  "title": "A Closer Look at Self-training for Zero-Label Semantic Segmentation",
  "authors": "Giuseppe Pastore, Fabio Cermelli, Yongqin Xian, Massimiliano Mancini, Zeynep Akata, Barbara Caputo",
  "year": 2021,
  "abstract": "Being able to segment unseen classes not observed during training is an\nimportant technical challenge in deep learning, because of its potential to\nreduce the expensive annotation required for semantic segmentation. Prior\nzero-label semantic segmentation works approach this task by learning\nvisual-semantic embeddings or generative models. However, they are prone to\noverfitting on the seen classes because there is no training signal for them.\nIn this paper, we study the challenging generalized zero-label semantic\nsegmentation task where the model has to segment both seen and unseen classes\nat test time. We assume that pixels of unseen classes could be present in the\ntraining images but without being annotated. Our idea is to capture the latent\ninformation on unseen classes by supervising the model with self-produced\npseudo-labels for unlabeled pixels. We propose a consistency regularizer to\nfilter out noisy pseudo-labels by taking the intersections of the pseudo-labels\ngenerated from different augmentations of the same image. Our framework\ngenerates pseudo-labels and then retrain the model with human-annotated and\npseudo-labelled data. This procedure is repeated for several iterations. As a\nresult, our approach achieves the new state-of-the-art on PascalVOC12 and\nCOCO-stuff datasets in the challenging generalized zero-label semantic\nsegmentation setting, surpassing other existing methods addressing this task\nwith more complex strategies.",
  "citation": 81
}