# COCO-Stuff: Thing and Stuff Classes in Context
Holger Caesar, Jasper Uijlings, Vittorio Ferrari

## 🧩 Problem to Solve
기존 객체 감지(object detection) 연구는 주로 형상이 명확한 '사물(thing)' 클래스(예: 자동차, 사람)에 집중해 왔습니다. 반면, '배경(stuff)' 클래스(예: 잔디, 하늘)는 형태가 불분명하고 식별 가능한 부분이 없어 상대적으로 적은 주목을 받았습니다. 그러나 배경 클래스는 (1) 장면 유형 파악, (2) 사물의 존재 가능성 및 위치 추론(맥락적 추론), (3) 물리적 속성, 재료 유형, 기하학적 특성 설명 등 이미지 이해에 중요한 역할을 합니다. 이 연구는 '사물'과 '배경'을 함께 이해하기 위해 기존 COCO 데이터셋에 배경 클래스 주석을 추가하고, 이들이 이미지 분석에서 얼마나 중요한지, 그리고 사물-배경 간의 관계를 탐구하는 것을 목표로 합니다.

## ✨ Key Contributions
*   **COCO-Stuff 데이터셋 도입**: COCO 2017 데이터셋의 164K 이미지 전체에 91개의 배경(stuff) 클래스에 대한 픽셀 단위 주석을 추가한 COCO-Stuff 데이터셋을 공개했습니다. 이 데이터셋은 사물(thing)과 배경(stuff) 클래스의 밀도 높은 주석을 가진 가장 큰 데이터셋입니다.
*   **효율적인 배경 주석 프로토콜 제안**: 기존 사물 주석과 슈퍼픽셀(superpixel)을 활용한 효율적인 배경 주석 프로토콜을 개발했습니다. 이 프로토콜은 높은 품질을 유지하면서 주석 시간을 크게 단축합니다.
*   **사물 및 배경 클래스 분석**: COCO-Stuff를 활용하여 다음을 분석했습니다:
    *   표면 점유율 및 이미지 캡션 언급 빈도를 통한 사물 및 배경 클래스의 중요성.
    *   사물과 배경 간의 공간적 관계 및 풍부한 맥락적 관계.
    *   현대적인 의미론적 분할(semantic segmentation) 방법이 사물 및 배경 클래스에서 어떻게 작동하는지, 그리고 배경 분할이 사물 분할보다 쉬운지에 대한 분석.

## 📎 Related Works
*   **사물과 배경의 정의**: 형상($\text{shape}$), 크기($\text{size}$), 부분($\text{parts}$), 인스턴스($\text{instances}$), 텍스처($\text{texture}$) 등 다양한 측면에서 사물과 배경을 정의하려는 시도(예: [21, 59, 28, 51, 55, 39, 17, 14]). 사물과 배경 감지에 다른 기술이 필요하며, 서로가 유용한 맥락적 단서가 됨을 보여준 연구들(예: [51, 53, 31, 14, 41, 27, 31, 38, 45])이 언급되었습니다.
*   **배경 전용 데이터셋**: 초기 질감 분류 데이터셋(예: [6, 15, 34, 9])과 최근의 Describable Textures Dataset [12], Materials in Context [5], Stanford Background dataset [24] 등이 있으나, 사물 클래스가 없어 사물-배경 관계 연구에는 부적합합니다.
*   **사물 전용 데이터셋**: PASCAL VOC [18], ILSVRC [43], COCO [35] 등은 컴퓨터 비전 연구 발전에 기여했지만, 배경 주석의 부재로 전체 장면 이해에 한계가 있습니다.
*   **사물 및 배경 혼합 데이터셋**: MSRC 21 [46], NYUD [47], CamVid [7], SIFT Flow [36], Barcelona [50], LM+SUN [52], PASCAL Context [38], Cityscapes [13], ADE20K [63] 등이 픽셀 단위의 사물 및 배경 주석을 제공하지만, COCO-Stuff는 이미지 수와 유용 클래스 수 면에서 가장 큰 규모입니다.
*   **데이터셋 주석 방법**: 상호작용 분할(interactive segmentation) [42, 57, 10], 슈퍼픽셀 주석 [61, 22, 40], 약한 감독 학습(weakly supervised learning) [3, 60, 30] 등 다양한 주석 방법들이 존재합니다. 본 연구는 슈퍼픽셀과 기존 COCO 사물 주석을 활용하여 효율적이고 고품질의 주석을 생성합니다.

## 🛠️ Methodology
1.  **COCO 2017 데이터셋 확장**: COCO 2017 데이터셋의 모든 164K 이미지(훈련 118K, 검증 5K, 테스트-개발 20K, 테스트-챌린지 20K)에 픽셀 단위 배경 주석을 추가했습니다.
2.  **배경 클래스 정의**:
    *   총 172개 클래스: 80개 사물 클래스(COCO와 동일), 91개 배경 클래스, 1개 알 수 없음(unlabeled) 클래스.
    *   전문 주석가가 91개 배경 클래스를 직접 선정했습니다.
    *   사전 정의된 상호 배타적인 레이블 셋을 사용하며, 이는 자유 형식(free-form) 텍스트 레이블의 문제점(클래스 과다, 동의어, 일관성 부족)을 방지합니다.
    *   **계층적 레이블 구조**: 배경 클래스를 계층 구조로 구성하여(예: `textile` 아래 `cloth`, `curtain`; `vegetation` 아래 `straw`, `moss`, `bush`, `grass` 등) 주석의 일관성과 검색 효율성을 높였습니다. `vegetation-other`와 같이 광범위한 하위 클래스를 두어 데이터의 산발적 분포를 피했습니다.
    *   일부 슈퍼 카테고리(`floor`, `wall`, `ceiling`)는 재료 유형(`wall-brick`, `wall-concrete`)을 포함하여 정의했습니다.
3.  **주석 프로토콜**:
    *   **슈퍼픽셀 기반**: 각 이미지를 SLICO [1]를 사용하여 1,000개의 슈퍼픽셀로 분할합니다. 슈퍼픽셀은 경계를 잘 따르고 균일한 크기를 가지므로 수동 경계 정의 필요 없이 효율적인 주석이 가능합니다.
    *   **페인트브러시 도구**: 크기 조절이 가능한 페인트브러시 도구를 제공하여 넓은 배경 영역을 효율적으로 주석할 수 있도록 했습니다.
    *   **기존 사물 주석 활용**: COCO의 정밀한 사물 주석을 오버레이로 제공하여, 주석가가 배경 클래스를 선택하고 브러시로 칠할 때 사물에 해당하는 픽셀은 고정되어 영향을 받지 않도록 했습니다. 이는 사물-배경 경계에서 매우 정확한 배경 윤곽선을 얻는 데 기여했습니다.
    *   **계층적 레이블 제시**: 주석가에게 전체 레이블 계층 구조를 제시하여 레이블 검색 시간을 단축했습니다.
    *   **효율성**: 이 프로토콜은 COCO의 복잡한 이미지 하나당 평균 3분이라는 매우 빠른 주석 시간을 달성했습니다.
4.  **주석 프로토콜 분석**:
    *   **품질-속도 균형**: 슈퍼픽셀 주석, 다각형(polygon) 주석, 자유 그리기(freedraw) 주석 세 가지 방식을 비교했습니다. 자유 그리기 주석을 지면 진실(ground-truth) 기준으로 사용했을 때, 슈퍼픽셀과 다각형 주석은 자유 그리기보다 각각 2.8배, 1.5배 빨랐고, 픽셀 단위 주석 일치도는 96%-97%로 매우 정확했습니다.
    *   **경계 복잡성($\text{boundary complexity}$)**: 이미지의 경계 복잡성이 높을수록 주석 시간이 길어지며, 슈퍼픽셀 방식이 다른 방식보다 경계 복잡성에 따른 시간 증가폭이 훨씬 작습니다.
    *   **사물 오버레이($\text{thing overlays}$)**: 사물 오버레이 사용 시 자유 그리기에서 1.8배, 슈퍼픽셀에서 1.2배의 유의미한 속도 향상이 있었으며, 주석 품질 저하는 없었습니다. COCO-Stuff의 경계 픽셀 중 46.8%가 사물 클래스에 인접하여 사물 오버레이가 경계 복잡성을 크게 줄여주었습니다.
    *   **주석가 간 일치도($\text{across-annotator agreement}$)**: COCO-Stuff의 평균 레이블 일치도는 73.6%로, ADE20K의 66.8%보다 높습니다.

## 📊 Results
*   **데이터셋 규모 및 유용성**:
    *   COCO-Stuff는 164K 이미지로 픽셀 단위 사물 및 배경 주석을 가진 데이터셋 중 가장 크고, PASCAL Context, ADE20K 등 다른 데이터셋보다 훨씬 많은 '유용(usable)' 배경 클래스(1,000개 이상 이미지에 나타나는 클래스가 84개)를 포함합니다.
    *   모든 레이블은 상호 배타적이고 유사한 세분화 수준을 가지며, 각 픽셀은 정확히 하나의 레이블을 가집니다(다른 데이터셋에서 발생할 수 있는 경계 모호성 없음).
*   **사물 및 배경의 중요성**:
    *   **표면 점유율**: COCO-Stuff 주석에서 픽셀의 69.1%, 영역(connected components)의 69.4%가 배경 클래스로 이루어져 이미지의 대부분을 배경이 차지합니다.
    *   **인간 설명**: COCO 이미지 캡션의 명사 중 38.2%가 배경 클래스를 지칭합니다. 이는 배경이 단순히 관련 없는 배경 픽셀이 아니라 인간에게도 장면 이해에 중요한 요소임을 나타냅니다.
*   **사물과 배경 간의 공간적 맥락**:
    *   **맥락 시각화**: 기차(train)는 보통 철도(railroad) 위에, TV는 사람(person) 앞에, 타일 벽(wall-tile)은 타일 바닥(floor-tile) 위에, 도로는 양쪽에 사람(person)이 있는 등 다양한 사물-배경, 사물-사물, 배경-배경 공간 관계 패턴을 시각적으로 확인했습니다.
    *   **정보량($\text{entropy}$)**: 배경 클래스는 사물 클래스(평균 3.40 vs. 3.02)보다 평균 엔트로피가 유의미하게 높아 더 다양한 맥락에서 나타납니다. 수직 관계(예: '위에')가 가장 정보적인 맥락 유형임을 시사합니다.
    *   **데이터셋 간 맥락 풍부도**: COCO-Stuff는 PASCAL Context (60개 유용 클래스), ADE20K (150개 유용 클래스), SIFT Flow 등 다른 데이터셋보다 전반적으로 가장 높은 평균 엔트로피를 보여 맥락적 풍부함이 가장 높습니다.
*   **의미론적 분할 성능**:
    *   DeepLab V2 [11] 모델(VGG-16 기반)을 사용하여 성능을 평가했습니다.
    *   **데이터셋 크기 영향**: 훈련 데이터셋 크기가 1K에서 118K로 증가함에 따라 모든 평가 지표(픽셀 정확도, 클래스 정확도, 평균 IOU, FW IOU)에서 성능이 유의미하게 향상됨을 확인했습니다.
    *   **배경이 사물보다 쉬운가?**: COCO-Stuff에서 DeepLab V2는 사물 클래스(mIOU 43.6%)보다 배경 클래스(mIOU 24.0%)에서 성능이 **현저히 낮게** 나타났습니다. 이는 기존 연구에서 배경이 사물보다 분할하기 쉽다는 통념에 반하는 결과이며, COCO-Stuff는 사물과 배경 클래스가 유사하게 분포하고 세분화된 특징을 가지기 때문임을 시사합니다.

## 🧠 Insights & Discussion
*   **배경 클래스의 중요성 재조명**: 배경은 단순히 이미지의 '배경'이 아니라 장면 이해에 필수적인 구성 요소임을 데이터(표면 점유율, 인간 캡션 언급)를 통해 강력하게 뒷받침합니다.
*   **풍부한 맥락적 관계**: COCO-Stuff는 사물-배경, 사물-사물, 배경-배경 간의 복잡하고 유용한 공간적 맥락 정보를 제공하며, 이는 장면의 3D 배치, 사물의 위치 제약, 관계 이해에 기여합니다. 배경 클래스가 사물 클래스보다 더 다양한 맥락에서 나타난다는 점은 배경 정보가 더욱 광범위하게 활용될 수 있음을 보여줍니다.
*   **배경 분할의 난이도 재평가**: 기존 연구와 달리 COCO-Stuff에서 배경 클래스가 사물 클래스보다 분할하기 어렵다는 결과는, 데이터셋의 클래스 정의(세분화 수준, 분포)가 의미론적 분할 난이도에 큰 영향을 미친다는 중요한 시사점을 제공합니다. 이는 실제 환경에서 배경 클래스도 사물 클래스만큼 복잡하고 다양한 형태를 가질 수 있음을 의미합니다.
*   **대규모 데이터셋의 지속적인 가치**: Deep Learning 모델의 성능이 데이터셋 규모가 커질수록 지속적으로 향상됨을 보여주며, 이는 대규모, 고품질 주석 데이터셋 구축의 중요성을 강조합니다.

## 📌 TL;DR
장면 이해에 필수적이지만 간과되었던 **배경(stuff) 클래스**의 중요성을 강조하기 위해, 이 논문은 기존 COCO 데이터셋에 91개 배경 클래스 주석을 추가한 **COCO-Stuff**를 제안합니다. 슈퍼픽셀과 기존 사물(thing) 주석을 활용한 효율적인 주석 프로토콜을 통해 고품질의 대규모 데이터셋을 구축했으며, 이를 통해 배경이 이미지 표면의 대다수를 차지하고 인간 설명에서도 중요한 역할을 함을 보였습니다. 또한, COCO-Stuff의 풍부한 **사물-배경 맥락적 관계**를 분석하고, 의미론적 분할 실험에서 **배경 분할이 사물 분할보다 어렵다**는 기존 통념과 다른 결과를 보여, 배경 클래스에 대한 새로운 연구 방향을 제시합니다.