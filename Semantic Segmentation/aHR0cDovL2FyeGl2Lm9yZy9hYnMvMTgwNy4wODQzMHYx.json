{
  "url": "http://arxiv.org/abs/1807.08430v1",
  "title": "Actor-Action Semantic Segmentation with Region Masks",
  "authors": "Kang Dang, Chunluan Zhou, Zhigang Tu, Michael Hoy, Justin Dauwels, Junsong Yuan",
  "year": 2018,
  "abstract": "In this paper, we study the actor-action semantic segmentation problem, which\nrequires joint labeling of both actor and action categories in video frames.\nOne major challenge for this task is that when an actor performs an action,\ndifferent body parts of the actor provide different types of cues for the\naction category and may receive inconsistent action labeling when they are\nlabeled independently. To address this issue, we propose an end-to-end\nregion-based actor-action segmentation approach which relies on region masks\nfrom an instance segmentation algorithm. Our main novelty is to avoid labeling\npixels in a region mask independently - instead we assign a single action label\nto these pixels to achieve consistent action labeling. When a pixel belongs to\nmultiple region masks, max pooling is applied to resolve labeling conflicts.\nOur approach uses a two-stream network as the front-end (which learns features\ncapturing both appearance and motion information), and uses two region-based\nsegmentation networks as the back-end (which takes the fused features from the\ntwo-stream network as the input and predicts actor-action labeling).\nExperiments on the A2D dataset demonstrate that both the region-based\nsegmentation strategy and the fused features from the two-stream network\ncontribute to the performance improvements. The proposed approach outperforms\nthe state-of-the-art results by more than 8% in mean class accuracy, and more\nthan 5% in mean class IOU, which validates its effectiveness.",
  "citation": 8
}