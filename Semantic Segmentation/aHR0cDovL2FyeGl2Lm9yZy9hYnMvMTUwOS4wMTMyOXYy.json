{
  "url": "http://arxiv.org/abs/1509.01329v2",
  "title": "Semantic Amodal Segmentation",
  "authors": "Yan Zhu, Yuandong Tian, Dimitris Mexatas, Piotr Doll√°r",
  "year": 2015,
  "abstract": "Common visual recognition tasks such as classification, object detection, and\nsemantic segmentation are rapidly reaching maturity, and given the recent rate\nof progress, it is not unreasonable to conjecture that techniques for many of\nthese problems will approach human levels of performance in the next few years.\nIn this paper we look to the future: what is the next frontier in visual\nrecognition?\n  We offer one possible answer to this question. We propose a detailed image\nannotation that captures information beyond the visible pixels and requires\ncomplex reasoning about full scene structure. Specifically, we create an amodal\nsegmentation of each image: the full extent of each region is marked, not just\nthe visible pixels. Annotators outline and name all salient regions in the\nimage and specify a partial depth order. The result is a rich scene structure,\nincluding visible and occluded portions of each region, figure-ground edge\ninformation, semantic labels, and object overlap.\n  We create two datasets for semantic amodal segmentation. First, we label 500\nimages in the BSDS dataset with multiple annotators per image, allowing us to\nstudy the statistics of human annotations. We show that the proposed full scene\nannotation is surprisingly consistent between annotators, including for regions\nand edges. Second, we annotate 5000 images from COCO. This larger dataset\nallows us to explore a number of algorithmic ideas for amodal segmentation and\ndepth ordering. We introduce novel metrics for these tasks, and along with our\nstrong baselines, define concrete new challenges for the community."
}