{
  "url": "http://arxiv.org/abs/2411.12814v2",
  "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
  "authors": "Junlong Cheng, Bin Fu, Jin Ye, Guoan Wang, Tianbin Li, Haoyu Wang, Ruoyu Li, He Yao, Junren Chen, Jingwen Li, Yanzhou Su, Min Zhu, Junjun He",
  "year": 2024,
  "abstract": "Interactive Medical Image Segmentation (IMIS) has long been constrained by\nthe limited availability of large-scale, diverse, and densely annotated\ndatasets, which hinders model generalization and consistent evaluation across\ndifferent models. In this paper, we introduce the IMed-361M benchmark dataset,\na significant advancement in general IMIS research. First, we collect and\nstandardize over 6.4 million medical images and their corresponding ground\ntruth masks from multiple data sources. Then, leveraging the strong object\nrecognition capabilities of a vision foundational model, we automatically\ngenerated dense interactive masks for each image and ensured their quality\nthrough rigorous quality control and granularity management. Unlike previous\ndatasets, which are limited by specific modalities or sparse annotations,\nIMed-361M spans 14 modalities and 204 segmentation targets, totaling 361\nmillion masks-an average of 56 masks per image. Finally, we developed an IMIS\nbaseline network on this dataset that supports high-quality mask generation\nthrough interactive inputs, including clicks, bounding boxes, text prompts, and\ntheir combinations. We evaluate its performance on medical image segmentation\ntasks from multiple perspectives, demonstrating superior accuracy and\nscalability compared to existing interactive segmentation models. To facilitate\nresearch on foundational models in medical computer vision, we release the\nIMed-361M and model at https://github.com/uni-medical/IMIS-Bench."
}