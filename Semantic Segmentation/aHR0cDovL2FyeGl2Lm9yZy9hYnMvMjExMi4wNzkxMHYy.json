{
  "url": "http://arxiv.org/abs/2112.07910v2",
  "title": "Decoupling Zero-Shot Semantic Segmentation",
  "authors": "Jian Ding, Nan Xue, Gui-Song Xia, Dengxin Dai",
  "year": 2021,
  "abstract": "Zero-shot semantic segmentation (ZS3) aims to segment the novel categories\nthat have not been seen in the training. Existing works formulate ZS3 as a\npixel-level zeroshot classification problem, and transfer semantic knowledge\nfrom seen classes to unseen ones with the help of language models pre-trained\nonly with texts. While simple, the pixel-level ZS3 formulation shows the\nlimited capability to integrate vision-language models that are often\npre-trained with image-text pairs and currently demonstrate great potential for\nvision tasks. Inspired by the observation that humans often perform\nsegment-level semantic labeling, we propose to decouple the ZS3 into two\nsub-tasks: 1) a classagnostic grouping task to group the pixels into segments.\n2) a zero-shot classification task on segments. The former task does not\ninvolve category information and can be directly transferred to group pixels\nfor unseen classes. The latter task performs at segment-level and provides a\nnatural way to leverage large-scale vision-language models pre-trained with\nimage-text pairs (e.g. CLIP) for ZS3. Based on the decoupling formulation, we\npropose a simple and effective zero-shot semantic segmentation model, called\nZegFormer, which outperforms the previous methods on ZS3 standard benchmarks by\nlarge margins, e.g., 22 points on the PASCAL VOC and 3 points on the COCO-Stuff\nin terms of mIoU for unseen classes. Code will be released at\nhttps://github.com/dingjiansw101/ZegFormer.",
  "citation": 434
}