{
  "url": "http://arxiv.org/abs/2003.09669v1",
  "title": "BiCANet: Bi-directional Contextual Aggregating Network for Image\n  Semantic Segmentation",
  "authors": "Quan Zhou, Dechun Cong, Bin Kang, Xiaofu Wu, Baoyu Zheng, Huimin Lu, Longin Jan Latecki",
  "year": 2020,
  "abstract": "Exploring contextual information in convolution neural networks (CNNs) has\ngained substantial attention in recent years for semantic segmentation. This\npaper introduces a Bi-directional Contextual Aggregating Network, called\nBiCANet, for semantic segmentation. Unlike previous approaches that encode\ncontext in feature space, BiCANet aggregates contextual cues from a categorical\nperspective, which is mainly consist of three parts: contextual condensed\nprojection block (CCPB), bi-directional context interaction block (BCIB), and\nmuti-scale contextual fusion block (MCFB). More specifically, CCPB learns a\ncategory-based mapping through a split-transform-merge architecture, which\ncondenses contextual cues with different receptive fields from intermediate\nlayer. BCIB, on the other hand, employs dense skipped-connections to enhance\nthe class-level context exchanging. Finally, MCFB integrates multi-scale\ncontextual cues by investigating short- and long-ranged spatial dependencies.\nTo evaluate BiCANet, we have conducted extensive experiments on three semantic\nsegmentation datasets: PASCAL VOC 2012, Cityscapes, and ADE20K. The\nexperimental results demonstrate that BiCANet outperforms recent\nstate-of-the-art networks without any postprocess techniques. Particularly,\nBiCANet achieves the mIoU score of 86.7%, 82.4% and 38.66% on PASCAL VOC 2012,\nCityscapes and ADE20K testset, respectively."
}