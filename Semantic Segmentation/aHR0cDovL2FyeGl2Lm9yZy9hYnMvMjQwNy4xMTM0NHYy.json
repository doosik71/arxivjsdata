{
  "url": "http://arxiv.org/abs/2407.11344v2",
  "title": "Centering the Value of Every Modality: Towards Efficient and Resilient\n  Modality-agnostic Semantic Segmentation",
  "authors": "Xu Zheng, Yuanhuiyi Lyu, Jiazhou Zhou, Lin Wang",
  "year": 2024,
  "abstract": "Fusing an arbitrary number of modalities is vital for achieving robust\nmulti-modal fusion of semantic segmentation yet remains less explored to date.\nRecent endeavors regard RGB modality as the center and the others as the\nauxiliary, yielding an asymmetric architecture with two branches. However, the\nRGB modality may struggle in certain circumstances, e.g., nighttime, while\nothers, e.g., event data, own their merits; thus, it is imperative for the\nfusion model to discern robust and fragile modalities, and incorporate the most\nrobust and fragile ones to learn a resilient multi-modal framework. To this\nend, we propose a novel method, named MAGIC, that can be flexibly paired with\nvarious backbones, ranging from compact to high-performance models. Our method\ncomprises two key plug-and-play modules. Firstly, we introduce a multi-modal\naggregation module to efficiently process features from multi-modal batches and\nextract complementary scene information. On top, a unified arbitrary-modal\nselection module is proposed to utilize the aggregated features as the\nbenchmark to rank the multi-modal features based on the similarity scores. This\nway, our method can eliminate the dependence on RGB modality and better\novercome sensor failures while ensuring the segmentation performance. Under the\ncommonly considered multi-modal setting, our method achieves state-of-the-art\nperformance while reducing the model parameters by 60%. Moreover, our method is\nsuperior in the novel modality-agnostic setting, where it outperforms prior\narts by a large margin of +19.41% mIoU",
  "citation": 19
}