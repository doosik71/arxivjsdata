{
  "title": "U-MixFormer: UNet-like Transformer with Mix-Attention for Efficient\n  Semantic Segmentation",
  "authors": "Seul-Ki Yeom, Julian von Klitzing",
  "year": 2023,
  "url": "http://arxiv.org/abs/2312.06272v1",
  "abstract": "Semantic segmentation has witnessed remarkable advancements with the\nadaptation of the Transformer architecture. Parallel to the strides made by the\nTransformer, CNN-based U-Net has seen significant progress, especially in\nhigh-resolution medical imaging and remote sensing. This dual success inspired\nus to merge the strengths of both, leading to the inception of a U-Net-based\nvision transformer decoder tailored for efficient contextual encoding. Here, we\npropose a novel transformer decoder, U-MixFormer, built upon the U-Net\nstructure, designed for efficient semantic segmentation. Our approach\ndistinguishes itself from the previous transformer methods by leveraging\nlateral connections between the encoder and decoder stages as feature queries\nfor the attention modules, apart from the traditional reliance on skip\nconnections. Moreover, we innovatively mix hierarchical feature maps from\nvarious encoder and decoder stages to form a unified representation for keys\nand values, giving rise to our unique mix-attention module. Our approach\ndemonstrates state-of-the-art performance across various configurations.\nExtensive experiments show that U-MixFormer outperforms SegFormer, FeedFormer,\nand SegNeXt by a large margin. For example, U-MixFormer-B0 surpasses\nSegFormer-B0 and FeedFormer-B0 with 3.8% and 2.0% higher mIoU and 27.3% and\n21.8% less computation and outperforms SegNext with 3.3% higher mIoU with\nMSCAN-T encoder on ADE20K. Code available at\nhttps://github.com/julian-klitzing/u-mixformer.",
  "citation": 28
}