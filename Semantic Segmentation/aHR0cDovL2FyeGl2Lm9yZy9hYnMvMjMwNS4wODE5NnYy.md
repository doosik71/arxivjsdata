# A Comprehensive Survey on Segment Anything Model for Vision and Beyond

Chunhui Zhang, Li Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, Yuehong Hu

## 🧩 Problem to Solve

인공지능(AI)은 특정 작업에 특화된 협소 AI를 넘어 인간과 유사한 수준의 지능으로 광범위한 작업을 수행할 수 있는 인공 일반 지능(AGI)을 목표로 진화하고 있습니다. 이를 위해 다양한 다운스트림 작업에 쉽게 적용될 수 있는 범용 모델, 즉 파운데이션 모델(Foundation Models)의 개발이 시급합니다. 특히 컴퓨터 비전(CV) 분야에서는 대규모 데이터셋으로 학습되어 강력한 제로샷(zero-shot) 일반화 능력을 갖춘 태스크에 구애받지 않는 분할 모델이 부족하다는 문제가 있었습니다. 최근 제안된 SAM(Segment Anything Model)은 이러한 분할 작업의 경계를 허물고 CV를 위한 파운데이션 모델 개발을 촉진하며 이 문제를 해결하고자 합니다.

## ✨ Key Contributions

- **SAM(Segment Anything Model)의 포괄적 분석:** SAM의 개발 배경, 최신 발전, 그리고 광범위한 응용 분야에 미치는 영향을 종합적으로 검토한 최초의 서베이 논문입니다.
- **프롬프트 기반 분할 패러다임 도입:** 사용자가 제공하는 다양한 프롬프트(점, 상자, 마스크, 텍스트)에 반응하여 객체 분할을 수행하는 새로운 방식을 제안하고, 이를 통해 강력한 제로샷 일반화 능력을 확보했습니다.
- **대규모 SA-1B 데이터셋 구축:** 1,100만 개의 라이선스 이미지에서 10억 개 이상의 마스크를 포함하는 세계 최대 규모의 분할 데이터셋 SA-1B를 혁신적인 데이터 엔진을 통해 구축했습니다.
- **놀라운 제로샷 일반화 성능 입증:** 학습 시 접하지 않은 객체, 낯선 장면(수중, 세포 현미경 등), 모호한 경우에도 객체 개념을 일반화하여 분할하는 능력을 보여, "CV의 GPT-3 모멘트"로 평가받습니다.
- **다양한 응용 분야로의 확장 가능성 제시:** 이미지 편집, 의료 영상 분석, 스타일 전이, 객체 탐지 및 추적, 3D 재구성, 로봇 공학 등 광범위한 시각 및 비시각 관련 작업에서의 SAM 활용 사례를 정리하고 분석했습니다.
- **미래 연구 방향 제시:** SAM의 장점과 한계를 분석하여, 더 다재다능한 파운데이션 모델을 개발하고 SAM 아키텍처를 개선하기 위한 연구 방향을 제시합니다.

## 📎 Related Works

- **파운데이션 모델:** NLP 분야의 BERT, T5, GPT-3/4, ChatGPT와 CV 분야의 ViT-G, CLIP, ALIGN, Florence, DALL-E, 그리고 멀티모달 학습의 ImageBind를 언급합니다.
- **동시 연구된 분할 모델:**
  - **OneFormer:** 태스크 조건부 학습 전략을 통해 다양한 전통적인 분할 태스크를 단일 모델로 통합합니다.
  - **SegGPT:** 문맥 내 학습(in-context learning) 프레임워크를 탐구하며, 다양한 분할 데이터 형식을 통일하고 무작위 색상 체계로 학습하여 강력한 제로샷 능력을 보여줍니다.
  - **SEEM:** SAM보다 더 다양한 프롬프트(시각, 텍스트, 참조 영역)를 도입하여 단일 분할 모델의 적용 범위를 확장합니다.
- **개방형 어휘 감지/분할 모델:** Grounding DINO, OVSeg, V3Det, OpenSeg와 같이 열린 어휘 시나리오에서 객체를 감지/분할하는 일반 AI 방법을 언급합니다.

## 🛠️ Methodology

SAM은 프롬프트 기반 분할 태스크를 수행하기 위해 세 가지 핵심 구성 요소로 이루어져 있습니다:

- **이미지 인코더 (Image Encoder):**

  - 대규모 데이터와 강력한 사전 학습 방법론을 기반으로 MAE(Masked Autoencoders)로 사전 학습된 Vision Transformer (ViT)를 사용합니다.
  - 이미지별로 한 번 실행되며, 모델에 프롬프트를 제공하기 전에 이미지 임베딩을 효율적으로 생성합니다.

- **프롬프트 인코더 (Prompt Encoder):**

  - **희소(Sparse) 프롬프트:** 점, 상자, 텍스트를 처리합니다. 점과 상자는 위치 인코딩과 학습된 임베딩을 사용하여 표현되며, 자유 형식 텍스트는 CLIP의 텍스트 인코더를 통해 처리됩니다.
  - **밀집(Dense) 프롬프트:** 마스크는 컨볼루션을 사용하여 임베딩되고 이미지 임베딩과 원소별로 합산됩니다.

- **마스크 디코더 (Mask Decoder):**

  - 이미지 임베딩, 프롬프트 임베딩, 출력 토큰을 효율적으로 마스크로 매핑합니다.
  - 수정된 트랜스포머 디코더 블록을 사용하며, 프롬프트 자가 어텐션(self-attention)과 이미지-프롬프트 양방향 교차 어텐션(cross-attention)을 통해 모든 임베딩을 업데이트합니다.
  - 모호한 프롬프트에 대해 여러 개의 유효한 마스크와 관련된 신뢰도 점수를 출력하여 모호성을 해결합니다.

- **데이터 엔진 (Data Engine) SA-1B 구축:** 분할 마스크 데이터의 부족을 해결하기 위해 모델 학습과 데이터셋 구축을 동시에 수행하는 3단계의 반복적인 데이터 수집 프로세스를 사용합니다.

  1. **수동 보조 단계 (Assisted-manual stage):** SAM 기반의 대화형 주석 도구를 사용하여 전문 주석가가 수동으로 마스크를 라벨링합니다.
  2. **반자동 단계 (Semi-automatic stage):** 모델이 높은 신뢰도로 예측한 마스크를 미리 채우고, 주석가가 상호작용하며 나머지 채워지지 않은 부분을 주석 처리합니다.
  3. **완전 자동 단계 (Fully automatic stage):** 충분히 많은 마스크가 수집된 후, 모호성을 인식하는 모델이 각 이미지에 32x32 그리드 프롬프트 포인트를 적용하여 마스크를 자동으로 생성하고 신뢰도를 기반으로 필터링/정렬합니다.

- **학습 손실 함수:** Focal Loss와 Dice Loss의 선형 조합을 사용하여 마스크 예측을 감독합니다.

## 📊 Results

SAM은 설계된 태스크, 모델 구조, 그리고 대규모 고품질 학습 데이터의 이점을 바탕으로 다양한 분할 작업에서 뛰어난 제로샷 전이 능력을 입증했습니다. 이는 단일 큐 포인트 분할, 엣지 검출, 객체 제안, 인스턴스 분할, 대화형 분할, 그리고 멀티모달 분할(Text-to-Mask) 태스크를 포함합니다. 주목할 만한 점은 일부 측면에서 기존의 완전 지도 학습(fully supervised) 모델보다 우수한 성능을 보여주기도 했다는 것입니다. 하지만 SAM은 저대비 장면이나 특정 도메인(예: 위장 동물, 산업 결함, 의료 병변, 유리/투명 객체)에서는 성능이 저하되거나, 특정 분야의 강한 사전 지식이 필요하다는 한계도 발견되었습니다.

## 🧠 Insights & Discussion

- **장점:**
  - **탁월한 일반화 능력:** SAM은 훈련 데이터에서 보지 못한 객체와 장면에 대해서도 뛰어난 제로샷 분할 능력을 보여줍니다. 이는 '객체가 무엇인지'에 대한 일반적인 개념을 학습했음을 시사합니다.
  - **높은 유연성 및 상호작용성:** 다양한 형태의 프롬프트(점, 상자, 텍스트)를 받아들여 사용자에게 유연하고 대화형적인 분할 경험을 제공합니다.
  - **광범위한 적용 가능성:** 이미지 편집, 스타일 전이, 객체 감지, 객체 카운팅, 이동 객체 분할, 의료 영상 분석, 데이터 주석, 3D 재구성, 비유클리드 도메인, 로봇 공학, 비디오 텍스트 인식, 시각 및 언어 통합, 오디오 및 비전 통합 등 다양한 분야에서 SAM의 잠재력이 확인되었습니다.
  - **데이터 주석 효율성 향상:** SAM은 대규모 고품질 의사 라벨(pseudo-labels) 생성을 매우 쉽고 효과적으로 만들어, 데이터 주석의 시간과 비용을 크게 절감할 수 있습니다.
- **한계:**
  - **복잡하거나 특정 도메인에서의 성능 저하:** 저대비 장면(위장 객체, 유리 객체), 의료 영상, 원격 감지 이미지와 같이 일반적인 장면과 특성이 다른 경우 SAM의 성능이 제한될 수 있으며, 특정 분야의 사전 지식이나 추가적인 미세 조정이 필요합니다.
  - **작고 혼잡한 객체 처리의 어려움:** 소수샷 객체 카운팅과 같은 작업에서 작거나 밀집된 객체에 대한 분할 및 구별 능력은 SOTA 모델에 비해 부족할 수 있습니다.
  - **적대적 강건성 취약:** 화이트박스 공격에 취약하며, 특정 유형의 이미지 섭동(perturbations)에 의해 성능이 저하될 수 있어, 안전이 중요한 애플리케이션에 배포 시 추가적인 방어 전략이 요구됩니다.
  - **의미론적 모호성:** 약지도 의미론적 분할(WSSS)에서 SAM이 때때로 부정확한 객체 마스크를 생성하거나, '하늘', '바다', '도로'와 같은 "stuff" 클래스 분할에 어려움을 겪을 수 있습니다.
- **향후 연구 방향:**
  - SAM 아키텍처의 지속적인 개선과 특정 도메인(예: 의료, 원격 감지) 지식의 통합을 통한 모델의 강건성 및 일반화 능력 향상.
  - SAM을 다른 파운데이션 모델(예: LLM, Grounding DINO, CLIP)과 결합하여 멀티모달 기능 강화 및 복합적인 AI 태스크 해결.
  - 데이터 효율적인 학습(원샷 학습) 및 적대적 방어 전략 연구.
  - 설명 가능한 AI(XAI) 분야에서 SAM의 활용과 잠재적 사회적 영향에 대한 심층 연구.

## 📌 TL;DR

- **문제:** 인공 일반 지능(AGI)을 향한 컴퓨터 비전 분야에서 광범위한 작업에 적용 가능한 범용 이미지 분할 파운데이션 모델의 부재.
- **제안 방법:** Meta AI Research는 프롬프트 기반 분할 태스크, 효율적인 모델 아키텍처(이미지 인코더, 프롬프트 인코더, 마스크 디코더), 그리고 10억 개 이상의 마스크를 포함하는 대규모 SA-1B 데이터셋을 통해 Segment Anything Model (SAM)을 제안합니다.
- **주요 발견:** SAM은 미지의 객체와 장면에도 뛰어난 제로샷 일반화 성능을 보여주며, 이미지 편집, 의료 영상 분석, 객체 추적 등 다양한 시각 및 비시각 관련 애플리케이션에서 강력한 잠재력을 입증합니다. 하지만 저대비 환경이나 적대적 공격에는 여전히 한계가 존재하며, 특정 도메인에 대한 추가적인 개선 및 다른 파운데이션 모델과의 통합이 향후 연구 방향으로 제시됩니다.
