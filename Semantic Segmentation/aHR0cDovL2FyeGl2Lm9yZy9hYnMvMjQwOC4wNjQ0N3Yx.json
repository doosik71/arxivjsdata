{
  "url": "http://arxiv.org/abs/2408.06447v1",
  "title": "S-SAM: SVD-based Fine-Tuning of Segment Anything Model for Medical Image\n  Segmentation",
  "authors": "Jay N. Paranjape, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel",
  "year": 2024,
  "abstract": "Medical image segmentation has been traditionally approached by training or\nfine-tuning the entire model to cater to any new modality or dataset. However,\nthis approach often requires tuning a large number of parameters during\ntraining. With the introduction of the Segment Anything Model (SAM) for\nprompted segmentation of natural images, many efforts have been made towards\nadapting it efficiently for medical imaging, thus reducing the training time\nand resources. However, these methods still require expert annotations for\nevery image in the form of point prompts or bounding box prompts during\ntraining and inference, making it tedious to employ them in practice. In this\npaper, we propose an adaptation technique, called S-SAM, that only trains\nparameters equal to 0.4% of SAM's parameters and at the same time uses simply\nthe label names as prompts for producing precise masks. This not only makes\ntuning SAM more efficient than the existing adaptation methods but also removes\nthe burden of providing expert prompts. We call this modified version S-SAM and\nevaluate it on five different modalities including endoscopic images, x-ray,\nultrasound, CT, and histology images. Our experiments show that S-SAM\noutperforms state-of-the-art methods as well as existing SAM adaptation methods\nwhile tuning a significantly less number of parameters. We release the code for\nS-SAM at https://github.com/JayParanjape/SVDSAM."
}