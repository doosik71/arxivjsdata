# Learning Video Object Segmentation from Static Images

Anna Khoreva, Federico Perazzi, Rodrigo Benenson, Bernt Schiele, Alexander Sorkine-Hornung

## 🧩 Problem to Solve

비디오 객체 분할(Video Object Segmentation, VOS)은 비디오의 초기 몇몇 프레임에 주어진 특정 객체의 마스크 주석을 바탕으로, 해당 객체의 모든 프레임을 정확하게 분할하는 문제입니다. 기존 컨볼루션 신경망(ConvNet) 기반 접근 방식은 픽셀 단위로 주석이 달린 대규모 비디오 데이터셋이 필요하지만, 이러한 데이터셋을 구축하는 것은 매우 비용이 많이 들고 어렵습니다. 따라서, 비디오 데이터를 사용하지 않고도 높은 정확도의 VOS를 달성하는 효율적이고 유연한 방법이 요구됩니다.

## ✨ Key Contributions

- **정적 이미지로 학습된 ConvNet:** 완전한 비디오 주석 없이 정적 이미지로만 훈련된 ConvNet을 사용하여 높은 정확도의 비디오 객체 분할을 가능하게 함을 시연했습니다.
- **가이드 인스턴스 분할(Guided Instance Segmentation) 개념 도입:** 이전 프레임의 마스크 추정치를 다음 프레임의 객체 분할을 위한 "가이드"로 사용하여 프레임별 인스턴스 분할을 수행하는 새로운 접근 방식을 제시했습니다. 이는 비디오 객체 분할을 위한 최초의 완전 학습(fully trained) 접근 방식입니다.
- **오프라인 및 온라인 학습 전략 결합:**
  - **오프라인 학습:** 대규모 정적 이미지 데이터셋에서 네트워크가 이전 프레임의 대략적인 추정치로부터 정제된 마스크를 생성하도록 학습시킵니다.
  - **온라인 학습:** 새로운 비디오의 첫 주석 프레임(또는 여러 프레임)을 사용하여 특정 객체 인스턴스의 외관에 맞게 모델을 미세 조정(fine-tuning)합니다.
- **유연한 입력 주석 처리:** 바운딩 박스(bounding box)와 세그먼트 마스크(segment mask) 등 다양한 형태의 초기 주석을 입력으로 받아들일 수 있으며, 여러 프레임의 주석을 통합할 수 있어 다양한 응용 분야에 적합합니다.
- **효율성:** 프레임별 순방향(feed-forward) 아키텍처로 효율적인 처리가 가능하며, 여러 프레임에 걸친 전역 최적화 없이 고품질 결과를 생성합니다.

## 📎 Related Works

- **픽셀 레벨 트래킹 기반 VOS:**
  - **지역 전파(Local Propagation):** JOTS [47], ObjFlow [44], BVS [26]와 같이 인접 프레임 간 정보 전파에 중점을 두나, 장거리 관계 및 전역 일관성 유지에 어려움이 있습니다.
  - **전역 전파(Global Propagation):** FCP [31], Z15 [51], W16 [45] 등 비디오 프레임 간 장거리 연결을 사용하지만, 속도와 메모리 사용을 위해 슈퍼픽셀이나 객체 제안(object proposals)에 의존하여 세밀한 이미지 처리에 한계가 있습니다.
- **비지도 분할(Unsupervised Segmentation):** NLC [14] 등 일반적인 움직이는 객체 분할을 수행하고 주석과 가장 잘 맞는 시공간 튜브를 선택하는 방법입니다.
- **바운딩 박스 트래킹(Box Tracking):** GOTURN [18], MDNet [29] 등 객체 추적을 활용하여 분할 품질을 향상시키며, 특히 MDNet의 온라인 미세 조정 아이디어는 본 연구의 온라인 학습 전략에 영향을 주었습니다.
- **인스턴스 분할(Instance Segmentation):** DeepMask [32], Hypercolumns [17] 등 객체 위치 및 크기 추정치를 기반으로 단일 인스턴스를 분할하는 ConvNet 아키텍처를 사용합니다. 본 연구는 DeepLabv2 [8]를 기반으로 합니다.

## 🛠️ Methodology

본 연구는 비디오 객체 분할 문제를 "가이드 인스턴스 분할"로 재정의하고, `MaskTrack`이라는 ConvNet 기반 시스템을 제안합니다.

- **기본 아키텍처:** DeepLabv2 [8]를 기반으로 하며, 입력은 RGB 이미지(3채널)와 이전 프레임의 마스크 추정치(1채널)를 합친 4채널입니다. 이 추가 마스크 채널은 네트워크에 현재 프레임에서 관심 객체의 대략적인 위치와 형태를 알려주는 "가이드" 역할을 합니다.
- **오프라인 학습 (Offline Training):**
  - **목표:** 네트워크가 "잡음이 있는" 이전 프레임 마스크 추정치로부터 정확한 현재 프레임 마스크를 정제하도록 학습시킵니다.
  - **학습 데이터:** 비용이 많이 드는 비디오 주석 데이터 대신 ECSSD [41], MSRA10K [9], SOD [28], PASCAL-S [25] 등 대규모 정적 이미지 saliency 분할 데이터셋(총 11,282개 이미지)을 활용합니다.
  - **마스크 변형(Mask Deformation):** 학습 샘플 생성을 위해 주석이 달린 마스크를 인위적으로 변형합니다. 이는 이전 프레임 출력 마스크의 "잡음"과 객체 움직임을 모방하여, 추정의 견고성을 높이고 오류 누적을 방지합니다.
    - **변형 종류:** 아핀 변환(랜덤 스케일링 $\pm 5\%$ 및 이동 $\pm 10\%$)과 비강체 변형(thin-plate splines [4]을 사용한 5개 제어점의 랜덤 이동 $\pm 10\%$)을 적용합니다.
    - **거칠게 만들기(Coarsening):** 팽창(dilation) 형태학적 연산을 사용하여 마스크의 세부 사항을 제거하고 뭉툭한 형태를 시뮬레이션합니다.
- **온라인 학습 (Online Training):**
  - **목표:** 테스트 시, 새로운 비디오의 첫 프레임(또는 여러 프레임)의 세그먼트 주석을 추가 학습 데이터로 사용하여, 특정 객체 인스턴스의 외관에 맞게 오프라인 학습된 모델을 미세 조정합니다.
  - **데이터 증강(Data Augmentation):** 오프라인 학습과 유사하게 아핀/비강체 변형 외에 이미지 뒤집기(flipping)와 회전 등을 통해 하나의 주석 프레임에서 약 $10^3$개의 학습 샘플을 생성합니다.
- **변형 모델 (Variants):**
  - **MaskTrack$_{Box}$:** 첫 프레임 입력 주석으로 세그먼트 마스크 대신 바운딩 박스를 사용합니다. 첫 프레임에서는 바운딩 박스를 입력으로 받는 별도의 ConvNet 모델을 사용하고, 다음 프레임부터는 표준 `MaskTrack` 모델을 사용합니다.
  - **MaskTrack+Flow:** `MaskTrack` 외에, 광류(optical flow) 정보를 추가 입력으로 활용합니다. EpicFlow [37]로 계산된 광류장의 크기(magnitude)를 3채널 이미지로 복제하여 별도의 네트워크에 입력하고, `MaskTrack` (RGB 입력) 결과와 평균을 내어 최종 점수를 융합합니다. 이는 RGB 이미지 모델이 광류 크기에서도 유용한 객체 형상 정보를 포착할 수 있음을 활용합니다.

## 📊 Results

- **주요 성능:** DAVIS [30], YoutubeObjects [35], SegTrack-v2 [24] 세 가지 이질적인 비디오 객체 분할 데이터셋에서 경쟁력 있는 성능을 달성했습니다.
  - DAVIS: `MaskTrack`은 74.8% mIoU를 달성했으며, 광류 및 CRF 후처리(`MaskTrack+Flow+CRF`)를 추가하면 80.3% mIoU를 기록하여 이 데이터셋에서 당시 최고 성능을 달성했습니다.
  - YoutubeObjects: 71.7% mIoU.
  - SegTrack-v2: 67.4% mIoU.
- **입력 주석 유연성:** `MaskTrack$_{Box}$` 변형은 첫 프레임에 바운딩 박스만 주어져도 모든 데이터셋에서 상위 3위 내에 드는 우수한 결과를 보여주었습니다 (예: DAVIS에서 73.7% mIoU).
- **효율성:** `MaskTrack`은 프레임당 약 12초로, `ObjFlow` (프레임당 2분)에 비해 상당히 빠릅니다.
- **구성 요소 영향 (Ablation Study):**
  - **온라인 미세 조정:** 온라인 미세 조정이 없을 경우 mIoU가 약 5%p 하락하여, 온라인 미세 조정이 추적 능력 확장에 매우 중요함을 입증했습니다.
  - **마스크 변형:** 학습 샘플 생성 시 마스크 변형(특히 팽창을 통한 거칠게 만들기)을 사용하는 것이 전체 접근 방식에 결정적인 요소로, 입력 마스크의 잡음에 대한 분할 추정의 견고성을 크게 향상시킵니다.
  - **정적 이미지 학습의 효과:** 비디오 데이터로 훈련했을 때 mIoU가 약간 감소하는 경향을 보여, 정적 이미지 사용이 성능 저하를 초래하지 않음을 입증했습니다.
- **속성 기반 분석 (DAVIS):** `MaskTrack`은 외관 변화, 폐색, 시야 밖 객체, 빠른 움직임, 모션 블러 등 다양한 비디오 도전 과제에 견고함을 보였습니다. 광류와 CRF 후처리를 추가하면 모든 범주에서 견고성이 향상되었습니다.
- **다중 프레임 주석:** DAVIS 데이터셋에서 10%의 프레임만 주석이 있어도 평균 mIoU 0.86을 달성할 수 있으며, 이는 다양한 응용 분야에 충분한 품질임을 시사합니다. 바운딩 박스 주석만으로도 만족스러운 결과를 얻을 수 있습니다.

## 🧠 Insights & Discussion

- **정적 이미지 학습의 잠재력:** 이 연구는 비디오 객체 분할을 위해 픽셀 단위로 주석이 달린 방대한 비디오 데이터셋이 필요하지 않으며, 정적 이미지 데이터만으로도 매우 효과적인 모델을 훈련할 수 있음을 보여줍니다. 이는 데이터 수집의 어려움을 크게 완화합니다.
- **가이드 인스턴스 분할의 유효성:** 이전 프레임의 마스크를 가이드로 사용하는 "가이드 인스턴스 분할" 접근 방식이 객체 인스턴스를 효과적으로 구분하고 추적하는 데 매우 효과적임을 입증했습니다. 이는 객체의 외관 변화, 폐색 등 다양한 난제를 처리하는 데 도움이 됩니다.
- **온라인 학습의 중요성:** 특정 객체 인스턴스에 대한 모델의 적응을 가능하게 하는 온라인 미세 조정은 시스템의 전반적인 성능 향상에 결정적인 역할을 합니다.
- **실용적 유연성:** 바운딩 박스 주석만으로도 경쟁력 있는 결과를 제공하여, 영상 편집과 같이 더 적은 주석 노력으로 고품질 결과를 얻고자 하는 다양한 실제 응용 분야에 대한 유연성을 높입니다.
- **한계 및 향후 연구:** 현재 시스템은 프레임별 인스턴스 분할에 중점을 두며 전역 최적화나 명시적인 시간적 차원을 통합하지 않습니다. 향후 연구에서는 더욱 정교한 네트워크 아키텍처, 시간적 차원 통합, 그리고 전역 최적화 전략 추가를 통해 품질을 더욱 향상시킬 수 있을 것입니다.

## 📌 TL;DR

이 논문은 대규모 비디오 주석 데이터의 필요성 없이 정적 이미지만으로 비디오 객체 분할(VOS)을 수행하는 `MaskTrack`이라는 새로운 접근 방식을 제안합니다. 핵심은 이전 프레임의 마스크를 가이드로 사용하여 현재 프레임의 객체를 분할하는 "가이드 인스턴스 분할"이며, 이는 오프라인 학습(정적 이미지)과 온라인 미세 조정(첫 주석 프레임)을 결합하여 구현됩니다. `MaskTrack`은 다양한 데이터셋에서 경쟁력 있는 결과를 달성하며, 바운딩 박스 입력만으로도 우수한 성능을 보여 비디오 편집 등 실제 응용 분야에서의 활용 가능성이 높습니다.
