{
  "url": "http://arxiv.org/abs/2310.02296v2",
  "title": "CLIP Is Also a Good Teacher: A New Learning Framework for Inductive\n  Zero-shot Semantic Segmentation",
  "authors": "Jialei Chen, Daisuke Deguchi, Chenkai Zhang, Xu Zheng, Hiroshi Murase",
  "year": 2023,
  "abstract": "Generalized Zero-shot Semantic Segmentation aims to segment both seen and\nunseen categories only under the supervision of the seen ones. To tackle this,\nexisting methods adopt the large-scale Vision Language Models (VLMs) which\nobtain outstanding zero-shot performance. However, as the VLMs are designed for\nclassification tasks, directly adapting the VLMs may lead to sub-optimal\nperformance. Consequently, we propose CLIP-ZSS (Zero-shot Semantic\nSegmentation), a simple but effective training framework that enables any image\nencoder designed for closed-set segmentation applied in zero-shot and\nopen-vocabulary tasks in testing without combining with VLMs or inserting new\nmodules. CLIP-ZSS consists of two key modules: Global Learning Module (GLM) and\nPixel Learning Module (PLM). GLM is proposed to probe the knowledge from the\nCLIP visual encoder by pulling the CLS token and the dense features from the\nimage encoder of the same image and pushing others apart. Moreover, to enhance\nthe ability to discriminate unseen categories, PLM consisting of pseudo labels\nand weight generation is designed. To generate semantically discriminated\npseudo labels, a multi-scale K-Means with mask fusion working on the dense\ntokens is proposed. In pseudo weight generation, a synthesizer generating\npseudo semantic features for the unannotated area is introduced. Experiments on\nthree benchmarks show large performance gains compared with SOTA methods."
}