{
  "url": "http://arxiv.org/abs/2311.15776v2",
  "title": "Stable Segment Anything Model",
  "authors": "Qi Fan, Xin Tao, Lei Ke, Mingqiao Ye, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Yu-Wing Tai, Chi-Keung Tang",
  "year": 2023,
  "abstract": "The Segment Anything Model (SAM) achieves remarkable promptable segmentation\ngiven high-quality prompts which, however, often require good skills to\nspecify. To make SAM robust to casual prompts, this paper presents the first\ncomprehensive analysis on SAM's segmentation stability across a diverse\nspectrum of prompt qualities, notably imprecise bounding boxes and insufficient\npoints. Our key finding reveals that given such low-quality prompts, SAM's mask\ndecoder tends to activate image features that are biased towards the background\nor confined to specific object parts. To mitigate this issue, our key idea\nconsists of calibrating solely SAM's mask attention by adjusting the sampling\nlocations and amplitudes of image features, while the original SAM model\narchitecture and weights remain unchanged. Consequently, our deformable\nsampling plugin (DSP) enables SAM to adaptively shift attention to the prompted\ntarget regions in a data-driven manner, facilitated by our effective robust\ntraining strategy (RTS). During inference, dynamic routing plugin (DRP) is\nproposed that toggles SAM between the deformable and regular grid sampling\nmodes, conditioned on the input prompt quality. Thus, our solution, termed\nStable-SAM, offers several advantages: 1) improved SAM's segmentation stability\nacross a wide range of prompt qualities, while 2) retaining SAM's powerful\npromptable segmentation efficiency and generality, with 3) minimal learnable\nparameters (0.08 M) and fast adaptation (by 1 training epoch). Extensive\nexperiments across multiple datasets validate the effectiveness and advantages\nof our approach, underscoring Stable-SAM as a more robust solution for\nsegmenting anything. Codes will be released upon acceptance.\nhttps://github.com/fanq15/Stable-SAM"
}