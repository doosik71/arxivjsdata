# A Generative Appearance Model for End-to-end Video Object Segmentation
Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan, Michael Felsberg

## 🧩 Problem to Solve
비디오 객체 분할(VOS)에서 핵심적인 과제는 타겟 및 배경 외형(appearance)에 대한 효과적인 표현을 찾는 것입니다. 기존 최고 성능 방법들은 이를 위해 컨볼루션 신경망(CNN)의 광범위한 미세 조정을 필요로 했습니다. 이러한 전략은 엄청난 계산 비용이 들 뿐만 아니라, 온라인 미세 조정(fine-tuning) 절차가 오프라인 네트워크 학습에 통합되지 않아 진정한 의미의 종단간(end-to-end) 학습이 불가능하다는 문제점이 있었습니다. 또한, 기존 외형 모델들은 단순하거나 미분 불가능한 구성 요소에 의존하여 종단간 학습이 어렵거나 식별 능력이 부족했습니다.

## ✨ Key Contributions
*   **단일 전방 전달(Single Forward Pass) 외형 모델 제안:** 타겟 및 배경 외형의 강력한 표현을 단 한 번의 전방 전달로 학습하는 신경망 아키텍처를 제안했습니다.
*   **미분 가능한 생성적 외형 모듈:** 타겟 및 배경 특징 분포의 확률적 생성 모델을 학습하는 모듈을 도입했습니다. 이 모듈의 학습 및 예측 단계는 모두 완전히 미분 가능하여 전체 분할 파이프라인의 진정한 종단간 학습을 가능하게 합니다.
*   **우수한 성능 및 효율성:**
    *   DAVIS2017 벤치마크에서 기존 온라인 미세 조정 방식에 버금가는 성능을 달성하면서 단일 GPU에서 15 FPS의 빠른 속도로 동작합니다.
    *   대규모 YouTube-VOS 데이터셋에서 모든 발표된 방법들보다 우수한 성능을 보였습니다.
*   **클래스 불가지론적(Class-agnostic) 일반화 능력:** 제안된 생성적 외형 모델이 학습 과정에서 보지 못한 임의의 객체에 대해서도 높은 일반화 능력을 가짐을 입증했습니다.

## 📎 Related Works
*   **온라인 미세 조정 기반 VOS:**
    *   초기 프레임에서 CNN을 미세 조정하여 전경/배경을 분할하는 접근 방식 (OSVOS [2], OnAVOS [30], CINM [1] 등).
    *   이러한 방법들은 정확도는 높지만, 높은 계산 비용으로 인해 실시간 처리가 어렵고 종단간 학습이 불가능하다는 공통적인 한계를 가집니다.
*   **온라인 최적화 회피 VOS:**
    *   광학 흐름(optical flow) 기반의 정제 방식 (DyeNet [18], CTN [15]).
    *   마스크 정제 문제로 VOS를 정의한 Perazzi et al. [23]의 MSK.
    *   이전 프레임의 마스크와 특징 맵을 현재 프레임에 연결하여 사용하는 RGMP [31].
    *   KNN(K-Nearest-Neighbor)을 사용하여 알려진 클래스 멤버십을 가진 특징과 입력 특징을 매칭하는 방식 (VM [13], PML [4]). 이 방법들은 타겟 외형을 모델링하지만, KNN의 비모수적 특성 때문에 전체 학습 데이터를 저장해야 하고, K-최근접 이웃을 찾는 과정이 미분 불가능하다는 단점이 있습니다.

## 🛠️ Methodology
본 논문은 비디오 객체 분할을 위해 타겟 및 배경 외형의 정확한 모델을 단일 전방 전달 방식으로 학습하는 신경망 아키텍처를 제안합니다. 제안된 모델은 총 다섯 가지 주요 구성 요소로 이루어져 있으며, 모두 종단간 방식으로 공동 학습됩니다.

1.  **전체 아키텍처:**
    *   **백본 특징 추출기(Backbone Feature Extractor):** ResNet101 [10]을 사용하여 입력 이미지에서 특징을 추출합니다.
    *   **생성적 외형 모듈(Generative Appearance Module):** 타겟 및 배경 특징 분포를 학습하여 후방 클래스 확률을 출력합니다.
    *   **마스크 전파 브랜치(Mask Propagation Branch):** 이전 프레임의 예측 마스크와 초기 프레임 정보를 활용하여 마스크 인코딩을 생성합니다. RGMP [31]의 개념을 기반으로 합니다.
    *   **융합 모듈(Fusion Component):** 외형 모듈과 마스크 전파 모듈의 출력을 결합하여 거친 마스크 인코딩을 생성합니다.
    *   **최종 업샘플링 및 예측 모듈(Upsampling and Prediction Module):** 융합 모듈의 거친 마스크 인코딩을 저수준 특징과 결합하여 최종 정제된 분할 마스크를 생성합니다.

2.  **생성적 외형 모듈 상세:**
    *   **모델 학습:**
        *   이미지에서 추출된 특징 $x_p$를 클래스 조건부 가우시안 혼합 모델(class-conditional mixture of Gaussians)로 모델링합니다: $p(x_p) = \sum_{k=1}^{K} p(z_p=k)p(x_p|z_p=k)$.
        *   각 클래스 조건부 밀도는 평균 $\mu_k$와 공분산 행렬 $\Sigma_k$를 가진 다변수 가우시안 분포 $N(x_p|\mu_k, \Sigma_k)$로 표현됩니다.
        *   본 모델은 전경/배경 각각에 대한 기본 구성 요소와 함께, 모델이 정확하게 표현하지 못하는 어려운 예시(hard examples, 방해물 등)를 모델링하기 위한 추가 가우시안 구성 요소를 사용합니다 (총 4개: 전경 기본, 배경 기본, 전경 잔여, 배경 잔여).
        *   첫 프레임에서는 초기 마스크를 사용하여 모델 파라미터를 직접 추론합니다. 이후 프레임에서는 네트워크 예측을 소프트 클래스 레이블로 사용하여 모델을 업데이트합니다.
        *   파라미터 업데이트는 E-M 알고리즘의 M-step과 유사한 방식으로 이루어지며, 학습률 $\lambda$를 사용하여 이전 프레임의 파라미터와 새로 계산된 파라미터를 결합합니다:
            $$ \tilde{\mu}_{i}{_k} = \frac{\sum_{p} \alpha_{i}{_pk} x_{i}{_p}}{\sum_{p} \alpha_{i}{_pk}} $$
            $$ \tilde{\Sigma}_{i}{_k} = \frac{\sum_{p} \alpha_{i}{_pk} \text{diag}\{(x_{i}{_p} - \tilde{\mu}_{i}{_k})^2 + r_k\}}{\sum_{p} \alpha_{i}{_pk}} $$
            $$ \mu_{i}{_k} = (1-\lambda)\mu_{i-1}{_k} + \lambda \tilde{\mu}_{i}{_k} $$
            $$ \Sigma_{i}{_k} = (1-\lambda)\Sigma_{i-1}{_k} + \lambda \tilde{\Sigma}_{i}{_k} $$
            여기서 $\alpha_{i}{_pk}$는 특징 $x_{i}{_p}$가 구성 요소 $k$에 할당되는 정도를 나타내는 소프트 할당 변수입니다.
    *   **할당 변수($\alpha_{i}{_pk}$) 계산:**
        *   첫 프레임에서는 초기 마스크 $y_p$를 사용하여 $\alpha_{0}{_p0} = 1-y_p$, $\alpha_{0}{_p1} = y_p$로 설정합니다.
        *   이후 프레임에서는 네트워크의 최종 예측 $\tilde{y}_p$를 사용하여 $\alpha_{i}{_p0} = 1-\tilde{y}_p$, $\alpha_{i}{_p1} = \tilde{y}_p$로 설정합니다.
        *   추가적인 가우시안 구성 요소 ($k=2,3$)에 대한 할당 변수는 기본 구성 요소의 예측 오류를 기반으로 계산됩니다:
            $$ \alpha_{i}{_p2} = \text{ReLU}(p(z_{i}{_p}=0|x_{i}{_p},\mu_{i}{_0},\Sigma_{i}{_0}) - \alpha_{i}{_p0}) $$
            $$ \alpha_{i}{_p3} = \text{ReLU}(p(z_{i}{_p}=1|x_{i}{_p},\mu_{i}{_1},\Sigma_{i}{_1}) - \alpha_{i}{_p1}) $$
    *   **모듈 출력:** 이전 프레임에서 계산된 모델 파라미터 $\theta_{i-1}$를 기반으로 구성 요소 후방 확률의 로그 값을 출력합니다. 이는 융합 모듈의 컨볼루션 레이어에 입력됩니다.
        $$ s_{i}{_pk} = -\frac{\ln|\Sigma_{i-1}{_k}| + (x_{i}{_p}-\mu_{i-1}{_k})^T(\Sigma_{i-1}{_k})^{-1}(x_{i}{_p}-\mu_{i-1}{_k})}{2} $$

3.  **네트워크 학습:**
    *   DAVIS2017 [26], YouTube-VOS [32], SynthVOS 데이터셋을 사용하여 종단간 반복 방식으로 학습합니다.
    *   각 프레임의 분할 마스크를 예측하고 교차 엔트로피 손실(cross-entropy loss)을 적용합니다. 거친 분할 예측($\tilde{y}_p$)에도 보조 손실을 적용합니다.
    *   학습은 두 단계로 진행됩니다:
        *   **초기 학습:** 세 데이터셋 모두를 사용하여 저해상도 이미지(240x432)로 80 에포크 학습.
        *   **미세 조정:** DAVIS2017 및 YouTube-VOS 학습셋을 사용하여 고해상도 이미지로 100 에포크 미세 조정.

## 📊 Results
*   **YouTube-VOS Ablation Study:**
    *   **외형 모듈:** 외형 모듈을 제거하면 전체 성능(G)이 66.0%에서 50.0%로 크게 감소했습니다. 특히 학습에서 보지 못한 클래스(unseen classes)에 대한 성능이 20.6% 감소하여, 외형 모듈이 일반화에 필수적임을 입증했습니다.
    *   **마스크 전파 모듈:** 마스크 전파 모듈을 제거하면 성능이 2.0% 감소했습니다.
    *   **다중 가우시안 구성 요소:** 하드 예시(hard examples)를 모델링하기 위한 추가 가우시안 구성 요소를 제거하면 성능이 1.6% 감소하여, 복잡한 배경 및 방해물 모델링의 중요성을 보여주었습니다.
    *   **모델 업데이트:** 매 프레임 모델 업데이트를 수행하지 않으면 성능이 1.1% 감소했습니다.
    *   **외형 모듈 출력:** 외형 모듈 출력을 로그 확률 대신 소프트맥스 확률로 변경하면 성능이 10.2% 크게 저하되었습니다.
    *   **종단간 학습:** 외형 모듈의 학습 단계에서 역전파를 수행하지 않는 (미분 불가능하게 만드는) 경우 전체 성능이 7.2% 감소하여 진정한 종단간 학습의 중요성을 강조했습니다.
*   **State-of-the-Art 비교:**
    *   **YouTube-VOS:** 제안된 방법은 66.0%의 최종 점수를 달성하여 온라인 미세 조정을 사용하지 않는 모든 기존 방법들을 크게 능가했습니다. 특히 unseen 카테고리에서 뛰어난 성능을 보였습니다.
    *   **DAVIS2017:** 67.2%의 점수를 달성하여 온라인 미세 조정을 사용하지 않는 모든 인과(causal) 방법들을 능가했으며, 심지어 온라인 미세 조정 기반 기술과 비인과적(non-causal) 방법들과도 비슷한 수준의 성능을 보였습니다.
    *   **DAVIS2016:** 82.0%의 경쟁력 있는 성능을 달성했으며, DAVIS2017 및 YouTube-VOS와 같은 크고 다양한 데이터셋에서 잘 일반화되지 않는 기존 최고 성능 방법들과 비교하여 뛰어난 일반화 능력을 보여주었습니다.
*   **정성적 평가:** 제안된 방법은 RGMP, CINM, FAVOS와 같은 기존 최신 기술에 비해 폐색(occlusion)에 강하고 다른 객체를 정확하게 구별하며, 정교한 경계와 세부 정보를 캡처하는 데 더 뛰어나다는 것을 보여주었습니다. 또한, 훨씬 빠른 속도로 동작합니다.

## 🧠 Insights & Discussion
*   이 연구의 핵심 통찰은 비디오 객체 분할에서 타겟 및 배경 외형 모델링이 매우 중요하다는 점과, 이를 확률적 생성 모델로 구현하고 전체 파이프라인에 종단간 미분 가능하게 통합함으로써 기존 방식의 한계를 극복할 수 있다는 것입니다.
*   특히, 단일 전방 전달만으로 외형 모델을 학습하고 업데이트할 수 있어, 실시간 추론이 불가능했던 기존의 온라인 미세 조정 방식에 비해 훨씬 효율적입니다.
*   제안된 생성적 외형 모듈은 학습 시 보지 못한 클래스(unseen classes)에 대해서도 높은 일반화 능력을 보여주었는데, 이는 모델이 특정 클래스에 의존하지 않고 객체 자체의 외형적 특징을 학습하기 때문입니다.
*   다중 가우시안 구성 요소를 사용하여 방해물(distractor)과 같은 '하드 예시'를 모델링한 것이 식별 능력 향상에 기여했음이 확인되었습니다.
*   외형 모듈의 출력을 로그 확률 스코어 형태로 유지하는 것이 최종 확률로 변환하는 것보다 성능에 유리하다는 점은 신경망의 내부 활성화가 최종 출력 계층 전까지 확률로 변환되지 않는 일반적인 딥러닝 관행과 일치합니다.
*   외형 모듈의 학습 단계가 종단간 미분 가능해야 전체 시스템의 성능이 크게 향상된다는 점은 제안된 아키텍처의 핵심 요소임을 강조합니다.
*   DAVIS2017에서 온라인 미세 조정 방식과 성능 격차를 줄이고 YouTube-VOS에서 최고 성능을 달성한 것은 이 방법이 실용성과 정확성 두 마리 토끼를 모두 잡았음을 시사합니다.

## 📌 TL;DR
본 논문은 비디오 객체 분할(VOS)에서 비효율적인 온라인 미세 조정 및 단순한 외형 모델링 문제를 해결하기 위해, 단일 전방 전달로 작동하며 완전히 미분 가능한 **생성적 외형 모듈**을 제안합니다. 이 모듈은 타겟 및 배경 특징 분포를 클래스 조건부 가우시안 혼합 모델로 학습하여 강력한 식별 단서를 제공하며, 전체 분할 파이프라인이 진정한 종단간 학습될 수 있도록 합니다. 제안된 방법은 DAVIS2017에서 기존 온라인 미세 조정 방식과 유사한 성능을 달성하면서 실시간 처리가 가능하며, 대규모 YouTube-VOS 데이터셋에서 최신 기술 중 가장 뛰어난 성능을 보여주며 탁월한 일반화 능력을 입증했습니다.