# CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation

Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, Seungryong Kim

## 🧩 Problem to Solve

오픈-어휘 의미론적 분할(Open-vocabulary semantic segmentation)은 이미지의 각 픽셀에 무한한 범위의 텍스트 설명으로 정의된 클래스 레이블을 할당하는 도전적인 과제입니다. CLIP과 같은 사전 학습된 비전-언어 모델(VLM)은 광범위한 이미지-텍스트 데이터셋으로 학습되어 강력한 오픈-어휘 인식 능력을 가지지만, 주로 이미지 레벨의 지도 학습으로 인해 픽셀 레벨의 분할 작업에 적용할 때 불일치가 발생합니다. 기존 방법들은 이 간극을 메우기 위해 마스크 제안 생성기를 사용하거나, CLIP 인코더를 미세 조정하려 할 때 학습된(seen) 클래스에 과적합되고 미학습(unseen) 클래스에 대한 임베딩 공간이 틀어지는 문제에 직면하여 대부분 인코더를 고정합니다.

## ✨ Key Contributions

* **새로운 비용 집계 기반 프레임워크 제안:** 이미지와 텍스트 임베딩 간의 코사인 유사도(비용 볼륨)를 집계하여 CLIP의 인코더를 미세 조정함으로써, 학습된 클래스뿐만 아니라 미학습 클래스에 대해서도 효과적으로 분할 작업에 적응시키는 CAT-Seg 프레임워크를 제안합니다.
* **멀티모달 비용 볼륨 처리를 위한 설계:** 이미지-텍스트 비용 볼륨의 멀티모달 특성을 고려하여 공간(spatial) 및 클래스(class) 집계 모듈로 구성된 프레임워크를 설계하고, 이를 개선하기 위한 다양한 방법을 탐구했습니다.
* **최첨단 성능 달성:** CAT-Seg는 표준 오픈-어휘 벤치마크 및 극한의 시나리오(예: MESS 벤치마크)에서 기존 최첨단 방법들을 큰 폭으로 능가하며, 방법의 다용성과 실용성을 입증했습니다.

## 📎 Related Works

* **오픈-어휘 의미론적 분할:**
  * **고전적 접근 방식:** SPNet [82], ZS3Net [6] 등은 미리 정의된 텍스트 임베딩에 시각적 임베딩을 정렬하려 했으나 어휘 한계가 있었습니다.
  * **CLIP 활용 방식:** LSeg [40]는 픽셀 수준 임베딩에 CLIP을 활용했습니다. OpenSeg [22], ZegFormer [15], ZSseg [84] 등은 마스크 제안 생성기를 활용하여 영역-텍스트 문제를 해결했으나, 제한된 데이터에 의존하는 한계가 있었습니다.
  * **원-스테이지 프레임워크:** ZegCLIP [100], SAN [85] 등은 CLIP 임베딩을 직접 활용했지만, 학습된 클래스에 과적합되는 경향이 있었습니다. FC-CLIP [89]는 미학습 클래스 성능 저하로 CLIP 인코더를 고정했습니다.
  * 본 연구는 CLIP에 외부 레이어를 추가하지 않고 비용 볼륨 집계를 통해 인코더 미세 조정을 가능하게 합니다.
* **비전-언어 모델 미세 조정:** CoOp [98], CoCoOp [97]는 프롬프트 토큰을 학습하고, CLIP-Adapter [21], TIP-Adapter [93]는 어댑터 레이어를 사용했습니다. 이들은 주로 소수샷(few-shot) 설정에 중점을 두었으며, 픽셀 레벨 예측을 위한 종단 간(end-to-end) 미세 조정은 기존에 실패한 시도가 많았습니다 [85, 89, 96].
* **비용 집계 (Cost Aggregation):** 스테레오 매칭 [38] 등 시각적 대응(visual correspondence) 문제에서 주로 사용되는 기술입니다. 이미지 쌍에서 추출된 밀집 특징 간의 유사도 점수(코사인 유사도 [45, 62])를 바탕으로 합니다. 본 연구는 이 개념을 이미지-텍스트 비용 볼륨에 적용하며, 비용 집계 레이어가 임베딩 자체가 아닌 유사도 점수에서 작동하므로 과적합에 강인하다는 점을 활용합니다 [45, 69].

## 🛠️ Methodology

CAT-Seg는 CLIP의 이미지 및 텍스트 임베딩으로부터 얻은 코사인 유사도 점수(비용 볼륨)를 개선하는 비용 기반 접근 방식을 사용합니다.

1. **비용 계산 및 임베딩 ($D_V$, $D_L$, $C$):**
    * 주어진 이미지 $I$와 클래스 집합 $C=\{T(n)\}$에 대해 CLIP 이미지 인코더 $\Phi_V(\cdot)$와 텍스트 인코더 $\Phi_L(\cdot)$를 사용하여 밀집 이미지 임베딩 $D_V \in R^{(H \times W) \times d}$와 텍스트 임베딩 $D_L \in R^{N_C \times d}$를 추출합니다.
    * $D_V$는 CLIP의 마지막 어텐션 레이어를 수정하여 풀링 효과를 제거함으로써 밀집 형태로 얻습니다 [96].
    * 각 픽셀 위치 $i$와 클래스 $n$에 대해 코사인 유사도를 사용하여 비용 볼륨 $C \in R^{(H \times W) \times N_C}$를 계산합니다:
        $$C(i, n) = \frac{D_V(i) \cdot D_L(n)}{\|D_V(i)\|\|D_L(n)\|}$$
    * 초기 비용 볼륨 $F \in R^{(H \times W) \times N_C \times d_F}$를 얻기 위해 $C$를 단일 컨볼루션 레이어에 통과시킵니다.
2. **비용 집계 ($F', F''$):** 비용 볼륨의 멀티모달 특성을 다루기 위해 두 가지 모듈을 사용합니다.
    * **공간 비용 집계 (Spatial Cost Aggregation):**
        * 이미지의 공간적 부드러움과 같은 특성을 고려합니다. 각 클래스 슬라이스 $F(:, n)$에 대해 독립적으로 적용됩니다.
        * 전역적/준전역적 수용 필드(receptive field)를 위해 Swin Transformer [47] 블록 두 개($T_{sa}$)를 사용합니다: $F'(:, n) = T_{sa}(F(:, n))$.
    * **클래스 비용 집계 (Class Cost Aggregation):**
        * 텍스트 모달리티를 고려하고, 가변적인 클래스 수와 순서 불변성(permutation invariance)을 처리합니다.
        * 위치 임베딩이 없는 Transformer [75] 레이어($T_{ca}$), 특히 선형 Transformer [37]를 사용하여 $F''(i, :) = T_{ca}(F'(i, :))$를 계산합니다.
    * 공간 및 클래스 집계를 교대로 적용합니다.
3. **CAT-Seg 프레임워크 향상:**
    * **업샘플링 디코더:** 집계된 비용 볼륨에 양선형 업샘플링을 적용하고, CLIP에서 추출된 중간 피쳐 맵과 연결한 후 컨볼루션 레이어를 적용합니다. 이를 $N_U$번 반복하여 고해상도 출력을 생성하고, 예측 헤드를 통해 최종 분할 예측을 수행합니다. CLIP ViT의 중간 레이어에서 고해상도 피쳐 맵을 효율적으로 추출합니다.
    * **임베딩 가이드 (Embedding Guidance):** CLIP의 임베딩 $D_L$과 $D_V$를 활용하여 집계 과정에 공간적 구조나 문맥적 정보를 제공합니다. $D_V$와 $D_L$을 선형 투영($P_V, P_L$)하여 공간 및 클래스 집계 레이어의 쿼리(query) 및 키(key) 프로젝션에 연결합니다:
        * $F'(:, n) = T_{sa}([F(:, n); P_V(D_V)])$
        * $F''(i, :) = T_{ca}([F'(i, :); P_L(D_L)])$
    * **CLIP 인코더의 효율적인 미세 조정:** 과적합에 강건성을 유지하면서 CLIP을 효과적이고 효율적으로 분할 작업에 적응시키기 위해 CLIP의 쿼리(query) 및 값(value) 프로젝션만 미세 조정하는 방식을 채택합니다.

## 📊 Results

* **표준 벤치마크 (Table 1):**
  * CAT-Seg는 모든 경쟁 방법들을 크게 능가하며, ViT-L/14 모델을 사용하는 다른 SOTA 모델들보다 뛰어난 성능을 보입니다.
  * 예를 들어, A-847 데이터셋에서 16.0 mIoU, PC-459에서 23.8 mIoU를 달성하여 이전 SOTA 대비 각각 29%, 52% 증가를 보였습니다.
* **다중 도메인 평가 (Table 2):**
  * MESS 벤치마크에서 가장 높은 평균 점수를 달성했습니다.
  * 일반 도메인과 농업 및 생물학 도메인에서 특히 강한 일반화 능력을 보였습니다. 다만, 의료 과학 및 공학 도메인에서는 CLIP의 해당 도메인에 대한 지식 부족으로 인해 성능에 일관성이 없거나 랜덤 예측과 유사한 결과를 보였습니다.
* **비용 집계의 효과 (Table 3):**
  * CLIP 인코더를 미세 조정했을 때, 피쳐 집계는 미미한 성능 향상을 보이는 반면, 비용 집계는 크게 향상됩니다. 이는 비용 집계가 CLIP을 분할 작업에 적응시키는 데 효과적임을 시사합니다.
  * 정성적 비교에서 비용 집계는 "새장(birdcage)"과 같은 미학습 클래스를 성공적으로 식별한 반면, 피쳐 집계는 "양동이(bucket)"와 같은 학습된 클래스에 과적합되는 경향을 보였습니다 (Fig. 5).
* **구성 요소 분석 (Table 4):**
  * 제안된 공간 및 클래스 집계 기술을 통합했을 때 (V) 기본 비용 집계 (II)보다 성능이 향상되었고, 임베딩 가이던스 (VI)는 모든 벤치마크에서 추가적인 성능 향상을 가져왔습니다.
* **CLIP 미세 조정 분석 (Table 6):**
  * CLIP의 미세 조정은 프레임워크의 성능을 향상시킵니다. 특히, 쿼리(query) 및 값(value) 프로젝션만 미세 조정하는 것이 가장 좋은 성능 향상과 높은 효율성을 제공합니다. 이미지와 텍스트 인코더를 모두 미세 조정하는 것이 한 가지만 미세 조정하는 것보다 더 나은 성능을 보였습니다.
  * t-SNE 시각화 (Fig. 6)를 통해 미세 조정된 CLIP은 학습된/미학습된 클래스 모두에서 잘 그룹화된 클러스터를 보여, 분할 작업에 대한 CLIP의 적응을 나타냈습니다.
* **효율성 비교 (Table 8):**
  * CAT-Seg는 학습 가능 파라미터 수, 총 파라미터 수, 학습 시간, 추론 시간, GFLOPs 측면에서 다른 SOTA 방법들보다 효율적입니다. 추가 마스크 생성기가 필요 없기 때문에 추론 시간이 3.7배 이상 빠릅니다.

## 🧠 Insights & Discussion

* **과적합 방지 및 일반화:** 기존의 CLIP 기반 분할 방법들이 인코더 미세 조정 시 과적합 문제에 직면했던 반면, CAT-Seg는 이미지와 텍스트 임베딩 간의 코사인 유사도를 집계하는 `비용 볼륨`이라는 새로운 접근 방식을 통해 이 문제를 효과적으로 해결합니다. 비용 집계 레이어가 특징 자체가 아닌 유사도 점수에서 작동하기 때문에 과적합에 더 강인하고 미학습 클래스에 대한 일반화 능력이 뛰어납니다.
* **효율성 및 실용성:** 추가적인 마스크 생성기 없이 CLIP의 임베딩을 직접 활용하고, 효율적인 미세 조정 전략(쿼리/값 프로젝션만 미세 조정)을 채택함으로써, CAT-Seg는 SOTA 성능을 달성하면서도 학습 및 추론 효율성 면에서 크게 우수합니다. 이는 실제 다양한 도메인 애플리케이션에서 CAT-Seg의 높은 실용성을 시사합니다.
* **멀티모달리티 활용:** 공간 및 클래스 집계 모듈을 통해 이미지-텍스트 비용 볼륨의 고유한 멀티모달 특성을 효과적으로 처리하며, 임베딩 가이던스를 통해 CLIP의 풍부한 시맨틱 정보를 활용하는 방식은 견고한 성능의 핵심 요인입니다.
* **한계:** 평가에 사용된 데이터셋의 그라운드-트루스 분할 맵이 일부 모호할 수 있어 평가 결과의 신뢰성에 의문을 제기할 수 있습니다. 향후에는 이러한 문제를 고려한 더 신뢰할 수 있는 데이터셋 구축이 필요합니다.

## 📌 TL;DR

CAT-Seg는 **오픈-어휘 의미론적 분할**에서 **CLIP의 이미지-레벨 학습과 픽셀-레벨 작업 간의 간극 및 미세 조정 시 과적합 문제**를 해결하기 위해 **이미지-텍스트 임베딩 간의 코사인 유사도(비용 볼륨)를 집계하는 새로운 프레임워크**를 제안합니다. 이 방법은 **공간 및 클래스 집계**를 통해 멀티모달 비용 볼륨을 처리하고, **효율적인 CLIP 인코더 미세 조정** (특히 쿼리/값 프로젝션) 및 **임베딩 가이던스**를 활용하여 학습된/미학습된 클래스 모두에서 **강력한 일반화 능력과 최첨단 성능**을 달성하며, **기존 방법 대비 뛰어난 효율성**을 보여줍니다.
