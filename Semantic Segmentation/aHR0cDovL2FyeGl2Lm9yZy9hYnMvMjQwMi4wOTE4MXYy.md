# OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM

Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo

## 🧩 Problem to Solve

기존의 대규모 시각-언어 모델(LVLM)은 다양한 멀티모달 태스크에서 뛰어난 성능을 보였지만, 의료 분야에서의 잠재력은 아직 충분히 탐구되지 않았습니다. 이는 실제 의료 애플리케이션에 필수적인 다양한 모달리티와 해부학적 영역을 포괄하는 의료 이미지 데이터의 부족 때문입니다. 특히, 기존 의료 시각 질의응답(VQA) 데이터셋은 규모가 작고, 제한된 모달리티와 특정 해부학적 영역에만 초점을 맞추어 LVLM의 포괄적인 평가에 적합하지 않다는 문제가 있습니다.

## ✨ Key Contributions

* **OmniMedVQA 벤치마크 제안**: 의료 분야에 특화된 대규모의 포괄적인 시각 질의응답 벤치마크인 OmniMedVQA를 제안했습니다. 이 벤치마크는 12가지 다른 모달리티의 이미지와 20개 이상의 독특한 인체 해부학적 영역을 포함하여 LVLM의 의료 관련 기본 능력을 평가하는 포괄적인 기준점을 확립합니다.
* **포괄적인 LVLM 평가 수행**: 일반 도메인 LVLM 8개와 의료 특화 LVLM 4개를 포함하여 총 12개의 대표적인 LVLM에 대한 철저한 평가를 수행했습니다. 이는 현재까지 의료 도메인 LVLM에 대한 가장 포괄적인 평가 중 하나입니다.
* **새로운 통찰력 및 향후 연구 방향 제시**: 평가 결과를 통해 여러 혁신적인 통찰력을 발견하고, 미래 의료 애플리케이션을 위한 LVLM 개선에 귀중한 지침을 제공합니다.

## 📎 Related Works

* **대규모 시각-언어 모델 (LVLM)**: Flamingo, BLIP2, InstructBLIP, LLaVA, LLaMA-Adapterv2, MiniGPT-4, mPLUG-Owl, Otter, VPGTrans 등 일반 도메인 LVLM의 발전. 이 모델들은 LLM의 지식을 활용하여 시각적 특징을 텍스트 공간에 정렬하고, 주로 지시 튜닝(instruction-tuning) 파이프라인을 통해 구축됩니다.
* **의료 특화 LVLM**: Med-Flamingo, LLaVA-Med, MedVInT, RadFM 등 의료 분야에 특화된 LVLM이 등장했으나, 이러한 모델들의 포괄적인 평가는 부족했습니다.
* **의료 VQA 데이터셋**: VQA-RAD, SLAKE, Path-VQA, VQA-Med 등 기존 의료 VQA 데이터셋은 이미지 수가 5K 미만으로 작고, CT, MRI, X-Ray 등 특정 모달리티에만 집중하며, 제한된 해부학적 영역만 다룬다는 한계가 있습니다. PMC-VQA는 대규모 의료 VQA 데이터셋이지만, 온라인 논문에서 추출되어 실제 의료 환경과의 괴리가 있습니다.

## 🛠️ Methodology

1. **원본 데이터셋 준비**: 실제 의료 이미지 활용을 극대화하기 위해, 12가지 다양한 영상 모달리티와 20개 이상의 인체 해부학적 영역을 포괄하는 73개의 의료 분류 데이터셋을 수집했습니다. RadImageNet이 데이터셋의 상당 부분을 차지합니다.
2. **QA 템플릿 설계**: 수집된 분류 데이터셋의 고유 속성(예: 카테고리, 모달리티, 해부학 정보)을 기반으로 질의응답(QA) 쌍으로 변환하기 위한 템플릿을 구축했습니다. 모든 QA 쌍은 모달리티 인식(Modality Recognition), 해부학적 식별(Anatomy Identification), 질병 진단(Disease Diagnosis), 병변 등급(Lesion Grading), 기타 생물학적 속성(Other Biological Attributes)의 5가지 질문 유형으로 분류됩니다. 각 템플릿의 항목 수를 역비례 샘플링 전략으로 제어하여 균형 잡힌 분포를 유지했습니다.
3. **QA 쌍 정제**: ChatGPT-3.5 API를 사용하여 질문의 표현 스타일과 구문 구조를 다양화하고, 각 항목에 대한 오답 선택지를 생성하여 다중 선택 질의응답 쌍으로 변환했습니다. 오답 선택지는 2개에서 4개까지 다양하게 구성됩니다.
4. **인간 검수**: 데이터 품질을 보장하기 위해 인간 검수 절차를 거쳐 OmniMedVQA 데이터셋의 유효성을 확인했습니다.
5. **평가 지표**: LVLM의 평가를 위해 두 가지 다른 지표를 사용했습니다.
    * **질의응답 점수 (Question-answering Score)**: 주어진 질문과 선택지를 LVLM에 입력하고, 모델이 생성한 응답과 후보 선택지 간의 유사도를 계산하여 가장 유사한 선택지를 최종 예측으로 간주합니다.
    * **접두사 기반 점수 (Prefix-based Score)**: 이미지와 텍스트(질문 + 각 후보 선택지)를 결합하여 LLM에 입력하고, 해당 텍스트 콘텐츠를 생성할 모델의 가능성(likelihood score)을 측정합니다. 가장 높은 점수를 얻은 선택지가 최종 답변으로 간주되며, 이는 모델의 내재된 생물의학 지식 수준을 반영합니다.

## 📊 Results

* **전반적인 성능**: OmniMedVQA는 매우 도전적인 벤치마크로, 대부분의 LVLM은 무작위 추측 성능을 약간 상회하는 수준을 보였습니다.
* **일반 LVLM의 우세**: 놀랍게도 일반 도메인 LVLM인 BLIP2가 모든 태스크에서 평균적으로 가장 좋은 성능을 보였으며, 많은 의료 도메인 LVLM을 큰 폭으로 능가했습니다. 이는 현재 의료 데이터로 일반 LVLM을 의료 태스크에 적응시킬 때 기대했던 새로운 특성(emerging property)이 나타나지 않음을 시사합니다.
* **의료 LVLM의 한계**: 의료 특화 LVLM인 MedVInT와 Med-Flamingo는 다른 일반 LVLM보다는 나은 성능을 보였으나, BLIP2와 InstructBLIP에는 미치지 못했습니다. 특히 LLaVA-Med는 최악의 성능을 보였는데, 이는 LLaVA 사전 학습 모델이 의료 도메인에 적합하지 않거나, GPT-4 API로 생성된 소량의 텍스트 기반 지시 데이터의 품질이 낮았기 때문으로 분석됩니다.
* **모달리티별 성능**:
  * 의료 LVLM은 CT, MRI와 같이 일반 이미지와 차이가 큰 모달리티에서는 상대적으로 좋은 성능을 보였습니다.
  * 그러나 일반 도메인 이미지와 분포가 유사한 모달리티(예: 안저 촬영(Fundus Photography), 적외선 반사 영상(Infrared Reflectance Imaging))에서는 의료 특화 LVLM이 일반 모델보다 두드러지게 우수한 성능을 보여주지 못했습니다.
  * RadFM은 CT 및 X-Ray 태스크에서 최고의 성능을 보였는데, 이는 해당 모달리티에 대한 대규모 고품질 방사선 이미지-텍스트 쌍으로 학습했기 때문입니다. 이는 고품질 지시 데이터의 중요성을 강조합니다.

## 🧠 Insights & Discussion

* **의료 LVLM의 현주소**: 기존의 의료 특화 LVLM은 주장하는 강건함에도 불구하고, 일반 도메인 모델보다 낮은 성능을 보이는 경우가 많아 기초 의료 지식의 부족을 드러냈습니다. 이는 현재 의료 LVLM의 단점을 명확히 보여줍니다.
* **고품질 데이터의 필요성**: 의료 도메인 LVLM의 발전을 위해서는 다양한 의료 도메인에서 수집된 대규모의 고품질 이미지 캡셔닝(설명) 데이터가 시급하게 필요합니다. BLIP2의 성공은 방대한 양의 고품질 이미지-텍스트 쌍으로 학습된 강력한 이미지-텍스트 정렬 모델의 중요성을 입증합니다.
* **다목적 의료 LVLM의 필요성**: 특정 모달리티에만 국한되지 않고, 안저 촬영(Fundus Photography), 적외선 반사 영상(Infrared Reflectance Imaging) 등 다양한 모달리티에 대한 추가적인 지식을 지속적으로 주입하여 더욱 다재다능한 의료 전문가 LVLM을 개발해야 합니다.
* **데이터 품질의 중요성**: MiniGPT-4의 사례는 소량이라도 고품질 데이터가 LVLM 훈련에 강력한 영향을 미칠 수 있음을 보여주지만, LLaVA-Med의 경우 텍스트 정보에만 의존하여 생성된 지시 데이터의 품질이 낮아 성능 저하로 이어졌습니다. 이는 의료 이미지에 대한 정확하고 포괄적인 설명을 생성할 수 있는 효과적인 캡션 생성 모델의 필요성을 강조합니다.
* **제한 사항**: OmniMedVQA의 분류 속성이 직관적이고 복잡하지 않을 수 있지만, 이는 의료 도메인에서 LVLM의 근본적인 능력을 평가하는 데 중요한 역할을 합니다. 평가 결과는 기존 LVLM, 특히 의료 특화 LVLM이 이 질문들을 잘 다루지 못하며, 이는 제안된 데이터셋의 도전 과제를 보여줍니다.

## 📌 TL;DR

새로운 대규모 의료 VQA 벤치마크인 OmniMedVQA를 제안하여, 12가지 모달리티와 20개 이상의 해부학적 영역을 포괄하는 11만 개 이상의 실제 의료 이미지를 기반으로 LVLM을 평가했습니다. 놀랍게도, 일반 도메인 LVLM(BLIP2)이 의료 특화 LVLM보다 뛰어난 성능을 보이는 경우가 많았으며, 이는 의료 LVLM의 기초 의료 지식 및 다양한 모달리티 처리 능력에 한계가 있음을 시사합니다. 연구는 의료 분야 LVLM의 발전을 위해 대규모의 고품질 의료 이미지-텍스트 정렬 데이터와 보다 다재다능한 모델이 필요함을 강조합니다.
