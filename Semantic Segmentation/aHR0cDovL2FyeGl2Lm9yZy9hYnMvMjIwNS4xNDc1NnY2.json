{
  "url": "http://arxiv.org/abs/2205.14756v6",
  "title": "EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense\n  Prediction",
  "authors": "Han Cai, Junyan Li, Muyan Hu, Chuang Gan, Song Han",
  "year": 2022,
  "abstract": "High-resolution dense prediction enables many appealing real-world\napplications, such as computational photography, autonomous driving, etc.\nHowever, the vast computational cost makes deploying state-of-the-art\nhigh-resolution dense prediction models on hardware devices difficult. This\nwork presents EfficientViT, a new family of high-resolution vision models with\nnovel multi-scale linear attention. Unlike prior high-resolution dense\nprediction models that rely on heavy softmax attention, hardware-inefficient\nlarge-kernel convolution, or complicated topology structure to obtain good\nperformances, our multi-scale linear attention achieves the global receptive\nfield and multi-scale learning (two desirable features for high-resolution\ndense prediction) with only lightweight and hardware-efficient operations. As\nsuch, EfficientViT delivers remarkable performance gains over previous\nstate-of-the-art models with significant speedup on diverse hardware platforms,\nincluding mobile CPU, edge GPU, and cloud GPU. Without performance loss on\nCityscapes, our EfficientViT provides up to 13.9$\\times$ and 6.2$\\times$ GPU\nlatency reduction over SegFormer and SegNeXt, respectively. For\nsuper-resolution, EfficientViT delivers up to 6.4x speedup over Restormer while\nproviding 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers\n48.9x higher throughput on A100 GPU while achieving slightly better zero-shot\ninstance segmentation performance on COCO."
}