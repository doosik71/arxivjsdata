{
  "title": "NamedMask: Distilling Segmenters from Complementary Foundation Models",
  "authors": "Gyungin Shin, Weidi Xie, Samuel Albanie",
  "year": 2022,
  "url": "http://arxiv.org/abs/2209.11228v1",
  "abstract": "The goal of this work is to segment and name regions of images without access\nto pixel-level labels during training. To tackle this task, we construct\nsegmenters by distilling the complementary strengths of two foundation models.\nThe first, CLIP (Radford et al. 2021), exhibits the ability to assign names to\nimage content but lacks an accessible representation of object structure. The\nsecond, DINO (Caron et al. 2021), captures the spatial extent of objects but\nhas no knowledge of object names. Our method, termed NamedMask, begins by using\nCLIP to construct category-specific archives of images. These images are\npseudo-labelled with a category-agnostic salient object detector bootstrapped\nfrom DINO, then refined by category-specific segmenters using the CLIP archive\nlabels. Thanks to the high quality of the refined masks, we show that a\nstandard segmentation architecture trained on these archives with appropriate\ndata augmentation achieves impressive semantic segmentation abilities for both\nsingle-object and multi-object images. As a result, our proposed NamedMask\nperforms favourably against a range of prior work on five benchmarks including\nthe VOC2012, COCO and large-scale ImageNet-S datasets."
}