# Feedforward semantic segmentation with zoom-out features

Mohammadreza Mostajabi, Payman Yadollahpour and Gregory Shakhnarovich

## 🧩 Problem to Solve

본 논문은 이미지 내 각 픽셀에 범주 수준 레이블을 할당하는 핵심 비전 작업인 의미론적 분할(semantic segmentation)을 다룹니다. 기존의 많은 최신 분할 연구는 인접하거나 멀리 떨어진 이미지 요소 간의 복잡한 상호작용 및 전역적 맥락의 중요성 때문에 이 작업을 조건부 랜덤 필드(CRF)와 같은 복잡한 구조적 예측(structured prediction) 문제로 취급해왔습니다. 그러나 이러한 접근 방식은 종종 추론 및 학습의 난해함과 계산 비용 문제를 야기합니다. 본 논문은 이러한 기존의 관습에서 벗어나, 복잡하고 비용이 많이 드는 추론 과정 없이도 통계적 구조를 활용하여 의미론적 분할을 순수한 피드포워드(feedforward) 단일 단계 분류 작업으로 접근하는 방법을 모색합니다.

## ✨ Key Contributions

- **순수 피드포워드 아키텍처 제안**: 복잡한 추론 과정 없이 슈퍼픽셀(superpixel)을 다층 신경망으로 분류하는 순수 피드포워드 아키텍처를 소개합니다.
- **줌아웃 특징(Zoom-out Features) 도입**: 각 슈퍼픽셀을 중심으로 확장되는 중첩된 영역(로컬, 근접, 원거리, 전역)에서 풍부한 특징 표현을 추출하고 이를 결합하여 사용합니다. 이는 명시적인 구조적 예측 메커니즘 없이 이미지와 레이블 공간의 통계적 구조를 암묵적으로 활용합니다.
- **비대칭 손실 함수(Asymmetric Loss Function) 적용**: 클래스 불균형 문제를 효과적으로 해결하기 위해 각 클래스의 빈도에 반비례하여 손실을 가중하는 비대칭 로그 손실(log-loss) 함수를 사용하여 모델을 훈련합니다.
- **최첨단 성능 달성**: PASCAL VOC 2012 테스트 세트에서 평균 64.4%의 IoU(Intersection over Union) 정확도를 달성하며 당시 의미론적 분할 분야의 최첨단 성능을 크게 향상시켰습니다 (기존 약 52%에서 대폭 상승).
- **심층 컨볼루션 신경망(CNN) 활용**: 이미지 분류에 효과적인 심층 CNN(예: CNN-S, VGG-16)을 원거리 및 전역 특징 추출에 활용하여 분할 작업에서도 심층 학습의 강력함을 입증합니다.

## 📎 Related Works

- **구조적 예측 모델**: CRF(Conditional Random Field) 기반의 분할 모델 [31, 21, 22, 3]들은 픽셀/슈퍼픽셀 노드에 대한 단항 포텐셜(unary potential) 및 쌍별/고차 포텐셜(pairwise/higher-order potential)을 사용하여 상호작용을 모델링합니다. 본 논문은 줌아웃 특징이 이러한 고차 구조를 암묵적으로 포착한다고 주장합니다.
- **다단계/영역 제안(Region Proposal) 기반 접근**: 범주 독립적 [6, 36] 또는 범주 인식 [2] 메커니즘으로 영역을 생성한 후 점수를 매기거나 순위를 매기는 방식 [4, 17, 2, 5, 24, 37]. 최근에는 컨브넷 기반 특징을 사용하는 [15, 8]이 포함됩니다. 본 논문은 이러한 영역 생성기에 의존하지 않습니다.
- **문맥 및 비지역(Non-local) 증거 활용**: 슈퍼픽셀 이웃을 통한 특징 계산 [11, 25] 및 문맥 사용에 대한 초기 연구 [31]. [27]은 단항 항만으로도 문맥 정보를 포착할 수 있다고 제안했으나 당시 성능은 낮았습니다.
- **최근 유사 연구**:
  - [10]: 이미지의 다른 해상도에 동일한 컨브넷을 적용하고 슈퍼픽셀 트리 구조 그래프와 결합하여 부드러움을 부여합니다.
  - [28]: 여러 수준(로컬, 근접, 전역과 유사)에 수작업 특징을 적용합니다.
  - [29]: 단일 컨브넷을 재귀적으로 적용합니다.
  - [16]: 3D 생물학적 데이터 경계 감지에 유사한 아이디어를 적용합니다.
  - **동시 연구**:
    - [26] (FCN-8s): 컨브넷의 중간 계층에서 나온 예측을 상위 계층 예측과 합산하여 공간 수준 정보를 융합합니다. 본 논문의 줌아웃 특징과 유사하나, 전역 및 로컬 수준이 부족하고 특징이 아닌 예측을 융합합니다.
    - [13] (Hypercolumns): 픽셀 주위의 중첩된 영역에서 추출된 증거를 풀링하여 하이퍼컬럼을 형성합니다. 본 논문의 줌아웃 특징과 유사하나, 입력이 전체 이미지가 아닌 탐지된 바운딩 박스이고 위치별 분류기를 사용합니다.

## 🛠️ Methodology

본 논문의 핵심 방법론은 의미론적 분할을 슈퍼픽셀 분류 문제로 전환하고, 각 슈퍼픽셀의 레이블을 결정하기 위해 **줌아웃 특징 융합** 및 **비대칭 손실**을 사용하는 다층 신경망(MLP)을 훈련하는 것입니다.

1. **슈퍼픽셀 생성**: SLIC [1] 알고리즘을 사용하여 이미지당 약 500개의 슈퍼픽셀을 생성합니다. 각 슈퍼픽셀은 평균 $21 \times 21$ 픽셀 크기이며, 실제 경계에 비교적 잘 맞춰집니다(슈퍼픽셀-정답 일치도 94.4%).

2. **줌아웃 특징 추출**: 각 슈퍼픽셀에 대해 네 가지 공간적 범위에서 특징을 추출하여 결합합니다.

   - **로컬 줌(Local Zoom)**: 슈퍼픽셀 자체에서 추출되는 특징으로, 로컬 증거(색상, 텍스처, 미세 패턴)를 포착합니다.
     - **수작업 특징**: L*a*b 색상 히스토그램 (32/8bin, 엔트로피, 적응형 binning), 텍스톤 히스토그램 (64-텍스톤, 엔트로피), SIFT 기반 BoW (500 시각 단어, 8/18픽셀 패치), 이미지 정규화된 슈퍼픽셀 위치.
     - **학습된 특징**: 슈퍼픽셀 분류 및 전경/배경 이진 분류를 위해 훈련된 소규모 컨브넷의 소프트맥스(softmax) 계층 출력(각각 21차원, 2차원).
   - **근접 줌(Proximal Zoom)**: 슈퍼픽셀 주변의 확장된 영역(반경 2 슈퍼픽셀 이웃, 평균 $100 \times 100$ 픽셀)에서 추출됩니다. 로컬 특징과 유사하나 더 넓은 지역의 통계적 속성을 포착합니다. 로컬 특징과 동일한 수작업 특징 세트(1818 차원)를 사용합니다.
   - **원거리 줌(Distant Zoom)**: 슈퍼픽셀 주변의 더 큰 영역(반경 3 슈퍼픽셀 이웃의 바운딩 박스, 평균 $170 \times 170$ 픽셀)에서 추출됩니다. 객체의 상당 부분 또는 전체를 포함할 만큼 충분히 넓어 형태, 복잡한 패턴, 공간적 배치에 대한 추론을 가능하게 합니다.
     - **특징**: ImageNet [30]에서 사전 학습된 심층 컨브넷(CNN-S [7] 또는 VGG-16 [32])의 마지막 완전 연결 계층(4096 유닛)의 활성화 값을 사용합니다. 입력 이미지는 $224 \times 224$ 픽셀로 크기가 조정됩니다.
   - **전역 줌(Global Zoom)**: 전체 이미지(장면)에서 추출되는 특징입니다. 이미지 수준 분류(즉, "이 이미지가 어떤 종류의 이미지인가?")와 전역 문맥 정보를 제공합니다.
     - **특징**: 원거리 줌과 동일한 심층 컨브넷(CNN-S 또는 VGG-16)의 활성화 값을 사용합니다.

3. **특징 결합**: 위 네 가지 수준에서 추출된 특징 벡터 $\phi_{\text{loc}}$, $\phi_{\text{prox}}$, $\phi_{\text{dist}}$, $\phi_{\text{glob}}$를 하나의 거대한 특징 벡터 $\phi_{\text{zoom-out}}(s,I)$로 연결($[\phi_{\text{loc}}(s,I), \phi_{\text{prox}}(s,I), \phi_{\text{dist}}(s,I), \phi_{\text{glob}}(I)]^T$)합니다.

4. **분류기 훈련**: 연결된 줌아웃 특징 벡터를 입력으로 받아 슈퍼픽셀의 범주 레이블을 예측하는 다층 신경망(MLP)을 훈련합니다.
   - **손실 함수**: 클래스 불균형을 해결하기 위해 비대칭 로그 손실 함수(수식 2)를 사용합니다. 각 클래스의 빈도 $f_c$에 역비례하여 손실에 가중치를 부여하여, 빈도가 낮은 클래스에 더 많은 중요도를 부여합니다.
     $$ -\frac{1}{N} \sum*{i=1}^{N} \frac{1}{f*{y_i}} \log \hat{p}(y_i | \phi(s_i, I_i)) $$
   - **최적화**: Caffe [18]를 사용하여 단일 GPU 머신에서 학습됩니다. 학습 속도 $0.0001$, 가중치 감쇠 $0.001$을 사용합니다.
   - **모델 선택**: CNN-S 대신 VGG-16을 사용했을 때 성능이 크게 향상되었으며, 최종 모델은 VGG-16 특징과 1024개 히든 유닛을 가진 2층 신경망으로 결정되었습니다.

## 📊 Results

본 논문은 PASCAL VOC 2012와 Stanford Background Dataset (SBD)에서 제안하는 줌아웃 아키텍처의 성능을 평가했습니다.

- **PASCAL VOC 2012 (평균 IoU)**:

  - **최종 결과**: 테스트 세트에서 **64.4%**의 평균 IoU를 달성하여 당시 모든 이전 공개 방법을 크게 능가했습니다. 이는 기존의 최첨단 성능인 약 52%에서 대폭 상승한 수치입니다.
  - **소프트맥스 선형 모델의 특징 중요도 (검증 세트)**:
    - 로컬 특징만: 14.6%
    - 근접 특징만: 15.5%
    - 로컬 + 근접: 17.7%
    - 로컬 + 원거리: 37.38% (원거리 특징의 중요성 확인)
    - 로컬 + 전역: 41.8% (전역 특징의 중요성 확인)
    - 전체 줌아웃 특징, 대칭 손실: 20.4%
    - **전체 줌아웃 특징, 비대칭 손실**: **52.4%** (비대칭 손실의 결정적 역할 입증)
  - **분류 모델 및 컨브넷 영향 (검증 세트)**:
    - CNN-S 특징, 2층 신경망 (512 유닛): 59.1%
    - VGG-16 특징, 2층 신경망 (1024 유닛): **63.5%** (VGG-16 및 더 큰 신경망의 우수성 입증)
  - 개별 클래스 정확도에서도 20개 객체 범주 중 15개에서 우수한 성능을 보였습니다.

- **Stanford Background Dataset (SBD)**:
  - 픽셀 정확도: **82.1%**
  - 클래스 정확도: **77.3%**
  - 이전 관련 연구([10], [29])보다 픽셀 및 클래스 정확도 모두에서 더 나은 결과를 달성했습니다.

전반적으로, 이 연구는 명시적인 구조적 모델링 없이도 피드포워드 아키텍처와 다중 스케일 특징, 비대칭 손실 함수를 통해 의미론적 분할에서 최첨단 성능을 달성할 수 있음을 성공적으로 입증했습니다.

## 🧠 Insights & Discussion

- **간결함 속의 강력함**: 본 논문의 가장 중요한 통찰은 겉보기에는 단순해 보이는 피드포워드 방식과 다단계 줌아웃 특징 구성을 통해 의미론적 분할에서 기존의 복잡한 구조적 예측 모델을 크게 뛰어넘는 성능을 달성할 수 있다는 점입니다. 이는 복잡하고 값비싼 추론 없이도 이미지와 레이블 공간의 통계적 구조를 효과적으로 활용할 수 있음을 보여줍니다.
- **심층 학습의 확장**: 이미지 분류, 객체 탐지 등 다른 인식 작업과 마찬가지로, 심층 컨볼루션 신경망이 의미론적 분할에도 큰 발전을 가져올 수 있음을 증명했습니다. 특히, 사전 학습된 CNN을 원거리 및 전역 특징 추출에 활용하는 것이 매우 효과적임을 보여주었습니다.
- **비대칭 손실의 중요성**: 클래스 불균형이 심한 분할 작업에서, 빈도가 낮은 클래스에 더 큰 가중치를 부여하는 비대칭 손실 함수가 모델 성능에 결정적인 영향을 미친다는 것을 경험적으로 확인했습니다.
- **한계점**:
  - 제안된 방법은 때때로 '언더-스무딩(under-smoothing)'된 분할 결과를 생성하며, 관련 없는 범주의 작은 '섬'과 같은 오류를 포함할 수 있습니다. 이는 고차 줌아웃 수준이 제공하는 부드러움에도 불구하고 나타나는 현상입니다.
  - 현재 구현은 여러 특징 추출기를 사용하고 있어 종단간 학습(end-to-end learning) 철학과는 거리가 있습니다.
- **향후 연구 방향**:
  - 수작업으로 제작된 로컬 및 근접 특징을 컨브넷을 통해 학습된 특징으로 대체하여 모든 특징 추출 과정을 데이터로부터 학습하도록 합니다.
  - 원거리 및 전역 특징 추출에 사용된 '기성품' 컨브넷(CNN-S, VGG-16)을 분할 작업에 맞게 미세 조정(fine-tune)합니다.
  - 추가적인 줌아웃 수준을 포함하여 더 다양한 스케일의 정보를 활용합니다.
  - 궁극적으로는 여러 특징 추출기를 하나의 '줌아웃 네트워크'로 통합하여 진정한 종단간 학습을 가능하게 합니다.
  - 피드포워드 방식의 장점을 유지하면서도, 구조적 모델의 추론(예: CRF)을 '풀어서(unrolling)' 신경망의 추가 계층으로 통합하는 방식으로 예측 결과를 '정제'하는 방법을 탐구할 수 있습니다.

## 📌 TL;DR

의미론적 분할은 픽셀 간 복잡한 상호작용으로 인해 전통적으로 복잡한 구조적 예측 모델(예: CRF)을 사용했으나, 이는 계산 비용이 높습니다. 본 논문은 이를 극복하기 위해 슈퍼픽셀을 중심으로 계층적으로 확장되는 **줌아웃 특징(local, proximal, distant, global)**을 추출하고, 이를 결합하여 **다층 신경망**에 입력하는 **순수 피드포워드 아키텍처**를 제안합니다. 특히, 클래스 불균형 문제를 해결하기 위해 **비대칭 손실 함수**를 사용합니다. 이 단순해 보이는 접근 방식은 PASCAL VOC 2012에서 **64.4%**의 평균 IoU를 달성하며 당시 최첨단 성능을 크게 경신했고, 명시적 구조적 모델링 없이도 심층 컨볼루션 신경망의 강력함을 분할 작업에 성공적으로 활용할 수 있음을 입증했습니다.
