{
  "url": "http://arxiv.org/abs/1902.09513v2",
  "title": "FEELVOS: Fast End-to-End Embedding Learning for Video Object\n  Segmentation",
  "authors": "Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen",
  "year": 2019,
  "abstract": "Many of the recent successful methods for video object segmentation (VOS) are\noverly complicated, heavily rely on fine-tuning on the first frame, and/or are\nslow, and are hence of limited practical use. In this work, we propose FEELVOS\nas a simple and fast method which does not rely on fine-tuning. In order to\nsegment a video, for each frame FEELVOS uses a semantic pixel-wise embedding\ntogether with a global and a local matching mechanism to transfer information\nfrom the first frame and from the previous frame of the video to the current\nframe. In contrast to previous work, our embedding is only used as an internal\nguidance of a convolutional network. Our novel dynamic segmentation head allows\nus to train the network, including the embedding, end-to-end for the multiple\nobject segmentation task with a cross entropy loss. We achieve a new state of\nthe art in video object segmentation without fine-tuning with a J&F measure of\n71.5% on the DAVIS 2017 validation set. We make our code and models available\nat https://github.com/tensorflow/models/tree/master/research/feelvos."
}