{
  "url": "http://arxiv.org/abs/2303.14396v1",
  "title": "IFSeg: Image-free Semantic Segmentation via Vision-Language Model",
  "authors": "Sukmin Yun, Seong Hyeon Park, Paul Hongsuck Seo, Jinwoo Shin",
  "year": 2023,
  "abstract": "Vision-language (VL) pre-training has recently gained much attention for its\ntransferability and flexibility in novel concepts (e.g., cross-modality\ntransfer) across various visual tasks. However, VL-driven segmentation has been\nunder-explored, and the existing approaches still have the burden of acquiring\nadditional training images or even segmentation annotations to adapt a VL model\nto downstream segmentation tasks. In this paper, we introduce a novel\nimage-free segmentation task where the goal is to perform semantic segmentation\ngiven only a set of the target semantic categories, but without any\ntask-specific images and annotations. To tackle this challenging task, our\nproposed method, coined IFSeg, generates VL-driven artificial\nimage-segmentation pairs and updates a pre-trained VL model to a segmentation\ntask. We construct this artificial training data by creating a 2D map of random\nsemantic categories and another map of their corresponding word tokens. Given\nthat a pre-trained VL model projects visual and text tokens into a common space\nwhere tokens that share the semantics are located closely, this artificially\ngenerated word map can replace the real image inputs for such a VL model.\nThrough an extensive set of experiments, our model not only establishes an\neffective baseline for this novel task but also demonstrates strong\nperformances compared to existing methods that rely on stronger supervision,\nsuch as task-specific images and segmentation masks. Code is available at\nhttps://github.com/alinlab/ifseg.",
  "citation": 26
}