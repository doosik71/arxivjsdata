# Microsoft COCO: Common Objects in Context

Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár

## 🧩 해결하려는 문제

이 논문은 컴퓨터 비전의 궁극적인 목표인 '장면 이해(scene understanding)'를 발전시키기 위해 객체 인식(object recognition)의 한계를 극복하는 것을 목표로 합니다. 기존 객체 인식 및 탐지 데이터셋(예: PASCAL, ImageNet)은 주로 객체의 '아이코닉 뷰(iconic view)'에 초점을 맞춰 실제 복잡하고 일상적인 장면에서의 객체 인식에 어려움이 있었습니다.

구체적으로 다음 세 가지 핵심 연구 문제를 다룹니다:

1. **비아이코닉 뷰(Non-iconic views) 객체 탐지:** 부분적으로 가려져 있거나, 배경에 있거나, 혼란스러운 환경 속에서 나타나는 객체 인식이 어렵습니다.
2. **객체 간 맥락적 추론(Contextual reasoning):** 작거나 모호한 객체의 식별을 위해 주변 맥락 정보가 필수적이지만, 기존 데이터셋은 주로 객체를 개별적으로 보여줍니다.
3. **정확한 2D 객체 위치 파악(Precise 2D localization):** 객체의 정확한 공간적 이해를 위해서는 바운딩 박스(bounding box)를 넘어 픽셀 단위의 정밀한 분할(segmentation) 정보가 필요하지만, 기존 데이터셋은 인스턴스 수준의 분할 마스크가 부족합니다.

## ✨ 주요 기여

- **MS COCO (Microsoft Common Objects in Context) 데이터셋 제안:** 복잡한 일상 장면에서 흔한 객체를 자연스러운 맥락에서 담은 대규모 데이터셋을 공개했습니다.
- **인스턴스별 분할(Per-instance segmentation) 마스크 제공:** 각 객체 인스턴스에 대한 픽셀 단위의 분할 마스크를 제공하여 정확한 객체 위치 파악을 지원합니다.
- **대규모 및 다양성:** 91개 객체 유형(4세 아이도 쉽게 인식 가능한)에 걸쳐 32만 8천 개의 이미지에 총 250만 개의 레이블된 인스턴스를 포함합니다.
- **풍부한 맥락 정보:** 이미지당 평균 7.7개의 객체 인스턴스와 3.5개의 카테고리를 포함하여 기존 데이터셋보다 훨씬 많은 객체와 맥락 정보를 제공합니다. 이는 비아이코닉 뷰 및 맥락적 추론 연구에 적합합니다.
- **새로운 어노테이션 파이프라인 개발:** Amazon Mechanical Turk(AMT)를 활용한 효율적이고 고품질의 크라우드소싱 파이프라인(카테고리 레이블링, 인스턴스 스포팅, 인스턴스 분할, 검증)을 구축했습니다.
- **상세한 통계 분석 및 비교:** PASCAL VOC, ImageNet, SUN 등 기존 데이터셋과 비교하여 MS COCO의 독특한 특성(인스턴스 수, 이미지당 객체 수, 객체 크기 분포 등)을 분석합니다.
- **벤치마크 성능 분석 제공:** DPM(Deformable Parts Model)을 사용하여 바운딩 박스 및 분할 탐지(segmentation detection)에 대한 기준 성능을 제시합니다.

## 📎 관련 연구

- **이미지 분류(Image Classification):** MNIST, COIL, Caltech 101/256, CIFAR-10/100, 그리고 대규모의 계층적 데이터베이스인 ImageNet [1]. ImageNet은 최근 딥러닝 알고리즘의 발전에 기여했습니다 [5, 6, 7].
- **객체 탐지(Object Detection):** 초기 얼굴 탐지 [32] 및 보행자 탐지 [4] 데이터셋. PASCAL VOC [2]는 20개 카테고리에 걸쳐 바운딩 박스 및 일부 분할 정보를 제공하며 널리 채택된 벤치마크입니다. ImageNet에서도 탐지 챌린지용으로 200개 객체 카테고리의 하위 집합이 생성되었습니다 [34].
- **시맨틱 장면 레이블링(Semantic Scene Labeling):** 이미지의 모든 픽셀에 카테고리(예: 하늘, 의자, 도로) 레이블을 부여하는 작업 [11, 35, 14]. SUN 데이터셋 [3]은 장면 이해를 위해 908개 장면 카테고리의 분할된 객체를 포함하지만, 객체 인스턴스 분포가 '롱테일' 현상을 보입니다.
- **기타 비전 데이터셋:** 스테레오 비전 [20], 멀티뷰 스테레오 [36], 광학 흐름 [21]을 위한 Middlebury 데이터셋; 분할 및 엣지 탐지 [37]를 위한 BSDS500; 장면 및 객체 속성 [9, 8, 38]을 위한 데이터셋 등이 있습니다.

## 🛠️ 방법론

MS COCO 데이터셋은 3가지 주요 작업으로 구성된 혁신적인 크라우드소싱 파이프라인을 통해 구축되었습니다:

1. **객체 카테고리 및 비아이코닉 이미지 수집:**

   - **카테고리 선정:** PASCAL VOC 카테고리, 시각적으로 식별 가능한 단어 목록, 4-8세 어린이의 의견 등을 종합하여 272개 후보를 선별했습니다. 최종적으로 "사물(thing)" 카테고리에 중점을 둔 91개 엔트리 레벨 카테고리를 선택했습니다. 이들은 실제 응용에 유용하고, 다양하며, 각 카테고리당 5,000개 이상의 인스턴스를 수집하기 용이하도록 고려되었습니다.
   - **비아이코닉 이미지 수집:** Flickr에서 이미지를 수집하고, 단일 객체 검색 대신 "개 + 자동차"와 같은 **객체 쌍 또는 장면/객체 쌍 조합**으로 검색하여 복잡하고 맥락이 풍부한 비아이코닉 이미지를 확보했습니다. 수집된 이미지는 필터링 단계를 거쳐 비아이코닉한 특성을 유지했습니다.

2. **이미지 어노테이션 (Amazon Mechanical Turk 활용):**
   - **카테고리 레이블링:** 91개 카테고리를 11개의 상위 카테고리로 그룹화하는 계층적 접근 방식 [18]을 사용하여 작업량을 줄였습니다. 작업자는 이미지에 특정 상위 카테고리의 객체가 있는지 먼저 확인한 후, 해당되는 하위 카테고리 아이콘을 이미지 내 한 인스턴스 위에 끌어다 놓습니다. 높은 리콜을 위해 각 이미지는 8명의 작업자가 레이블링했습니다.
   - **인스턴스 스포팅:** 이전 단계에서 레이블된 카테고리에 대해, 작업자는 이미지 내 해당 카테고리의 **모든 인스턴스** (최대 10개) 위에 십자 표시를 합니다. 작은 객체를 찾기 위해 돋보기 기능이 제공되었으며, 역시 8명의 작업자가 참여했습니다.
   - **인스턴스 분할:** Bell et al. [16]의 사용자 인터페이스를 수정하여 각 객체 인스턴스에 대한 픽셀 단위 분할 마스크를 생성합니다.
     - **훈련 작업:** 모든 작업자는 각 객체 카테고리에 대한 훈련 작업을 이수해야 했습니다. 이 훈련은 작업자의 분할이 정답과 충분히 일치해야만 통과할 수 있었고, 이는 어노테이션 품질을 크게 향상시켰습니다.
     - **검증 단계:** 시간 소모적인 작업이므로 각 인스턴스는 한 번만 분할되지만, 품질 보장을 위해 3~5명의 작업자가 분할된 마스크를 검증합니다. 불충분한 품질의 분할은 다시 풀에 추가되어 재어노테이션됩니다.
     - **"군중(Crowds)" 처리:** 한 카테고리의 인스턴스가 10-15개 이상 밀집된 경우, 개별 분할 대신 단일 (다중 부분) 세그먼트로 "군중"으로 표시하여 평가에서 무시됩니다.

## 📊 결과

- **데이터셋 통계:**
  - **규모:** MS COCO는 총 32만 8천 개 이미지에 91개 카테고리의 250만 개 인스턴스를 포함합니다 (2014년 초기 릴리스는 80개 카테고리, 16만 5천 개 이미지 중 학습/검증 데이터에 88만 6천 개 인스턴스).
  - **인스턴스 밀도:** 이미지당 평균 3.5개 카테고리 및 7.7개 인스턴스를 포함합니다. 이는 ImageNet (평균 3.0 인스턴스) 및 PASCAL VOC (평균 2.3 인스턴스)보다 훨씬 높습니다. MS COCO 이미지의 10%만이 단일 카테고리를 포함하는 반면, ImageNet과 PASCAL VOC는 60% 이상이 단일 카테고리입니다.
  - **객체 크기:** MS COCO의 객체는 ImageNet 및 PASCAL VOC보다 평균적으로 작습니다.
- **알고리즘 분석 (DPMv5 기반 기준선):**
  - **바운딩 박스 탐지:** PASCAL VOC 데이터로 학습된 DPMv5-P 모델이 MS COCO에서 테스트될 때 성능이 약 2배 저하되었습니다. 이는 MS COCO가 부분 가려짐, 복잡한 배경 등으로 인해 훨씬 더 어려운 데이터셋임을 시사합니다.
  - MS COCO 데이터로 학습된 DPMv5-C 모델 또한 MS COCO에서는 유사한 성능 저하를 보였으나, PASCAL VOC 데이터에서 테스트 시 DPMv5-P보다 6/20개 카테고리에서 더 나은 성능을 보였습니다. 이는 MS COCO에서 학습된 모델이 더 많은 훈련 데이터를 통해 더 쉬운 데이터셋에 대한 **일반화 능력(generalization ability)**을 향상시킬 수 있음을 나타냅니다.
  - **분할 기반 탐지 평가:** DPM 부분 마스크를 투영하여 분할을 생성하는 간단한 기준선 모델을 사용했습니다. 이 기준선 결과는 픽셀 수준 분할이 탐지 정확도와는 별개로 매우 어려운 작업임을 보여줍니다 (예: 사람 카테고리의 경우 바운딩 박스 겹침($\text{IoU} \ge 0.5$)이 좋더라도 분할 겹침은 훨씬 낮을 수 있음).

## 🧠 통찰 및 논의

MS COCO는 일상 생활의 자연스러운 환경에서 발견되는 객체들을 탐지하고 분할하기 위한 새로운 데이터셋입니다. 7만 시간 이상의 크라우드 워커 작업 시간을 투입하여 방대한 객체 인스턴스 컬렉션을 구축하고, 객체 탐지 및 분할 알고리즘의 발전을 촉진하도록 설계되었습니다. 특히, 다양한 시점에서 나타나는 **비아이코닉 이미지**와 **풍부한 맥락 정보**를 담는 데 중점을 두었습니다.

데이터셋 통계는 MS COCO가 기존 데이터셋에 비해 훨씬 복잡하고 맥락 정보가 풍부하며, 작은 객체들이 많이 포함되어 있어 실제 환경에서의 객체 인식 문제에 더 가깝다는 것을 보여줍니다. DPM 모델을 사용한 초기 기준선 분석은 MS COCO가 기존 PASCAL VOC에 비해 탐지 및 분할 측면에서 훨씬 더 어려운 도전 과제임을 확인시켰습니다. 동시에, MS COCO와 같은 복잡한 데이터셋으로 학습된 모델이 더 넓은 범위의 이미지에 대한 일반화 능력을 향상시킬 잠재력을 시사합니다.

향후 연구 방향으로는 "사물(thing)" 카테고리 외에 "재료/배경(stuff)" 카테고리를 레이블링하여 더 많은 맥락 정보를 제공하거나, 객체 가려짐(occlusion) 정도나 키포인트(keypoints)와 같은 추가적인 어노테이션을 포함하는 것이 유용할 수 있습니다. 또한, 장면 유형, 속성, 전체 문장으로 된 이미지 설명 등 다양한 유형의 레이블링에 대한 벤치마크로 활용될 가능성도 있습니다.

## 📌 한 줄 요약 (TL;DR)

**문제:** 기존 객체 인식 데이터셋은 실제 복잡한 장면의 비아이코닉 뷰, 맥락적 추론, 정밀한 픽셀 단위 인스턴스 분할 정보가 부족하여 장면 이해에 한계가 있었습니다.
**방법:** Microsoft COCO 데이터셋은 일상생활의 복잡한 장면에서 비아이코닉 뷰와 풍부한 맥락 정보를 담은 91개 객체 카테고리(총 250만 인스턴스)에 대한 **인스턴스별 픽셀 단위 분할 마스크**를 제공합니다. 이를 위해 아마존 메카니컬 터크를 활용한 독창적이고 고품질의 크라우드소싱 파이프라인을 구축했습니다.
**결과:** MS COCO는 이미지당 더 많은 객체와 카테고리를 포함하며, 기존 데이터셋보다 객체 크기가 작아 탐지 및 분할 난이도가 훨씬 높지만, 모델의 일반화 능력을 향상시킬 잠재력을 보여주었습니다.
