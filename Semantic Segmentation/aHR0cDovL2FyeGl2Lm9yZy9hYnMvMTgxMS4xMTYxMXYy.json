{
  "url": "http://arxiv.org/abs/1811.11611v2",
  "title": "A Generative Appearance Model for End-to-end Video Object Segmentation",
  "authors": "Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan, Michael Felsberg",
  "year": 2018,
  "abstract": "One of the fundamental challenges in video object segmentation is to find an\neffective representation of the target and background appearance. The best\nperforming approaches resort to extensive fine-tuning of a convolutional neural\nnetwork for this purpose. Besides being prohibitively expensive, this strategy\ncannot be truly trained end-to-end since the online fine-tuning procedure is\nnot integrated into the offline training of the network.\n  To address these issues, we propose a network architecture that learns a\npowerful representation of the target and background appearance in a single\nforward pass. The introduced appearance module learns a probabilistic\ngenerative model of target and background feature distributions. Given a new\nimage, it predicts the posterior class probabilities, providing a highly\ndiscriminative cue, which is processed in later network modules. Both the\nlearning and prediction stages of our appearance module are fully\ndifferentiable, enabling true end-to-end training of the entire segmentation\npipeline. Comprehensive experiments demonstrate the effectiveness of the\nproposed approach on three video object segmentation benchmarks. We close the\ngap to approaches based on online fine-tuning on DAVIS17, while operating at 15\nFPS on a single GPU. Furthermore, our method outperforms all published\napproaches on the large-scale YouTube-VOS dataset."
}