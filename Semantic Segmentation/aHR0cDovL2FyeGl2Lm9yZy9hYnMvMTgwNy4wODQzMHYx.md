# Actor-Action Semantic Segmentation with Region Masks
Kang Dang, Chunluan Zhou, Zhigang Tu, Michael Hoy, Justin Dauwels, Junsong Yuan

## 🧩 Problem to Solve
이 논문은 비디오 프레임 내에서 주체(actor)와 행동(action) 카테고리를 동시에 레이블링하는 '주체-행동 의미론적 분할(actor-action semantic segmentation)' 문제를 다룹니다. 이 작업의 주요 과제는 주체가 행동을 수행할 때, 주체의 다른 신체 부위가 행동 카테고리에 대해 상이한 단서를 제공하여 픽셀들을 독립적으로 레이블링할 경우 **불일치한 행동 레이블링**이 발생할 수 있다는 점입니다.

## ✨ Key Contributions
*   **영역 마스크 활용을 통한 일관성 확보**: 픽셀들이 독립적으로 레이블링될 때 발생할 수 있는 행동 레이블링의 불일치 문제를 해결하기 위해 영역 마스크(region mask)를 활용하는 방법을 제안합니다. 포괄적인 실험을 통해 일관된 주체-행동 레이블링을 위한 영역 마스크 활용의 중요성을 입증했습니다.
*   **종단 간(End-to-End) 주체-행동 의미론적 분할 네트워크 설계**:
    *   각 객체에 대해 여러 영역 마스크를 획득하고, 예측된 카테고리를 영역-픽셀 레이어(region-to-pixel layer)를 통해 최종 예측으로 융합합니다. 이를 통해 영역 내 픽셀들이 하나의 동일한 행동 레이블을 가지도록 강제하여 일관성을 높입니다.
    *   RGB 이미지와 광학 흐름(optical flow) 이미지를 입력으로 받는 이중 스트림(two-stream) 네트워크 아키텍처를 사용하여 의미론적 분할 성능을 추가로 향상시킵니다.

## 📎 Related Works
*   **주체-행동 의미론적 분할**: Xu et al. [29]에 의해 소개된 이 분야는 주체 및 행동 카테고리의 동시 레이블링을 목표로 합니다. 이전 연구들은 레이블 간의 상호작용 모델링 [13, 29], 장거리 상호작용 모델링 [28], 약지도 학습(weakly-supervised learning) [30], 3D 컨볼루션 네트워크 및 LSTM [23] 등을 탐구했습니다. 본 논문은 고품질 영역 마스크를 딥 네트워크에 직접 통합하여 일관성을 높이는 데 중점을 둡니다.
*   **영역 기반 의미론적 분할**: Caesar et al. [3]은 미분 가능한 영역-픽셀 레이어와 ROI 풀링(pooling)을 사용하여 객체 경계 정확도를 개선했습니다. Arnab et al. [1]과 He et al. [11]은 각각 객체 감지 경계 및 슈퍼픽셀 영역 제약을 활용했습니다. 본 논문은 이러한 연구 방향을 확장하여 영역 마스크 제약을 주체-행동 레이블링의 일관성 향상에 적용합니다.
*   **인스턴스 분할(Instance Segmentation)**: FCIS [33] 및 Dai et al. [6]과 같은 다양한 인스턴스 분할 알고리즘은 본 제안 방법의 입력으로 사용될 수 있는 후보 영역 마스크를 생성합니다. 본 연구는 FCIS를 활용하며, 특히 여러 후보 마스크에 대한 영역-픽셀 레이어를 통해 객체 레이블링을 생성하고, RGB 및 광학 흐름 정보를 모두 활용하는 이중 스트림 아키텍처를 사용한다는 점에서 이전 연구와 차별화됩니다.

## 🛠️ Methodology
본 논문은 종단 간 영역 기반 주체-행동 의미론적 분할 접근 방식을 제안하며, 다음 단계로 구성됩니다.

1.  **영역 마스크 생성**: 각 프레임에 대해 FCIS(Fully Convolutional Instance Segmentation) [33] 알고리즘을 사용하여 후보 영역 마스크 $ \{b_i, M_i\}_{i=1}^N $를 생성합니다. 이 때, 특정 데이터셋(예: A2D)에 대해 FCIS를 미세 조정(fine-tuning)하여 관련 없는 배경 영역을 줄이고 고품질 마스크를 얻습니다.
2.  **프론트엔드 모듈 (이중 스트림 네트워크)**:
    *   **외형(Appearance) 스트림**: RGB 이미지를 입력으로 받아 외형 정보를 학습합니다.
    *   **움직임(Motion) 스트림**: 광학 흐름 이미지를 입력으로 받아 움직임 정보를 학습합니다 (FlowNet [8]을 사용하여 광학 흐름 계산).
    *   각 스트림은 ResNet-101 [9]을 기반으로 하며, 공간 디테일 보존을 위해 마지막 두 레이어를 확장 컨볼루션(dilated convolution)으로 대체하여 출력 해상도를 원본 이미지 크기의 1/8로 높입니다.
    *   두 스트림의 특징 맵은 연결(concatenate)되어 백엔드 모듈의 입력으로 사용됩니다 (조기 융합 전략).
3.  **백엔드 모듈 (영역 기반 분할 네트워크)**:
    *   프론트엔드에서 얻은 융합된 특징 $X \in \mathbb{R}^{W \times H \times 2C}$와 후보 영역 마스크 $ \{b_i, M_i\}_{i=1}^N $를 입력으로 받습니다.
    *   두 개의 네트워크를 사용하여 주체 레이블 $Y_{\text{actor}} \in \mathbb{R}^{W \times H \times K_{\text{actor}}}$와 행동 레이블 $Y_{\text{action}} \in \mathbb{R}^{W \times H \times K_{\text{action}}}$을 각각 예측합니다.
    *   **영역 분류**: 각 바운딩 박스 $b_i$에 대해 ROI 풀링 레이어와 여러 개의 완전 연결(FC) 레이어를 통해 전체 영역 마스크 점수 $s_{i, \text{actor}}$ 및 $s_{i, \text{action}}$를 얻습니다.
    *   **영역-픽셀 레이어**: 영역 분류 점수를 픽셀 점수로 변환하여 일관된 레이블링을 강제합니다. 픽셀 $j$에 대한 점수는 다음과 같이 계산됩니다:
        $$ y_j^{\text{actor}} = \max_{i=1,...,N, j \in b_i} (m_{i,j} \times s_{i, \text{actor}}) $$
        $$ y_j^{\text{action}} = \max_{i=1,...,N, j \in b_i} (m_{i,j} \times s_{i, \text{action}}) $$
        여기서 $m_{i,j}$는 픽셀 $j$가 마스크 $i$에 속할 확률을 나타냅니다. 여러 영역 마스크에 속하는 픽셀의 경우, 최대 풀링(max pooling)을 적용하여 레이블링 충돌을 해결합니다.
    *   결과로 나온 픽셀 점수는 소프트맥스(softmax) 레이어를 통과하여 클래스 확률을 출력합니다.
    *   훈련 시 각 픽셀 $j$에 대한 손실 함수는 다음과 같습니다:
        $$ L_{j, \text{loss}} = -\log(p_{j, \text{actor}}(o_{j, \text{actor}})) - \log(p_{j, \text{action}}(o_{j, \text{action}})) $$
        여기서 $o_{j, \text{actor}}$ 및 $o_{j, \text{action}}$는 각각 주체 및 행동의 실제 레이블입니다.
4.  **네트워크 훈련**: 훈련 가속화를 위해 2단계 훈련 절차를 채택합니다.
    *   **1단계**: 프론트엔드 네트워크를 사전 훈련된 모델로 초기화하고, 매개변수가 적은 가벼운 백엔드 네트워크(DeepLab CNN [4] 기반)와 함께 훈련합니다.
    *   **2단계**: 프론트엔드 네트워크의 매개변수를 고정하고, 제안하는 영역 기반 백엔드 네트워크의 완전 연결 레이어만 훈련합니다.

## 📊 Results
A2D 데이터셋 [29]을 사용하여 제안된 방법의 효과를 검증했습니다.
*   **DeepLab CNN 기반 베이스라인 대비 성능**:
    *   전역 픽셀 정확도(global pixel accuracy)에서는 경계 부정확성으로 인해 베이스라인보다 약간 낮게 나타납니다.
    *   하지만 **평균 클래스 정확도(mean class accuracy)에서는 모든 설정(주체, 행동, 주체-행동)에서 베이스라인보다 약 10% 더 우수**하며, **평균 클래스 IoU(mean class IoU)에서는 행동 및 주체-행동 설정에서 약 3% 더 우수**합니다. 이는 특히 도전적인 자세나 작은 객체에 대한 일관된 분할과 더 나은 캡처 능력에서 기인합니다.
    *   비객체 경계(non-boundary) 영역에 대한 평가에서도 제안된 방법은 평균 클래스 정확도에서 6%, 평균 클래스 IoU에서 3% 더 나은 성능을 보여, 일관된 레이블링 해결이 경계 처리 능력보다 더 큰 성능 향상에 기여함을 입증합니다.
*   **영역 마스크 품질의 영향**: A2D 데이터셋에서 훈련된 FCIS 모델이 COCO 데이터셋에서 훈련된 모델보다 일관되게 더 좋은 성능을 보였습니다. 이는 고품질의 데이터셋 특정 후보 영역 마스크가 성능 향상에 기여함을 보여줍니다. 또한, 실제 영역 마스크(ground truth region masks)를 사용할 경우 성능이 크게 향상되어(평균 클래스 IoU에서 7% 이상), 이상적인 인스턴스 분할 성능이 달성된다면 주체-행동 의미론적 분할의 잠재력이 매우 크다는 것을 시사합니다.
*   **광학 흐름 통합의 영향**: 광학 흐름 정보는 빠르게 움직이는 주체-행동 카테고리(예: adult-jumping, car-running, ball-flying 등)의 성능을 10% 이상 크게 향상시켰습니다. 전반적으로 광학 흐름을 포함했을 때 주체-행동 설정에서 평균 클래스 정확도 및 평균 클래스 IoU가 4% 이상 개선되었습니다.
*   **기존 최첨단 방법과의 비교**: 제안된 방법은 A2D 데이터셋에서 기존 최첨단 기술보다 **평균 클래스 정확도에서 8% 이상, 평균 클래스 IoU에서 5% 이상 우수한 성능**을 달성했습니다.
*   **계산 복잡성**: 프레임당 약 1100ms (광학 흐름 350ms, 영역 마스크 300ms, 분할 네트워크 450ms)가 소요되어 베이스라인(395ms)보다 높지만, CUDA 구현을 통해 최적화될 여지가 있습니다.

## 🧠 Insights & Discussion
*   **일관된 레이블링의 중요성**: 주체의 여러 신체 부위가 특정 행동을 다르게 반영할 수 있다는 점을 고려할 때, 영역 마스크를 사용하여 영역 내 픽셀에 일관된 행동 레이블을 부여하는 전략은 의미론적 분할의 정확도를 크게 향상시킵니다. 특히 'adult-jumping', 'bird-rolling'과 같이 특정 신체 움직임이 두드러지는 행동에서 효과적입니다.
*   **작은 객체 처리 개선**: 영역 마스크를 활용한 적응형 수용 필드(adaptive receptive field)는 기존 방법이 놓칠 수 있는 작은 객체(예: 새, 공)의 분할 정확도를 높이는 데 기여합니다.
*   **광학 흐름의 보완적 역할**: 광학 흐름은 움직임 정보에 대한 강력한 단서를 제공하여 특히 빠른 움직임과 관련된 행동 카테고리의 성능을 유의미하게 개선합니다. 이는 외형 정보와 움직임 정보의 융합이 복잡한 비디오 이해 작업에 필수적임을 시사합니다.
*   **제한 사항 및 향후 연구**:
    *   현재 접근 방식은 영역 마스크 생성 단계의 품질에 크게 의존합니다. 부정확한 영역 마스크는 최종 분할 결과에 부정적인 영향을 미칠 수 있습니다.
    *   높은 계산 복잡성은 실시간 애플리케이션에 대한 한계로 작용할 수 있습니다. 향후에는 광학 흐름 추정 및 영역 마스크 생성과 주체-행동 의미론적 분할을 하나의 통합된 딥 네트워크로 결합하여 효율성과 정확도를 동시에 개선할 필요가 있습니다.
    *   실제 인스턴스 분할 마스크가 완벽할 경우 상당한 성능 향상 가능성을 보여주므로, 인스턴스 분할 방법론의 지속적인 개선이 주체-행동 의미론적 분할 성능을 더욱 높일 것입니다.

## 📌 TL;DR
본 논문은 비디오 주체-행동 의미론적 분할에서 신체 부위별 불일치한 행동 레이블링 문제를 해결하기 위해 **영역 마스크 기반의 종단 간 접근 방식**을 제안합니다. **이중 스트림 프론트엔드** (RGB 및 광학 흐름)와 **영역 기반 백엔드** (영역 마스크를 활용한 일관된 픽셀 레이블링 및 최대 풀링 기반 충돌 해결)를 사용합니다. A2D 데이터셋에서 제안 방법은 **기존 최첨단 대비 평균 클래스 정확도 8%, 평균 클래스 IoU 5% 이상 향상**되었으며, 영역 마스크와 광학 흐름의 중요성을 입증했습니다.