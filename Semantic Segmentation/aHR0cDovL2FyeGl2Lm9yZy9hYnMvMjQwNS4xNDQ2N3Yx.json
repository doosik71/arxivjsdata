{
  "title": "Segformer++: Efficient Token-Merging Strategies for High-Resolution\n  Semantic Segmentation",
  "authors": "Daniel Kienzle, Marco Kantonis, Robin Sch√∂n, Rainer Lienhart",
  "year": 2024,
  "url": "http://arxiv.org/abs/2405.14467v1",
  "abstract": "Utilizing transformer architectures for semantic segmentation of\nhigh-resolution images is hindered by the attention's quadratic computational\ncomplexity in the number of tokens. A solution to this challenge involves\ndecreasing the number of tokens through token merging, which has exhibited\nremarkable enhancements in inference speed, training efficiency, and memory\nutilization for image classification tasks. In this paper, we explore various\ntoken merging strategies within the framework of the Segformer architecture and\nperform experiments on multiple semantic segmentation and human pose estimation\ndatasets. Notably, without model re-training, we, for example, achieve an\ninference acceleration of 61% on the Cityscapes dataset while maintaining the\nmIoU performance. Consequently, this paper facilitates the deployment of\ntransformer-based architectures on resource-constrained devices and in\nreal-time applications."
}