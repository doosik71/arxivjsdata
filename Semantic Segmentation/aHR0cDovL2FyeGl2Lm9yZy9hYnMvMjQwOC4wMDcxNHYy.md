# SAM 2: Segment Anything in Images and Videos

Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, Christoph Feichtenhofer

## 🧩 Problem to Solve

기존 Segment Anything 모델(SAM)은 이미지 내 객체를 프롬프트 기반으로 분할하는 데 성공했지만, 실제 세계의 시각적 객체는 복잡한 움직임을 보이며 특히 비디오 데이터에서 시간적 차원을 가집니다. 증강 현실/가상 현실(AR/VR), 로봇 공학, 자율 주행, 비디오 편집 등 다양한 응용 분야에서는 이미지 수준의 분할을 넘어선 시간적 지역화가 필수적입니다. 기존 비디오 분할 모델과 데이터셋은 "비디오 내 무엇이든 분할"하는 능력이 부족하며, 객체의 외형 변화(움직임, 변형, 가려짐, 조명 변화), 낮은 비디오 품질, 그리고 대량의 프레임을 효율적으로 처리하는 문제가 존재합니다. 이 논문은 이미지와 비디오 모두에 적용 가능한 범용적인 시각 분할 시스템을 구축하고, 프롬프트 기반 시각 분할(Promptable Visual Segmentation, PVS)이라는 새로운 태스크를 제시하여 이 문제를 해결하고자 합니다.

## ✨ Key Contributions

* **SAM 2 도입**: 이미지와 비디오 모두에서 프롬프트 기반 시각 분할을 위한 통합 기반 모델인 Segment Anything Model 2 (SAM 2)를 제안합니다.
* **SA-V 데이터셋 구축**: 사용자 상호작용을 통해 모델과 데이터를 개선하는 데이터 엔진을 구축하여, 현재까지 가장 큰 규모의 비디오 분할 데이터셋인 Segment Anything Video (SA-V)를 수집했습니다. SA-V는 기존 비디오 분할 데이터셋보다 53배 많은 마스크를 포함하며, 전체 객체뿐만 아니라 객체 부분(parts)도 광범위하게 포함합니다.
* **스트리밍 메모리 기반 모델 아키텍처**: 실시간 비디오 처리를 위해 스트리밍 메모리를 갖춘 간단한 트랜스포머 아키텍처를 도입하여 SAM을 비디오 도메인으로 자연스럽게 확장했습니다.
* **우수한 성능 입증**:
  * **비디오 분할**: 이전 접근 방식보다 3배 적은 상호작용으로 더 나은 정확도를 달성했습니다.
  * **이미지 분할**: 기존 SAM보다 더 정확하고 6배 더 빠릅니다.
* **오픈 소스 공개**: 핵심 모델, 데이터셋, 모델 학습 코드를 개방형 라이선스로 공개하여 후속 연구와 응용을 촉진합니다.

## 📎 Related Works

* **이미지 분할 (Image segmentation)**:
  * **Segment Anything (SAM)**: Kirillov et al. (2023)이 프롬프트 기반 이미지 분할 태스크와 SA-1B 데이터셋을 통해 제로샷 분할 능력을 제시했습니다.
  * **확장 및 응용**: HQ-SAM (Ke et al., 2024)과 같은 고품질 출력 개선, EfficientSAM (Xiong et al., 2023)과 같은 효율성 개선 연구가 있었으며, 의료 영상, 원격 감지 등 다양한 분야에 적용되었습니다.
* **대화형 비디오 객체 분할 (Interactive Video Object Segmentation, iVOS)**:
  * **초기 접근 방식**: Wang et al. (2005) 등은 그래프 기반 최적화를 사용했습니다.
  * **최근 접근 방식**: Heo et al. (2020) 등은 사용자 입력을 마스크로 변환하고 다른 프레임으로 전파하는 모듈식 설계를 채택했습니다. 클릭 기반 입력이 수집 용이합니다.
  * **SAM과 추적기 결합**: SAM을 이미지에 적용하고 비디오 추적기와 결합하는 연구 (Cheng et al., 2023b)가 있었지만, 모든 객체에 대한 추적 한계, 비디오 프레임에서의 SAM 성능 문제, 그리고 대화형 정제 메커니즘의 부재 등의 한계가 있었습니다.
* **비디오 객체 분할 (Video Object Segmentation, VOS)**:
  * **반지도 VOS (Semi-supervised VOS)**: 첫 프레임에 주어진 객체 마스크를 비디오 전체에서 정확하게 추적하는 태스크입니다. 관련 응용 분야가 많습니다.
  * **접근 방식**: 초기 딥러닝 기반은 첫 프레임에 대한 온라인 미세 조정 (Caelles et al., 2016) 또는 오프라인 학습 모델 (Hu et al., 2018b), RNN/트랜스포머 (Xu et al., 2018a)를 사용했습니다.
  * **SAM 2와의 관계**: 반지도 VOS는 PVS 태스크의 특수한 경우로 간주될 수 있습니다 (첫 프레임에 마스크 프롬프트만 주어지는 경우). 첫 프레임 마스크 주석의 어려움이 있습니다.
* **비디오 분할 데이터셋 (Video segmentation datasets)**:
  * **기존 데이터셋**: DAVIS (Pont-Tuset et al., 2017), YouTube-VOS (Xu et al., 2018b) 등은 고품질 주석을 제공하지만 규모가 작습니다. 최근에는 가려짐, 긴 비디오, 극단적 변형, 객체/장면 다양성 등에 초점을 맞춘 데이터셋이 등장했습니다.
  * **한계**: 기존 데이터셋은 주로 전체 객체(부분 아님)를 다루고 특정 객체 클래스(사람, 차량, 동물)에 집중하여 "비디오 내 무엇이든 분할"하는 능력 구현에 부족함이 있었습니다.

## 🛠️ Methodology

SAM 2는 이미지와 비디오 도메인 모두에서 객체의 공간적 범위를 정의하기 위해 개별 프레임에 점, 상자 및 마스크 프롬프트를 입력으로 받는 SAM의 일반화 버전입니다.

* **PVS 태스크**:
  * 비디오의 **어떤 프레임**에서든 모델에 프롬프트(긍정/부정 클릭, 상자, 마스크)를 제공하여 관심 객체를 정의하거나 모델 예측을 정제할 수 있습니다.
  * 프롬프트 수신 시, 모델은 해당 프레임에 대한 유효한 분할 마스크를 즉시 응답하고, 비디오 전체에 걸쳐 해당 객체의 마스크렛(spatio-temporal mask)을 전파해야 합니다.
  * 추가 프롬프트는 비디오 전체의 **어떤 프레임**에서도 세그먼트 정제를 위해 제공될 수 있습니다.

* **SAM 2 모델 아키텍처**:
  * **이미지 인코더**: MAE로 사전 학습된 Hiera (Ryali et al., 2023; Bolya et al., 2023) 계층적 이미지 인코더를 사용합니다. 실시간 처리를 위해 비디오 프레임을 스트리밍 방식으로 처리하며, 이미지 인코더는 전체 상호작용 동안 한 번만 실행됩니다. 다중 스케일 특징을 활용합니다.
  * **메모리 어텐션**: 현재 프레임 특징을 과거 프레임 특징, 예측, 새로운 프롬프트에 조건화합니다. `L`개의 트랜스포머 블록으로 구성되며, 메모리 뱅크에 저장된 공간 특징과 객체 포인터에 대한 교차 어텐션을 수행합니다. 바닐라 어텐션 연산을 사용합니다.
  * **프롬프트 인코더**: SAM과 동일하며 클릭, 상자, 마스크 프롬프트를 처리합니다.
  * **마스크 디코더**: SAM의 "양방향" 트랜스포머 블록을 따릅니다. 모호한 프롬프트(예: 단일 클릭)에 대해 여러 마스크를 예측하여 유효성을 보장합니다. **새로운 요소**:
    * 객체가 현재 프레임에 존재하는지 예측하는 추가 **가려짐 예측 헤드**를 포함합니다.
    * 계층적 이미지 인코더로부터의 **스킵 연결**을 사용하여 고해상도 임베딩을 마스크 디코딩에 통합합니다.
    * 마스크 토큰은 객체 포인터 토큰으로 사용됩니다.
  * **메모리 인코더**: 예측 마스크를 다운샘플링하고 이미지 인코더의 무조건화된 프레임 임베딩과 융합하여 메모리를 생성합니다.
  * **메모리 뱅크**: 최대 $N$개의 최근 프레임 메모리(FIFO 큐)와 최대 $M$개의 프롬프트 프레임 메모리를 유지합니다. 공간 특징 맵과 객체 포인터(경량 벡터)를 저장합니다. 최근 프레임 메모리에만 시간적 위치 정보를 임베딩합니다.

* **데이터 엔진**: 대규모 SA-V 데이터셋 수집을 위해 모델-인-더-루프(model-in-the-loop) 방식을 사용합니다.
  * **Phase 1 (SAM per frame)**: 초기 단계. 이미지 기반 SAM으로 프레임별 수동 주석. 느리지만 (37.8초/프레임) 고품질 공간 주석 생성. 1.4K 비디오에서 16K 마스크렛 수집. SA-V 검증/테스트 세트 주석에 사용.
  * **Phase 2 (SAM + SAM 2 Mask)**: SAM 2 Mask (마스크 프롬프트만 허용)를 도입하여 마스크의 시간적 전파를 지원. 여전히 중간 프레임에서 마스크를 처음부터 주석해야 함. 7.4초/프레임으로 5.1배 속도 향상. 63.5K 마스크렛 수집.
  * **Phase 3 (SAM 2)**: 완전한 SAM 2 (점, 마스크 프롬프트 모두 허용)를 활용. 객체에 대한 메모리 이점을 통해 예측 마스크렛 편집 시 간헐적인 정제 클릭만 필요. 4.5초/프레임으로 Phase 1 대비 8.4배 속도 향상. 197.0K 마스크렛 수집.
  * **품질 검증**: 별도의 주석자가 마스크렛 품질을 "만족" 또는 "불만족"으로 검증. 불만족 마스크렛은 정제 파이프라인으로 반환.
  * **자동 마스크렛 생성**: SAM 2를 정규 그리드 점으로 프롬프트하여 후보 마스크렛을 생성하고 검증 단계에서 필터링. 다양성을 높이고 모델 실패 사례를 식별.

* **SA-V 데이터셋**: 데이터 엔진을 통해 수집된 SA-V는 50.9K 비디오와 642.6K 마스크렛(190.9K 수동, 451.7K 자동)으로 구성되며, 총 35.5M개의 마스크를 포함합니다. 이는 기존 VOS 데이터셋보다 53배 큰 규모입니다.

* **학습**: 이미지 및 비디오 데이터에 대해 **공동 학습**을 수행합니다.
  * **상호작용 시뮬레이션**: 8프레임 시퀀스를 샘플링하고 최대 2개 프레임(첫 프레임 포함)에 대해 확률적 교정 클릭을 시뮬레이션합니다. 초기 프롬프트는 실제 마스크(0.5), 긍정 클릭(0.25), 경계 상자(0.25)입니다.
  * **데이터 혼합**: SA-1B, SA-V, Internal 데이터셋 및 공개 소스 VOS 데이터셋(DAVIS, MOSE, YouTubeVOS)의 혼합을 사용합니다. 이미지와 비디오 데이터셋 간의 **교대 학습 전략**을 채택합니다.
  * **데이터 증강**: 수평 뒤집기, 아핀 변환, 색상 지터링, 그레이스케일 변환을 포함합니다. 유사한 객체가 있는 시나리오를 시뮬레이션하기 위해 **모자이크 변환** (10% 확률로 동일 비디오를 2x2 그리드로 타일링)을 사용합니다.
  * **미세 조정**: 긴 비디오의 품질 향상을 위해 가장 많이 편집된 비디오들을 대상으로 16프레임 시퀀스로 추가 미세 조정을 수행합니다.
  * **손실 함수**: 마스크 예측을 위한 focal 및 dice 손실의 선형 조합, IoU 예측을 위한 MAE 손실, 객체 존재 예측을 위한 교차 엔트로피 손실을 사용합니다.

## 📊 Results

* **프롬프트 기반 비디오 분할 (대화형 설정)**:
  * 9개의 제로샷 비디오 데이터셋(밀집 주석)에서 오프라인/온라인 평가를 수행했습니다.
  * SAM 2는 SAM+XMem++ 및 SAM+Cutie와 같은 강력한 베이스라인보다 뛰어난 성능을 보였습니다.
  * SAM 2는 **3배 적은 상호작용**으로도 더 나은 분할 정확도를 제공합니다.
* **반지도 비디오 객체 분할 (VOS)**:
  * 17개 비디오 데이터셋에서 클릭, 상자, 마스크 프롬프트를 **첫 프레임에만** 제공하는 방식으로 평가했습니다.
  * SAM 2는 XMem++ 및 Cutie (심지어 이들이 실제 마스크를 입력으로 사용할 때도)보다 뛰어난 성능을 보여, SAM 2가 기존의 비대화형 VOS 태스크에도 탁월함을 입증했습니다.
* **이미지 분할**:
  * 37개 제로샷 데이터셋(SAM이 사용했던 23개 + 14개 새 비디오 데이터셋)에서 평가했습니다.
  * SA-1B로만 학습된 SAM 2 (Hiera-B+)는 기존 SAM (ViT-H)보다 1-클릭 정확도에서 더 높고 (**58.9 mIoU vs 58.1 mIoU**), **6배 빠릅니다**. 이는 Hiera 이미지 인코더의 효율성 때문입니다.
  * SA-1B와 비디오 데이터를 혼합하여 학습하면 23개 데이터셋에서 평균 정확도가 **61.4%**로 더욱 향상되며, 특히 비디오 도메인의 벤치마크(이미지로 평가)에서 탁월한 성능 향상을 보였습니다.
* **반지도 VOS 최신 기술 (SOTA)과의 비교**:
  * SAM 2는 Hiera-B+ 및 Hiera-L 인코더 크기별로 현존하는 최고의 방법들보다 상당한 성능 향상을 보였습니다 (예: MOSE val에서 76.6 J&F vs Cutie-base+의 71.7 J&F).
  * 특히 SA-V val/test 벤치마크에서 **76.8/77.0 J&F**로 기존 방법들(약 60 J&F)을 압도적으로 능가하여 "비디오 내 무엇이든 분할" 능력의 격차를 보여줍니다.
  * 실시간 속도(A100 GPU 기준): Hiera-B+는 43.8 FPS, Hiera-L은 30.2 FPS를 달성했습니다.

## 🧠 Insights & Discussion

SAM 2는 이미지와 비디오 도메인을 아우르는 프롬프트 기반 시각 분할의 중대한 진전을 나타냅니다. SAM 아키텍처에 메모리 개념을 도입하고, 대규모의 다양한 SA-V 데이터셋을 구축함으로써, 시각 인지 분야의 후속 연구와 응용에 중요한 이정표를 제시합니다.

* **함의**: 이 연구는 기존 이미지 중심의 분할 모델을 시간적 차원을 가진 비디오 데이터로 성공적으로 확장하여, 실제 세계의 복잡한 시각 환경에 대응할 수 있는 범용적인 시각 분할 시스템의 가능성을 열었습니다. 특히, "무엇이든 분할"하는 SAM의 철학을 비디오 도메인으로 가져와 AR/VR, 로봇 공학 등 다양한 산업 분야에 잠재적인 영향을 미칠 수 있습니다.
* **한계**:
  * **장면 전환 및 복잡한 환경**: 장면 전환, 혼잡한 장면, 긴 가려짐 또는 긴 비디오에서 객체를 추적하거나 혼동하는 데 어려움을 겪을 수 있습니다. 하지만, **어떤 프레임에서든** 추가 프롬프트를 통해 오류를 빠르게 수정할 수 있습니다.
  * **세밀한 객체 및 빠른 움직임**: 매우 얇거나 세밀한 객체, 특히 빠르게 움직이는 객체를 정확하게 추적하는 데 어려움이 있습니다. 명시적인 모션 모델링 도입이 개선에 도움이 될 수 있습니다.
  * **다중 객체 처리**: SAM 2는 비디오 내 여러 객체를 동시에 추적할 수 있지만, 각 객체를 독립적으로 처리하며 객체 간 통신이 없습니다. 객체 수준의 공유 컨텍스트 정보를 통합하면 효율성이 향상될 수 있습니다.
  * **데이터 엔진의 자동화**: 데이터 엔진이 주석자의 수동 검증에 의존하므로, 이 프로세스를 자동화하여 효율성을 높일 여지가 있습니다.

## 📌 TL;DR

**문제**: 기존 이미지 분할 모델(SAM)은 비디오의 복잡한 움직임과 시간적 연속성을 처리하지 못하여 "비디오 내 무엇이든 분할"하는 범용 시스템이 부재했습니다.

**해결책**: 이 논문은 **SAM 2**를 제안하며, 이는 스트리밍 메모리를 갖춘 통합 트랜스포머 기반 모델로 이미지와 비디오 모두에서 프롬프트 기반 시각 분할을 수행합니다. 또한, 모델과 데이터를 상호작용적으로 개선하는 **데이터 엔진**을 구축하여, 가장 큰 규모의 비디오 분할 데이터셋인 **SA-V**를 수집하고 SAM 2 학습에 활용했습니다.

**주요 결과**: SAM 2는 비디오 분할에서 기존 방법보다 **3배 적은 상호작용으로 더 높은 정확도**를 달성하고, 이미지 분할에서는 기존 SAM보다 **더 정확하고 6배 더 빠릅니다.** 이 연구는 핵심 모델, 데이터셋, 학습 코드를 모두 공개합니다.
