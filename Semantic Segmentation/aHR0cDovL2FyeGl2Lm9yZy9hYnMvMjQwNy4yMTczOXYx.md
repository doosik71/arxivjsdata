# A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation

Mothilal Asokan, Joseph Geo Benjamin, Mohammad Yaqub, and Karthik Nandakumar

## 🧩 Problem to Solve

의료 영상 분석을 위한 기반 모델(Foundation Models)을 적응시키려면 자연 영상으로 사전 훈련된 모델과 의료 영상 데이터 간의 **극심한 분포 변화(distribution shifts)**로 인해 상당한 양의 데이터에 대한 미세 조정(fine-tuning)이 필요합니다. 그러나 이러한 미세 조정을 위한 **작업별 의료 데이터를 중앙에서 수집하는 것은 개인 정보 보호 문제**를 야기합니다. 연합 학습(Federated Learning, FL)이 분산된 개인 데이터에 대한 훈련의 효과적인 수단을 제공하지만, **대규모 기반 모델을 연합 학습하는 데 드는 통신 비용은 심각한 병목 현상**이 되어 솔루션의 확장성에 영향을 미칩니다. 이 연구는 FL에서 **효과적인 학습을 보장하면서 효율적인 통신**이라는 문제를 해결하는 것을 목표로 합니다.

## ✨ Key Contributions

* **FLAP-SAM 프레임워크 제안**: 파라미터 효율적인 미세 조정(PEFT)과 연합 학습(FL)의 강점을 결합하여 3D 의료 영상 분할을 위한 SAM(Segment Anything Model)을 적응시키는 FL 친화적인 접근 방식인 FLAP-SAM을 제안했습니다.
* **세분화된 SAM 구성 요소 분석**: 기존 연구들이 LoRA를 활용하고 전체 디코더를 미세 조정하는 것과 달리, SAM의 각 세분화된 구성 요소가 미세 조정 성능에 미치는 기여를 비판적으로 분석했습니다.
* **효율적인 미세 조정 계층 식별**: 통신 비용 측면에서 매우 효율적이면서도 동등한 정확도를 제공하는 특정 계층(LoRA 파라미터와 마스크 디코더의 최종 출력 계층)을 연합 학습 대상으로 식별했습니다.
* **통신 비용 대폭 감소 및 성능 향상**: Fed-KiTS 데이터셋에서 전체 미세 조정(`FullFT`) 대비 통신 비용을 `~48x` 감소시키면서 Dice 점수를 `~6%` 향상시켰습니다.
* **효율성 우위 입증**: `SAMed`와 유사한 성능을 달성하면서도 미세 조정할 파라미터와 통신량에서 `~2.8x` 감소를 보였습니다.
* **기반 모델의 고유 능력 보존**: 적응 과정에서 SAM 모델의 파라미터(대부분의 디코더 포함)를 원래 상태로 유지하는 것이 소규모 데이터셋에서의 미세 조정으로 인한 기반 모델의 고유 능력 왜곡을 방지하는 데 유익함을 보여주었습니다.

## 📎 Related Works

* **SAM(Segment Anything Model)** [14]: 자연 이미지에 대한 제로샷 일반화 성능이 뛰어난 기반 모델이지만, 의료 영상 도메인에서는 분포 변화로 인해 성능 저하가 발생합니다.
* **의료 이미지용 SAM 적응 연구**:
  * **MSA** [29] 및 **SAM-Med2D** [4]: 2D 의료 영상에서 맞춤형 프롬프팅 기법을 사용하여 SAM을 개선합니다.
  * **MedSAM** [20]: 대규모 작업별 데이터셋을 사용하여 SAM을 의료 영상용으로 미세 조정합니다.
* **파라미터 효율적인 미세 조정(PEFT)**: 전체 파라미터를 미세 조정하는 대신 최소한의 파라미터만 미세 조정하는 방법입니다.
  * **Prompt Tuning** [12].
  * **LoRA(Low-Rank Adapters)** [9]: 대규모 언어 모델 적응에 널리 사용되는 PEFT 기법으로, 사전 훈련된 가중치 행렬에 대한 수정 사항을 저랭크 분해(`$\Delta W = BA$`)를 사용하여 제약합니다.
  * **LoRA를 활용한 분할 작업 연구**: [22, 28, 32]에서 2D 분할에 LoRA를 적용했습니다. 특히 **SAMed** [32]는 2D 분할을 위해 LoRA와 전체 SAM 디코더를 미세 조정합니다.
* **3D 분할을 위한 SAM 적응 연구**:
  * **MA-SAM** [2]: 3D 분할을 위해 FacT(Factor Tuning) 어댑터를 사용합니다. 그러나 텐서 분해의 복잡성으로 인해 FL에 적용하기 어렵습니다.
* **연합 학습(Federated Learning, FL)** [21, 16]: 여러 클라이언트가 데이터를 공유하지 않고 협력적으로 모델을 훈련하는 분산 학습 패러다임입니다. **FedAvg** [21]는 FL에서 널리 사용되는 모델 업데이트 방법입니다.

## 🛠️ Methodology

1. **SAM 아키텍처 개요**: SAM의 아키텍처는 세 가지 주요 구성 요소로 구성됩니다.
    * **Image Encoder (IE)**: 이미지 임베딩을 계산합니다. ViT [5]를 백본으로 사용합니다.
    * **Prompt Encoder (PE)**: 포인트, 박스, 마스크 형태의 다양한 입력 프롬프트를 인코딩하여 프롬프트 임베딩을 생성합니다. 본 연구에서는 **완전 자동 모드(fully automatic mode)**에서 정규 격자(regular grid)의 전경 포인트(foreground points)를 입력 프롬프트로 사용합니다.
    * **Mask Decoder (MD)**: 이미지 임베딩과 프롬프트 임베딩을 결합하여 분할 마스크를 생성합니다. 전치 합성곱 계층(transposed convolutional layers, UP)과 하이퍼 다층 퍼셉트론(HYP)을 사용합니다. [2]를 따라 두 개의 추가 전치 합성곱 계층을 포함하도록 MD를 수정하여 해상도를 `16x` 업샘플링합니다.
2. **다양한 PEFT 접근 방식 비교**:
    * **FullFT (Full Fine-Tuning)**: `$\theta_{IE}, \theta_{PE}, \theta_{MD}$`의 모든 파라미터를 미세 조정합니다.
    * **AttnFT (Attention Fine-Tuning)**: 이미지 인코더의 어텐션 계층(`$\theta_{IE-AT}$`)과 마스크 디코더(`$\theta_{MD}$`)를 업데이트합니다.
    * **DecFT (Decoder Fine-Tuning)**: 이미지 인코더를 고정하고 마스크 디코더(`$\theta_{MD}$`)만 미세 조정합니다.
    * **PDecFT (Partial Decoder Fine-Tuning)**: `UP` 및 `HYP` 출력 계층(`$\theta_{MD-UP}, \theta_{MD-HYP}$`)만 미세 조정합니다.
    * **LoRAFT (LoRA Fine-Tuning)**: `IE` 및 `MD`의 어텐션 계층에 `LoRA` 파라미터(`$\theta_{LoRA}$`)만 업데이트합니다.
    * **LoRADecFT (LoRA + Decoder Fine-Tuning)**: `SAMed` [32]에서 사용된 방식으로, `$\theta_{LoRA}$`와 `$\theta_{MD}$`를 함께 업데이트합니다.
3. **제안하는 FLAP-SAM 접근 방식**:
    * FLAP-SAM은 `PDecFT`와 `LoRAFT`의 **하이브리드** 형태입니다.
    * 업데이트할 파라미터: **LoRA 파라미터(`$\theta_{LoRA}$`)**와 **최종 디코더 출력 계층(`$\theta_{MD-UP}, \theta_{MD-HYP}$`)**만 업데이트합니다.
    * **LoRA 구현**: 모든 어텐션 계층의 쿼리(`$W_q$`) 및 값(`$W_v$`) 프로젝션 행렬에만 LoRA를 적용합니다. 이때 `$W$`는 고정되고 `$A$`와 `$B$` 행렬만 업데이트됩니다.
4. **연합 학습(Federated Learning) 구현**:
    * `FedAvg` [21] 방식을 사용하여 연합 학습을 수행합니다.
    * **LoRA 파라미터(`$\theta_{LoRA}$`)의 연합 집계**:
        * 각 클라이언트 `$k$`는 자신의 `$A_{\ell,k}^q, A_{\ell,k}^v, B_{\ell,k}^q, B_{\ell,k}^v$` 파라미터를 서버로 전송합니다.
        * 서버는 각 계층 `$\ell$`과 클라이언트 `$k$`에 대해 `$\Delta W_{\ell,k}^q = B_{\ell,k}^q \cdot A_{\ell,k}^q$` 및 `$\Delta W_{\ell,k}^v = B_{\ell,k}^v \cdot A_{\ell,k}^v$`를 재구성합니다.
        * 재구성된 `$\Delta W$` 행렬들을 `FedAvg` 방식으로 집계하여 글로벌 `$\Delta W_{\ell}^q$` 및 `$\Delta W_{\ell}^v$`를 얻습니다.
        * 마지막으로, 서버는 집계된 행렬에 특이값 분해(Singular Value Decomposition, SVD)를 적용하여 글로벌 LoRA 파라미터 `$\{A_{\ell}^q, B_{\ell}^q, A_{\ell}^v, B_{\ell}^v\}$`를 다시 분해한 후 클라이언트로 전송합니다.
5. **훈련 상세**:
    * SAM의 "vit_b" 버전 사용.
    * 입력 크기: `$N \times H \times W$` (연속 슬라이스 `N=5`).
    * LoRA 랭크 `$r=32$`. `$A$`는 가우시안 분포로 초기화, `$B$`는 0으로 설정.
    * 손실 함수: 교차 엔트로피 손실(`$L_{CE}$`)과 Dice 손실(`$L_{Dice}$`)을 결합한 하이브리드 손실(`$L_{seg} = \alpha L_{CE} + \beta L_{Dice}$`, `$\alpha=0.2, \beta=0.8$`).
    * 최적화기: Adam, 배치 크기 32.

## 📊 Results

* **데이터셋**: Fed-KiTS2019, Fed-IXI, Prostate MRI 데이터셋을 사용하여 3D 의료 영상 분할 작업에서 FLAP-SAM의 성능을 평가했습니다.
* **Fed-KiTS2019 결과**:
  * **통신 효율성 및 성능**: FLAP-SAM은 전체 미세 조정(`FullFT`) 대비 통신 비용을 `~49x` 감소시키면서 Dice 점수를 `~6%` 향상시켰습니다.
  * **`SAMed (LoRADecFT)`와 비교**: `SAMed`와 동등한 Dice 점수를 달성했지만, 파라미터 수와 통신 효율성 면에서 `~2.8x` 더 효율적입니다.
  * **LoRA 랭크 Ablation**: 낮은 LoRA 랭크는 Dice 점수의 미미한 저하와 함께 훈련 가능한 파라미터를 상당히 줄였습니다. (예: 랭크 32에서 Dice 0.605, 랭크 4에서 Dice 0.600)
  * **다른 저랭크 어댑터와 비교**: DoRA [19] 및 MoLE [30]와 같은 다른 저랭크 어댑터와 비교했을 때 성능 차이는 미미했습니다.
* **`MA-SAM`과 비교 (중앙 집중식 설정)**:
  * `MA-SAM` [2]은 FacT 어댑터를 사용하는데, 연합 집계 후 FacT 텐서를 분해하는 것이 복잡하여 FL 설정에서는 비교할 수 없었습니다.
  * 중앙 집중식 설정에서 `MA-SAM`은 FLAP-SAM과 비슷한 결과를 보였지만, 훈련 가능한 파라미터 수는 `28.7M`으로 FLAP-SAM(`1.712M`)보다 `~16x` 더 많았습니다. 이는 FLAP-SAM이 파라미터 효율적이고 FL 친화적이라는 점을 입증합니다.
* **종합 결과 (모든 데이터셋)**: FLAP-SAM은 모든 데이터셋에서 `FullFT` 및 다른 PEFT 방법들보다 전반적으로 우수하거나 경쟁력 있는 성능을 보이면서도 훨씬 적은 파라미터를 사용했습니다.

## 🧠 Insights & Discussion

* **PEFT의 중요성**: 소규모 데이터셋에 대한 `FullFT`는 쉽게 과적합(overfitting)으로 이어지므로, 제한된 데이터 환경에서 PEFT 방법의 중요성이 강조됩니다.
* **기반 모델의 고유 능력 보존**: SAM 모델의 파라미터 대부분(대부분의 디코더 포함)을 원래 상태로 유지하는 것이 유익합니다. 이는 소규모 데이터셋에 대한 미세 조정이 기반 모델의 고유 능력을 왜곡하는 경향이 있기 때문입니다. FLAP-SAM은 이러한 왜곡을 방지하면서 충분한 유연성을 제공합니다.
* **통신 비용 및 과적합 해결**: 제안된 FLAP-SAM은 파라미터 효율성이 높아 FL의 통신 비용을 크게 줄이고, 데이터가 제한된 시나리오에서 과적합을 방지하는 데 효과적입니다.
* **LoRA의 FL 친화성**: `MA-SAM`의 `FacT` 어댑터와 달리, `LoRA`는 연합 집계 후 파라미터 재구성이 간단하여 FL에 적합합니다.
* **향후 연구 방향**: 현재 연구는 `FedAvg` 컨텍스트에서 다양한 미세 조정 방법을 분석했지만, 향후에는 상당한 분포 변화가 있는 데이터셋에서 저랭크 어댑터에 대한 다양한 연합 최적화 전략의 효과를 연구할 계획입니다.

## 📌 TL;DR

* **문제**: 의료 영상 분할을 위한 대규모 기반 모델(SAM)을 연합 학습(FL)에 적용할 때, 데이터 프라이버시 문제와 데이터 희소성으로 인해 발생하는 높은 통신 비용과 과적합이 주요 도전 과제입니다.
* **제안 방법**: **FLAP-SAM**은 LoRA(Low-Rank Adapters)를 FL과 결합하여, SAM의 LoRA 파라미터와 마스크 디코더의 최종 출력 계층(`$\theta_{MD-UP}, \theta_{MD-HYP}$`)만을 선택적으로 미세 조정합니다.
* **주요 결과**: `FullFT` 대비 통신 비용을 `~49x` 대폭 감소시키고 3D 분할 Dice 점수를 `~6%` 향상시켰습니다. `SAMed`와 유사한 성능을 유지하면서도 파라미터와 통신량 면에서 `~2.8x` 더 효율적임을 입증하여, 제한된 의료 데이터 환경에서 효율적이고 효과적인 솔루션을 제공합니다.
