{
  "url": "http://arxiv.org/abs/2303.17225v1",
  "title": "FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation",
  "authors": "Jie Qin, Jie Wu, Pengxiang Yan, Ming Li, Ren Yuxi, Xuefeng Xiao, Yitong Wang, Rui Wang, Shilei Wen, Xin Pan, Xingang Wang",
  "year": 2023,
  "abstract": "Recently, open-vocabulary learning has emerged to accomplish segmentation for\narbitrary categories of text-based descriptions, which popularizes the\nsegmentation system to more general-purpose application scenarios. However,\nexisting methods devote to designing specialized architectures or parameters\nfor specific segmentation tasks. These customized design paradigms lead to\nfragmentation between various segmentation tasks, thus hindering the uniformity\nof segmentation models. Hence in this paper, we propose FreeSeg, a generic\nframework to accomplish Unified, Universal and Open-Vocabulary Image\nSegmentation. FreeSeg optimizes an all-in-one network via one-shot training and\nemploys the same architecture and parameters to handle diverse segmentation\ntasks seamlessly in the inference procedure. Additionally, adaptive prompt\nlearning facilitates the unified model to capture task-aware and\ncategory-sensitive concepts, improving model robustness in multi-task and\nvaried scenarios. Extensive experimental results demonstrate that FreeSeg\nestablishes new state-of-the-art results in performance and generalization on\nthree segmentation tasks, which outperforms the best task-specific\narchitectures by a large margin: 5.5% mIoU on semantic segmentation, 17.6% mAP\non instance segmentation, 20.1% PQ on panoptic segmentation for the unseen\nclass on COCO."
}