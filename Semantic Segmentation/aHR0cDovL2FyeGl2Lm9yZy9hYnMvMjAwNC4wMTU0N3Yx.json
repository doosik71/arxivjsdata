{
  "url": "http://arxiv.org/abs/2004.01547v1",
  "title": "Context Prior for Scene Segmentation",
  "authors": "Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, Nong Sang",
  "year": 2020,
  "abstract": "Recent works have widely explored the contextual dependencies to achieve more\naccurate segmentation results. However, most approaches rarely distinguish\ndifferent types of contextual dependencies, which may pollute the scene\nunderstanding. In this work, we directly supervise the feature aggregation to\ndistinguish the intra-class and inter-class context clearly. Specifically, we\ndevelop a Context Prior with the supervision of the Affinity Loss. Given an\ninput image and corresponding ground truth, Affinity Loss constructs an ideal\naffinity map to supervise the learning of Context Prior. The learned Context\nPrior extracts the pixels belonging to the same category, while the reversed\nprior focuses on the pixels of different classes. Embedded into a conventional\ndeep CNN, the proposed Context Prior Layer can selectively capture the\nintra-class and inter-class contextual dependencies, leading to robust feature\nrepresentation. To validate the effectiveness, we design an effective Context\nPrior Network (CPNet). Extensive quantitative and qualitative evaluations\ndemonstrate that the proposed model performs favorably against state-of-the-art\nsemantic segmentation approaches. More specifically, our algorithm achieves\n46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on\nCityscapes. Code is available at https://git.io/ContextPrior.",
  "citation": 352
}