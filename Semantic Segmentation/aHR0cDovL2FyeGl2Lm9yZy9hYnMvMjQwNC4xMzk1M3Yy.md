# 360VOTS: Visual Object Tracking and Segmentation in Omnidirectional Videos

Yinzhe Xu, Huajian Huang, Yingshu Chen and Sai-Kit Yeung

## 🧩 Problem to Solve

옴니디렉셔널(360°) 비디오에서의 시각 객체 추적(VOT) 및 객체 분할(VOS)은 광범위한 시야(FoV)와 구형 왜곡으로 인해 어려운 과제입니다. 특히, 에쿼리탱귤러 투영(ERP) 이미지는 객체가 이미지 경계를 넘나들 수 있고, 위도 증가에 따라 왜곡이 심해지며, 카메라 자체의 스티칭 아티팩트나 촬영자 노출로 인한 방해 및 가려짐 현상이 발생할 수 있습니다. 기존의 연구는 대부분 원근 투영(perspective) 장면에 초점을 맞춰왔기 때문에, 360° 비디오의 고유한 특성을 고려한 추적 및 분할 방법론과 데이터셋이 부족합니다.

## ✨ Key Contributions

* 360° 이미지에서 객체 위치를 나타내는 새로운 표현 방식인 **확장 바운딩 시야(extended BFoV, eBFoV)**와 이를 기반으로 한 일반적인 **360 추적 프레임워크**를 제안합니다. 이 프레임워크는 옴니디렉셔널 시각 객체 추적 및 분할 작업 모두에 적용 가능합니다.
* 기존 360VOT 데이터셋을 확장하여, 옴니디렉셔널 비디오 객체 분할(360VOS) 요소를 추가한 포괄적인 벤치마크 데이터셋 **360VOTS**를 구축했습니다. 360VOS는 조밀한 픽셀 단위 마스크가 포함된 290개 시퀀스를 포함하며, 더 넓은 범위의 객체 카테고리를 다룹니다.
* 옴니디렉셔널 추적 및 분할 알고리즘의 엄격한 성능 평가를 위해 특별히 고안된 **새로운 평가 지표 세트**를 제안합니다. 여기에는 이중 성공률, 이중 정밀도, 각도 정밀도, 구형 영역 유사도, 구형 윤곽선 정확도가 포함됩니다.
* 광범위한 실험을 통해 24개 추적 알고리즘과 16개 분할 알고리즘을 360VOT 및 360VOS에서 벤치마킹하고, 제안하는 360 추적 프레임워크와 훈련 데이터셋의 효과를 입증하며 **새로운 SOTA 기준선**을 제시합니다.

## 📎 Related Works

* **시각 객체 추적(VOT) 벤치마크**: OTB100, UAV123, LaSOT, GOT-10k 등 다양한 시나리오와 속성을 가진 기존 벤치마크들이 존재합니다. 그러나 이들은 주로 원근 투영 이미지에 초점을 맞추며, 본 논문의 360VOT는 옴니디렉셔널 비디오에서 새로운 표현 방식을 탐구합니다.
* **비디오 객체 분할(VOS) 벤치마크**: DAVIS, YouTube-VOS, LVOS와 같은 데이터셋이 지난 10년간 제안되었습니다. 이 또한 원근 투영 비디오에 중점을 두며, 본 논문의 360VOS는 옴니디렉셔널 이미지의 픽셀 단위 마스크를 제공합니다.
* **기타 옴니디렉셔널 비전 벤치마크**: 360-Indoor, PANDORA는 정적 360° 이미지의 객체 감지를 위한 데이터셋이며, 2D-3D-S, SynPASS는 파노라마 이미지의 의미론적 분할에 중점을 둡니다. 본 논문은 비디오 기반의 옴니디렉셔널 객체 추적 및 분할을 다룹니다.
* **분할에서의 객체 추적 방식 적용**: DaSiamRPN, SiamX, RTS, SiamMask와 같은 기존 연구들은 추적 속도 향상 및 재현지화 전략을 탐구했습니다. 본 논문은 eBFoV를 활용하여 덜 왜곡된 검색 영역을 얻음으로써 기존 로컬 추적기들을 360° 비디오에 효과적으로 적용합니다.

## 🛠️ Methodology

* **타겟 위치 표현: 확장 바운딩 시야 (Extended BFoV, eBFoV)**
  * 360° 이미지에서 객체 위치를 정확하게 나타내기 위해 기존 바운딩 박스(BBox)의 한계를 극복하고 BFoV([$c_{lon}, c_{lat}, \theta, \phi, \gamma$])를 사용합니다. 이는 객체의 중심 경도($c_{lon}$)/위도($c_{lat}$) 좌표와 최대 수평($\theta$)/수직($\phi$) 시야각, 그리고 회전 각도($\gamma$)를 구면 좌표계에서 정의합니다.
  * 작은 FoV(<90°)의 경우 접평면 $T(\theta, \phi)$를 사용하여 왜곡이 덜한 이미지를 추출합니다.
  * FoV가 90°를 초과할 경우 기존 접평면 정의가 심한 왜곡을 유발하는 단점을 해결하기 위해, 구면 $S(\theta, \phi)$를 활용하는 eBFoV를 제안합니다. 이 정의는 큰 FoV에서도 왜곡이 적은 검색 영역을 추출할 수 있도록 합니다.
  * BFoV로 표현되는 영역 $I$는 다음과 같이 정의됩니다:
        $$I((r)BFoV|\Omega) = P(R_y(c_{lon}) \cdot R_x(c_{lat}) \cdot R_z(\gamma) \cdot \Omega)$$
        여기서 $P$는 3D 카메라 공간을 2D 이미지 공간으로 투영하는 함수이며, $\Omega$는 객체의 FoV에 따라 다음과 같이 정의됩니다:
        $$\Omega = \begin{cases} T(\theta, \phi), & \theta < 90^\circ, \phi < 90^\circ \\ S(\theta, \phi), & \text{otherwise} \end{cases}$$
* **360 추적 프레임워크 (360 Tracking Framework)**
  * 이 프레임워크는 기존 로컬 VOT 또는 VOS 추적기를 옴니디렉셔널 환경에서 활용하기 위해 고안되었습니다.
  * **처리 과정**:
        1. 이전 프레임의 BFoV 또는 마스크를 기반으로 현재 프레임의 360° 이미지에서 해당 **검색 영역 $I$**를 계산합니다.
        2. $I$에 기록된 픽셀 좌표를 이용하여 360° 이미지를 재매핑($\tau$)하여 **왜곡이 덜한 국소 이미지**를 추출합니다.
        3. 추출된 국소 이미지 내에서 기존의 **로컬 시각 추적기**가 (회전된) BBox 또는 분할 마스크를 추론합니다.
        4. 추론된 국소 예측을 **역투영($\tau^{-1}$)**하여 360° 이미지의 전역 바운딩 영역으로 다시 변환합니다. (BBox의 경우 최소 면적 직사각형, BFoV의 경우 최대 바운딩 FoV, 마스크의 경우 역투영 후 팽창).
        5. 연속적인 추적을 위해 다음 프레임의 BFoV 검색 영역은 현재 예측된 BFoV 또는 마스크로 **동적으로 업데이트**됩니다.
  * **주요 하이퍼파라미터**:
    * **SR Ratio**: 다음 프레임에서 검색 영역을 확장하는 스케일링 인자입니다.
    * **SR Min**: 검색 영역의 최소 BFoV를 조절하여 시스템이 과도하게 작은 영역에 고정되는 것을 방지합니다.
    * **Max Loss**: 추적기가 대상을 놓쳤을 때 검색 영역이 변경되지 않고 유지되는 연속 프레임 수를 나타내며, 이후 점진적으로 확장됩니다.
  * 이 프레임워크는 추적기의 네트워크 아키텍처에 독립적이므로, 다양한 기존 로컬 추적기를 옴니디렉셔널 추적 및 분할에 적용할 수 있습니다.
* **벤치마크 데이터셋: 360VOTS 구축**
  * YouTube 및 360° 카메라로 수집된 비디오를 기반으로 총 290개 시퀀스를 포함하며, 170개는 훈련, 120개는 테스트용으로 분할되었습니다.
  * **주석**: 360VOS 데이터셋은 밀집된 픽셀 단위 마스크 주석을 제공하며, 이를 통해 360VOT를 위한 편향되지 않은 (회전된) BBox 및 BFoV 주석을 파생합니다.
  * **속성**: 조명 변화(IV), 배경 혼란(BC), 변형 가능한 객체(DEF) 등 13개의 일반적인 속성과 함께 스티칭 아티팩트(SA), 경계 넘나들기(CB), 구면 상 빠른 움직임(FMS), 넓은 FoV(LFoV), 위도 변화(LV), 고위도(HL), 큰 왜곡(LD) 등 7가지 360° 이미지 특유의 도전적인 속성을 포함하여 총 20가지 속성으로 주석됩니다.

## 📊 Results

* **VOT 성능 평가 (360VOT BBox)**:
  * 제안된 360 추적 프레임워크를 통합한 **AiATrack-360**은 기존 SOTA 추적기들을 월등히 능가하는 성능을 보였습니다. 특히, S$_{dual}$, P$_{dual}$, $\tilde{P}_{dual}$, P$_{angle}$ 지표에서 AiATrack 대비 각각 12.9%, 13.7%, 13.6%, 15.1%의 상당한 성능 향상을 달성했습니다.
  * 다른 추적기인 SiamX에 프레임워크를 적용한 **SiamX-360**도 유사하게 높은 성능 향상을 보이며, 제안된 프레임워크의 일반성과 효과를 입증했습니다.
* **VOT 기타 주석 기반 성능**:
  * (r)BBox 주석에서는 성능이 크게 하락하지만, (r)BFoV 주석에서는 더 합리적이고 일관된 결과가 나타났습니다. 이는 (r)BFoV가 옴니디렉셔널 장면에서 객체 현지화에 더 효과적인 표현 방식임을 시사합니다.
* **VOS 성능 평가 (360VOS)**:
  * 메모리 기반 방법론인 STCN, XMem, XMem++가 기존 추적기 중 가장 높은 성능을 보였습니다.
  * 제안된 프레임워크와 XMem을 결합한 **XMem-360** 및 360VOS 훈련 세트로 재훈련된 모델을 결합한 **XMem-360$^\star$**은 기존의 모든 추적기를 능가하는 뛰어난 분할 성능을 달성했습니다.
* **VOS 추적기의 VOT 성능**:
  * XMem의 VOS 출력을 VOT 형식으로 변환하여 360VOT 벤치마크에서 평가한 결과, **XMem-360**은 AiATrack-360과 비슷한 성능을 보였으며, 특히 rBBox 각도 정밀도 P$_{angle}$에서 3.9% 더 높은 결과를 기록했습니다. 이는 VOS 추적기가 360° 비디오에서 복잡한 방향의 객체를 처리하는 데 탁월한 능력을 가짐을 나타냅니다.
  * 재훈련된 XMem-360$^\star$은 모든 추적 지표에서 일관된 성능 향상을 보여, 훈련 세트의 효과를 입증했습니다.
* **구면 지표의 효율성**:
  * 제안된 구면 지표($J_{sphere}, F_{sphere}$)는 왜곡이 심한 고위도 영역에서 표준 지표($J, F$)보다 객체 분할 품질을 더 정확하게 반영함을 정량적 및 시각적으로 보여주었습니다 (예: 극 근처에서 표준 지표는 최대 27.4% 과대평가).
* **360 추적 프레임워크의 견고성**:
  * SR Ratio, SR Min, Max Loss 하이퍼파라미터에 대한 어블레이션 연구 결과, 프레임워크는 넓은 범위의 파라미터 값에 대해 안정적이고 견고한 성능을 유지함을 확인했습니다. 극단적인 설정에서만 성능 저하가 발생했습니다.

## 🧠 Insights & Discussion

* **의의**: 360VOTS는 옴니디렉셔널 시각 객체 추적 및 분할 연구를 위한 중요한 기반을 마련했습니다. 새로운 표현 방식(eBFoV), 일반 프레임워크, 포괄적인 데이터셋, 그리고 특화된 평가 지표의 도입은 이 분야의 발전을 가속화할 것입니다. 또한, 새로운 훈련 데이터는 기존 모델의 360° 환경 적응력을 크게 향상시킴을 입증했습니다.
* **프레임워크의 장점**: 제안된 360 추적 프레임워크는 기존의 로컬 추적기(VOT/VOS)를 360° 비디오의 도전 과제(예: 경계 넘나들기, 심한 왜곡, 스티칭 아티팩트)에 효과적으로 적용하고 성능을 향상시킬 수 있는 일반적인 솔루션을 제공합니다. 이는 특히 추적기가 대상을 중앙에 유지하여 왜곡에도 불구하고 효과적인 현지화를 가능하게 합니다.
* **향후 연구 방향**:
  * **장기 옴니디렉셔널 추적 알고리즘 개발**: 현재 프레임워크로 강화된 추적기들은 기술적으로 단기 추적기로 분류됩니다. 360VOTS에서 두드러지는 객체 가려짐과 같은 문제를 해결하고, 전체 시야와 장기 관찰의 이점을 활용하여 대상 재현지화가 가능한 장기 추적기 개발이 필요합니다.
  * **새로운 네트워크 아키텍처 탐구**: 옴니디렉셔널 이미지에 특화된 네트워크 아키텍처(예: SphereNet, DeepSphere)를 탐구하여, 360° 비디오에서 더욱 강력한 특징을 추출하고 객체 추적 및 분할의 견고성을 높이는 연구가 필요합니다.
  * **데이터셋 확장 및 엣지 케이스 평가**: 데이터셋 내 극단적인 왜곡 시나리오나 카메라에 가까운 대형 객체에 대한 추가적인 평가와 더 광범위하고 다양한 엣지 케이스 시퀀스 수집이 필요합니다.

## 📌 TL;DR

360VOTS는 360° 비디오에서 객체 추적 및 분할의 고유한 도전 과제(심한 왜곡, 경계 넘나들기, 스티칭 아티팩트 등)를 해결하기 위해 **eBFoV(확장 바운딩 시야)**라는 새로운 객체 표현 방식과 **360 추적 프레임워크**를 제안합니다. 이와 함께, 밀집된 픽셀 단위 마스크와 다양한 바운딩 박스/BFoV 주석을 포함하는 최초의 포괄적인 **360° 비디오 데이터셋(360VOTS)**을 구축하고, 360° 이미지에 특화된 새로운 **평가 지표**를 도입했습니다. 제안된 프레임워크와 훈련 데이터셋으로 기존 SOTA 추적기들을 벤치마킹한 결과, 옴니디렉셔널 환경에서 뛰어난 성능 향상을 달성하여, 이 분야의 발전을 위한 중요한 기반을 마련했습니다.
