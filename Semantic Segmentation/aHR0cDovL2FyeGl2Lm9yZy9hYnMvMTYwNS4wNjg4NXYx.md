# Bridging Category-level and Instance-level Semantic Image Segmentation
Zifeng Wu, Chunhua Shen, Anton van den Hengel

## 🧩 Problem to Solve
이 논문은 이미지 내의 개별 객체 인스턴스를 식별하고 분할하는 **인스턴스 레벨 시맨틱 분할(semantic instance-level segmentation)** 문제를 다룹니다. 기존의 최신 인스턴스 분할 방법들은 대부분 객체의 경계 상자를 먼저 탐지한 다음, 해당 경계 상자 내에서 이진 마스크를 생성하는 "탐지 후 분할(detect-then-segment)" 파이프라인을 따릅니다. 그러나 이러한 접근 방식은 다음과 같은 문제점을 안고 있습니다:
*   강력한 **범주 레벨 시맨틱 분할(semantic category-level segmentation)** 모델의 이점을 충분히 활용하지 못합니다.
*   일부 기존 방법(예: PFN [12])은 복잡한 아키텍처와 훈련의 복잡성을 가지며, 인스턴스 개수 예측에 지나치게 의존하여 실제 적용에 어려움이 있습니다.
*   다른 방법(예: Uhrig et al. [13])은 특정 시나리오(예: 정확한 깊이 추정 필요)에만 효과적인 정교한 템플릿 매칭에 의존합니다.
이 논문은 범주 레벨 시맨틱 분할 모델의 강점을 활용하여 보다 간결하고 일반적이면서도 정확한 인스턴스 분할 방법을 개발하고자 합니다.

## ✨ Key Contributions
*   **새로운 인스턴스 분할 접근 방식 제안**: 의미론적 스코어 맵을 Hough-like 맵으로 변환하여 서로 다른 의미 객체의 인스턴스를 쉽게 탐지할 수 있는 새로운 인스턴스 레벨 분할 방법을 제안합니다.
*   **온라인 부트스트랩 학습 방법 제안**: 훈련 중 온라인 부트스트랩(online bootstrapping) 방법을 제안하며, 이는 시맨틱 범주 분할과 인스턴스 레벨 분할 모두에서 우수한 성능을 달성하는 데 매우 중요함을 입증합니다.
*   **FCRN 구성의 광범위한 평가**: 최상의 시맨틱 범주 분할 정확도를 위해 네트워크 깊이, 특징 맵 해상도, 시야(Field-of-View, FoV) 크기 등 완전 합성곱 잔차 네트워크(Fully Convolutional Residual Network, FCRN)의 다양한 변형을 광범위하게 평가하여 최적의 구성을 찾아냅니다.
*   **시맨틱 분할 최신 성능 달성**: PASCAL VOC 2012 데이터셋에서 79.1%의 평균 IoU(Intersection-over-Union) 점수를 달성하여 당시 시맨틱 분할 분야에서 최고 성능을 기록했습니다 (강화된 PASCAL VOC 훈련 데이터만 사용). 이는 드롭아웃, 다층 비선형 분류기, 멀티뷰 테스팅 기법을 적용하여 이루어졌습니다.
*   **인스턴스 분할 최신 성능 달성**: PASCAL VOC 2012 데이터셋에서 이전 최고 성능과 동등하거나 그 이상의 인스턴스 분할 결과를 달성했으며, 특히 0.7 중첩에서의 평균 영역 평균 정밀도(mean region average precision at an overlap of 0.7, mAP$_{r}^{0.7}$)를 41.5%에서 46.6%로 5.1%p 크게 향상시켰습니다. 시맨틱 분할 정확도가 향상될수록 본 방법의 성능이 크게 개선될 잠재력이 있음을 경험적으로 보여주었습니다.

## 📎 Related Works
*   **매우 깊은 합성곱 네트워크(Very deep convolutional networks)**: AlexNet [14], VGGNets [5], GoogLeNet [15]을 거쳐 ResNets [6, 7]에 이르기까지 깊은 네트워크가 더 나은 특징을 학습함을 보여주었습니다. 특히, ResNet의 잔차 블록(residual blocks)은 그래디언트 소실 문제를 해결하며 깊은 네트워크 훈련을 가능하게 했습니다.
*   **시맨틱 분할을 위한 완전 합성곱 네트워크(Fully convolutional networks for semantic segmentation)**: Long et al. [1]이 FCN 프레임워크를 제안하여 효과적이고 효율적인 시맨틱 분할을 가능하게 했습니다. DeepLab [2]은 down-sampling을 제거하고 커널 팽창(dilation)을 도입하여 특징 맵 해상도를 높였습니다. 또한 Dense CRFs [17]를 후처리로 사용했습니다. CRFasRNN [18]과 UoA-Context [3]는 CRFs를 CNN과 함께 엔드-투-엔드로 학습하는 방법을 제시했습니다.
*   **경계 상자 탐지 기반 인스턴스 분할 접근 방식(Bounding-box detection based approaches to instance segmentation)**: SDS [8], Hypercolumn [9], MNC [10]와 같은 대부분의 최신 인스턴스 분할 방법들은 경계 상자 탐지를 첫 단계로 사용하고, 그 후 이진 마스크를 생성합니다. 본 논문은 이와 달리 시맨틱 분할 위에 직접 구축됩니다.
*   **깊은 합성곱 네트워크 훈련을 위한 온라인 부트스트랩(Online bootstrapping for training deep convolutional networks)**: Loshchilov and Hutter [20]와 Shrivastava et al. [21]은 훈련 중 어려운 샘플을 선택하는 샘플링 방법을 연구했습니다. 본 논문은 픽셀 레벨 시맨틱 및 인스턴스 분할 문제에 온라인 부트스트랩을 적용한 최초의 연구임을 강조합니다.

## 🛠️ Methodology
본 논문은 의미론적 범주 분할(semantic category-level segmentation) 위에 인스턴스 분할을 구축하는 새로운 파이프라인을 제안하며, 핵심 구성 요소로 온라인 부트스트랩(online bootstrapping)과 최적화된 완전 합성곱 잔차 네트워크(FCRN)를 사용합니다.

1.  **인스턴스 분할 파이프라인**:
    *   **1단계**: **범주별 스코어 맵 계산**: 시맨틱 범주 분할 네트워크를 통해 입력 이미지의 픽셀별 범주 스코어 맵을 생성합니다.
    *   **2단계**: **경계 상자 변환 맵 계산**: 각 픽셀에 대해 해당 픽셀이 속한 인스턴스의 경계 상자 중심까지의 수직/수평 오프셋($\Delta x, \Delta y$)과 인스턴스의 높이($h$) 및 너비($w$)를 회귀하는 변환 맵을 예측합니다.
    *   **3단계**: **변환 맵 적용**: 얻은 변환 맵을 해당 시맨틱 스코어 맵에 적용합니다. 이 과정에서 배경 픽셀은 무시하고, 전경 픽셀만 변환합니다. 인스턴스 회수율을 높이기 위해 범주별 Top-n 마스크를 사용합니다.
    *   **4단계**: **지역 최대값 탐색**: 변환된 맵에서 Non-Maximal Suppression (NMS)을 적용하여 지역 최대값을 찾아 인스턴스 가설로 간주합니다. 이 과정은 Hough 변환과 유사합니다.
    *   **5단계**: **마스크 복구**: NMS 과정에서 억제된(suppressed) 픽셀들을 추적하여 각 인스턴스 가설에 대한 최종 마스크를 복구합니다.
    *   **6단계**: **최종 인스턴스 분할**: 영역 기반 NMS [8]를 통해 최종 인스턴스 분할 결과를 생성합니다.
    *   **훈련**: 시맨틱 분할 네트워크는 Logistic regression loss로, 지역화(localization) 네트워크는 smoothed $L_{1}$ loss [11]로 각각 독립적으로 훈련됩니다. 다양한 크기의 인스턴스 기여도를 균형 있게 맞추기 위해 훈련 손실에서 픽셀에 가중치를 부여합니다.

2.  **온라인 부트스트랩(Online Bootstrapping)을 통한 어려운 훈련 픽셀 학습**:
    *   **목적**: 네트워크가 학습 과정에서 오류가 많거나 예측하기 어려운 "하드(hard) 픽셀"에 집중하도록 하여 학습 효율을 높입니다.
    *   **시맨틱 범주 분할**:
        *   현재 모델에 "너무 쉬운(too easy)" 픽셀(예: 예측 확률 $p_{ij}$가 임계값 $t$ 이상인 픽셀)은 학습에서 제외합니다.
        *   $$ \mathcal{L} = -\frac{1}{\sum_{i} \sum_{j} \mathbf{1}\{y_i=j \text{ and } p_{ij} < t\}} \left( \sum_{i} \sum_{j} \mathbf{1}\{y_i=j \text{ and } p_{ij} < t\} \log p_{ij} \right) $$
        *   임계값 $t$는 미니 배치당 일정 수 이상의 픽셀이 유지되도록 모델의 성능에 따라 동적으로 조절됩니다.
    *   **지역화 네트워크**: 픽셀의 경계 상자 회귀 손실이 아닌, 예측된 경계 상자와 실제 경계 상자 간의 IoU(Intersection-over-Union) 점수가 낮은(어려운) 픽셀을 선택합니다.
    *   **이점**: 배경 픽셀이 객체 픽셀보다 압도적으로 많은 경우처럼 편향된(biased) 훈련 데이터를 자동으로 균형 있게 처리하는 데 효과적입니다.

3.  **완전 합성곱 잔차 네트워크(Fully Convolutional Residual Network, FCRN)**:
    *   ResNet [6]을 기반으로 시작하며, 마지막 선형 분류 레이어를 각 공간 위치별 예측을 위한 합성곱 레이어로 대체합니다.
    *   7x7 풀링 레이어 제거: 이 레이스는 FoV를 확장하지만 특징을 평활화하여 픽셀별 분할에 방해가 될 수 있습니다. 대신 분류기가 FoV를 처리하도록 커널 크기를 확장하거나 팽창(dilation)을 사용하여 FoV를 확장합니다.
    *   **특징 맵 해상도 증가**: 낮은 해상도의 특징 맵(1/32) 문제를 해결하기 위해 DeepLab [2]에서 사용된 "hole algorithm" (또는 `atrous algorithm`)을 채택합니다. 이는 down-sampling을 건너뛰고 후속 합성곱 커널의 팽창을 증가시켜 가중치를 변경하지 않고도 고해상도 특징 맵을 생성합니다.
    *   이러한 설계를 통해 시맨틱 범주 분할을 위한 강력한 FCRN을 구축하며, 지역화 네트워크도 유사하게 설계됩니다 (배경 픽셀에는 경계 상자 회귀를 수행하지 않음).

## 📊 Results
*   **시맨틱 범주 레벨 분할**:
    *   **PASCAL VOC 2012**: 증강된 데이터셋만으로 79.1%의 평균 IoU를 달성하여 이전 최고 기록(75.3%)을 1.3%p 초과하며, 20개 객체 범주 중 18개에서 최고 성능을 기록했습니다. 네트워크 깊이를 50에서 101로 늘리고, 특징 맵 해상도를 높이며, FoV를 392까지 확대하는 것이 성능 향상에 기여했습니다. 특히 온라인 부트스트랩은 "cow", "horse"와 같이 훈련 데이터에서 빈번하지 않은 범주에서 성능을 크게 개선했습니다.
    *   **Cityscapes**: 74.6%의 평균 IoU를 달성하여 이전 최고 기록(68.6%)을 능가했습니다. 이 데이터셋에서 온라인 부트스트랩은 평균 IoU를 3.1%p 향상시키는 중요한 기여를 했습니다.
    *   **PASCAL-Context**: 44.5%의 평균 IoU를 달성하여 이전 최고 기록(43.3%)을 초과했습니다.
*   **인스턴스 레벨 분할**:
    *   **PASCAL VOC 2012**: mAP$_{r}^{0.5}$와 mAP$_{r}^{0.7}$ 모두에서 이전 최고 성능과 동등하거나 우수한 결과를 달성했습니다. 특히, mAP$_{r}^{0.7}$에서는 41.5%에서 46.6%로 5.1%p의 현저한 개선을 이루었습니다. 시맨틱 분할 네트워크를 COCO 데이터셋으로 사전 학습하면 성능이 2.0%p 더 향상될 수 있음을 보여주었습니다.
    *   **향상 잠재력**: Ground-truth 마스크를 사용하여 시맨틱 스코어 맵을 생성했을 때 인스턴스 분할 성능이 mAP$_{r}^{0.5}$에서 73.0%, mAP$_{r}^{0.7}$에서 60.6%까지 상승하는 것을 보여주어, 시맨틱 범주 레벨 분할의 정확도 개선이 인스턴스 분할에 큰 잠재적 이득을 가져올 수 있음을 시사했습니다.
*   **온라인 부트스트랩의 중요성**: PASCAL VOC 2012와 Cityscapes 데이터셋 모두에서 온라인 부트스트랩 방법이 시맨틱 분할 및 인스턴스 분할 성능 향상에 크게 기여함을 정량적으로 입증했습니다.

## 🧠 Insights & Discussion
*   **새로운 패러다임의 유효성**: 기존의 "탐지 후 분할" 방식과 달리, 고성능 시맨틱 범주 분할 모델을 기반으로 인스턴스 분할을 수행하는 본 논문의 접근 방식이 매우 효과적임을 입증했습니다. 이는 시맨틱 분할 기술의 발전이 인스턴스 분할 분야에 직접적으로 기여할 수 있는 새로운 가능성을 제시합니다.
*   **시맨틱 분할 정확도의 중요성**: 실험 결과는 시맨틱 범주 분할의 정확도가 인스턴스 레벨 분할 성능에 결정적인 영향을 미친다는 것을 명확히 보여주었습니다. 이는 의미론적 정보가 개별 인스턴스를 구분하고 마스킹하는 데 필수적인 기반이 됨을 시사합니다.
*   **온라인 부트스트랩의 강력함**: 훈련 과정에서 "어려운 픽셀"에 선택적으로 집중하는 온라인 부트스트랩 방법은 데이터셋의 편향(예: 배경 픽셀이 객체 픽셀보다 많은 경우)을 효과적으로 완화하고, 학습이 덜 된 소수 범주나 경계 영역의 성능을 크게 향상시키는 데 기여합니다. 이는 일반적인 픽셀 레이블링 작업의 고질적인 문제를 해결하는 실용적인 방법입니다.
*   **FCRN 구성의 세밀한 최적화**: 네트워크 깊이, 특징 맵 해상도, FoV 등 FCRN의 하이퍼파라미터에 대한 광범위한 평가는 모델 성능에 이들이 미치는 중요한 영향을 강조하며, 특정 데이터셋 특성(예: 이미지 크기)에 따른 최적 FoV 설정의 필요성을 보여줍니다.
*   **한계 및 향후 연구**: PASCAL VOC 2012의 "bicycle", "chair"와 같이 다양성이 높거나 복잡한 어노테이션을 가진 범주에서는 여전히 개선의 여지가 있습니다. 이는 더 많은 훈련 데이터 또는 더 정교한 특징 학습이 필요할 수 있음을 시사합니다. 또한 COCO 데이터셋으로 사전 학습된 모델을 PASCAL VOC에 적용할 때의 도메인 적응 문제는 향후 연구 과제로 남아있습니다.

## 📌 TL;DR
이 논문은 의미론적 범주 분할 결과를 활용하여 인스턴스 경계 상자를 회귀하고 Hough 변환 방식으로 인스턴스를 식별하는 새로운 인스턴스 분할 파이프라인을 제안합니다. 특히, 학습 시 어려운 픽셀에 중점을 두는 온라인 부트스트랩 방법을 도입하여 모델 성능을 크게 향상시켰습니다. 그 결과, PASCAL VOC 2012 데이터셋에서 의미론적 분할의 최고 평균 IoU(79.1%)를 달성하고, 인스턴스 분할에서도 뛰어난 성능을 기록하며, 시맨틱 분할의 발전이 인스턴스 분할로 이어질 수 있음을 효과적으로 입증했습니다.