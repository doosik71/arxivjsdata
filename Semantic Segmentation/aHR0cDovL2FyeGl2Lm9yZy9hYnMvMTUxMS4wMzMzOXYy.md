# Attention to Scale: Scale-aware Semantic Image Segmentation
Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille

## 🧩 Problem to Solve
시맨틱 이미지 분할(Semantic Image Segmentation)에서 최첨단 성능을 달성하는 데 다중 스케일(multi-scale) 특징을 통합하는 것이 중요하지만, 기존 방법들은 몇 가지 한계가 있습니다. 특히, FCN(Fully Convolutional Neural Network) 기반 모델에서 다중 스케일 특징을 통합하는 두 가지 주요 접근 방식인 'skip-net'과 'share-net'이 있습니다. 'skip-net'은 여러 계층의 특징을 결합하지만 훈련 과정이 비효율적이고 시간이 오래 걸릴 수 있습니다. 'share-net'은 여러 스케일의 입력 이미지를 공유 네트워크에 통과시킨 후 특징을 융합하지만, 주로 평균 풀링(average-pooling) 또는 최대 풀링(max-pooling)을 사용하여 각 스케일의 특징이 동등하게 중요하거나 드물게 선택되는 단점이 있습니다. 이러한 비효율적인 특징 융합 방식과 추가적인 감독(supervision)의 부족은 성능 향상을 제한합니다.

## ✨ Key Contributions
*   **스케일 인지 어텐션 메커니즘 제안:** 각 픽셀 위치에서 다중 스케일 특징에 대한 가중치를 학습하는 어텐션(attention) 모델을 제안합니다. 이는 기존의 평균 풀링이나 최대 풀링보다 뛰어난 성능을 보입니다.
*   **가중치 맵 시각화:** 제안된 어텐션 모델은 각 위치와 스케일별 특징의 중요도를 진단적으로 시각화할 수 있게 하여 네트워크의 동작을 해석하는 데 도움을 줍니다.
*   **추가 감독의 중요성 입증:** 각 스케일의 FCN 출력에 추가적인 감독을 적용하는 것이 다중 스케일 특징을 융합할 때 뛰어난 성능을 달성하는 데 필수적임을 보여줍니다.
*   **엔드-투-엔드(End-to-End) 학습:** 어텐션 모델과 다중 스케일 네트워크를 공동으로(jointly) 엔드-투-엔드 방식으로 훈련하여, 각 픽셀에 대한 "정답 스케일"을 수동으로 주석 달 필요 없이 모델이 최적의 가중치를 적응적으로 찾도록 합니다.
*   **광범위한 실험을 통한 효과 입증:** PASCAL-Person-Part, PASCAL VOC 2012, MS-COCO 2014 데이터셋에서 강력한 기준선 대비 일관된 성능 향상을 시연합니다.

## 📎 Related Works
*   **Deep Convolutional Neural Networks (DCNNs) 및 FCNs:** 이미지 분류, 객체 감지, 시맨틱 분할 등 다양한 컴퓨터 비전 작업에서 성공적으로 활용됩니다. 본 연구는 DeepLab 모델($[11]$)을 기반으로 합니다.
*   **Multi-scale Features:**
    *   **Skip-net:** FCN의 중간 계층에서 특징을 결합합니다($[27, 38, 41, 11]$). 예를 들어, FCN-8s는 더 낮은 계층에서 더 미세한 스케일 예측을 학습합니다. DeepLab-MSc($[11]$)는 MLP를 사용하여 다중 스케일 특징을 추출합니다.
    *   **Share-net:** 다중 스케일 입력 이미지를 공유 네트워크에 적용합니다($[19, 34]$). Farabet et al.($[19]$)은 라플라시안 피라미드를 사용하고, Lin et al.($[34]$)은 여러 스케일의 입력 이미지에서 특징을 연결합니다.
*   **Attention Models for Deep Networks:** 이미지 분류($[8, 25, 53]$), 객체 감지($[4, 7, 57]$)에 사용되었으며, Mnih et al.($[40]$)은 이미지 영역을 선택하는 어텐션 모델을 학습했습니다. Bahdanau et al.($[5]$)은 기계 번역에서 입력 단어의 중요도를 부드럽게 가중하는 어텐션 모델을 제안했습니다. Xu et al.($[55]$)과 Yao et al.($[56]$)은 이미지 및 비디오 캡셔닝에서 어텐션 모델을 활용했습니다. 기존 연구는 2D 공간 및/또는 시간 차원에서 어텐션을 적용했지만, 본 연구는 스케일 차원에 어텐션을 적용합니다.

## 🛠️ Methodology
1.  **DeepLab 기반의 Share-net 구조 채택:** 최첨단 DeepLab-LargeFOV 모델($[11]$)을 `share-net` 구조로 변형합니다. 입력 이미지는 여러 스케일 $s \in \{1, ..., S\}$로 크기가 조정되고, 각 스케일은 가중치를 공유하는 DeepLab FCN을 통해 전달되어 스코어 맵 $f_{i,c}^{s}$를 생성합니다. 여기서 $i$는 공간 위치, $c$는 클래스 채널입니다.
2.  **어텐션 모델 설계:**
    *   다중 스케일 스코어 맵 $f_{i,c}^{s}$를 융합하기 위해 픽셀별 가중치 $w_{i}^{s}$를 학습하는 어텐션 모델을 제안합니다. 최종 출력 $g_{i,c}$는 각 스케일 스코어 맵의 가중치 합으로 계산됩니다:
        $$g_{i,c} = \sum_{s=1}^{S} w_{i}^{s} \cdot f_{i,c}^{s}$$
    *   가중치 $w_{i}^{s}$는 소프트맥스 함수를 통해 계산됩니다:
        $$w_{i}^{s} = \frac{\exp(h_{i}^{s})}{\sum_{t=1}^{S} \exp(h_{i}^{t})}$$
        여기서 $h_{i}^{s}$는 어텐션 모델이 스케일 $s$와 위치 $i$에 대해 생성하는 스코어 맵입니다. $w_{i}^{s}$는 모든 채널에 걸쳐 공유됩니다.
    *   어텐션 모델 자체는 별도의 FCN으로 구현되며, VGG-16의 `fc7` 특징을 입력으로 받습니다. 이 FCN은 2개의 컨볼루션 계층으로 구성됩니다 (첫 번째 계층은 $3 \times 3$ 커널의 512개 필터, 두 번째 계층은 $1 \times 1$ 커널의 $S$개 필터).
3.  **추가 감독(Extra Supervision):**
    *   최종 출력에 대한 주 감독 외에도, 각 스케일의 FCN 출력에도 추가적인 크로스 엔트로피 손실을 적용합니다. 이는 융합될 특징들이 더 판별적(discriminative)이도록 학습시키기 위함입니다.
    *   총 손실 함수는 $1 + S$개의 크로스 엔트로피 손실 함수로 구성됩니다 (최종 출력용 1개, 각 스케일 출력용 $S$개).
4.  **공동(Joint) 학습:** 제안된 어텐션 모델과 DeepLab FCN 부분은 엔드-투-엔드로 공동 훈련됩니다. 이를 통해 모델이 픽셀별 "정답 스케일" 주석 없이도 최적의 스케일 가중치를 적응적으로 학습할 수 있습니다.
5.  **훈련 및 평가:** SGD를 사용하여 훈련하며, ImageNet 사전 훈련된 VGG-16 모델($[49]$)로 가중치를 초기화합니다. 성능은 픽셀 IoU(Intersection-Over-Union)를 클래스별로 평균하여 측정합니다.

## 📊 Results
*   **PASCAL-Person-Part 데이터셋:**
    *   단일 스케일 입력(DeepLab-LargeFOV 51.91%) 대비 다중 스케일 입력이 성능을 향상시켰습니다.
    *   제안된 어텐션 모델은 평균 풀링(55.17%) 및 최대 풀링(55.26%)보다 더 나은 성능(55.85% for scales={1, 0.5}, 56.39% for scales={1, 0.75, 0.5})을 보였습니다.
    *   각 FCN 스케일에 추가 감독(Extra Supervision, E-Supv)을 적용했을 때 성능이 크게 향상되었습니다 (예: 어텐션 모델 53.49% $\rightarrow$ 55.85% for scales={1, 0.5}). 이는 다중 스케일 특징 융합에 추가 감독이 필수적임을 보여줍니다.
    *   최고 모델(56.39%)은 DeepLab-MSc-LargeFOV(skip-net 타입, 53.72%)보다 2.67% 더 높은 성능을 달성했습니다.
    *   어텐션 맵 시각화 결과, 스케일 1 어텐션은 작은 객체, 스케일 0.75 어텐션은 중간 객체, 스케일 0.5 어텐션은 큰 객체나 배경에 더 큰 가중치를 부여하는 등 더 해석 가능한 가중치 맵을 학습했습니다.
*   **PASCAL VOC 2012 데이터셋:**
    *   ImageNet 사전 훈련 모델에서 DeepLab-LargeFOV 기준선(62.28%) 대비 6.8% 성능 향상(69.08%)을 달성했습니다.
    *   MS-COCO 사전 훈련 모델에서도 DeepLab-LargeFOV 기준선(67.58%) 대비 3.84% 성능 향상(71.42%)을 보였습니다.
    *   테스트 셋에서는 CRF 후처리 적용 시 75.1% IoU를 달성하여 DeepLab-CRF-LargeFOV 및 DeepLab-MSc-CRF-LargeFOV를 능가했습니다.
*   **MS-COCO 2014 서브셋 데이터셋:**
    *   MS-COCO 데이터셋의 난이도(객체 스케일 변화, 많은 클래스)로 인해 기준선 성능은 낮았지만(31.22%), 제안된 방법은 DeepLab-LargeFOV 기준선 대비 4.6% 향상(35.78%)을 보였습니다.
    *   특히, `person` 클래스에서는 제안된 방법이 더욱 두드러진 성능 향상을 보였습니다 (68.76% $\rightarrow$ 72.72%).

## 🧠 Insights & Discussion
*   **다중 스케일 입력의 중요성:** 단일 스케일 입력보다 다중 스케일 입력을 활용하는 것이 시맨틱 분할 성능 향상에 필수적임을 모든 실험에서 입증했습니다.
*   **어텐션 모델의 우월성:** 제안된 어텐션 모델은 기존의 평균 풀링이나 최대 풀링보다 다중 스케일 특징을 효과적으로 융합하며 더 나은 성능을 달성했습니다. 특히, 어텐션 맵 시각화를 통해 모델이 작은 객체는 고해상도 스케일에, 큰 객체나 배경은 저해상도 스케일에 더 집중하는 등 합리적이고 해석 가능한 방식으로 스케일별 중요도를 학습함을 보여주었습니다. 이는 모델의 "블랙박스" 특성을 일부 해소하는 데 기여합니다.
*   **추가 감독의 핵심 역할:** 각 스케일 네트워크의 최종 출력에 추가적인 감독을 제공하는 것이 융합될 특징들을 더욱 판별적으로 만들어 최종 성능을 크게 향상시키는 데 결정적인 요소임을 확인했습니다.
*   **엔드-투-엔드 학습의 장점:** 복잡한 "정답 스케일" 주석 없이도 어텐션 모델과 FCN을 공동으로 훈련함으로써 효율적인 학습이 가능합니다.
*   **한계 및 향후 연구:** MS-COCO 데이터셋에서 작은 객체 클래스나 불균형 클래스에 대한 성능 향상이 상대적으로 적었는데, 이는 이러한 문제들을 해결하기 위한 추가 연구가 필요함을 시사합니다. 또한, 본 모델은 현재 최고의 성능을 보이는 일부 모델($[34, 37]$)보다 낮지만, 이는 해당 모델들이 CRF와 FCN의 공동 훈련과 같은 추가적인 요소를 통합하기 때문입니다. 제안된 어텐션 모델은 이러한 다른 기법들과 상호보완적으로 활용될 수 있을 것으로 예상됩니다.

## 📌 TL;DR
시맨틱 이미지 분할에서 다중 스케일 특징을 효과적으로 융합하는 문제 해결을 위해, 본 논문은 스케일별 특징의 중요도를 픽셀 단위로 학습하는 **어텐션 모델**을 제안합니다. 이 모델은 DeepLab을 기반으로 하며, 여러 스케일의 입력 이미지로부터 생성된 특징들을 가중치 합으로 융합합니다. 또한, 각 스케일 네트워크의 최종 출력에 **추가 감독**을 적용하여 특징의 판별력을 높였습니다. 실험 결과, 제안된 어텐션 모델은 기존 평균/최대 풀링 방식보다 뛰어난 성능을 보였고, 추가 감독이 성능 향상에 필수적임을 입증했습니다. 어텐션 맵 시각화를 통해 모델이 객체 스케일에 따라 적절한 특징에 집중함을 보여주며, 복잡한 주석 없이 엔드-투-엔드 학습이 가능함을 확인했습니다.