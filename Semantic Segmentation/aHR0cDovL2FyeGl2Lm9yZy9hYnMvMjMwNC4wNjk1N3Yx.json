{
  "url": "http://arxiv.org/abs/2304.06957v1",
  "title": "MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic\n  Segmentation",
  "authors": "Jie Guo, Qimeng Wang, Yan Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Baochang Zhang",
  "year": 2023,
  "abstract": "CLIP (Contrastive Language-Image Pretraining) is well-developed for\nopen-vocabulary zero-shot image-level recognition, while its applications in\npixel-level tasks are less investigated, where most efforts directly adopt CLIP\nfeatures without deliberative adaptations. In this work, we first demonstrate\nthe necessity of image-pixel CLIP feature adaption, then provide Multi-View\nPrompt learning (MVP-SEG) as an effective solution to achieve image-pixel\nadaptation and to solve open-vocabulary semantic segmentation. Concretely,\nMVP-SEG deliberately learns multiple prompts trained by our Orthogonal\nConstraint Loss (OCLoss), by which each prompt is supervised to exploit CLIP\nfeature on different object parts, and collaborative segmentation masks\ngenerated by all prompts promote better segmentation. Moreover, MVP-SEG\nintroduces Global Prompt Refining (GPR) to further eliminate class-wise\nsegmentation noise. Experiments show that the multi-view prompts learned from\nseen categories have strong generalization to unseen categories, and MVP-SEG+\nwhich combines the knowledge transfer stage significantly outperforms previous\nmethods on several benchmarks. Moreover, qualitative results justify that\nMVP-SEG does lead to better focus on different local parts.",
  "citation": 14
}