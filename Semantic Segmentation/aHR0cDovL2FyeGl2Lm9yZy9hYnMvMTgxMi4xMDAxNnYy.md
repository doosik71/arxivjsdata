# A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation

Kai Wang, Yimin Lin, Luowei Wang, Liming Han, Minjie Hua, Xiang Wang, Shiguo Lian, Bill Huang

## 🧩 Problem to Solve

로봇 공학에서 중요한 시각 기반 작업인 동시 위치 추정 및 지도 작성(SLAM)과 시맨틱 분할은 기존에 독립적인 문제로 간주되어 왔습니다. 이 두 기술은 각각 다음과 같은 어려움을 겪고 있습니다:

- **SLAM (특히 vSLAM):** 동적인 환경(움직이는 객체, 장기적인 환경 변화)에서 지도 일관성이 저해되어 위치 추정의 정확성과 강건성이 떨어집니다. 기존 방법들은 주로 정적인 환경을 가정하거나 동적 객체를 단순히 제거하여 정확도가 제한적이었습니다.
- **시맨틱 분할:** 부정확한 수동 라벨링과 훈련 데이터 부족으로 인해 딥러닝 기반 분할 결과가 부정확해질 수 있습니다. 특히 객체의 정확한 경계 분할에서 어려움이 있습니다.
- **두 기술 간의 상호작용 부족:** SLAM과 시맨틱 분할은 각각의 중간 결과를 서로에게 활용하여 성능을 상호 개선할 수 있는 잠재력이 있음에도 불구하고, 과거에는 거의 독립적으로 수행되었습니다.

## ✨ Key Contributions

이 논문의 주요 기여는 다음과 같습니다:

- **vSLAM과 시맨틱 분할의 상호 개선을 위한 통합 프레임워크 제안:** 두 작업을 엮어 동시에 성능을 향상시키는 새로운 방식을 제시합니다.
- **움직이는(moving) 객체와 잠재적으로 움직일 수 있는(potentially moveable) 객체를 식별하여 vSLAM의 지도 작성 및 위치 추정 정확도 향상:** 동적 객체에 대한 새로운 처리 방식을 통해 SLAM의 안정성과 정밀도를 높입니다.
- **3D 포즈 정보를 활용한 이미지 분할 결과의 효과적인 개선 방안 제시:** SLAM에서 얻은 정밀한 3D 포즈 정보를 사용하여 분할 결과의 오류를 수정하고 경계 정확도를 높입니다.

## 📎 Related Works

- **동적 환경을 위한 vSLAM:** 대부분의 기존 vSLAM(예: PTAM, ORB-SLAM2)은 정적인 환경을 가정합니다. 동적 환경을 다루기 위한 시도로는 광학 흐름 기반 동적 객체 분할([7]), 딥러닝 기반 마스크를 이용한 특징점 제거(Mask-SLAM [9], DeepLab v2 [10]), 멀티뷰 기하학과 딥러닝 결합([1]), 깊이 맵, 희소 장면 흐름, 시맨틱 정보를 결합한 분류([11]) 등이 있습니다. 본 논문은 [12]와 유사하게 동적 객체 식별을 통해 vSLAM을 개선하고 분할 네트워크 훈련을 위한 정제된 데이터를 제공하지만, 객체 검출의 정확성 및 오프라인 방식에서 차이가 있습니다.
- **이미지 및 비디오 분할:** 딥러닝 기반 이미지 분할의 선구적인 연구로는 FCN([13]), 인코더-디코더 구조의 SegNet([14]), Atrous Spatial Pyramid Pooling (ASPP)를 활용한 DeepLabv3([15]) 및 DeepLabv3+([17]), Pyramid Scene Parsing Network (PSPNet)([18]) 등이 있습니다. 인스턴스 레벨 분할로는 R-CNN([19]), MNC([21]), FCIS([22]), Mask R-CNN([23]) 등이 있습니다. 비디오 시퀀스 기반 분할에서는 3D-Conv([26]) 및 Conv-LSTM([27]) 모듈을 사용하여 시공간 정보를 활용하는 연구([25])가 있습니다. 본 논문은 인접 프레임의 3D 공간 정보를 활용하여 경계 정보를 개선합니다.

## 🛠️ Methodology

제안하는 프레임워크는 RGB 이미지 시퀀스와 깊이 맵 시퀀스를 입력으로 받으며, vSLAM 모듈과 분할 모듈의 두 가지 주요 모듈로 구성됩니다. 각 입력 프레임에 대해 vSLAM 모듈은 카메라의 포즈 정보를 출력하고 환경 지도를 업데이트하며, 분할 모듈은 각 픽셀의 시맨틱 정보를 포함하는 이미지 분할 결과를 생성합니다.

1. **초기 분할 (Initial Segmentation):**

   - 입력 RGB 프레임에 대해 FCIS([22]) 알고리즘을 사용하여 초기 분할을 수행합니다. MS COCO 데이터셋으로 훈련되어 80가지 객체 클래스를 인식합니다.
   - 분할 결과에서 미리 정의된 '움직일 수 있는 객체' 목록(사람, 자동차, 컵, 의자 등)에 따라 잠재적으로 움직일 수 있는 객체를 식별합니다. 이 정보는 마스크 이미지 형태로 vSLAM 모듈에 전달됩니다.

2. **분할 결과 기반 vSLAM:**

   - ORB-SLAM2 RGB-D 버전([5])을 기본 알고리즘으로 사용합니다.
   - **초기 추적:** 각 새 프레임에 대해 ORB 특징점을 추출하고 깊이 맵과 정렬하여 3D 좌표를 얻은 후, 재투영 오차를 최소화하여 대략적인 카메라 포즈($R_c$, $T_c$)를 계산합니다.
   - **특징점 분류:** 추출된 특징점은 초기 분할 결과에 따라 배경($A$) 또는 분할된 인스턴스($B_i$)로 분류됩니다.
   - **객체 운동 상태 판단:** 대략적인 포즈($R_c$, $T_c$)를 사용하여 트래킹 맵의 점들을 현재 프레임에 투영하고, 각 점의 유클리드 거리를 기준으로 정적/움직임을 판단합니다. $B_i$ 세트 내 움직이는 점의 비율이 임계값보다 낮으면 정적 객체로, 그렇지 않으면 움직이는 객체로 간주합니다.
   - **정밀한 포즈 추정:** 배경($A$)과 정적 객체로 분류된 $B_s$ 세트의 특징점만을 사용하여 2D-3D 매칭을 통해 재투영 오차를 최소화하여 정밀한 포즈($R_f$, $T_f$)를 얻습니다. 이 정밀한 포즈는 분할 모듈로 전달되어 초기 분할 결과를 개선하는 데 사용됩니다.
   - **지도 관리:**
     - **트래킹 맵 (Tracking map):** 현재 스캔에서 카메라 궤적 계산에 사용됩니다. 배경($A$) 및 정적 객체($B_s$)의 점만 포함하여 추적 안정성과 궤적 정확도를 높입니다.
     - **장기 맵 (Long-term map):** 동일한 영역 재방문 시 재매핑을 피하기 위해 사용됩니다. 시간이 지나도 위치가 고정될 가능성이 있는 점(배경 $A$의 점)만 포함하여 안정적인 환경 정보를 제공합니다.

3. **분할 결과 개선 (Refinement of Segmentation Result):**
   - 현재 프레임의 대략적인 포즈($R_c$, $T_c$)와 이전 프레임의 정밀한 포즈($R_f$, $T_f$) 및 분할 결과(정확하다고 가정)를 활용합니다.
   - 이전 프레임의 분할된 영역의 2D 점 $(p_u, p_v)$을 다음 수식을 통해 현재 프레임의 $(p'_u, p'_v)$로 투영합니다:
     $$P_z = D(p_u, p_v)/DF \tag{1}$$
     $$P_x = (p_u - c_x) * P_z / f_x \tag{2}$$
     $$P_y = (p_v - c_y) * P_z / f_y \tag{3}$$
     $$\begin{pmatrix} p'_u \\ p'_v \end{pmatrix} = \begin{pmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{pmatrix} [R|T] \begin{pmatrix} P_x \\ P_y \\ P_z \\ 1 \end{pmatrix} / s \tag{4}$$
     여기서 $f_x, f_y$는 초점 거리, $(c_x, c_y)$는 주점, $D(p_u, p_v)$는 깊이 값, $DF$는 깊이 계수, $R=R^{-1}_c R_f$ 및 $T=T_f - T_x$는 상대 회전 및 변환, $s$는 이미지 스케일 팩터입니다.
   - 각 투영된 영역 $Re_p$에 대해 현재 프레임의 초기 분할 결과에서 가장 유사한 매칭 영역 $Re_c$를 찾습니다. 유사도 $S_{cp}$는 두 영역의 무게 중심 간 거리와 모양 차이를 결합하여 측정합니다:
     $$S_{cp} = w1 \cdot Dist(Re_c, Re_p) + w2 \cdot \frac{Area((Re_c - Re_p) \cup (Re_p - Re_c))}{Area(Re_c) + Area(Re_p)} \tag{5}$$
     여기서 $Dist(\cdot)$는 유클리드 거리, $Area(\cdot)$는 픽셀 수입니다.
   - 매칭된 영역이 있으면, 교차 영역과 각 영역의 비율을 비교하여 더 신뢰할 수 있는 영역을 최종 분할 결과로 보존합니다.
   - 매칭된 영역이 없지만 현재 프레임의 분할된 인스턴스 수가 이전 프레임보다 적은 경우, $Re_p$를 분할 결과에 추가하여 누락된 부분을 보정합니다.

## 📊 Results

- **TUM 데이터셋 (vSLAM):**

  - 'walking' 및 'sitting' 시퀀스(총 6개)에서 ORB-SLAM2, DynaSLAM과 Absolute Trajectory Error (ATE)를 비교했습니다.
  - **'walking' 데이터셋에서 ATE가 크게 개선됨** (예: `Walkinghalfsphere`에서 ORB-SLAM2의 0.351m, DynaSLAM의 0.025m 대비 본 논문은 0.019m). 이는 동적 특징점을 정확히 제거한 덕분입니다. 본 논문의 알고리즘이 DynaSLAM보다 우수한 이유는 3D 포즈 정보를 사용하여 분할 결과와 경계를 더 정확하게 정제했기 때문입니다.
  - 'sitting' 데이터셋에서는 동적 객체가 적어 개선 효과가 덜 두드러졌습니다.
  - 궤적 시각화 결과(그림 3)에서 제안하는 vSLAM 모듈이 ORB-SLAM2보다 그라운드 트루스에 훨씬 더 유사함을 보여줍니다.
  - 평균 처리 시간: 초기 추적 6ms, 정밀 추적 및 매핑 22ms.

- **ScanNet 데이터셋 (시맨틱 분할):**

  - mAP (mean Average Precision)와 mIoU (mean Intersection over Union)를 기준으로 FCIS([22])와 본 논문의 분할 모듈을 비교했습니다.
  - **FCIS 대비 mAP (0.6314 -> 0.6504), mIoU (0.5620 -> 0.5751) 모두 개선**되었습니다. 이는 3D 포즈 정보를 활용한 분할 영역 개선이 효과적임을 증명합니다.
  - 분할 개선 예시(그림 4)는 누락된 부분(테이블)이 추가되고, 과대 분할된 부분(의자)이 정확한 범위로 축소되는 것을 보여줍니다.
  - 평균 처리 시간: 초기 분할 113ms, 개선 50ms (병렬 처리로 가속 가능).

- **AirSim 생성 데이터셋 (동시 평가 및 재위치 추정):**
  - 가상 도시 환경에서 40개 경로를 두 번 주행하며 데이터를 생성하여 vSLAM의 재위치 추정 성능과 두 작업의 동시 개선 효과를 평가했습니다.
  - **vSLAM 재위치 추정:** ORB-SLAM2 대비 ATE가 크게 개선됨 (0.82m -> 0.39m). 이는 장기 맵에서 동적 객체를 제거함으로써 재방문 시 추적 정확도가 향상되었기 때문입니다.
  - **시맨틱 분할:** FCIS 대비 mAP (0.6702 -> 0.6893), mIoU (0.6491 -> 0.6611) 모두 개선되었습니다.
  - 세 가지 데이터셋에 대한 테스트 결과는 제안하는 프레임워크가 실내외 환경 모두에서 vSLAM과 분할의 정확도를 효과적으로 향상시키며, 특히 동적 객체가 있는 장면에서 성능 향상이 두드러짐을 보여줍니다.

## 🧠 Insights & Discussion

- **상호 개선의 효과:** 이 논문은 SLAM과 시맨틱 분할이 서로의 중간 결과를 활용하여 어떻게 상호 보완적으로 성능을 향상시킬 수 있는지 성공적으로 보여줍니다. SLAM의 정확한 포즈 추정은 분할 결과의 경계 정밀도를 높이고, 정제된 분할 결과는 SLAM이 동적 객체를 효과적으로 처리하여 추적 및 매핑의 강건성과 정확도를 높입니다.
- **동적 환경 처리의 강점:** 특히 움직이는 객체나 재배치된 객체가 있는 동적 환경에서 제안하는 프레임워크의 성능 향상이 두드러집니다. 이는 정적 특징점만을 사용하여 카메라 포즈를 추정하고, 동적/잠재적으로 움직일 수 있는 객체를 체계적으로 관리하는 접근 방식 덕분입니다.
- **장기 맵의 활용:** 장기 맵 개념은 동일한 영역을 재방문할 때 중복 매핑 계산을 피하고 안정적인 환경 정보를 제공함으로써 재위치 추정의 정확도를 높이는 데 기여합니다.
- **한계점:** 두 인접 프레임 사이에 급격한 변화가 없는 상황을 가정합니다. 카메라 프레임 속도가 충분히 높지 않아 빠르게 움직이는 객체가 잘 포착되지 않는 극단적인 상황에서는 영역 대응 판단에 실패하여 잘못된 결과가 발생할 수 있습니다. 프레임 보간을 통해 완화될 수 있지만, 실제 응용에서는 드문 경우라고 언급됩니다.

## 📌 TL;DR

이 논문은 SLAM과 시맨틱 분할을 통합하여 상호 성능을 개선하는 프레임워크를 제안합니다. 주요 문제는 동적 환경에서 SLAM의 불안정성과 분할의 부정확성, 그리고 두 작업 간의 정보 단절이었습니다. 제안하는 방법은 초기 분할 결과로 동적 객체를 식별하고, 이를 통해 SLAM의 정밀한 포즈를 얻은 다음, 이 포즈 정보를 활용하여 분할 결과를 정제합니다. TUM, ScanNet, AirSim 데이터셋 실험을 통해 이 상호 보완적인 접근 방식이 특히 동적 장면에서 vSLAM의 정확도와 강건성, 그리고 시맨틱 분할의 정밀도를 효과적으로 향상시킴을 입증했습니다.
