# Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation

Xiaoxiao Li, Chen Change Loy

## 🧩 Problem to Solve

이 논문은 비디오 객체 분할, 특히 다수의 객체 인스턴스가 존재하며 다음의 도전 과제들을 가질 때 발생하는 문제들을 해결하고자 합니다:

- **다중 객체 간 가려짐:** 비디오 내 객체들이 서로를 가리거나 부분적/전체적으로 가려져 추적 및 분할이 실패할 수 있습니다.
- **큰 스케일 및 자세 변화:** 객체들이 시간 경과에 따라 크기와 자세가 크게 변하여 기존 템플릿 매칭 기반 방법들이 한계를 드러냅니다.
- **긴 가려짐 후 재식별 불가:** 기존 시간적 전파(temporal propagation) 방식은 객체가 사라진 후 다시 나타날 때 재식별 메커니즘이 없어 오류에 취약합니다.
- **방해 요소(distractors)의 영향:** 복잡한 배경이나 다른 객체의 세그먼트가 마스크 전파 과정에서 방해 요소로 작용하여 추적 정확도를 저하시킵니다.

## ✨ Key Contributions

이 연구의 주요 기여는 다음과 같습니다:

- **재식별(Re-ID) 및 어텐션 기반 반복 마스크 전파(Re-MP)를 결합한 통합 딥러닝 네트워크 (DyeNet) 제안:** 템플릿 매칭과 시간적 전파의 장점을 하나의 엔드투엔드(end-to-end) 학습 가능한 프레임워크로 통합하여 다중 객체 비디오 객체 분할 문제를 해결합니다. 별도의 온라인 학습 없이도 경쟁력 있는 성능을 내며, 온라인 학습 시 더 좋은 결과를 얻습니다.
- **효과적인 템플릿 확장(template expansion) 방법을 포함하는 재식별 모듈 제시:** 자세 및 스케일 변화가 큰 재등장 객체를 더 잘 재식별하기 위해 예측된 고신뢰도 마스크를 템플릿 세트에 동적으로 추가하는 방법을 제시합니다.
- **새로운 어텐션 기반 반복 마스크 전파 모듈 제시:** 배경 클러터나 다른 객체와 같은 방해 요소에 강건한 어텐션 메커니즘이 적용된 RNN 기반 마스크 전파 방식을 제안합니다.
- **DAVIS 2017 벤치마크 최고 성능 달성:** DAVIS 2017(test-dev 세트)에서 68.2의 G-mean(Region Jaccard 및 Boundary F-measure)으로 이전 최고 기록인 66.1을 능가하는 새로운 SOTA(State-of-the-Art) 성능을 달성했습니다. DAVIS 2016, SegTrack$_{\text{v2}}$, YouTubeObjects 데이터셋에서도 SOTA를 기록했습니다.

## 📎 Related Works

이 연구는 비디오 객체 분할의 기존 주요 접근 방식들(템플릿 매칭, 시간적 전파)의 한계를 언급하며, 이를 결합하려는 이전 시도들과의 차별점을 강조합니다.

- **템플릿 매칭 기반 방법:** 초기 프레임의 마스크를 템플릿으로 사용하는 방식(예: Caelles et al. [3], Yoon et al. [38])은 큰 스케일 및 자세 변화에 취약합니다.
- **시간적 전파 기반 방법:** 시간적 연속성을 활용하는 방식(예: Jampani et al. [15], Perazzi et al. [26])은 가려짐 후 재식별 메커니즘이 없어 오류에 취약하고, 방해 요소가 있을 때 추적에 실패할 수 있습니다.
- **템플릿 매칭과 시간적 연속성 결합 시도:**
  - Khoreva et al. [16]는 훈련 데이터 합성 및 마스크 전파를 통해 성능을 향상시켰습니다.
  - IRIF [17]는 사람 객체에 대해 재식별을 적용했습니다.
  - Li et al. [21] (VS-ReID)는 일반 객체 재식별 모델과 two-stream 마스크 전파를 사용했으며, 2017 DAVIS 챌린지에서 최고 성능을 달성했습니다.
- **DyeNet의 차별점 (특히 VS-ReID [21] 대비):**
  - DyeNet은 재식별과 전파를 **단일 엔드투엔드 네트워크**에서 최적화합니다.
  - DyeNet은 **템플릿 확장**을 통해 동적으로 템플릿 세트를 풍부하게 하여 자세 변화에 더 강건합니다. VS-ReID는 초기 프레임 마스크만 사용합니다.
  - DyeNet의 마스크 전파는 **어텐션 메커니즘**을 포함하여 방해 요소에 대한 강건성이 우수합니다. VS-ReID는 이 기능이 없습니다.
  - DyeNet은 VS-ReID보다 훨씬 빠릅니다 (DyeNet 약 0.43 FPS vs VS-ReID 약 3 FPS).

## 🛠️ Methodology

DyeNet은 재식별(Re-ID) 모듈과 반복 마스크 전파(Re-MP) 모듈이라는 두 가지 핵심 구성 요소로 이루어진 딥 재귀 네트워크입니다.

1. **특징 추출 (Feature Extraction):**
   - ResNet-101을 $N_{feat}$ 백본으로 사용하여 각 프레임 $I_i$에서 특징 $f_i$를 추출합니다. 특징 맵의 해상도를 높이기 위해 팽창 컨볼루션(dilated convolutions)을 사용합니다.
2. **반복 추론 및 템플릿 확장 (Iterative Inference with Template Expansion):**
   - 주어진 첫 프레임 마스크를 초기 템플릿으로 사용합니다.
   - **Re-ID 모듈**이 객체 제안(object proposals)으로부터 마스크를 생성하고 이를 템플릿과 비교하여 높은 유사도를 가진 마스크를 Re-MP의 시작점으로 선택합니다.
   - **Re-MP 모듈**이 선택된 각 마스크(시작점)를 양방향(forward 및 backward)으로 전파하여 세그멘테이션 마스크 시퀀스(트랙렛)를 생성합니다.
   - 각 반복 후에 DyeNet은 높은 신뢰도로 예측된 마스크를 선택하여 템플릿 세트를 확장합니다. 이 과정은 더 이상 높은 신뢰도의 마스크를 찾을 수 없을 때까지 반복됩니다.
   - 생성된 트랙렛들은 탐욕적(greedy) 알고리즘을 통해 일관된 마스크 튜브로 연결됩니다.
3. **재식별 모듈 (Re-ID Module):**
   - RPN(Region Proposal Network) [30]을 사용하여 각 프레임에서 후보 바운딩 박스 $b_{ij}$를 제안합니다.
   - RoIAlign [11]을 사용하여 각 $b_{ij}$의 특징을 $f_i$에서 추출하고 고정 크기($m \times m$)로 조정합니다.
   - 이 특징은 **마스크 네트워크** (이진 마스크 예측)와 **재식별 네트워크** (L2 정규화된 256차원 마스크 특징 추출)에 입력됩니다.
   - 마스크 특징과 템플릿 특징 간의 코사인 유사도를 계산하여 임계값 $\rho_{reid}$보다 유사도가 높은 마스크를 전파를 위한 시작점으로 선택합니다.
4. **반복 마스크 전파 모듈 (Re-MP Module):**
   - RNN으로 모델링되며, 검색된 마스크(시작점)를 양방향으로 확장하여 트랙렛을 형성합니다.
   - FlowNet 2.0 [13]으로 광학 흐름 $F_{(j-1) \to j}$을 추정하고, 이전 프레임의 마스크 $y_{j-1}$와 은닉 상태 $h_{j-1}$를 현재 프레임으로 워핑하여 공간적 일관성을 유지합니다.
   - **영역 어텐션 (Region Attention):** 워핑된 은닉 상태 $h_{(j-1) \to j}$로부터 어텐션 분포 $a_j \in \mathbb{R}^{m \times m \times 1}$를 생성하여 현재 은닉 상태 $h_j$에 곱함으로써 관심 영역에 집중하고 방해 요소의 영향을 줄입니다.
   - 강화된 $h_j$로부터 최종 마스크 $y_j$를 예측합니다. 객체 마스크의 크기가 너무 작아지면 전파를 중단합니다.
5. **학습 (Training):**
   - 전체 손실 함수는 재식별 손실 $L_{reid}$ (OIM 손실 [35] 기반)과 픽셀 단위 세그멘테이션 손실 $L_{mask}$ (마스크 네트워크), $L_{remp}$ (Re-MP 모듈)의 선형 결합 $L = L_{reid} + \lambda(L_{mask} + L_{remp})$로 구성됩니다.
   - 모델 가중치는 의미론적 분할 네트워크 [39]로 초기화되며, 재식별 서브 네트워크는 ImageNet [6]에서 사전 학습됩니다.

## 📊 Results

DyeNet은 다양한 벤치마크 데이터셋에서 최첨단 성능을 달성하며, 특히 DAVIS 2017에서 우수성을 입증했습니다.

- **DAVIS 2017 (test-dev set) 성능:**
  - **G-mean 68.2:** 온라인 파인튜닝(per-dataset) 적용 시 VS-ReID [21]의 66.1을 뛰어넘는 SOTA를 달성했습니다 (J-mean 65.8, F-mean 70.5).
  - **G-mean 62.5:** 온라인 훈련 없이 오프라인 훈련만으로도 경쟁력 있는 성능을 보여주었습니다.
- **각 모듈의 효과 (DAVIS 2017 val set, 오프라인 훈련):**
  - **Re-MP 모듈:** 기존 MSK [26] 대비 G-mean을 2.2 향상시켰고, 특히 어텐션 메커니즘을 추가하여 1.6의 추가적인 G-mean 향상을 달성했습니다.
  - **Re-ID 모듈과 템플릿 확장:** 반복을 통해 recall과 G-mean이 꾸준히 증가하여, 초기 프레임 마스크에 대한 의존도를 낮추고 전반적인 성능을 향상시켰습니다.
- **특정 속성별 성능 분석 (DAVIS 2017 test-dev):**
  - Re-MP 모듈은 작은 객체 추적 및 부분 가려짐 상황에서 방해 요소 회피에 강합니다.
  - Re-ID 모듈은 심한 가려짐으로 인해 사라진 인스턴스를 효과적으로 재검색하여 해당 경우의 성능을 크게 향상시킵니다. 템플릿 확장은 큰 자세 변화에도 Re-ID가 잘 작동하도록 합니다.
- **다른 데이터셋 (DAVIS 2016, SegTrack$_{\text{v2}}$, YouTubeObjects) 성능 (mIoU):**
  - 오프라인 및 온라인 파인튜닝 DyeNet 모두 모든 데이터셋에서 SOTA 성능을 달성했습니다. 특히 DAVIS 2017에서 훈련된 모델이 파인튜닝 없이도 다른 데이터셋에서 뛰어난 일반화 및 전이 학습 능력을 보여주었습니다.
- **속도 분석:**
  - 오프라인 DyeNet은 단일 Titan Xp GPU에서 2.4 FPS(프레임당 약 0.42초)로 빠른 추론 속도를 보여줍니다.
  - 온라인 파인튜닝 적용 시 0.43 FPS(프레임당 약 2.3초)로, 기존 SOTA 방법들에 비해 훨씬 빠르거나 경쟁력 있는 속도를 제공합니다.

## 🧠 Insights & Discussion

- **통합적 접근의 효율성:** 비디오 객체 분할의 핵심 과제를 개별적으로 해결하는 대신, 재식별과 시간적 전파를 단일 엔드투엔드 프레임워크로 통합함으로써 각 구성 요소의 시너지를 극대화했습니다.
- **템플릿 확장과 동적 업데이트의 중요성:** 고정된 템플릿에 의존하지 않고 고신뢰도 예측 마스크를 동적으로 템플릿 세트에 추가하는 전략은 객체의 복잡한 자세 및 스케일 변화에 대응하는 데 매우 효과적입니다.
- **어텐션 메커니즘의 선구적 적용:** 마스크 전파 과정에 어텐션 메커니즘을 적용하여 목표 객체에 집중하고 방해 요소의 영향을 줄인 것은 비디오 객체 분할 분야에서 새로운 방향을 제시합니다.
- **실용성과 유연성:** DyeNet은 온라인 훈련 없이도 경쟁력 있는 정확도를 달성하여 더 빠른 추론 속도를 제공하며, 온라인 훈련을 통해 추가적인 정확도 향상을 얻을 수 있어 다양한 실제 적용 시나리오에 유연하게 대응할 수 있습니다.
- **강력한 일반화 능력:** DAVIS 2017에서 훈련된 모델이 다른 특성의 데이터셋에서도 뛰어난 성능을 보이는 것은 DyeNet의 강력한 특징 학습 및 일반화 능력을 입증합니다.
- **향후 개선 방향:** 현재 트랙렛 연결에 탐욕적 알고리즘을 사용하고 있으나, 향후 조건부 랜덤 필드와 같은 더 정교한 연결 방법을 탐색할 여지가 있습니다.

## 📌 TL;DR

이 논문은 다중 객체 비디오 객체 분할의 난제(가려짐, 스케일/자세 변화)를 해결하는 **DyeNet**을 제안합니다. DyeNet은 사라진 객체를 재식별하는 **Re-ID 모듈**과 어텐션 메커니즘으로 방해 요소에 강건하게 마스크를 전파하는 **Re-MP 모듈**을 하나의 엔드투엔드 프레임워크에 통합합니다. 특히, **템플릿 확장**을 통해 동적으로 템플릿을 보강하고, **어텐션 메커니즘**을 통해 정확한 마스크 전파를 수행합니다. DyeNet은 DAVIS 2017 벤치마크에서 SOTA인 68.2의 G-mean을 달성했으며, 빠른 추론 속도를 유지하면서도 기존 방법들의 한계를 효과적으로 극복했습니다.
