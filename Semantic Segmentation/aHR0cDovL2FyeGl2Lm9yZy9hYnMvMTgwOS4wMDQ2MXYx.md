# YouTube-VOS: Sequence-to-Sequence Video Object Segmentation

Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang

## 🧩 Problem to Solve

기존 비디오 객체 분할(Video Object Segmentation, VOS) 방법론들은 주로 정적 이미지 분할 기술에 의존하거나, 시간적 의존성을 포착하기 위해 사전 학습된 옵티컬 플로우(optical flow) 모델에 의존하여 최적의 성능을 달성하지 못했습니다. 또한, 비디오 분할을 위한 공간-시간(spatial-temporal) 특징을 엔드-투-엔드(end-to-end) 방식으로 학습하는 것은 가용한 비디오 분할 데이터셋의 규모가 작아 (가장 큰 데이터셋도 90개의 짧은 비디오 클립에 불과) 큰 제약을 받았습니다.

## ✨ Key Contributions

- **대규모 비디오 객체 분할 데이터셋 구축**: 3,252개의 YouTube 비디오 클립과 78개 카테고리를 포함하는 대규모 비디오 객체 분할 데이터셋인 YouTube-VOS를 구축하고 공개했습니다. 이는 현재까지 가장 큰 VOS 데이터셋입니다.
- **새로운 시퀀스-투-시퀀스(Sequence-to-Sequence) 네트워크 제안**: YouTube-VOS 데이터셋을 기반으로 비디오 내 장기적인 공간-시간 정보를 완전히 활용하는 새로운 시퀀스-투-시퀀스 학습 알고리즘을 제안했습니다. 이 모델은 기존 옵티컬 플로우나 모션 분할 모델에 의존하지 않고 엔드-투-엔드 방식으로 학습됩니다.
- **최고 성능 달성**: 제안된 방법은 YouTube-VOS 테스트 세트에서 최고의 결과를 달성했으며, DAVIS 2016에서도 최신 기술과 비교할 만한 성능을 보여주었습니다. 실험을 통해 대규모 데이터셋이 모델 성공의 핵심 요소임을 입증했습니다.

## 📎 Related Works

- **기존 VOS 데이터셋의 한계**: DAVIS [33, 34]를 포함한 기존 VOS 데이터셋([21, 13, 26, 4, 30, 15])은 수십 개의 비디오 클립만을 포함하는 등 규모가 작고, 비디오 콘텐츠가 단순하며 해상도가 낮은 경향이 있습니다. 이는 엔드-투-엔드 모델 학습에 부족합니다.
- **심층 학습 기반 VOS 방법**: 대부분의 심층 학습 기반 방법([6, 32, 8, 11, 50, 44])은 이미지 분할 네트워크를 기반으로 하며 시퀀스 모델링을 포함하지 않습니다. 이들은 온라인 학습([6])을 통해 성능을 향상시키거나 이전 프레임의 마스크를 가이드로 사용합니다([32, 50, 19]).
- **시간적 정보 활용 시도**: Jampani et al. [22]은 공간-시간 일관성을 사용하여 마스크를 전파하고, Tokmakov et al. [41]은 투 스트림 네트워크와 순환 레이어를 사용하지만, 이들은 사전 학습된 모션 분할 모델 [40]이나 옵티컬 플로우 모델 [20]에 의존하여 최적의 결과를 얻지 못하는 한계가 있습니다.

## 🛠️ Methodology

본 논문에서는 장기적인 공간-시간 정보를 직접적으로 학습하는 시퀀스-투-시퀀스 비디오 객체 분할 알고리즘을 제안합니다.

1. **문제 정의**: 시간 $t$에서의 입력 프레임 $x_t$와 시간 $0$에서의 초기 객체 마스크 $y_0$가 주어졌을 때, 시간 $1$부터 $T-1$까지의 객체 마스크 $\hat{y}_t$를 예측하는 문제입니다. 이는 이전 프레임 $x_{t-1}$의 정보만 활용하는 것이 아니라, $\hat{y}_t = \mathrm{arg max}_{\forall \bar{y}_t} P(\bar{y}_t | x_0, x_1, ..., x_t, y_0)$와 같이 전체 과거 시퀀스 정보를 활용하는 것을 목표로 합니다.
2. **모델 구조**:
   - **Initializer**: 초기 프레임 $x_0$와 초기 마스크 $y_0$를 입력받아 ConvLSTM의 초기 기억 상태 $c_0$와 히든 상태 $h_0$를 생성합니다. 이는 객체의 외형, 위치, 스케일 정보를 인코딩합니다. VGG-16 [37] 네트워크를 백본으로 사용합니다.
   - **Encoder**: 각 시간 단계 $t$에서 현재 프레임 $x_t$를 입력받아 특징 맵 $\tilde{x}_t$를 추출합니다. Initializer와 동일하게 VGG-16 백본을 사용합니다.
   - **ConvLSTM**: Encoder에서 추출된 $\tilde{x}_t$와 이전 시간 단계의 $c_{t-1}$, $h_{t-1}$를 입력으로 받아 내부 상태 $c_t$와 $h_t$를 업데이트합니다. 이 과정에서 객체의 새로운 특징이 포착되며, 장기적인 공간-시간 의존성을 학습합니다.
   - **Decoder**: ConvLSTM의 출력인 히든 상태 $h_t$를 입력받아 전체 해상도의 분할 결과 $\hat{y}_t$를 생성합니다. 여러 개의 업샘플링(upsampling) 레이어로 구성됩니다.
3. **학습**:
   - YouTube-VOS 학습 세트에서 임의의 비디오 시퀀스에서 객체와 5~11프레임을 샘플링합니다.
   - 원래 RGB 프레임과 마스크는 $256 \times 448$로 크기가 조정됩니다.
   - 학습 초기에는 Ground Truth (GT) 마스크가 있는 프레임만 사용하여 손실을 계산하고 역전파합니다 (교사 강요, teacher forcing [45]).
   - 학습이 안정화되면 GT 마스크가 없는 프레임도 추가하며, 이전 스텝의 모델 예측을 입력으로 사용합니다 (커리큘럼 학습, curriculum learning [2]).
   - 이진 교차 엔트로피(binary cross-entropy) 손실을 사용하며, Adam [24] 옵티마이저로 학습합니다. 전체 모델은 엔드-투-엔드 방식으로 학습됩니다.
4. **추론**:
   - 오프라인으로 학습된 모델은 새로운 테스트 비디오에 직접 적용하여 좋은 분할 결과를 생성할 수 있습니다. 이는 온라인 학습(online learning)이 필요한 기존 SOTA(state-of-the-art) 방법들과 대조됩니다.
   - 온라인 학습을 추가하면 성능을 더욱 향상시킬 수 있습니다. 초기 프레임 $(x_0, y_0)$에 아핀 변환(affine transformation)을 적용하여 임의의 온라인 학습 샘플 쌍을 생성하고, 이를 통해 Initializer, Encoder, Decoder 네트워크를 미세 조정합니다. ConvLSTM의 파라미터는 고정됩니다.

## 📊 Results

- **YouTube-VOS 성능**:
  - 본 제안 모델(온라인 학습 없음)은 기존 온라인 학습 기반 방법인 OSVOS(59.1% J$_{mean}$)보다 우수한 60.9%의 J$_{mean}$을 달성하며 장기 공간-시간 정보의 중요성을 입증했습니다.
  - 온라인 학습을 추가했을 때, 본 모델은 66.9%의 J$_{mean}$을 달성하며 OSVOS 대비 약 8%p 절대 성능 향상을 보였습니다. F$_{mean}$ 및 Decay rate에서도 크게 우수했습니다.
  - 시간에 따른 J$_{mean}$ 변화를 보면, 초기 프레임에서는 온라인 학습 기반 방법이 우수할 수 있으나, 비디오 진행에 따라 본 모델이 성능 저하가 느리며 OSVOS를 능가하기 시작합니다.
  - "Unseen" 카테고리에 대한 일반화 능력 또한 뛰어나며, 이는 대규모 YouTube-VOS 데이터셋의 다양한 학습 카테고리가 모델의 일반화에 기여함을 보여줍니다.
- **DAVIS 2016 성능**:
  - DAVIS 2016 데이터셋에 사전 학습된 모델을 미세 조정(fine-tune)하여 평가했습니다.
  - 본 모델(온라인 학습 없음)은 76.5%의 mean IoU를 달성하여 MaskTrack(79.7%), OSVOS(79.8%) 등 다른 온라인 학습 방법과 비슷한 결과를 보였지만, 추론 속도는 60배 빨랐습니다 (0.16s vs. 10~12s).
  - 온라인 학습을 추가하면 79.1%의 mean IoU로 성능이 더욱 향상되어 다른 SOTA 방법들과 경쟁력 있는 결과를 보였습니다.
- **데이터셋 규모의 영향 (Ablation Study)**:
  - 기존 데이터셋 (DAVIS 학습 세트 + SegTrackv2, JumpCut, YoutubeObjects 합쳐 192개 비디오)만으로 모델을 학습했을 때 mean IoU는 51.9%에 불과하여, 기존 데이터셋만으로는 충분한 성능을 얻기 어렵다는 것을 보여주었습니다.
  - YouTube-VOS 학습 데이터의 25%, 50%, 75%를 사용했을 때 성능이 46.7%, 51.5%, 56.8%로 점진적으로 증가하여 데이터셋 규모가 모델 성능에 매우 중요함을 입증했습니다.

## 🧠 Insights & Discussion

- **대규모 데이터셋의 중요성**: YouTube-VOS와 같은 대규모 데이터셋의 존재가 엔드-투-엔드 시퀀스-투-시퀀스 모델의 성공에 결정적인 역할을 한다는 것을 명확히 보여주었습니다. 기존 데이터셋은 그 규모와 복잡성 면에서 심층 학습 기반의 시공간 특징 학습에 부족합니다.
- **장기 공간-시간 정보의 효과**: 본 제안 모델은 옵티컬 플로우나 사전 학습된 모션 모델 없이도 장기적인 공간-시간 의존성을 직접 학습하여 비디오 객체 분할에서 뛰어난 성능을 달성했습니다. 이는 특히 비디오 길이가 길어질수록 객체의 외형 변화나 빠른 움직임이 심할 때, 이전 프레임의 정보만을 활용하는 방법보다 강건함을 의미합니다.
- **실용적인 적용 가능성**: 온라인 학습 없이도 준수한 성능을 보이며 빠른 추론 속도를 제공하므로, 실시간 또는 대규모 비디오 처리 애플리케이션에 더 적합합니다. 온라인 학습을 추가하면 성능을 더욱 끌어올릴 수 있습니다.
- **모델 구성 요소의 중요성**: Initializer의 역할이 객체 위치 파악에 중요하며, 단순히 마스크만을 초기 상태로 사용하는 것은 성능 저하를 가져옵니다. Encoder의 입력으로 이전 단계의 마스크를 추가하는 것은 성능에 큰 영향을 미치지 않으면서 오차 전파 문제를 야기할 수 있음을 확인했습니다.
- **향후 연구 방향**: 데이터셋 규모가 100%일 때도 정확도 개선 추세가 정체되지 않았으므로, 더 많은 데이터 수집을 통해 알고리즘에 미치는 영향을 추가로 탐색할 가치가 있습니다.

## 📌 TL;DR

본 논문은 기존 비디오 객체 분할(VOS) 연구의 데이터셋 규모 한계를 극복하기 위해 3,252개 비디오와 78개 카테고리를 포함하는 대규모 YouTube-VOS 데이터셋을 구축했습니다. 이 데이터셋을 기반으로 초기 프레임 마스크와 연속적인 비디오 프레임을 ConvLSTM 기반의 시퀀스-투-시퀀스 네트워크에 입력하여 장기적인 공간-시간 특징을 엔드-투-엔드 방식으로 학습하는 새로운 VOS 방법론을 제안했습니다. 제안된 모델은 YouTube-VOS에서 SOTA 성능을 달성했으며, DAVIS 2016에서도 경쟁력 있는 결과를 보이면서 대규모 데이터셋이 효과적인 비디오 시공간 특징 학습과 VOS 성능 향상에 필수적임을 입증했습니다.
