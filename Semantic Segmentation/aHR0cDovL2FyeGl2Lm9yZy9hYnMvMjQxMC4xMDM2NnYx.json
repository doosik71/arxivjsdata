{
  "url": "http://arxiv.org/abs/2410.10366v1",
  "title": "Affinity-Graph-Guided Contractive Learning for Pretext-Free Medical\n  Image Segmentation with Minimal Annotation",
  "authors": "Zehua Cheng, Di Yuan, Thomas Lukasiewicz",
  "year": 2024,
  "abstract": "The combination of semi-supervised learning (SemiSL) and contrastive learning\n(CL) has been successful in medical image segmentation with limited\nannotations. However, these works often rely on pretext tasks that lack the\nspecificity required for pixel-level segmentation, and still face overfitting\nissues due to insufficient supervision signals resulting from too few\nannotations. Therefore, this paper proposes an affinity-graph-guided\nsemi-supervised contrastive learning framework (Semi-AGCL) by establishing\nadditional affinity-graph-based supervision signals between the student and\nteacher network, to achieve medical image segmentation with minimal annotations\nwithout pretext. The framework first designs an average-patch-entropy-driven\ninter-patch sampling method, which can provide a robust initial feature space\nwithout relying on pretext tasks. Furthermore, the framework designs an\naffinity-graph-guided loss function, which can improve the quality of the\nlearned representation and the model generalization ability by exploiting the\ninherent structure of the data, thus mitigating overfitting. Our experiments\nindicate that with merely 10% of the complete annotation set, our model\napproaches the accuracy of the fully annotated baseline, manifesting a marginal\ndeviation of only 2.52%. Under the stringent conditions where only 5% of the\nannotations are employed, our model exhibits a significant enhancement in\nperformance surpassing the second best baseline by 23.09% on the dice metric\nand achieving an improvement of 26.57% on the notably arduous CRAG and ACDC\ndatasets.",
  "citation": 0
}