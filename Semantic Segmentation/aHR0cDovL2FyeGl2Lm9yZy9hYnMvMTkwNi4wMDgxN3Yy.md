# Zero-Shot Semantic Segmentation

Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, Patrick Pérez

## 🧩 Problem to Solve

기존의 시맨틱 분할(Semantic Segmentation) 모델은 방대한 양의 주석(annotation) 데이터가 필요하므로 새로운 객체 클래스로 확장하는 데 한계가 있습니다. 이 연구는 **제로샷 시맨틱 분할(Zero-Shot Semantic Segmentation, ZS3)**이라는 새로운 과제를 도입합니다. 이는 훈련 예제가 전혀 없는(never-seen) 객체 범주에 대해 픽셀 단위 분류기를 학습하는 것을 목표로 합니다. 특히, 테스트 시에 학습된(seen) 클래스와 학습되지 않은(unseen) 클래스 모두를 처리해야 하는 "일반화된(generalized)" 제로샷 분류 상황을 다룹니다.

## ✨ Key Contributions

- **제로샷 시맨틱 분할(ZS3)**이라는 새로운 컴퓨터 비전 과제를 최초로 정의하고 제안했습니다.
- **ZS3Net**이라는 혁신적인 아키텍처를 제안했습니다. 이 아키텍처는 딥 비주얼 분할 모델(DeepLabv3+)과 시맨틱 단어 임베딩으로부터 시각적 표현을 생성하는 접근 방식을 결합합니다.
- ZS3Net이 테스트 시에 학습된 클래스와 학습되지 않은 클래스 모두에 대한 픽셀 분류 작업을 성공적으로 수행하여 일반화된 제로샷 분류 문제를 해결함을 입증했습니다.
- 학습되지 않은 클래스 픽셀에 대한 자동 의사 라벨링(pseudo-labeling)을 활용한 **셀프 트레이닝(self-training) 단계**를 도입하여 성능을 추가적으로 향상시켰습니다 (모델명: **ZS5Net**).
- Pascal-VOC 및 Pascal-Context 두 표준 분할 데이터셋에 대한 제로샷 벤치마크를 제안하고, 경쟁력 있는 기준선을 제시했습니다.
- Pascal-Context와 같이 복잡한 장면의 경우, 클래스별 분할 맵에서 얻은 공간적 컨텍스트 사전 정보(spatial context priors)를 최대한 활용하기 위해 **그래프 컨텍스트 인코딩(graph-context encoding)**을 통한 모델 확장을 제안했습니다.

## 📎 Related Works

- **시맨틱 분할(Semantic Segmentation):** FCN(Fully Convolutional Networks) [28]을 시작으로 U-Net [37], SegNet [3], DeepLab 시리즈 [10, 11], PSPNet [50] 등 CNN 기반의 발전과 atrous/dilated convolution [10, 46], pyramid context pooling [50], context encoding [47]을 통한 컨텍스트 정보 활용 전략이 언급됩니다.
- **약지도 분할(Weakly-Supervised Segmentation):** 이미지 수준 [33, 34] 또는 바운딩 박스 수준 [13]의 주석을 사용하는 연구들이 소개됩니다.
- **제로샷 학습(Zero-Shot Learning, ZSL):** 이미지 분류 분야에서 활발히 연구되었으며, 이미지 데이터와 클래스 설명을 공통 공간으로 매핑하는 임베딩 문제 [1, 2, 6, 8, 16, 24, 32, 36, 39, 42, 43, 48] 또는 조건부 생성 모델을 통해 학습되지 않은 클래스의 합성 인스턴스를 생성하는 방식 [7, 25, 45]이 있습니다.
- **제로샷 객체 탐지(Zero-Shot Object Detection):** 최근 [4, 14, 35, 51] 등에서 연구가 시작되었지만, 제로샷 시맨틱 분할에 대한 선행 연구는 없다고 언급합니다.

## 🛠️ Methodology

ZS3Net은 기존 시맨틱 분할 모델(DeepLabv3+)을 기반으로 하며, 다음과 같은 단계로 구성됩니다.

1. **픽셀별 데이터 정의 및 수집 (Step 0):**

   - 학습된(seen) 클래스에 대해 완전히 지도 학습된 DeepLabv3+ 모델을 사용합니다.
   - 마지막 $1 \times 1$ 컨볼루션 분류 레이어와 그 입력 특징을 분류기 $f$와 대상 특징 $x$로 선택합니다.
   - 학습되지 않은 클래스에 대해서는 어떠한 훈련 데이터도 없으며, 해당 범주의 임베딩 $a[c]$만 사용합니다.
   - 학습되지 않은 클래스에서의 정보 유출을 막기 위해 백본 네트워크는 학습된 클래스만으로 ImageNet [38]에서 재학습됩니다.

2. **생성 모델 (Step 1):**

   - 클래스 임베딩 벡터 $a$에 따라 이미지 특징 $\hat{x}$를 생성하는 모델 $G(a, z; w)$를 학습합니다. 여기서 $z$는 고정된 다변수 가우시안 분포에서 샘플링된 무작위 노이즈입니다.
   - 생성 모델로는 **Generative Moment Matching Network (GMMN)** [27]을 채택했습니다. GMMN은 실제 데이터 분포와 생성된 분포 간의 **Maximum Mean Discrepancy (MMD)**를 최소화하여 학습됩니다.
   - GMMN 손실 함수:
     $$ L*{GMMN}(a) = \sum*{x,x' \in X(a)} k(x,x') + \sum*{\hat{x},\hat{x}' \in \hat{X}(a;w)} k(\hat{x},\hat{x}') - 2 \sum*{x \in X(a)} \sum\_{\hat{x} \in \hat{X}(a,w)} k(x,\hat{x}) $$
        여기서 $k$는 가우시안 커널입니다.

3. **분류 모델 (Step 2):**

   - GMMN 학습 후, 학습되지 않은 클래스에 대해 임의의 수의 픽셀 수준 합성 특징 $\hat{D}_u$를 생성합니다.
   - 이 합성 특징을 학습된 클래스의 실제 특징 $D_s$와 결합하여 새로운 훈련 데이터셋을 구성합니다.
   - DeepLabv3+의 $1 \times 1$ 컨볼루션 분류 레이어 $f$를 이 새로운 데이터셋으로 파인튜닝하여 학습된 클래스와 학습되지 않은 클래스 모두를 처리할 수 있도록 합니다.

4. **제로샷 학습 및 셀프 트레이닝 (ZS5Net):**

   - 학습되지 않은 클래스의 객체를 포함하는 라벨링되지 않은 이미지가 있을 때 적용됩니다 (완화된 제로샷 설정).
   - ZS3Net 모델을 사용하여 이 이미지들에 대한 의사 라벨(pseudo-labels)을 생성합니다.
   - 가장 높은 확신도를 가진 상위 $p\%$의 의사 라벨만 새로운 훈련 특징으로 사용하고, 시맨틱 분할 네트워크를 재학습시킵니다.

5. **그래프 컨텍스트 인코딩 (Graph-context encoding):**
   - 복잡한 장면에 대한 인식을 향상시키기 위해 공간적 객체 배열의 컨텍스트 정보를 활용합니다.
   - 시맨틱 분할 마스크를 의미론적으로 연결된 구성 요소들의 인접 그래프 $G=(V, E)$로 표현합니다.
   - 각 노드 $\nu \in V$는 해당 시맨틱 임베딩 $a_{\nu}$와 무작위 가우시안 샘플 $z_{\nu}$를 연결하여 표현됩니다.
   - GMMN의 선형 레이어를 그래프 컨볼루션 레이어 [23]로 대체하여 그래프 $G$를 추가 입력으로 받아들여 그래프 노드에 부착된 시각적 특징을 생성합니다.

## 📊 Results

- **데이터셋:** Pascal-VOC 2012와 Pascal-Context 데이터셋에서 2, 4, 6, 8, 10개로 다양하게 학습되지 않은 클래스 세트를 구성하여 실험했습니다.
- **평가 지표:** 픽셀 정확도(PA), 평균 정확도(MA), 평균 Intersection-over-Union(mIoU)을 사용했으며, 학습된/학습되지 않은 클래스 mIoU의 조화 평균(hIoU)을 제안하여 일반화된 제로샷 성능을 더 정확히 평가했습니다.
- **ZS3Net 성능:**
  - 기존 ZSL 분류 접근 방식을 분할에 적용한 기준선(DeViSe [16] 기반)보다 훨씬 우수한 성능을 보였습니다. 특히 학습되지 않은 클래스에서 mIoU가 크게 향상되었고, hIoU 측면에서도 상당한 개선을 이루었습니다.
  - 예를 들어, Pascal-VOC 2-split에서 ZS3Net은 학습되지 않은 클래스 mIoU에서 기준선 대비 +32.2% 향상을 달성했습니다.
  - 일반화된 ZSL 평가(학습된 클래스와 학습되지 않은 클래스 모두 고려)에서 ZS3Net은 학습된 클래스에 대한 예측 편향을 효과적으로 줄여 우수한 성능을 보였습니다.
- **그래프 컨텍스트 인코딩 (ZS3Net + GC):**
  - Pascal-Context와 같이 복잡하고 객체 주석이 밀집된 데이터셋에서 ZS3Net + GC는 ZS3Net보다 일관되게 향상된 성능을 보여 공간적 컨텍스트 정보 활용의 이점을 입증했습니다.
- **셀프 트레이닝 (ZS5Net):**
  - 셀프 트레이닝 단계를 추가한 ZS5Net은 학습된, 학습되지 않은, 그리고 전체 클래스에서 성능을 크게 향상시켰습니다.
  - 일부 설정(예: 두 데이터셋의 2-unseen split)에서는 전체 성능이 완전한 지도 학습 모델의 성능에 매우 근접했습니다.
- **정성적 결과:**
  - 제로샷 학습 없이 훈련된 모델이 학습되지 않은 객체를 배경이나 학습된 다른 클래스로 잘못 분류하는 반면, ZS3Net과 ZS5Net은 이전에 보지 못한 객체들을 정확하게 분할하는 모습을 보여주었습니다.

## 🧠 Insights & Discussion

- 이 연구는 시맨틱 분할에 제로샷 학습 패러다임을 성공적으로 도입하여, 주석된 훈련 데이터가 없는 새로운 객체 범주에 대한 모델의 확장성 문제를 해결할 수 있는 가능성을 제시했습니다.
- 생성 모델을 통해 학습되지 않은 클래스의 시각적 특징을 합성하는 접근 방식은 일반화된 제로샷 시나리오에서 흔히 발생하는 '학습된 클래스 편향' 문제를 효과적으로 완화합니다. 이는 모델이 학습된 클래스에 과도하게 치우치지 않고 학습되지 않은 클래스에 대해서도 합리적인 성능을 낼 수 있게 합니다.
- 셀프 트레이닝은 강력한 보완 전략임이 입증되었습니다. 라벨링되지 않은 학습되지 않은 클래스 데이터가 존재할 경우, 모델은 스스로 지식을 확장하여 성능을 크게 끌어올릴 수 있으며, 이는 실제 응용 분야에서 매우 유용할 수 있습니다.
- 그래프 컨텍스트 인코딩은 특히 복잡한 장면에서 객체 간의 공간적 관계를 활용하는 것이 시맨틱 분할 성능에 중요함을 보여줍니다. 이는 미래의 시맨틱 분할 연구에서 컨텍스트 모델링의 중요성을 강조합니다.
- 본 연구의 방법론은 ZS3Net이 단어 임베딩을 통해 시맨틱 정보를 효과적으로 시각적 특징 공간으로 변환할 수 있음을 입증했으며, 이는 멀티모달 학습의 중요성을 시사합니다.

## 📌 TL;DR

**문제:** 기존 시맨틱 분할 모델은 새로운 객체 클래스에 대해 훈련 데이터가 없으면 분할을 수행할 수 없어 확장성이 제한됩니다.
**제안 방법:** 이 논문은 제로샷 시맨틱 분할(ZS3) 과제를 도입하고, **ZS3Net**이라는 새로운 모델을 제안합니다. ZS3Net은 DeepLabv3+ 백본과 단어 임베딩으로부터 학습되지 않은 클래스의 합성 시각적 특징을 생성하는 생성 모델(GMMN)을 결합합니다. 학습된 클래스의 실제 특징과 학습되지 않은 클래스의 합성 특징을 함께 사용하여 최종 분류기를 훈련합니다. 나아가 **ZS5Net**은 학습되지 않은 클래스에 대한 의사 라벨링 기반의 셀프 트레이닝을 통해 성능을 향상시키며, 복잡한 장면에서는 그래프 컨텍스트 인코딩을 추가하여 공간적 컨텍스트를 활용합니다.
**주요 결과:** ZS3Net은 Pascal-VOC 및 Pascal-Context 데이터셋에서 제로샷 시맨틱 분할 벤치마크를 설정하고, 기존 기준선보다 학습되지 않은 클래스 및 전반적인 조화 평균 IoU(hIoU)에서 월등히 뛰어난 성능을 보였습니다. 셀프 트레이닝(ZS5Net)은 성능을 크게 향상시켜 일부 경우 완전 지도 학습 모델에 근접한 결과를 달성했으며, 그래프 컨텍스트 인코딩은 복잡한 장면에서 추가적인 성능 개선을 가져왔습니다.
