{
  "url": "http://arxiv.org/abs/2310.00240v1",
  "title": "Learning Mask-aware CLIP Representations for Zero-Shot Segmentation",
  "authors": "Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, Humphrey Shi",
  "year": 2023,
  "abstract": "Recently, pre-trained vision-language models have been increasingly used to\ntackle the challenging zero-shot segmentation task. Typical solutions follow\nthe paradigm of first generating mask proposals and then adopting CLIP to\nclassify them. To maintain the CLIP's zero-shot transferability, previous\npractices favour to freeze CLIP during training. However, in the paper, we\nreveal that CLIP is insensitive to different mask proposals and tends to\nproduce similar predictions for various mask proposals of the same image. This\ninsensitivity results in numerous false positives when classifying mask\nproposals. This issue mainly relates to the fact that CLIP is trained with\nimage-level supervision. To alleviate this issue, we propose a simple yet\neffective method, named Mask-aware Fine-tuning (MAFT). Specifically,\nImage-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary\nnumbers of image and mask proposals simultaneously. Then, mask-aware loss and\nself-distillation loss are designed to fine-tune IP-CLIP Encoder, ensuring CLIP\nis responsive to different mask proposals while not sacrificing\ntransferability. In this way, mask-aware representations can be easily learned\nto make the true positives stand out. Notably, our solution can seamlessly plug\ninto most existing methods without introducing any new parameters during the\nfine-tuning process. We conduct extensive experiments on the popular zero-shot\nbenchmarks. With MAFT, the performance of the state-of-the-art methods is\npromoted by a large margin: 50.4% (+ 8.2%) on COCO, 81.8% (+ 3.2%) on\nPascal-VOC, and 8.7% (+4.3%) on ADE20K in terms of mIoU for unseen classes. The\ncode is available at https://github.com/jiaosiyu1999/MAFT.git.",
  "citation": 73
}