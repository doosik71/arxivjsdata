# Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation

Daizong Liu, Shuangjie Xu, Xiao-Yang Liu, Zichuan Xu, Wei Wei, Pan Zhou

## 🧩 Problem to Solve

반지도 학습(semi-supervised) 비디오 객체 분할(VOS)은 첫 프레임의 주석을 기반으로 클래스에 구애받지 않는 객체를 배경으로부터 분할하는 것을 목표로 합니다. 기존의 탐지 기반(detection-based) VOS 방법들은 대개 그리디(greedy) 전략을 사용하여 "최고의" 후보(proposal)만을 선택하며, 이로 인해 선택되지 않은 다른 후보들에 포함된 중요한 국소 패치(local patch) 세부 정보를 잃을 수 있습니다. 또한, 이러한 방법들은 후보의 품질과 Re-ID 네트워크의 신뢰성에 크게 의존하여 배경 노이즈, 객체 누락, 심각한 가려짐(occlusion)과 같은 실제 시나리오에서 여전히 어려움을 겪습니다.

## ✨ Key Contributions

* **새로운 시공간 그래프 신경망(STG-Net) 제안:** 인스턴스 수준(instance level)에서 국소 패치 세부 정보를 복구하기 위한 시공간 그래프를 기반으로 하는 새로운 VOS 방법인 STG-Net을 제안합니다. 공간 및 시간 그래프 네트워크의 협력을 통해 STG-Net은 더 정확한 마스크 재구성을 위한 충분한 용량을 가집니다. VOS 태스크에서 GNN을 사용하여 공간적 및 시간적 상관관계를 모두 활용한 것은 본 연구가 처음입니다.
* **공간 그래프 네트워크를 통한 마스크 세부 정보 집계:** 그리디 방식으로 최적의 후보를 찾는 대신, 제안된 공간 그래프 네트워크는 모든 객체 후보를 고려하고, 후보 쌍의 특징 유사도로 측정되는 엣지 가중치(edge weight) 전략을 사용하여 마스크 세부 정보를 집계합니다. 이후 모션 추정(motion estimation)과 마스크 전파(mask propagation)를 고려한 점수 함수를 통해 재구성된 마스크 중 최적의 마스크를 선택합니다.
* **메모리 네트워크 기반 시간 그래프를 통한 마스크 정제:** 공간 그래프에서 선택된 마스크를 기반으로 시간 차원(time dimension)을 따라 시간 그래프를 개발합니다. 각 노드에 대해 메모리 네트워크를 활용하여 과거(historic) 마스크에서 마스크 컨텍스트를 검색하고, 이를 시간 그래프 네트워크로 현재 마스크를 정제합니다.
* **우수한 성능 달성:** 온라인 학습 및 미세 조정(fine-tuning) 없이 DAVIS, YouTube-VOS, SegTrack-v2, YouTube-Objects의 4가지 주요 벤치마크에서 SOTA 성능을 달성하여 제안된 접근 방식의 효과성을 입증합니다. 특히 가려지거나 누락된 객체를 처리하는 데 강점을 보입니다.

## 📎 Related Works

* **반지도 학습 비디오 객체 분할 (Semi-Supervised Video Object Segmentation):**
  * **매칭 기반(Matching-based):** 첫 프레임 마스크에서 외관 정보를 추출하여 후속 프레임에서 유사 객체를 찾습니다 (예: OSMN, SiamMask, FEEL).
  * **전파 기반(Propagation-based):** 이전 프레임에서 전파된 마스크를 시간 정보를 활용하여 정제합니다 (예: RGMP, AGAME, RVOS).
  * **탐지 기반(Detection-based):** 각 프레임에서 최적의 바운딩 박스를 먼저 탐지한 후 분할 모델에 입력합니다 (예: PReMVOS, Re-ID). 본 논문은 이러한 방법의 단점(국소 세부 정보 손실)을 개선합니다.
* **그래프 신경망 (Graph Neural Networks, GNN):** 노드별 상관관계 설정을 통해 정보를 집계합니다. 기존 VOS GNN 연구는 프레임을 노드로 보아 시간적 상관관계를 탐색하거나, 픽셀을 노드로 보아 인스턴스 수준 관계를 놓치는 경우가 있었습니다. 본 논문은 마스크 후보를 노드로 사용하여 공간 그래프를, 과거 마스크를 노드로 사용하여 시간 그래프를 구성합니다.
* **메모리 네트워크 (Memory Networks):** 외부 메모리를 사용하여 정보를 쓰고 읽습니다 (예: Vaswani et al. 2017). 본 논문에서는 각 프레임의 마스크 결과를 값으로 사용하여 시간 그래프에서 현재 마스크를 정제하기 위해 과거 마스크를 검색합니다.

## 🛠️ Methodology

STG-Net은 다음 단계로 구성됩니다.

1. **후보 생성 (Proposal Generation):**
    * **바운딩 박스 후보 생성:** Mask R-CNN을 사용하여 비디오 프레임에서 클래스에 구애받지 않는 객체 바운딩 박스(bbox) 후보 $b_v$를 오프라인으로 생성합니다.
    * **마스크 생성:** Deeplabv3+ 네트워크를 사용하여 해당 마스크 $M_v$를 분할합니다. 이전 프레임에서 광학 흐름(optical flow)으로 워핑(warp)된 마스크 $Q$를 추가 입력으로 사용하여 분할 모듈의 정확도를 높입니다.
2. **공간 그래프 구성 (Spatial Graph Construction):**
    * **준비 (모션 모델):** 모션 모델을 사용하여 $M_v$ 후보들을 객체 $o$ 클래스로 분류합니다. 이전 $n$단계 움직임 이력에 기반하여 현재 프레임에서 객체 $o$의 목표 확률 위치 $p = (c_t, s_t)$를 예측합니다. $b_v$와 $p$ 간의 IoU를 계산하여 $b_v$를 객체 $o$ 클래스로 분류합니다.
    * **그래프 구성:** 각 객체 $o$에 대해 마스크 $\{M_v\}$를 노드로 하는 완전 연결 공간 그래프를 구성합니다.
    * **엣지 가중치 ($W_{vu}$):** 노드 $v$와 $u$ 사이의 엣지 가중치는 다음과 같이 정의됩니다:
        $$W_{vu} = \begin{cases} \alpha \cos(X_v, X_u) + \beta \text{IoU}(b_v, b_u), & v \ne u \\ 0, & v = u \end{cases}$$
        여기서 $X_v, X_u$는 학습 가능한 CNN으로 추출된 특징이며, $\alpha$와 $\beta$는 특징 유사도와 IoU 점수 간의 비율을 제어합니다.
    * **그래프 업데이트:** 노드 $v$는 이웃 $N(v)$로부터 마스크 메시지 $m_v = \sum_u W_{vu} h_u$를 집계하고, 자신의 상태를 $(h_v)' = ( (1-W_{vv})h_v + m_v ) / (1 + \sum_u W_{vu})$로 업데이트하여 마스크를 재구성합니다. 이 과정은 2-3회 반복됩니다.
3. **시간 그래프 구성 (Temporal Graph Construction):**
    * **마스크 선택:** 공간 그래프에서 재구성된 마스크 $\{\hat{M}_v\}$ 중 다음 점수 함수를 최대화하는 최적의 마스크 $\hat{M}$을 선택합니다.
        $$S(\hat{M}_v|(p,Q)) = \lambda_1 \text{IoU}(B(\hat{M}_v),p) + \lambda_2 \frac{\hat{M}_v \cap Q}{\hat{M}_v \cup Q}$$
        여기서 $B(\cdot)$는 $\hat{M}_v$의 bbox를 추출하는 함수이고, $Q$는 이전 프레임에서 워핑된 마스크입니다.
    * **그래프 구성:** 현재 프레임의 선택된 마스크 $\hat{M}_t$와 이전 프레임의 과거 마스크 $\hat{M}_r$ ($r \le t$)를 노드로 사용하여 시간 그래프를 구성합니다. $B(\hat{M}_r)$ 기반으로 이미지를 크롭하여 $I_r$을 얻고, CNN을 통해 키 특징 맵 $K_r$을 추출합니다. 크롭된 $\hat{M}_r$은 값 맵(value map)으로 사용됩니다.
    * **그래프 업데이트:** 노드 $t$는 이전 $t$개 프레임에서 마스크 메모리를 검색합니다.
        $$(m_t)_i = \sum_{r=0}^{t-1} \sum_j \frac{\exp((K_t)_i \cdot (K_r)_j)}{\sum_j \exp((K_t)_i \cdot (K_r)_j)} (h_r)_j$$
        그리고 자신의 상태를 $(h_t)' = \eta (h_t + m_t)$로 정제합니다. 여기서 $\eta = 1/(t+1)$입니다. 이 과정은 1회 반복됩니다.
4. **손실 함수 (Loss Function):** 총 손실 함수 $L(\hat{M})$은 재구성된 마스크에 대한 이진 교차 엔트로피(binary cross-entropy) 손실과, ground truth 기반 점수와 예측 점수 사이의 거리 손실을 결합하여 구성됩니다.

## 📊 Results

* **DAVIS 데이터셋:** DAVIS2016 및 DAVIS2017 벤치마크에서 기존 SOTA 방법들을 능가하며 최신 성능을 달성했습니다. 특히 DAVIS2017 test-dev 세트에서 63.1 G$_M$의 전역 평균 값(global mean value)을 기록하며 가려짐 및 객체 누락 시나리오에 대한 강력한 성능을 보여주었습니다. (FEEL 대비 J$_M$ 4.6%, F$_M$ 6.1%, G$_M$ 5.3% 개선).
* **YouTube-VOS 데이터셋:** YouTube-VOS 검증 세트에서 G$_M$ 73.0을 달성하여 새로운 SOTA를 기록했습니다. AGSS 대비 1.7% 개선을 이루었으며, 성능과 실행 시간(FPS) 사이에서 우수한 균형을 보여주었습니다.
* **기타 데이터셋:** SegTrack-v2 및 YouTube-Objects 데이터셋에서도 온라인 학습 없이 SOTA 성능을 달성했습니다.
* **정성적 시각화:** 시각적 비교를 통해 STG-Net이 다른 방법들(탐지 기반 방법의 국소 패치 손실, 매칭 기반 방법의 배경 노이즈)보다 더 나은 마스크 세부 정보를 재구성함을 보여주었습니다.
* **어블레이션 연구 (Ablation Study):**
  * 모션 모델 사용 시 $0.9$ G$_M$ 개선.
  * 공간 그래프 구성 시 $4.3$ G$_M$의 가장 큰 개선 효과 (엣지 가중치 $\alpha$는 추가로 $0.8$ G$_M$ 개선).
  * 시간 그래프 사용 시 $2.1$ G$_M$ 개선 (두 번째로 큰 개선 효과, 마스크 선택 $\lambda_1$은 추가로 $0.6$ G$_M$ 개선).
  * 공간 그래프 업데이트 반복 횟수는 2단계에서 가장 좋은 성능(63.1 G$_M$)을 보였습니다.
  * 온라인 학습, 데이터 증강을 통한 미세 조정, YouTube-VOS 사전 학습은 추가적인 성능 개선으로 이어졌지만, 본 논문의 주요 결과는 이러한 추가 훈련 없이 달성되었습니다.

## 🧠 Insights & Discussion

STG-Net은 공간 그래프를 통해 프레임 내 모든 객체 후보들의 풍부한 컨텍스트를 집계하고, 시간 그래프를 통해 이전 프레임의 마스크 메모리를 활용함으로써 VOS의 핵심 과제인 국소 세부 정보 손실 및 시공간적 일관성 부족 문제를 효과적으로 해결합니다. 특히, 그리디 전략에 의존하는 기존 탐지 기반 방법의 한계를 극복하고, 객체 가려짐이나 누락과 같은 어려운 시나리오에서 우수한 강건성(robustness)을 보여줍니다. 온라인 학습이나 미세 조정 없이도 여러 벤치마크에서 SOTA 성능을 달성했다는 점은 모델의 강력한 일반화 능력과 효율성을 입증합니다. 이러한 결과는 STG-Net이 클래스에 구애받지 않는 객체 분할을 위한 뛰어난 표현(representation)을 제공함을 시사합니다.

## 📌 TL;DR

**문제:** 기존 VOS 방법은 그리디 후보 선택으로 국소 세부 정보를 놓치고, 가려짐/누락 시나리오에 취약합니다.
**제안 방법:** STG-Net은 시공간 그래프 신경망을 활용합니다. 공간 그래프는 엣지 가중치 전략을 통해 프레임 내 *모든* 후보로부터 세부 정보를 집계하고, 시간 그래프는 메모리 네트워크를 사용하여 과거 마스크로 현재 마스크를 정제합니다.
**주요 결과:** 온라인 학습 없이도 DAVIS, YouTube-VOS 등 4가지 주요 벤치마크에서 SOTA 성능을 달성하며, 도전적인 시나리오에서 강건하고 정확한 마스크 재구성 능력을 입증했습니다.
