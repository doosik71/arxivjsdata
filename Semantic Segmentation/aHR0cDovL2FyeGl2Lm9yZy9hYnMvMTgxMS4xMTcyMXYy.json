{
  "url": "http://arxiv.org/abs/1811.11721v2",
  "title": "CCNet: Criss-Cross Attention for Semantic Segmentation",
  "authors": "Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, Thomas S. Huang",
  "year": 2018,
  "abstract": "Contextual information is vital in visual understanding problems, such as\nsemantic segmentation and object detection. We propose a Criss-Cross Network\n(CCNet) for obtaining full-image contextual information in a very effective and\nefficient way. Concretely, for each pixel, a novel criss-cross attention module\nharvests the contextual information of all the pixels on its criss-cross path.\nBy taking a further recurrent operation, each pixel can finally capture the\nfull-image dependencies. Besides, a category consistent loss is proposed to\nenforce the criss-cross attention module to produce more discriminative\nfeatures. Overall, CCNet is with the following merits: 1) GPU memory friendly.\nCompared with the non-local block, the proposed recurrent criss-cross attention\nmodule requires 11x less GPU memory usage. 2) High computational efficiency.\nThe recurrent criss-cross attention significantly reduces FLOPs by about 85% of\nthe non-local block. 3) The state-of-the-art performance. We conduct extensive\nexperiments on semantic segmentation benchmarks including Cityscapes, ADE20K,\nhuman parsing benchmark LIP, instance segmentation benchmark COCO, video\nsegmentation benchmark CamVid. In particular, our CCNet achieves the mIoU\nscores of 81.9%, 45.76% and 55.47% on the Cityscapes test set, the ADE20K\nvalidation set and the LIP validation set respectively, which are the new\nstate-of-the-art results. The source codes are available at\n\\url{https://github.com/speedinghzl/CCNet}.",
  "citation": 4166
}