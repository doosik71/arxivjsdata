# AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene Segmentation
Jay N. Paranjape, Nithin Gopalakrishnan Nair, Shameema Sikder, S. Swaroop Vedula, and Vishal M. Patel

## 🧩 Problem to Solve
수술 장면 분석에서 영상 분할은 핵심적인 문제이나, 이 분야의 고질적인 데이터 부족으로 인해 전통적인 분할 기술을 적용하기 어렵습니다. 기존 연구들은 사전 훈련된 모델을 미세 조정하여 이 문제를 해결하려 하지만, 새로운 데이터가 나올 때마다 수백만 개의 매개변수를 가진 딥 네트워크를 다시 훈련해야 하는 비효율성이 존재합니다. 최근 발표된 기반 모델인 SAM(Segment-Anything Model)은 자연 이미지에 대해서는 뛰어난 일반화 능력을 보이지만, 의료 분야에서는 그대로 사용 시 성능이 저조하며, 미세 조정에는 많은 계산 자원이 필요하고, 바운딩 박스나 점과 같은 수동적인 작업이 필요한 프롬프트에 의존해야 합니다. 이러한 수동 프롬프트는 데이터 규모가 커질수록 전문가의 개입이 늘어 비효율적입니다.

## ✨ Key Contributions
*   **AdaptiveSAM 제안**: 수술 데이터셋을 위한 텍스트 프롬프트 기반 분할 방법으로, 전문가의 개입 없이도 높은 정밀도의 결과를 제공합니다.
*   **바이어스 튜닝(Bias-tuning) 학습 전략 개발**: SAM과 같은 기반 모델을 최소한의 훈련과 자원으로 특정 작업에 적응시킬 수 있는 효율적인 전략입니다. AdaptiveSAM에 적용 시 SAM 훈련 매개변수의 2% 미만만 미세 조정하여 단일 GPU로 훈련 가능합니다.
*   **최신 기술(SOTA) 성능 달성**: Endovis17, Endovis18, Cholec-Seg8k 등 세 가지 공개 수술 장면 분할 데이터셋에서 SOTA 결과를 달성했으며, 초음파 및 X-레이와 같은 비수술 데이터셋에서도 AdaptiveSAM의 일반화 가능성을 입증했습니다.

## 📎 Related Works
*   **전통적인 의료 영상 분할**: UNet 및 그 변형 모델([3]-[7])과 Mask RCNN([17])과 같은 CNN 기반 방법이 주로 사용되었으며, 최근에는 트랜스포머 기반 모델([21], [22])도 등장했습니다. 이들 방법은 새로운 데이터셋마다 많은 매개변수를 훈련해야 하는 한계가 있습니다.
*   **SAM의 의료 분야 적용**: SAM은 프롬프트 기반 분할을 위해 출시된 기반 모델로, MedSAM([14])은 SAM 디코더만 미세 조정하고, Medical SAM Adapter([15])는 이미지 및 프롬프트 인코더에 LoRA(Low-Rank Adaptation) 레이어를 추가합니다. AutoSAM([12])과 [13]은 인코더를 고정하고 CNN 기반 예측 헤드 또는 프롬프트 레이어를 추가합니다. 하지만 이 모든 방법은 바운딩 박스나 점과 같은 수동 프롬프트에 의존해야 합니다.
*   **매개변수 효율적인 미세 조정**: NLP 분야에서 대규모 언어 모델 미세 조정을 위해 바이어스(bias) 매개변수만 튜닝하는 BitFit([16])과 같은 접근 방식이 연구되었습니다.

## 🛠️ Methodology
AdaptiveSAM은 SAM의 이미지 인코더, 프롬프트 인코더, 마스크 디코더의 사전 훈련된 가중치를 그대로 사용하면서 효율적으로 의료 데이터에 적응하도록 설계되었습니다.

1.  **텍스트 어파인 레이어 (Text Affine Layer, TAL) 도입**:
    *   CLIP은 일반 데이터로 훈련되어 의료 용어에 대한 의미 있는 임베딩을 제공하지 못합니다.
    *   이를 극복하기 위해 CLIP 임베딩을 입력받아 의료 관련 텍스트 임베딩으로 정제하는 경량의 어파인 변환 레이어(TAL)를 추가합니다.
    *   TAL의 연산은 다음과 같습니다:
        $$
        y = \text{BatchNorm}(\text{ReLU}(W_{TAL}^T X + b_{TAL}))
        $$
        여기서 $X$는 입력 CLIP 임베딩, $y$는 변환된 임베딩, $W_{TAL}$과 $b_{TAL}$은 TAL의 가중치와 바이어스입니다.
    *   이미지는 SAM의 이미지 인코더로, 텍스트는 CLIP을 거쳐 TAL로 인코딩된 후 SAM의 프롬프트 인코더로 전달됩니다. 이 두 인코딩된 출력은 SAM의 마스크 디코더에서 융합되어 최종 마스크를 생성합니다.

2.  **바이어스 튜닝(Bias-tuning) 훈련 전략**:
    *   SAM의 인코더는 일반 객체를 분할하는 데 필요한 일반적인 가중치 행렬을 이미 학습했습니다.
    *   새로운 의료 영상 컨텍스트에 적응하기 위해 SAM 인코더 내의 어파인 변환 출력에 학습 가능한 **시프트 매개변수 $\bar{b}$**를 추가합니다.
    *   이는 쿼리, 키, 값 생성 및 MLP 레이어에 적용됩니다:
        $$
        qkv = (W_{n}^{qkv})^T x + b_{n}^{k} + \bar{b}_{n}^{qkv}
        $$
        $$
        o_{n} = (W_{n}^{MLP})^T x + b_{n}^{MLP} + \bar{b}_{n}^{MLP}
        $$
        여기서 $x$는 입력 이미지 패치, $W$와 $b$는 기존 가중치와 바이어스, $n$은 레이어 인덱스, $\bar{b}$는 학습 가능한 시프트 매개변수입니다.
    *   훈련 중 SAM의 모든 다른 가중치와 바이어스는 고정하고, 오직 시프트 매개변수 $\bar{b}$와 LayerNorm 레이어만 훈련합니다. 이들은 0으로 초기화되고 낮은 학습률로 훈련됩니다.
    *   디코더는 의료 텍스트 프롬프트와 수술 이미지 임베딩을 올바르게 융합하도록 완전히 훈련합니다.
    *   전체 학습 가능한 매개변수 수는 SAM의 훈련 매개변수의 2% 미만입니다.

3.  **'빈 라벨(Blank Labels)'을 사용한 훈련**:
    *   의료 분야에서는 객체가 이미지에 없을 때 모델이 빈 마스크를 생성하는 것이 중요합니다.
    *   이를 위해 각 이미지에 대해 가능한 모든 관심 객체에 대한 이미지-텍스트 쌍을 생성하고, 해당 객체가 없으면 Ground Truth 마스크를 비워둡니다.
    *   이로 인한 데이터 불균형을 완화하기 위해 다음과 같은 포컬 손실(Focal Loss)을 사용합니다:
        $$
        L_{focal} = \frac{1}{B} \sum_{i=1}^{B} \sum_{j=1}^{HW} L_{ij}
        $$
        $$
        L_{ij} = \begin{cases}
        -\alpha(1-p)^\gamma \log(p), & \text{if } y=1 \\
        -(1-\alpha)(p)^\gamma \log(1-p), & \text{if } y=0.
        \end{cases}
        $$
        여기서 $p$는 예측 확률, $y$는 실제 라벨, $\alpha$와 $\gamma$는 하이퍼파라미터입니다. 이 손실은 훈련 중 사소한 예측에 주어지는 가중치를 줄여 모델이 항상 빈 마스크를 생성하는 것을 방지합니다.

## 📊 Results
AdaptiveSAM은 다양한 의료 영상 데이터셋에서 기존 SOTA 방법들을 능가하는 성능을 보였습니다.

*   **Endovis 17 (EV17)**: 기존 UNet, TransUNet, Med-T보다 현저히 높은 Dice 계수(DSC) 및 IoU를 달성했습니다. SAM의 제로샷(SAM-ZS) 성능 대비 DSC에서 68%의 큰 폭으로 성능이 향상되었습니다.
*   **Endovis 18 (EV18)**: 기존 SOTA 방법들 대비 DSC에서 8%, IoU에서 14% 향상을 보였습니다. SAM-ZS 대비 DSC에서 21% 향상되었습니다.
*   **Cholec-Seg8k**: 모든 클래스에서 평균적으로 SOTA 방법 대비 2% 향상을 보였습니다. SAM-ZS 대비 DSC에서 60% 향상되었습니다. 특히 희귀 클래스(Blood, Hepatic Vein 등)를 개별적으로 처리하며 뛰어난 성능을 보였습니다.
*   **비수술 데이터셋으로의 일반화**:
    *   **복부 초음파 데이터셋**: 합성 테스트셋에서 0.48, 실제 테스트셋에서 0.58의 평균 DSC를 기록하며 다른 방법들을 크게 앞섰습니다.
    *   **X-ray (ChestXDet) 데이터셋**: 데이터셋의 불균형을 더 잘 처리하여 다른 방법들보다 훨씬 높은 성능을 달성했습니다 (평균 DSC 0.83).
*   **정성적 평가**: AdaptiveSAM은 다양한 텍스트 프롬프트에 대해 매우 정밀하고 노이즈가 적은 마스크를 생성합니다. 객체가 이미지에 없을 때 올바르게 빈 마스크를 출력하는 능력을 보였습니다.
*   **위치 학습 능력**: "Left Large Needle Driver"와 같은 텍스트 프롬프트를 통해 이미지 내 특정 위치의 객체를 분할할 수 있는 능력을 보여주었으며, 해당 객체가 없으면 빈 마스크를 반환합니다.

## 🧠 Insights & Discussion
AdaptiveSAM은 SAM의 강력한 일반화 능력을 활용하면서도 의료 데이터의 특수성에 효과적으로 적응할 수 있음을 입증했습니다. 핵심적인 통찰은 다음과 같습니다.

*   **효율적인 적응**: '바이어스 튜닝' 전략을 통해 SAM의 방대한 매개변수 중 극히 일부(2% 미만)만 훈련하면서도, 자연 영상과 의료 영상 간의 도메인 시프트에 성공적으로 대응합니다. 이는 단일 GPU로도 훈련이 가능하여 리소스 제약을 크게 줄입니다.
*   **전문가 개입 최소화**: 기존 SAM 적용 방법들이 이미지별 바운딩 박스나 점과 같은 수동 프롬프트를 요구하는 반면, AdaptiveSAM은 '강제 겸자'와 같은 자유 형식 텍스트 프롬프트만 필요로 하므로, 대규모 데이터셋에 대한 배포가 훨씬 용이합니다. 이는 의료 전문가의 부담을 크게 줄이는 실용적인 장점입니다.
*   **의료 특화 학습**: CLIP 임베딩을 의료 용어에 맞게 정제하는 텍스트 어파인 레이어(TAL)와 '빈 라벨' 훈련 전략, 포컬 손실의 조합은 모델이 의료 영상의 클래스 불균형과 엄격한 정밀도 요구사항을 더 잘 처리할 수 있게 합니다.
*   **일반화 및 확장 가능성**: 수술 장면뿐만 아니라 초음파, X-레이와 같은 다른 의료 모달리티에도 성공적으로 적용될 수 있음을 보여줌으로써, AdaptiveSAM의 광범위한 활용 가능성을 시사합니다. 또한 텍스트 프롬프트 입력은 미래에 더 복잡한 질의(예: "왼쪽 큰 바늘 운반기")를 이해하는 모델로의 확장을 가능하게 합니다.
*   **제한 사항**: 현재 모델은 텍스트 프롬프트를 사용하므로, 텍스트 라벨이 명확하지 않은 미분화된 병변 등에는 한계가 있을 수 있습니다.

## 📌 TL;DR
수술 장면 분할은 데이터 부족과 SAM의 비효율적인 미세 조정 및 수동 프롬프트 문제에 직면합니다. 본 논문은 이러한 문제를 해결하기 위해 SAM의 텍스트 프롬프트 기반 분할 기능을 의료 데이터에 적응시키는 **AdaptiveSAM**을 제안합니다. 핵심은 SAM 매개변수의 2% 미만만 훈련하는 효율적인 **바이어스 튜닝** 전략과 의료 용어 이해를 위한 **텍스트 어파인 레이어(TAL)**입니다. AdaptiveSAM은 전문가의 수동 개입 없이 자유 형식 텍스트 프롬프트로 정밀한 분할을 수행하며, 수술 및 비수술 의료 데이터셋 전반에서 기존 SOTA 방법들을 능가하는 성능과 일반화 가능성을 입증했습니다.