# 2017 Robotic Instrument Segmentation Challenge

M. Allan, A. Shvets, T. Kurmann, Z. Zhang, R. Duggal, Y.H. Su, N. Rieke, I. Laina, N. Kalavakonda, S. Bodenstedt, L.C. Garcia-Peraza-Herrera, W. Li, V. Iglovikov, H. Luo, J. Yang, D. Stoyanov, L. Maier-Hein, S. Speidel, M. Azizian

## 🧩 Problem to Solve

주류 컴퓨터 비전 분야와 달리, 로봇 보조 수술 분야에서는 알고리즘 성능 비교 및 개선을 위한 공통 데이터셋과 벤치마킹 방법이 부족합니다. 2015년 EndoVis 워크숍에서 로봇 전방 운동학을 통해 자동으로 생성된 주석을 포함한 데이터셋이 제공되었으나, 배경 변화의 제한, 복잡한 움직임의 부족, 주석의 부정확성 등의 문제가 있었습니다. 정확한 수술 도구 분할(segmentation)은 수술 중 보조 시스템(예: 도구 추적, 증강 현실 오버레이 마스킹) 개발에 필수적이며, 이를 위한 고품질 데이터셋이 절실히 요구됩니다.

## ✨ Key Contributions

- 다빈치 로봇 수술 도구의 이진, 부품 및 유형 기반 분할에 대한 2017년 로봇 도구 분할 챌린지의 결과를 발표했습니다.
- 수동으로 주석을 단 고품질의 다빈치 로봇 수술 도구 이미지 데이터셋을 제공하여 이전 챌린지의 한계를 개선했습니다. 이 데이터셋은 10가지 개별 수술 절차를 통해 배경 다양성을 높였으며, 더 세분화된 도구 부품 및 유형 레이블을 포함합니다.
- 10개 팀의 참여를 통해 다양한 심층 학습 기반 분할 알고리즘의 성능을 벤치마킹하고 비교할 수 있는 기회를 제공했습니다.
- 현재 심층 학습 기반 방법론의 강점과 한계점(예: 드문 도구 유형, 복잡한 조명, 연기 발생 시 성능 저하)을 식별하고 논의했습니다.

## 📎 Related Works

- **일반 컴퓨터 비전 데이터셋:** ImageNet [1], COCO [2], KITTI [3] 등은 주류 컴퓨터 비전 분야의 발전을 이끈 데이터셋으로 언급됩니다.
- **이전 수술 비전 챌린지:** 2015년 EndoVis 기기 분할 하위 챌린지 [1]는 로봇 전방 운동학(dVRK [13])을 통해 자동 생성된 부정확한 주석의 한계를 가졌습니다.
- **고전적 기계 학습 기법:** Naïve Bayes [5] 및 SVM (Support Vector Machine) [6]과 같은 방법이 도구 분할에 사용되었습니다.
- **심층 콘볼루션 신경망 (CNNs):** 최근 도구 분할의 지배적인 방법론으로 [7]–[9], [11], [12]에서 성능 향상을 보였습니다.
  - **FCN (Fully Convolutional Network) [23]:** 픽셀 단위 분할을 위한 핵심 기반 구조입니다.
  - **U-Net [14]:** FCN을 개선한 형태로, 스킵 연결(skip connection)을 사용하여 정밀한 픽셀 수준의 위치 파악을 가능하게 합니다.
  - **사전 학습된 인코더:** VGG [16] (TernausNet [17], [28]), ResNet [21] 등을 인코더로 사용하는 U-Net 기반 아키텍처가 흔히 사용됩니다.
- **기타 특정 방법론:** SegNet [18], ToolNet [11], CSL (Concurrent Segmentation and Localization) [7], FCRN (Fully Convolutional Residual Network) [20], CRF (Conditional Random Field) [22] 등 다양한 네트워크 및 후처리 기법이 논의되었습니다.

## 🛠️ Methodology

- **챌린지 개요:**
  - **이진 도구 분할:** 다빈치 Xi 도구와 배경(초음파 프로브, 수술용 클립, 돼지 조직 등)으로 분류합니다.
  - **도구 부품 분할:** 이진 도구 레이블을 샤프트(shaft), 손목(wrist), 집게(jaws)로 분할합니다.
  - **도구 유형 분할:** Large Needle Driver, Prograsp Forceps, Monopolar Curved Scissors 등 7가지 도구 유형을 식별합니다.
- **데이터 수집:**
  - 다빈치 Xi 시스템을 사용하여 복부 돼지 수술 절차에서 기록된 10개의 영상 시퀀스로 구성됩니다.
  - 각 절차에서 활성 시퀀스(도구 움직임 및 가시성이 높은)를 선택하여 1Hz로 300프레임을 샘플링했습니다.
  - 스테레오 카메라의 왼쪽/오른쪽 영상이 모두 제공되었으나, 주석은 왼쪽 영상에 대해서만 수행되었습니다.
  - 8개 시퀀스는 처음 225프레임을 훈련 데이터로, 마지막 75프레임을 테스트 데이터로 사용했습니다. 2개 시퀀스는 전체(300프레임)를 테스트 데이터로 사용했습니다.
  - 공개된 비수술 데이터로 사전 학습된 CNN 사용은 허용되었으나, 외부 수술 데이터로 데이터셋을 증강하는 것은 금지되었습니다.
- **데이터 라벨링:**
  - Intuitive Surgical의 전담 분할 팀이 오픈 소스 소프트웨어 Viame를 사용하여 프레임별 다각형 생성 방식으로 수동 라벨링을 수행했습니다.
  - 객체별로 별도의 주석 이미지를 사용하여 인스턴스 수준으로 라벨링했습니다.
  - **제한점:** 주석 도구가 다각형 내 구멍 생성을 허용하지 않아 집게 부분의 구멍 등이 정확히 라벨링되지 못했습니다.
- **참가 팀 방법론 (주요 특징):**
  - 대부분의 팀은 **U-Net [14]** 및 **FCN [23]** 기반의 인코더-디코더 구조를 가진 심층 학습 모델을 사용했습니다.
  - 많은 팀이 ImageNet으로 **사전 학습된 인코더**(VGG11/16 [16], ResNet-50 [21] 등)를 활용했습니다.
  - **데이터 증강:** 수평/수직 뒤집기, 확대/축소, 회전, 색상/대비 변경 등 일반적인 공간적/사진측량적 증강 기법을 사용했습니다. 뮌헨 공과대학교(TUM) 팀은 계측기 표면의 반사를 모방하는 독특한 증강 기법을 적용했습니다.
  - **손실 함수:** 범주형 교차 엔트로피(categorical cross entropy), Dice loss, IoU (Intersection-over-Union) 손실이 사용되었습니다.
    - SIAT 팀은 클래스 불균형 처리를 위해 가중치 보상 전략을 사용했습니다:
      $$ \sigma*i(z) = \frac{k_i \exp(z_i)}{\sum*{j=1}^{m} \exp(z_j)}, \quad i=1,...,m $$
            여기서 $k_i$는 클래스 가중치를 나타냅니다.
    - UCL 팀은 다중 스케일 손실을 집계하는 ToolNet [11]을 사용했습니다:
      $$ \hat{y}^{{(\bar{s})}}(z,\theta) = \sum*{j=1}^{M} w_j \hat{y}^{{(s_j)}}(z,\theta) $$
            $$ L*{\text{{MSIoU}}}(y,z,\theta) = \bar{\lambda}L*{\text{{IoU}}}(\hat{y}^{{(\bar{s})}},(z,\theta),y) + \sum*{j=1}^{M} \lambda*j L*{\text{{IoU}}}(\hat{y}^{{(s_j)}},(z,\theta),y) $$
  - **최적화 기법:** Adam, SGD(Stochastic Gradient Descent) 등이 사용되었습니다.
  - **후처리:** 조건부 랜덤 필드(CRF) [22] (Delhi), 작은 연결 영역 제거 및 경계 제약 적용 (Alberta, TUM) 등이 활용되었습니다.
  - **비(非)심층 학습 접근 방식:** 워싱턴 대학교(UW) 팀은 기계 학습 없이 전통적인 컴퓨터 비전 기법(색상 필터링, GrabCut [26], 이미지 특징 및 흐릿함 평가)을 사용하여 도구 분할을 시도했습니다.

## 📊 Results

- **평가 기준:** 모든 클래스에 대한 평균 IoU (Intersection-over-Union)를 사용했습니다. IoU는 다음과 같이 정의됩니다.
  $$ \text{IOU}=\text{TP}/(\text{TP}+\text{FP}+\text{FN}) $$
    여기서 $\text{TP}$는 참 양성, $\text{FP}$는 거짓 양성, $\text{FN}$는 거짓 음성 예측의 수를 나타냅니다.
- **이진 분할:**
  - 가장 높은 평균 IoU는 MIT 팀 (0.888)이 달성했습니다.
  - 전체 10개 팀 중 5개 팀이 평균 IoU 0.8 이상을 기록하며 높은 성능을 보였습니다.
  - 데이터셋 1과 7(초음파 프로브 및 복잡한 조명)은 도전적인 환경이었습니다.
- **부품 분할:**
  - TUM 팀이 가장 높은 평균 IoU (0.751)를 달성했습니다. MIT 팀은 개별 데이터셋에서 7번 최고 점수를 기록했습니다.
  - Vessel Sealer 도구의 샤프트(shaft) 부분에서 불일치 라벨링으로 인해 문제가 발생했습니다.
- **유형 분할:**
  - 난이도가 크게 증가하여 6개 팀만 참여했습니다.
  - MIT 팀이 가장 높은 평균 IoU (0.542)를 달성했습니다.
  - 정확한 분할 마스크를 생성했음에도 불구하고 도구 유형 식별에서는 많은 도구가 부분적 또는 완전히 오분류되는 등 성능이 크게 저하되었습니다.
- **일반적인 결과:** 심층 학습 기반 방법(특히 사전 학습된 인코더와 적절한 데이터 증강을 활용한 모델)이 전통적인 컴퓨터 비전 접근 방식보다 우수한 성능을 보였습니다.

## 🧠 Insights & Discussion

- **CNN의 효율성:** FCN 및 U-Net과 같은 심층 신경망은 픽셀 단위 분할에 매우 효과적이며, 스킵 연결(U-Net)은 정밀한 픽셀 수준 위치 파악에 중요합니다. ImageNet에서 사전 학습된 인코더를 사용하는 것이 성능 향상에 기여했습니다.
- **데이터 증강의 중요성:** 훈련 데이터셋의 크기가 작고 시퀀스 데이터의 상관 관계가 높기 때문에 과적합을 줄이고 일반화 성능을 향상시키는 데 필수적입니다. 공간적 증강(뒤집기, 회전)이 일반적이었으며, 사진측량적 증강(색상, 대비)도 중요했습니다. TUM 팀의 반사 증강 기법은 특히 흥미로운 접근 방식이었습니다.
- **챌린지 설계의 한계:**
  - **데이터셋 크기:** 총 3000프레임(훈련용 1800프레임)으로, COCO 같은 대규모 데이터셋에 비해 작아 과적합 위험이 높고 일반화 테스트에 한계가 있었습니다.
  - **절차 영상 소스 다양성 부족:** 제한된 수의 수술 절차 영상에서 데이터를 샘플링하여 랭킹에 편향이 발생하고 도구 유형 분포가 실제 사용 빈도를 반영하지 못했습니다.
  - **라벨링 오류:** 주석자의 부주의, 빠른 도구 움직임으로 인한 이미지 흐림, 주석 도구의 한계(예: 집게의 구멍 라벨링 불가)로 인해 불일치가 발생했습니다.
  - **계산 부담:** 10개 데이터셋에 대해 9개의 개별 모델을 훈련해야 하는 요구 사항은 참가 팀에 과도한 계산 부담을 주어 적절한 실험을 방해했습니다.
- **향후 챌린지 방향:**
  - 각 데이터셋당 프레임 수를 줄이고, 개별 절차의 수를 늘려 데이터 다양성을 확보해야 합니다.
  - 훈련 및 테스트 세트를 완전히 분리하여 단일 모델 평가를 가능하게 해야 합니다.
  - 도구뿐만 아니라 해부학적 구조 및 기타 장치(클립, 실, 흡입/관류 도구 등)까지 포함하는 전체 장면의 밀집 분할(dense segmentation)을 목표로 해야 합니다 (2018년 후속 데이터셋에서 시도됨). 이는 수술 의사에게 더 많은 상황 인식 지원을 제공할 잠재력이 있습니다.

## 📌 TL;DR

수술 도구 분할을 위한 표준 데이터셋과 벤치마킹 방법의 부족을 해결하고자, 2017년 로봇 도구 분할 챌린지에서는 **수동으로 주석된 고품질의 다빈치 로봇 수술 도구 데이터셋**을 제공했습니다. 이 데이터셋은 이진, 부품, 유형별 분할 작업을 포함하며, 10개 팀이 주로 사전 학습된 인코더와 다양한 데이터 증강 기법을 활용한 **CNN 기반 모델(U-Net/FCN 변형)**을 사용하여 성능을 겨루었습니다. 결과적으로 심층 학습 모델이 이진 및 부품 분할에서 높은 성능을 보였으나, 도구 유형 분할은 훨씬 더 어려웠습니다. 연구는 데이터셋 크기의 한계, 라벨링 불일치, 특정 시각적 아티팩트(연기, 반사)의 문제를 지적하며, 미래에는 더욱 다양하고 포괄적으로 주석된 데이터셋의 필요성을 강조합니다.
