{
  "url": "http://arxiv.org/abs/2207.10892v1",
  "title": "Bi-directional Contrastive Learning for Domain Adaptive Semantic\n  Segmentation",
  "authors": "Geon Lee, Chanho Eom, Wonkyung Lee, Hyekang Park, Bumsub Ham",
  "year": 2022,
  "abstract": "We present a novel unsupervised domain adaptation method for semantic\nsegmentation that generalizes a model trained with source images and\ncorresponding ground-truth labels to a target domain. A key to domain adaptive\nsemantic segmentation is to learn domain-invariant and discriminative features\nwithout target ground-truth labels. To this end, we propose a bi-directional\npixel-prototype contrastive learning framework that minimizes intra-class\nvariations of features for the same object class, while maximizing inter-class\nvariations for different ones, regardless of domains. Specifically, our\nframework aligns pixel-level features and a prototype of the same object class\nin target and source images (i.e., positive pairs), respectively, sets them\napart for different classes (i.e., negative pairs), and performs the alignment\nand separation processes toward the other direction with pixel-level features\nin the source image and a prototype in the target image. The cross-domain\nmatching encourages domain-invariant feature representations, while the\nbidirectional pixel-prototype correspondences aggregate features for the same\nobject class, providing discriminative features. To establish training pairs\nfor contrastive learning, we propose to generate dynamic pseudo labels of\ntarget images using a non-parametric label transfer, that is, pixel-prototype\ncorrespondences across different domains. We also present a calibration method\ncompensating class-wise domain biases of prototypes gradually during training.",
  "citation": 31
}