{
  "url": "http://arxiv.org/abs/2403.10780v1",
  "title": "Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy\n  for Multi-Class Multi-Instance Segmentation",
  "authors": "Mariia Khan, Yue Qiu, Yuren Cong, Jumana Abu-Khalaf, David Suter, Bodo Rosenhahn",
  "year": 2024,
  "abstract": "Multi-class multi-instance segmentation is the task of identifying masks for\nmultiple object classes and multiple instances of the same class within an\nimage. The foundational Segment Anything Model (SAM) is designed for promptable\nmulti-class multi-instance segmentation but tends to output part or sub-part\nmasks in the \"everything\" mode for various real-world applications. Whole\nobject segmentation masks play a crucial role for indoor scene understanding,\nespecially in robotics applications. We propose a new domain invariant\nReal-to-Simulation (Real-Sim) fine-tuning strategy for SAM. We use object\nimages and ground truth data collected from Ai2Thor simulator during\nfine-tuning (real-to-sim). To allow our Segment Any Object Model (SAOM) to work\nin the \"everything\" mode, we propose the novel nearest neighbour assignment\nmethod, updating point embeddings for each ground-truth mask. SAOM is evaluated\non our own dataset collected from Ai2Thor simulator. SAOM significantly\nimproves on SAM, with a 28% increase in mIoU and a 25% increase in mAcc for 54\nfrequently-seen indoor object classes. Moreover, our Real-to-Simulation\nfine-tuning strategy demonstrates promising generalization performance in real\nenvironments without being trained on the real-world data (sim-to-real). The\ndataset and the code will be released after publication.",
  "citation": 4
}