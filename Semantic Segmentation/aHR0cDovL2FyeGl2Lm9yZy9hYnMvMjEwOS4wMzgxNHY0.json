{
  "title": "Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with\n  Transformers",
  "authors": "Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo, Tong Lu",
  "year": 2021,
  "url": "http://arxiv.org/abs/2109.03814v4",
  "abstract": "Panoptic segmentation involves a combination of joint semantic segmentation\nand instance segmentation, where image contents are divided into two types:\nthings and stuff. We present Panoptic SegFormer, a general framework for\npanoptic segmentation with transformers. It contains three innovative\ncomponents: an efficient deeply-supervised mask decoder, a query decoupling\nstrategy, and an improved post-processing method. We also use Deformable DETR\nto efficiently process multi-scale features, which is a fast and efficient\nversion of DETR. Specifically, we supervise the attention modules in the mask\ndecoder in a layer-wise manner. This deep supervision strategy lets the\nattention modules quickly focus on meaningful semantic regions. It improves\nperformance and reduces the number of required training epochs by half compared\nto Deformable DETR. Our query decoupling strategy decouples the\nresponsibilities of the query set and avoids mutual interference between things\nand stuff. In addition, our post-processing strategy improves performance\nwithout additional costs by jointly considering classification and segmentation\nqualities to resolve conflicting mask overlaps. Our approach increases the\naccuracy 6.2\\% PQ over the baseline DETR model. Panoptic SegFormer achieves\nstate-of-the-art results on COCO test-dev with 56.2\\% PQ. It also shows\nstronger zero-shot robustness over existing methods. The code is released at\n\\url{https://github.com/zhiqi-li/Panoptic-SegFormer}.",
  "citation": 193
}