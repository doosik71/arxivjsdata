# One-Shot Video Object Segmentation
S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taixé, D. Cremers, L. Van Gool

## 🧩 Problem to Solve
이 논문은 비디오 객체 분할(Video Object Segmentation, VOS) 중에서도 준지도 학습(semi-supervised) 상황을 다룹니다. 특히, 비디오의 첫 프레임에 주어진 객체의 마스크(mask) 정보만을 사용하여 해당 객체를 비디오의 나머지 프레임에서 배경과 분리하는 문제를 해결하는 것을 목표로 합니다. 이는 '원샷(one-shot)' 학습 상황에 해당하며, 딥러닝 모델이 극히 제한된 초기 레이블 데이터만으로 특정 객체의 외형을 학습하고 비디오 전체에서 추적 및 분할하는 것이 가능한지에 대한 질문에 답하고자 합니다.

## ✨ Key Contributions
*   **원샷 학습 기반 CNN 적응:** ImageNet에서 학습된 일반적인 의미 정보를 시작으로, DAVIS 훈련 세트에서 객체의 일반적인 형태를 학습한 후, 최종적으로 테스트 시 특정 비디오의 첫 프레임에 주석이 달린 단일 객체 인스턴스에 네트워크를 미세 조정(fine-tune)하여 비디오 객체 분할을 수행하는 CNN 아키텍처(OSVOS)를 제안합니다.
*   **독립적인 프레임 처리:** OSVOS는 각 프레임을 독립적으로 처리하여 시간적 일관성을 명시적으로 강제하지 않음에도 불구하고 안정적이고 일관된 분할 결과를 얻습니다. 이는 기존의 광학 흐름(optical flow)이나 시간적 제약에 의존하는 방식과 대조되며, 오류 전파를 방지하고, 움직임 범위의 제약이 없으며, 순차적 처리 없이도 부분적인 가려짐(occlusion)을 처리할 수 있는 장점을 가집니다.
*   **속도-정확도 트레이드오프 조절:** 사용자가 미세 조정 수준을 조절하여 속도와 정확도 사이의 균형을 선택할 수 있도록 합니다. 또한, 사용자가 추가 프레임에 주석을 달면(예: 만족스럽지 않은 프레임) OSVOS가 이를 학습하여 점진적으로 결과를 개선할 수 있음을 보여줍니다.
*   **FCN 기반 비디오 분할 최초 적용:** 완전 합성곱 신경망(Fully Convolutional Network, FCN)을 비디오 객체 분할 작업에 적용한 최초의 연구 중 하나로, 깊은 레이어의 거친 예측 문제를 해결하기 위해 스킵 연결(skip connection) 및 학습 가능한 필터를 활용합니다.
*   **최첨단 성능 달성:** DAVIS 및 Youtube-Objects 데이터셋에서 기존 최첨단 방법 대비 획기적인 성능 향상(DAVIS Jaccard Measure 기준 79.8% vs 68.0%)과 빠른 처리 속도(480p 프레임당 102ms)를 입증합니다.

## 📎 Related Works
*   **비디오 객체 분할 및 추적:** 대부분의 기존 준지도 비디오 객체 분할 연구들은 초기 마스크를 다음 프레임으로 전파하기 위해 시간적 일관성(temporal consistency)을 강조합니다. 슈퍼픽셀(superpixel), 패치(patch), 객체 제안(object proposal) 등을 사용하며, 일부는 광학 흐름(optical flow) 계산을 포함하여 속도를 저해합니다. 최근 딥러닝 기반 접근법으로는 MaskTrack [22]이나 [21]이 있지만, OSVOS는 더 단순한 파이프라인으로 각 프레임을 독립적으로 분할하면서 더 정확하고 빠른 결과를 제공합니다. 시각적 추적(visual tracking) 분야에서는 MDNET [32]과 같이 CNN을 사용하여 객체 표현을 학습하는 방식이 있지만, OSVOS는 단일 모델을 학습하여 나머지 프레임을 분할합니다.
*   **분할을 위한 FCNs:** 이미지 인식 분야의 CNN 발전이 분할 연구에도 영향을 미쳤습니다. [12]와 [27]에서 FCN이 밀도 높은 예측(dense prediction)을 위해 도입되었으며, 마지막 완전 연결 레이어를 $1 \times 1$ 합성곱으로 변경하여 임의 크기의 이미지에 대한 예측이 가능하게 했습니다. FCN은 파라미터 수가 적어 적은 레이블 데이터로도 훈련이 용이합니다. 깊은 레이어의 공간적 정확도 손실 문제는 스킵 연결 [27, 18, 51, 30]이나 역합성곱 레이어 [34, 52]를 통해 해결됩니다. 이 연구는 FCN을 비디오 분할에 활용하여 제한된 주석으로 정확한 지역화된 예측을 달성합니다.

## 🛠️ Methodology
OSVOS는 비디오 객체 분할을 위해 FCN 기반의 이진 분류 모델을 사용하며, 인간의 점진적인 학습 방식에서 영감을 받은 2단계 훈련 전략을 채택합니다.

1.  **엔드투엔드 훈련 가능 전경 FCN:**
    *   **아키텍처:** [30]의 CNN 아키텍처를 기반으로 VGG [47] 네트워크를 수정하여 사용합니다. 정확한 지역화된 밀도 예측을 위해 VGG의 5개 단계 각각의 마지막 레이어(풀링 전)에서 별도의 스킵 경로를 형성하고, 필요에 따라 업스케일링(upscaling)을 수행하여 다양한 상세 수준의 정보를 통합합니다. 최종적으로 입력 이미지와 동일한 차원의 단일 출력으로 선형적으로 융합합니다. 완전 연결 레이어를 제거하여 파라미터 수를 줄이고 효율적인 추론을 가능하게 합니다.
    *   **손실 함수:** 픽셀 단위 이진 분류를 위한 교차 엔트로피 손실 함수를 사용합니다. 두 이진 클래스 간의 불균형을 처리하기 위해 가중치 $\beta = |Y^-| / |Y|$를 적용한 수정된 손실 함수 $L_{mod}$를 사용합니다:
        $$L_{mod} = -\beta \sum_{j \in Y^+} \log P(y_j=1|X) - (1-\beta) \sum_{j \in Y^-} \log P(y_j=0|X)$$
        여기서 $Y^+$는 전경 픽셀, $Y^-$는 배경 픽셀, $X$는 입력 이미지, $y_j$는 픽셀 레이블입니다.

2.  **훈련 상세:**
    *   **오프라인 훈련 (부모 네트워크):**
        *   ImageNet [44]에서 이미지 레이블링을 위해 사전 훈련된 VGG "기본 네트워크"를 사용합니다.
        *   이후 DAVIS [37] 훈련 세트의 이진 마스크를 사용하여 네트워크를 추가로 훈련하여 객체를 배경으로부터 분할하는 일반적인 개념과 형태를 학습합니다.
        *   SGD(모멘텀 0.9)를 50,000회 반복, 학습률 $10^{-8}$로 설정하고 데이터 증강(미러링, 확대)을 적용합니다. 이 단계를 거친 네트워크를 "부모 네트워크"라고 합니다.
    *   **온라인 훈련/테스트 (테스트 네트워크):**
        *   부모 네트워크를 특정 비디오의 첫 프레임(이미지 및 분할 마스크)에 대해 추가로 미세 조정(fine-tuning)합니다.
        *   미세 조정된 네트워크 가중치를 사용하여 비디오의 나머지 모든 프레임을 독립적으로 분할합니다.
        *   미세 조정 시간에 따라 품질과 속도 간의 트레이드오프가 발생합니다 (10초~10분).

3.  **경계선 스내핑 (Contour Snapping):**
    *   FCN의 거친 예측으로 인한 윤곽선 지역화의 부정확성을 개선하기 위한 두 가지 전략:
        1.  **Fast Bilateral Solver (FBS) [2] 사용:** 전경 예측 결과를 이미지 경계선에 맞추는 방식으로, 5차원 색상-위치 공간에서 가우시안 스무딩을 수행하여 이미지의 경계를 보존하면서 입력 신호를 부드럽게 합니다. 빠르고(프레임당 약 60ms) 미분 가능합니다.
        2.  **두 스트림 FCN 아키텍처:**
            *   주요 전경 분할 스트림 외에, 객체 윤곽선 감지를 위해 오프라인으로 훈련된 보완적인 "윤곽선 스트림" CNN을 추가합니다 (PASCAL-Context [31] 데이터셋 사용). 이 스트림은 장면의 모든 윤곽선을 감지하며, 특정 객체에 대한 미세 조정이 필요 없어 온라인 타이밍에 영향을 주지 않습니다.
            *   경계선 스내핑 단계에서는 계산된 윤곽선에 맞춰 슈퍼픽셀(superpixel)을 형성하고(Ultrametric Contour Map [1, 40] 활용), 전경 마스크와 50% 이상 겹치는 슈퍼픽셀을 다수결 투표로 선택하여 최종 분할을 생성합니다. 이 방식은 더 정확하지만 느립니다(프레임당 400ms).
    *   두 정제 과정은 모듈식으로, 요구사항에 따라 선택적으로 사용할 수 있습니다.

## 📊 Results
*   **DAVIS 데이터셋 성능:**
    *   OSVOS는 Jaccard Measure (J) 79.8%, Contour Accuracy (F) 80.6%로 기존 최첨단 방법(OFL의 J 68.0%)을 크게 능가했습니다.
    *   객체 제안 기술(COB) 중 최상의 것을 선택하는 오라클보다도 우수했으며, 최상의 슈퍼픽셀 세트(COB|SP)를 선택하는 오라클에 비해 불과 6.7%p 낮았습니다.
*   **어블레이션 연구 (Ablation Study):**
    *   부모 네트워크 훈련(-PN)이 없으면 J 점수가 15.2p 하락하고, 원샷 학습(-OS)이 없으면 27.3p 하락했습니다. 둘 다 없으면(ImageNet만 사용) J는 17.6%로 무작위 수준이었습니다. 이는 두 훈련 단계 모두 핵심적임을 보여줍니다.
    *   경계선 스내핑(-BS)은 J 점수를 2.4p 향상시키며, 주로 오분류(False Positives)를 줄이는 데 기여했습니다.
*   **도전 속성별 성능:** DAVIS 데이터셋의 다양한 속성(예: 외형 변화, 모션 블러, 가려짐)에 대해 모든 속성에서 최고의 성능을 보였으며, 이러한 도전 과제가 있을 때 성능 저하가 가장 작아 높은 강건성을 입증했습니다.
*   **훈련 데이터 양:** 부모 네트워크 훈련에 DAVIS 훈련 세트의 약 200개 주석 프레임만 사용해도 전체 데이터 사용 시와 거의 동일한 성능에 도달하여, 비디오 전체 주석이 필요하지 않음을 보여주었습니다.
*   **타이밍:**
    *   OSVOS는 480p DAVIS 프레임을 102ms에 처리할 수 있습니다.
    *   미세 조정 시간에 따라 속도와 정확도 간 트레이드오프가 존재하여, 프레임당 181ms에서 71.5% 정확도, 7.83초에서 79.7% 정확도를 달성할 수 있습니다.
    *   경쟁 기술 대비 모든 조건에서 훨씬 빠르거나 정확했습니다.
*   **점진적 개선:**
    *   주석 프레임 수가 증가함에 따라 품질이 점진적으로 향상됩니다. 1개 주석에서 J=79.8%였던 것이 2개에서 84.6%, 4개에서 86.9%로 증가하며, 약 5개 주석에서 성능이 포화됩니다.
    *   이를 통해 매우 유사한 두 객체(예: 두 마리의 낙타)를 구분하는 것과 같이 어려운 상황에서도 더 많은 주석을 통해 학습하여 개선될 수 있음을 보여주었습니다.
*   **추적기 평가:** 바운딩 박스(bounding box) 기준으로 평가했을 때, VOT Challenge 2015 우승자인 MDNET [32]보다 모든 겹침(overlap) 임계값에서 훨씬 우수한 성능을 보였습니다.
*   **Youtube-Objects 데이터셋:** OFL보다 약간 더 나은 성능을 보였습니다.

## 🧠 Insights & Discussion
*   이 논문의 핵심 통찰은 딥러닝이 충분히 정확한 객체 모델을 제공하여, 비디오 객체 분할에서 시간적 일관성을 명시적으로 모델링하는(예: 광학 흐름 알고리즘, 시간적 스무딩) 것이 더 이상 필수가 아니라는 점입니다. OSVOS는 각 프레임을 독립적으로 처리하여 시간적 오류 전파(drift) 문제를 겪지 않으면서도 높은 정확도와 시간적 일관성을 달성합니다.
*   인간이 단일 학습 예제만으로 새로운 객체를 인식하고 추적하는 능력, 즉 '원샷 학습' 능력을 기계 학습 시스템에서도 재현할 수 있음을 입증했습니다. 이는 일반적인 데이터셋으로 사전 학습된 모델이 특정 객체 인스턴스에 대한 미세 조정을 통해 강력한 적응 능력을 발휘한다는 것을 의미합니다.
*   제안된 방법은 실제 적용 시 유연성을 제공합니다. 사용자는 필요한 속도에 따라 미세 조정 시간을 조절하거나, 결과의 품질이 만족스럽지 않을 때 추가적인 주석을 제공하여 성능을 점진적으로 개선할 수 있습니다. 이는 로토스코핑(rotoscoping)과 같은 작업의 효율성을 크게 높일 수 있습니다.
*   FCN의 고유한 한계(거친 윤곽선 예측)를 보완하기 위한 경계선 스내핑 메커니즘의 필요성과 효과를 명확히 보여주었습니다. 특히 학습된 윤곽선을 활용하는 이중 스트림 접근법은 이러한 한계를 효과적으로 극복하는 방법이었습니다.

## 📌 TL;DR
OSVOS는 비디오의 첫 프레임에 주어진 단일 마스크만을 사용하여 객체를 분할하는 **원샷 준지도 비디오 객체 분할** 문제입니다. 이 문제를 해결하기 위해, ImageNet 사전 훈련, DAVIS 데이터셋을 통한 일반 객체 학습, 그리고 테스트 시 특정 비디오의 첫 프레임에 대한 **미세 조정**으로 구성된 3단계 학습 전략을 가진 **FCN 기반 신경망**을 제안합니다. OSVOS의 핵심 기여는 **각 프레임을 독립적으로 처리**하여 기존 광학 흐름 기반 방법의 시간적 일관성 제약과 오류 전파를 피하고도 높은 성능을 달성한다는 점입니다. 이 방법은 DAVIS 벤치마크에서 **최첨단 성능(79.8% Jaccard)을 크게 능가**하고 빠르며(프레임당 102ms), 추가 주석을 통해 성능을 점진적으로 향상시킬 수 있음을 입증했습니다.