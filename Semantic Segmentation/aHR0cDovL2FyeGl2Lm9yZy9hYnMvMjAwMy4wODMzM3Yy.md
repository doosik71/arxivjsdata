# Collaborative Video Object Segmentation by Foreground-Background Integration
Zongxin Yang, Yunchao Wei, and Yi Yang

## 🧩 Problem to Solve
비디오 객체 분할(VOS)은 컴퓨터 비전의 핵심 과제 중 하나로, 특히 첫 프레임에서 주어진 객체 마스크를 기반으로 비디오 시퀀스 전체에서 특정 객체를 분할하는 반지도 학습(semi-supervised) VOS는 여전히 도전적입니다. 기존 방법들은 주로 전경(foreground) 객체 픽셀의 임베딩 학습에만 집중하여, 유사한 객체가 배경에 존재할 때 전경 객체와 배경을 혼동하는 '배경 혼동 문제'에 직면했습니다. 또한, 기존 접근 방식은 추론 속도가 느리거나(미세 조정 기반), 대규모 시뮬레이션 데이터를 필요로 하거나, 객체 규모 변화에 대한 견고성이 부족하다는 한계가 있었습니다.

## ✨ Key Contributions
*   **전경-배경 통합(Foreground-Background Integration)을 통한 협업 임베딩 학습:** 전경 객체뿐만 아니라 해당 배경의 피처 임베딩도 함께 학습하여 전경과 배경이 대조되도록 유도합니다. 이는 '배경 혼동 문제'를 완화하고 분할 정확도를 높이는 데 기여합니다.
*   **픽셀 레벨 및 인스턴스 레벨 매칭 도입:** 객체의 세부 정보와 전체적인 스케일을 아우르기 위해 픽셀 레벨(pixel-level) 임베딩과 인스턴스 레벨(instance-level) 임베딩을 모두 활용하여 다양한 객체 스케일에 강건한 매칭 프로세스를 수행합니다.
*   **전경-배경 다중 로컬 매칭(Multi-Local Matching):** 이전 프레임과의 로컬 매칭에 배경 정보를 포함하고 여러 윈도우 크기를 적용하여 객체의 다양한 움직임 속도에 더욱 강건하게 대응합니다.
*   **협업 인스턴스 레벨 어텐션(Collaborative Instance-level Attention):** 효율적인 인스턴스 레벨 어텐션 메커니즘을 설계하여 픽셀 레벨 매칭을 보완하고 대규모 객체의 분할을 돕습니다.
*   **협업 인스턴스 앙상블러(Collaborative Ensembler, CE):** 전경/배경 정보와 픽셀 레벨/인스턴스 레벨 정보를 암묵적으로 통합하는 앙상블러를 제안하여 더 넓은 수용 영역(receptive field)을 확보하고 정확한 예측을 수행합니다.
*   **균형 잡힌 무작위 자르기(Balanced Random-Crop) 스키마:** 학습 시 전경-배경 픽셀 수의 불균형으로 인해 모델이 배경 특성에 편향되는 것을 방지하기 위해 사용됩니다.
*   DAVIS 2016, DAVIS 2017, YouTube-VOS 세 가지 주요 벤치마크에서 미세 조정, 후처리, 시뮬레이션 데이터 없이 최신 기술(state-of-the-art) 성능을 달성했습니다.

## 📎 Related Works
*   **반지도 VOS (미세 조정 기반):** OSVOS [2], MoNet [39], OnAVOS [35], MaskTrack [29], PReMVOS [23] 등.
*   **반지도 VOS (미세 조정 없음):** OSMN [42], PML [5], VideoMatch [18], FEELVOS [34], RGMP [38], STMVOS [27] 등. 특히 STMVOS는 메모리 네트워크와 광범위한 시뮬레이션 데이터를 사용했습니다.
*   **어텐션 메커니즘:** SE-Nets [17]. CFBI의 인스턴스 레벨 어텐션에 영감을 주었습니다.

## 🛠️ Methodology
CFBI(Collaborative video object segmentation by Foreground-Background Integration)는 전경 및 배경 임베딩 학습과 픽셀 및 인스턴스 레벨 매칭을 통합합니다.

1.  **백본 네트워크(Backbone Network):**
    *   DeepLabv3+ [4] 아키텍처를 기반으로 하며, 확장된(dilated) ResNet-101 [4]을 사용하여 픽셀 단위 임베딩을 추출합니다. ImageNet [10] 및 COCO [22]에서 사전 학습됩니다.

2.  **협업 픽셀 레벨 매칭(Collaborative Pixel-level Matching):**
    *   **거리 메트릭 재정의:** 현재 프레임 픽셀 $p$와 프레임 $t$의 픽셀 $q$ 간의 거리 $D_t(p, q)$를 정의합니다. 전경 바이어스 $b_F$와 배경 바이어스 $b_B$를 도입하여 전경과 배경 거리를 구분합니다.
    $$D_t(p, q) = \begin{cases} 1 - \frac{2}{1 + \exp(||e_p - e_q||^2 + b_B)} & \text{if } q \in B_t \\ 1 - \frac{2}{1 + \exp(||e_p - e_q||^2 + b_F)} & \text{if } q \in F_t \end{cases}$$
    *   **전경-배경 전역 매칭(Global Matching):** 첫 번째 참조 프레임($t=1$)의 전경/배경 픽셀과 현재 프레임 픽셀 간의 최소 거리 ($G_{T,o}(p)$)를 계산합니다.
    *   **전경-배경 다중 로컬 매칭(Multi-Local Matching):** 이전 프레임($T-1$)의 전경/배경 픽셀과 현재 프레임 픽셀 간의 최소 거리 ($ML_{T,o}(p, K)$)를 여러 로컬 윈도우 크기 $K = \{k_1, ..., k_n\}$(예: $\{2,4,6,8,10,12\}$)에서 계산하여 객체의 움직임 속도 변화에 대응합니다.
    *   최종 출력은 현재 프레임 임베딩, 이전 프레임 임베딩 및 마스크, 다중 로컬 매칭 맵, 전역 매칭 맵의 연결(concatenation)입니다.

3.  **협업 인스턴스 레벨 어텐션(Collaborative Instance-level Attention):**
    *   첫 번째 프레임과 이전 프레임의 픽셀 임베딩을 마스크에 따라 전경/배경 픽셀로 분리합니다.
    *   각 그룹에 채널 단위 평균 풀링(channel-wise average pooling)을 적용하여 네 개의 인스턴스 레벨 임베딩 벡터를 생성합니다.
    *   이 벡터들을 연결하여 '협업 인스턴스 레벨 가이드 벡터'를 만듭니다.
    *   이 가이드 벡터를 이용하여 협업 앙상블러(CE)의 Res-Block 입력 피처 채널의 스케일을 조정하는 어텐션 메커니즘(SE-Nets [17]에서 영감)을 구현합니다.

4.  **협업 앙상블러(Collaborative Ensembler, CE):**
    *   ResNets [16]과 Deeplabs [3,4]에서 영감을 받은 다운샘플-업샘플 구조를 가집니다.
    *   세 단계의 Res-Blocks와 Atrous Spatial Pyramid Pooling (ASPP) [4] 모듈을 포함하며, 확장된 컨볼루션(dilated convolution)을 사용하여 수용 영역을 효율적으로 확장합니다.
    *   낮은 레벨 백본 피처와 협업하여 예측을 세밀하게 조정합니다.

5.  **학습 상세(Implementation Details):**
    *   **균형 잡힌 무작위 자르기(Balanced Random-Crop):** 학습 시 잘린 영역이 충분한 전경 픽셀을 포함하도록 강제하여 모델이 배경 특성에 편향되는 것을 방지합니다.
    *   **순차 학습(Sequential Training):** 추론 단계와 일관성을 유지하기 위해 이전 마스크로 네트워크 예측(ground-truth 대신)을 사용하여 네트워크를 순차적으로 학습합니다.
    *   가장 어려운 15% 픽셀만 고려하는 부트스트랩 교차 엔트로피 손실(bootstrapped cross-entropy loss)을 사용합니다.
    *   배치 크기가 작을 때 학습 안정성을 높이기 위해 그룹 정규화(Group Normalization, GN) [37] 및 Gated Channel Transformation [43]을 적용합니다.

## 📊 Results
CFBI는 DAVIS 2016, DAVIS 2017, YouTube-VOS 세 가지 주요 벤치마크에서 기존의 최신 기술들을 능가하는 성능을 달성했습니다.

*   **YouTube-VOS:**
    *   **Validation 2018:** J&F 81.4%로 이전 SOTA인 STMVOS(79.4%)보다 2.0% 높게 측정되었습니다.
    *   **Testing 2019:** 단일 모델로 J&F 81.5%를 달성했으며, 다중 스케일 및 플립 전략 적용 시 82.2%로 대회 1위(EMN, 81.8%)를 능가했습니다. 특히, STMVOS와 같은 시뮬레이션 데이터를 사용하지 않고도 높은 성능을 보였습니다.
*   **DAVIS 2016 (YouTube-VOS로 추가 학습):** J&F 89.4%를 달성하여 STMVOS(89.3%)보다 약간 우수했으며, FEELVOS(81.7%)보다 훨씬 높은 정확도를 보였습니다(추론 속도 0.18초 vs. 0.45초로 비슷).
*   **DAVIS 2017 (YouTube-VOS로 추가 학습):**
    *   **Validation:** J&F 81.9%로 FEELVOS(71.5%) 및 STMVOS(81.8%)를 능가했습니다.
    *   **Testing:** J&F 74.8%로 STMVOS(72.2%)를 크게 앞질렀습니다.
*   **Multi-scale & Flip Augmentation:** 평가 단계에서 이 증강 기법을 적용하면 모든 데이터셋에서 성능이 추가적으로 향상됩니다 (예: YouTube-VOS 82.7%, DAVIS 2016 90.1%, DAVIS 2017 Validation 83.3%).
*   **추론 속도:** 단일 객체 추론 시 약 5 FPS를 유지하여 경쟁력을 갖습니다.
*   **어블레이션 연구(Ablation Study):**
    *   **배경 임베딩의 중요성:** 배경 임베딩을 제거하면 DAVIS-2017에서 J&F 점수가 74.9%에서 70.9%로 크게 하락하여 배경 정보의 중요성을 입증했습니다.
    *   **바이어스 $b_F$, $b_B$의 효과:** 전경 및 배경 바이어스를 제거하면 2.1% 성능 하락이 발생하여, 전경 및 배경 픽셀 간의 거리를 개별적으로 고려하는 것이 중요함을 보여줍니다.
    *   **다른 구성 요소들의 기여:** 다중 로컬 윈도우, 순차 학습, 협업 앙상블러, 균형 잡힌 무작위 자르기, 인스턴스 레벨 어텐션 모두 CFBI의 성능 향상에 유의미하게 기여함이 확인되었습니다.

## 🧠 Insights & Discussion
*   **배경 정보의 중요성:** 이 연구의 가장 중요한 통찰은 비디오 객체 분할에서 전경 객체뿐만 아니라 배경 정보 또한 동등하게 중요하게 다루어야 한다는 점입니다. 전경과 배경 피처를 대조적으로 학습시킴으로써, 특히 유사한 객체가 많거나 혼잡한 장면에서 발생하는 '배경 혼동 문제'를 효과적으로 해결할 수 있었습니다.
*   **다중 레벨 정보 통합의 시너지:** 픽셀 레벨 임베딩(세부 정보)과 인스턴스 레벨 임베딩(대규모 객체 정보)을 결합함으로써, CFBI는 객체의 다양한 스케일에 걸쳐 견고한 분할을 수행할 수 있게 됩니다. 이는 지역적 모호성(local ambiguities)을 해소하는 데 도움을 줍니다.
*   **움직임 변화에 대한 강건성:** 다중 로컬 매칭 메커니즘은 프레임 간 객체의 움직임 속도가 크게 달라지는 상황에서도 효과적으로 대응할 수 있게 설계되어 모델의 전반적인 강건성을 향상시킵니다.
*   **학습 효율성 및 안정성:** '균형 잡힌 무작위 자르기'는 데이터셋의 전경-배경 픽셀 불균형 문제를 해소하여 모델의 편향을 줄이고 학습 성능을 향상시킵니다. 또한, '순차 학습'은 추론 과정과 일관된 학습 환경을 제공하여 모델의 실질적인 성능 향상에 기여합니다.
*   **실용성:** CFBI는 미세 조정이나 대규모 시뮬레이션 데이터 없이도 최첨단 성능을 달성하면서 경쟁력 있는 추론 속도를 유지합니다. 이는 실제 응용 분야에서 높은 효율성을 제공함을 시사합니다.
*   **한계:** 매우 유사한 객체가 근접해 있거나, 빠른 움직임으로 인한 블러 현상이 심할 경우 여전히 분할 오류가 발생할 수 있습니다 (예: 유도 비디오의 손). 이는 향후 연구의 과제로 남습니다.

## 📌 TL;DR
*   **문제:** 기존 반지도 VOS는 배경 혼동 문제와 객체 스케일/움직임 변화에 대한 낮은 견고성, 그리고 느리거나 비효율적인 학습/추론 방식에 직면했습니다.
*   **방법:** CFBI는 전경 및 배경 피처를 대조적으로 통합하는 새로운 VOS 프레임워크를 제안합니다. 이는 픽셀 레벨 및 인스턴스 레벨 임베딩을 다중 로컬 매칭 및 협업 앙상블러와 결합하며, 균형 잡힌 무작위 자르기와 순차 학습 기법을 통해 훈련됩니다.
*   **결과:** CFBI는 미세 조정이나 시뮬레이션 데이터 없이 YouTube-VOS, DAVIS 2016, DAVIS 2017에서 새로운 SOTA 성능을 달성하여, 정확도, 일반화 능력, 강건성 측면에서 우수함을 입증했습니다.