# ZegFormer: Decoupling Zero-Shot Semantic Segmentation

Jian Ding, Nan Xue, Gui-Song Xia, Dengxin Dai

---

## 🧩 Problem to Solve

기존의 제로샷 의미론적 분할(Zero-shot Semantic Segmentation, ZS3)은 훈련 과정에서 보지 못한 새로운 카테고리(novel categories)를 분할하는 것을 목표로 합니다. 현재 접근 방식들은 ZS3를 픽셀 수준의 제로샷 분류 문제로 정식화하고, 텍스트로만 사전 훈련된 언어 모델을 사용하여 의미론적 지식을 학습된(seen) 클래스에서 학습되지 않은(unseen) 클래스로 전달합니다. 하지만 이러한 픽셀 수준의 정식화는 다음과 같은 한계를 가집니다:

1. **시각-언어 모델 통합의 어려움**: 이미지-텍스트 쌍으로 사전 훈련된 CLIP과 같은 대규모 시각-언어 모델을 픽셀 수준 ZS3 문제에 효과적으로 통합하기 어렵습니다.
2. **비자연적인 지식 전달**: 인간은 일반적으로 이미지의 픽셀 단위가 아닌 객체/세그먼트 단위를 텍스트로 설명하므로, 픽셀 수준의 시각적 특징과 의미론적 특징 간의 상관관계를 구축하는 것은 자연스럽지 않습니다. 이로 인해 학습되지 않은 클래스에 대한 픽셀 수준 분류 정확도가 낮아져 최종 분할 품질이 저하됩니다. 특히 학습되지 않은 카테고리의 수가 많을 때 이러한 현상이 두드러집니다.

## ✨ Key Contributions

* ZS3 작업을 **클래스에 구애받지 않는 그룹화(class-agnostic grouping)**와 **세그먼트 수준 제로샷 분류(segment-level zero-shot classification)**의 두 가지 하위 작업으로 분리하는 새로운 정식화를 제안합니다. 이는 대규모 사전 훈련된 시각-언어 모델을 ZS3에 통합하는 더 자연스럽고 유연한 방법을 제공합니다.
* 새로운 정식화를 기반으로, 트랜스포머 디코더를 사용하여 세그먼트 수준 임베딩을 생성하는 간단하고 효과적인 ZegFormer 모델을 제안합니다. 이는 ZS3를 위해 사전 훈련된 대규모 시각-언어 모델(예: CLIP)을 완전히 활용하는 최초의 모델입니다.
* ZS3 표준 벤치마크에서 최첨단(SOTA) 결과를 달성했습니다. 분리 정식화가 픽셀 수준 제로샷 분류보다 훨씬 우수함을 입증합니다.

## 📎 Related Works

* **제로샷 이미지 분류(Zero-Shot Image Classification)**: 의미론적 속성, 온톨로지, 워드 벡터 등을 통해 학습된 클래스에서 학습되지 않은 클래스로 지식을 전달합니다. CLIP과 ALIGN과 같은 대규모 시각-언어 사전 훈련 모델이 뛰어난 성능을 보였습니다.
* **제로샷 분할(Zero-Shot Segmentation)**: 이 분야의 초기 연구들은 ZS3를 픽셀 수준의 제로샷 분류 문제로 정식화했으며, 대표적인 방법으로 SPNet과 ZS3Net이 있습니다. 이후의 연구들도 이 정식화를 따랐습니다. 자기 학습(self-training) 방법도 사용되었지만, 새로운 클래스 등장 시 모델을 재훈련해야 하는 단점이 있습니다. 본 연구는 픽셀 수준 분류와 달리 세그먼트 수준 분류를 제안합니다.
* **클래스에 구애받지 않는 분할(Class-Agnostic Segmentation)**: 인스턴스 분할(instance segmentation) 작업에서 학습된 클래스에서 학습되지 않은 클래스로의 높은 일반화 가능성이 입증되었습니다. 엔티티 분할(entity segmentation)과 같은 관련 작업도 존재하지만, 본 연구는 novel 클래스의 의미론적 분할과 상세 클래스 이름 예측에 초점을 맞춥니다.

## 🛠️ Methodology

ZegFormer는 ZS3를 클래스에 구애받지 않는 그룹화와 세그먼트 수준 제로샷 분류로 분리하여 수행합니다.

1. **세그먼트 임베딩 생성**: MaskFormer [12]를 기반으로, 백본 및 픽셀 디코더를 통해 추출된 특징 맵과 $N$개의 세그먼트 쿼리를 트랜스포머 디코더에 입력하여 $N$개의 세그먼트 수준 임베딩을 생성합니다. 각 세그먼트 임베딩은 마스크 임베딩($B_q$)과 의미론적 임베딩($G_q$)으로 투영됩니다.
2. **클래스에 구애받지 않는 그룹화 (CAG)**:
    * 마스크 임베딩 $B_q$를 픽셀 디코더의 특징 맵 $F(I)$와 내적하여 클래스에 구애받지 않는 이진 마스크 $m_q = \sigma(B_q \cdot F(I))$를 예측합니다.
3. **세그먼트 분류 (s-ZSC)**:
    * **의미론적 세그먼트 임베딩(SSE)을 이용한 분류**: 각 클래스 이름을 프롬프트 템플릿(예: "A photo of a {class name} in the scene")에 넣어 사전 훈련된 텍스트 인코더(CLIP)를 통해 텍스트 임베딩($T_c$)을 얻습니다. 의미론적 임베딩 $G_q$와 $T_c$ 간의 코사인 유사도를 기반으로 클래스 확률 분포 $p_q(c)$를 계산합니다. "no object" 카테고리를 위한 학습 가능한 임베딩도 추가합니다.
    * **이미지 임베딩을 이용한 분류**: 예측된 마스크 $m_q$와 원본 이미지 $I$를 사용하여 세그먼트별 하위 이미지 $I_q$를 생성합니다(크롭, 마스크, 또는 둘의 조합). 이 $I_q$를 사전 훈련된 이미지 인코더(CLIP)에 입력하여 이미지 임베딩 $A_q$를 얻고, 이를 통해 확률 분포 $p'_q(c)$를 계산합니다.
4. **훈련**: 학습된 클래스에 속하는 픽셀 레이블만 사용합니다. 예측된 마스크와 실제 마스크 간에 이분 매칭(bipartite matching)을 수행하며, 분류 손실과 마스크 손실(Dice Loss + Focal Loss)을 최적화합니다.
5. **추론**:
    * **ZegFormer-seg**: SSE를 이용한 분류 점수 $p_q(c)$를 사용하며, 학습된 클래스에 대한 점수를 보정하여 불균형 문제를 해결합니다.
    * **ZegFormer-img**: 이미지 임베딩을 이용한 분류 점수 $p'_q(c)$를 사용합니다.
    * **ZegFormer (전체 모델)**: $p_q(c)$와 $p'_q(c)$를 융합하여 최종 세그먼트 클래스 예측을 얻습니다. 학습되지 않은 클래스에 대해서는 기하 평균을 사용하고, 학습된 클래스에 대해서는 $p_q(c)$와 학습된 클래스들의 $p'_q(c)$ 평균의 기하 평균을 사용하여, 두 분류 점수의 장점을 모두 활용합니다.

## 📊 Results

* **COCO-Stuff 및 PASCAL VOC 벤치마크**:
  * ZegFormer는 PASCAL VOC에서 학습되지 않은 클래스 mIoU에서 이전 최첨단 모델인 SIGN [13]보다 22점, Joint [5]보다 31점 향상된 성능을 보였습니다.
  * COCO-Stuff에서 ZegFormer는 학습되지 않은 클래스 mIoU에서 STRICT [42]보다 3점, SIGN [13]보다 18점 향상되었습니다.
  * 특히, 복잡한 다단계 훈련 방식이나 재훈련이 필요한 생성/자기 학습 방식과 달리, ZegFormer는 판별(discriminative) 방식이며 새로운 클래스에 즉시 적용 가능합니다.
* **ADE20k-Full 벤치마크**:
  * 275개의 학습되지 않은 클래스를 포함하는 도전적인 ADE20k-Full 데이터셋에서 ZegFormer는 SPNet-FPN 베이스라인을 mIoU에서 4점 이상 크게 앞질렀으며, 심지어 완전 지도 학습 모델과 견줄 만한 성능을 보였습니다.
* **클래스에 구애받지 않는 그룹화 성능**:
  * 클래스에 구애받지 않는 그룹화 지표(F$_b$, P$_b$, R$_b$)에서 ZegFormer는 SPNet-FPN 베이스라인보다 훨씬 우수한 성능을 보여, 제안된 분리 정식화가 학습되지 않은 클래스의 픽셀을 그룹화하는 데 훨씬 강력한 일반화 능력을 가짐을 입증했습니다.
* **속도 및 정확도**:
  * 클래스 수가 많을 때 픽셀 수준 제로샷 분류 방식보다 ZegFormer-seg가 속도와 정확도 면에서 모두 우수함을 보였습니다. 이는 픽셀 수준 방식의 계산 복잡도가 클래스 수에 크게 의존하기 때문입니다.

## 🧠 Insights & Discussion

* **분리 정식화의 우수성**: ZS3를 클래스에 구애받지 않는 그룹화와 세그먼트 수준 제로샷 분류로 분리하는 방식이 픽셀 수준 분류 방식보다 훨씬 우수하다는 것이 정량적, 정성적 결과로 입증되었습니다. 특히 학습되지 않은 클래스 수가 많아질수록 그 차이가 명확해집니다. 이는 인간이 객체를 인식하고 분할하는 방식과 더 유사하여 더욱 자연스러운 접근 방식임을 시사합니다.
* **CLIP 활용의 이점**: 세그먼트 수준의 시각적 특징이 CLIP과 같은 사전 훈련된 시각-언어 모델의 특징과 더 잘 정렬되어, 지식 전달의 효율성을 높입니다.
* **보완적 분류 점수**: 의미론적 세그먼트 임베딩과 이미지 임베딩을 통한 세그먼트 분류 점수가 서로 보완적인 역할을 함을 확인했습니다(예: 전자는 'stuff'에, 후자는 'things'에 강점). 이러한 점수 융합은 전체 성능 향상에 기여합니다.
* **한계점**: ZegFormer-seg는 훈련 데이터의 규모가 작을 때 성능이 저하될 수 있습니다. 이는 트랜스포머 구조가 대규모 데이터를 필요로 하기 때문으로 보이며, 향후 연구에서 더 효율적인 훈련 전략이나 다른 마스크 분류 방법을 통해 개선될 수 있습니다.

## 📌 TL;DR

기존 ZS3의 픽셀 수준 접근 방식이 시각-언어 모델 통합 및 학습되지 않은 클래스에 대한 지식 전달에서 한계를 보인다는 문제점을 해결하기 위해, 이 논문은 ZS3를 **클래스에 구애받지 않는 그룹화**와 **세그먼트 수준 제로샷 분류**로 **분리**하는 새로운 정식화를 제안합니다. 이를 바탕으로 **ZegFormer**라는 모델을 개발했으며, 이는 CLIP과 같은 대규모 시각-언어 모델을 효과적으로 활용합니다. ZegFormer는 표준 벤치마크에서 기존 최첨단 방법들을 크게 능가하는 성능을 달성하여, ZS3 문제에 대한 새로운 접근 방식의 우수성을 입증했습니다.
