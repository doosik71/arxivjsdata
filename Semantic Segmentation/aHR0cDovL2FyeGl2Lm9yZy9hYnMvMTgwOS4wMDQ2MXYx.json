{
  "url": "http://arxiv.org/abs/1809.00461v1",
  "title": "YouTube-VOS: Sequence-to-Sequence Video Object Segmentation",
  "authors": "Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, Thomas Huang",
  "year": 2018,
  "abstract": "Learning long-term spatial-temporal features are critical for many video\nanalysis tasks. However, existing video segmentation methods predominantly rely\non static image segmentation techniques, and methods capturing temporal\ndependency for segmentation have to depend on pretrained optical flow models,\nleading to suboptimal solutions for the problem. End-to-end sequential learning\nto explore spatial-temporal features for video segmentation is largely limited\nby the scale of available video segmentation datasets, i.e., even the largest\nvideo segmentation dataset only contains 90 short video clips. To solve this\nproblem, we build a new large-scale video object segmentation dataset called\nYouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains\n3,252 YouTube video clips and 78 categories including common objects and human\nactivities. This is by far the largest video object segmentation dataset to our\nknowledge and we have released it at https://youtube-vos.org. Based on this\ndataset, we propose a novel sequence-to-sequence network to fully exploit\nlong-term spatial-temporal information in videos for segmentation. We\ndemonstrate that our method is able to achieve the best results on our\nYouTube-VOS test set and comparable results on DAVIS 2016 compared to the\ncurrent state-of-the-art methods. Experiments show that the large scale dataset\nis indeed a key factor to the success of our model."
}