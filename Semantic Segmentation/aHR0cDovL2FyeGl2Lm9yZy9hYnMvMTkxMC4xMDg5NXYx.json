{
  "url": "http://arxiv.org/abs/1910.10895v1",
  "title": "Anchor Diffusion for Unsupervised Video Object Segmentation",
  "authors": "Zhao Yang, Qiang Wang, Luca Bertinetto, Weiming Hu, Song Bai, Philip H. S. Torr",
  "year": 2019,
  "abstract": "Unsupervised video object segmentation has often been tackled by methods\nbased on recurrent neural networks and optical flow. Despite their complexity,\nthese kinds of approaches tend to favour short-term temporal dependencies and\nare thus prone to accumulating inaccuracies, which cause drift over time.\nMoreover, simple (static) image segmentation models, alone, can perform\ncompetitively against these methods, which further suggests that the way\ntemporal dependencies are modelled should be reconsidered. Motivated by these\nobservations, in this paper we explore simple yet effective strategies to model\nlong-term temporal dependencies. Inspired by the non-local operators of [70],\nwe introduce a technique to establish dense correspondences between pixel\nembeddings of a reference \"anchor\" frame and the current one. This allows the\nlearning of pairwise dependencies at arbitrarily long distances without\nconditioning on intermediate frames. Without online supervision, our approach\ncan suppress the background and precisely segment the foreground object even in\nchallenging scenarios, while maintaining consistent performance over time. With\na mean IoU of $81.7\\%$, our method ranks first on the DAVIS-2016 leaderboard of\nunsupervised methods, while still being competitive against state-of-the-art\nonline semi-supervised approaches. We further evaluate our method on the FBMS\ndataset and the ViSal video saliency dataset, showing results competitive with\nthe state of the art.",
  "citation": 150
}