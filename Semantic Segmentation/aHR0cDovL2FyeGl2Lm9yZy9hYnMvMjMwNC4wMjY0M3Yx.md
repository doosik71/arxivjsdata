# Segment Anything
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick

## 🧩 Problem to Solve
본 논문은 이미지 세그멘테이션 분야에서 "기초 모델(foundation model)"을 구축하는 것을 목표로 합니다. 이는 대규모 데이터셋으로 사전 훈련되어 프롬프트를 통해 새로운 이미지 분포 및 다양한 다운스트림 세그멘테이션 문제에 제로샷(zero-shot)으로 일반화될 수 있는 모델을 개발하는 것을 의미합니다. 이를 위해 다음 세 가지 핵심 질문에 대한 포괄적인 해결책을 제시합니다:
1.  제로샷 일반화를 가능하게 할 **세그멘테이션 태스크**는 무엇인가?
2.  해당 태스크에 적합한 **모델 아키텍처**는 무엇인가?
3.  이 태스크와 모델을 구동할 수 있는 **데이터**는 어떻게 확보할 수 있는가?

## ✨ Key Contributions
*   **프롬프트 가능한 세그멘테이션 (Promptable Segmentation) 태스크 정의**: 이미지에 어떤 세그멘테이션 프롬프트(예: 점, 바운딩 박스, 텍스트)가 주어져도 유효한 세그멘테이션 마스크를 반환하는 새로운 태스크를 제안합니다. 이는 프롬프트가 모호할 경우에도 합리적인 마스크를 출력하도록 설계되었습니다.
*   **Segment Anything Model (SAM) 개발**: 프롬프트 가능한 세그멘테이션 태스크를 효율적으로 수행하도록 설계된 모델입니다. 강력한 이미지 인코더, 유연한 프롬프트 인코더, 경량 마스크 디코더로 구성되며, 실시간 상호작용이 가능하고 모호한 입력에 대해 여러 개의 유효한 마스크를 예측할 수 있습니다.
*   **Segment Anything 1 Billion (SA-1B) 데이터셋 구축**: SAM을 모델-인-더-루프(model-in-the-loop) 방식의 데이터 수집 루프에 활용하여, 1,100만 개의 라이선스 이미지에서 11억 개 이상의 마스크를 포함하는 현존하는 가장 큰 세그멘테이션 데이터셋을 구축했습니다. 이 데이터셋의 대부분은 SAM에 의해 완전 자동으로 생성되었으며 높은 품질과 다양성을 보여줍니다.
*   **제로샷 전이 성능 입증**: SAM이 훈련 시 보지 못했던 새로운 이미지 분포 및 태스크(예: 엣지 검출, 객체 제안, 인스턴스 세그멘테이션, 텍스트-마스크 예측)에 대해 프롬프트 엔지니어링을 통해 경쟁력 있는, 때로는 기존 완전 지도 학습 방식보다 우수한 제로샷 성능을 달성했음을 광범위한 실험을 통해 보여주었습니다.

## 📎 Related Works
*   **NLP 기초 모델**: GPT-3 [10], PaLM [21] 등 웹 스케일 데이터셋으로 사전 훈련되어 강력한 제로샷 및 Few-shot 일반화를 보이는 자연어 처리(NLP) 분야의 '기초 모델' 개념에서 영감을 받았습니다.
*   **비전-언어 기초 모델**: CLIP [82], ALIGN [55]와 같이 텍스트와 이미지를 정렬하여 시각적 개념 및 데이터 분포에 대한 제로샷 일반화를 가능하게 하는 비전 분야의 기초 모델 개발 노력을 언급합니다.
*   **대화형 세그멘테이션 (Interactive Segmentation)**: RITM [92], FocalClick [18], SimpleClick [67] 등 사용자의 입력(점, 박스)을 기반으로 마스크를 생성하는 기존 연구의 훈련 패러다임과 아이디어를 활용했습니다.
*   **트랜스포머 기반 비전 모델**: SAM의 이미지 인코더는 MAE [47]로 사전 훈련된 Vision Transformer (ViT) [33] 기반이며, 마스크 디코더는 DETR [14] 및 Mask2Former [20]와 같은 트랜스포머 기반 세그멘테이션 모델에서 영감을 받았습니다.
*   **다양한 세그멘테이션 태스크**: 엣지 검출 [3], 객체 제안 [2], 시맨틱 세그멘테이션 [90], 인스턴스 세그멘테이션 [66], 파놉틱 세그멘테이션 [59] 등 세그멘테이션의 다양한 하위 분야를 언급하며, SAM이 이러한 태스크에 적응할 수 있는 범용 모델을 목표로 합니다.

## 🛠️ Methodology
본 논문은 프롬프트 가능한 세그멘테이션 태스크, SAM 모델, 그리고 SA-1B 데이터셋이라는 세 가지 상호 연결된 구성 요소를 중심으로 방법론을 설명합니다.

### 1. 프롬프트 가능한 세그멘테이션 태스크
*   **태스크 정의**: 사용자로부터 다양한 형태의 "프롬프트"(예: 전경/배경 점, 대략적인 박스, 마스크, 자유 형식 텍스트)를 받아 해당 이미지 내의 유효한 세그멘테이션 마스크를 반환하는 것을 목표로 합니다. 여기서 "유효한" 마스크는 프롬프트가 모호하더라도 (예: 셔츠 위의 점이 셔츠나 사람 중 하나를 의미할 수 있을 때) 적어도 하나의 객체에 대해 합리적인 마스크를 제공해야 함을 의미합니다.
*   **사전 훈련**: 대화형 세그멘테이션 설정에서 영감을 받아, 각 훈련 샘플에 대해 프롬프트 시퀀스(점, 박스, 마스크)를 시뮬레이션하고 모델의 마스크 예측을 실제 마스크와 비교하여 훈련합니다. 특히 모델이 어떤 프롬프트에도 항상 유효한 마스크를 예측하도록 훈련하여 모호한 경우에도 효과적으로 작동하도록 합니다.

### 2. Segment Anything Model (SAM) 아키텍처
SAM은 세 가지 주요 구성 요소로 구성됩니다:
*   **이미지 인코더**: MAE [47]로 사전 훈련된 Vision Transformer (ViT) [33]를 기반으로 고해상도 입력($1024 \times 1024$ 픽셀)을 처리하여 이미지 임베딩을 생성합니다. 이 임베딩은 한 번만 계산되고 여러 프롬프트에 재사용될 수 있어 효율성을 극대화합니다.
*   **프롬프트 인코더**:
    *   **스파스 프롬프트 (점, 박스, 텍스트)**: 점과 박스는 위치 인코딩($P_E$)과 학습된 임베딩($E_{type}$)의 합으로 표현됩니다. 텍스트는 CLIP [82]의 텍스트 인코더를 통해 임베딩됩니다.
    *   **밀집 프롬프트 (마스크)**: 컨볼루션을 사용하여 임베딩되고 이미지 임베딩과 원소별로 합쳐집니다.
*   **경량 마스크 디코더**: 이미지 임베딩과 프롬프트 임베딩을 효율적으로 매핑하여 마스크를 예측합니다.
    *   트랜스포머 디코더 블록 [103]을 수정하여 프롬프트 자기-어텐션 및 교차-어텐션을 수행합니다.
    *   **모호성 해결**: 단일 프롬프트에 대해 여러 개의 마스크(기본 3개)를 예측하여 모호성을 자연스럽게 처리하며, 각 마스크에 대한 신뢰도 점수(IoU 예측)를 함께 제공합니다. 훈련 시에는 가장 낮은 손실을 가진 마스크에 대해서만 역전파를 수행합니다.
    *   이미지 임베딩이 미리 계산되면, 프롬프트 인코더와 마스크 디코더는 웹 브라우저 CPU에서 약 $50\text{ms}$ 내에 마스크를 예측할 수 있어 실시간 상호작용이 가능합니다.
*   **손실 및 훈련**: 마스크 예측은 Focal Loss [65]와 Dice Loss [73]의 선형 조합으로 감독되며, IoU 예측 헤드는 MSE Loss로 훈련됩니다. 모델 훈련은 기하학적 프롬프트(점, 박스)를 사용하여 11 라운드의 대화형 설정을 시뮬레이션합니다.

### 3. 데이터 엔진 (Data Engine) 구축
세그멘테이션 마스크의 희소성으로 인해 "데이터 엔진"이라는 모델-인-더-루프 데이터 수집 시스템을 구축했습니다:
*   **보조-수동 단계 (Assisted-manual stage)**: SAM이 주석가들이 클릭으로 마스크를 주석하고 세부적으로 조정하는 것을 실시간으로 보조합니다. 모델이 개선됨에 따라 주석 시간이 단축되고 이미지당 마스크 수가 증가했습니다 (120k 이미지에서 4.3M 마스크).
*   **반자동 단계 (Semi-automatic stage)**: SAM이 객체 탐지기를 통해 신뢰도 높은 마스크를 자동으로 생성하고, 주석가들은 모델이 놓친 덜 눈에 띄는 객체를 추가로 주석하여 마스크의 다양성을 높였습니다 (180k 이미지에서 5.9M 마스크 추가).
*   **완전 자동 단계 (Fully automatic stage)**: 모델 성능 향상과 모호성 인식 모델 개발 덕분에 주석가의 개입 없이 SAM이 마스크를 완전 자동으로 생성합니다. $32 \times 32$ 정규 그리드 포인트를 프롬프트로 사용하여 이미지당 평균 $~100$개의 고품질 마스크를 생성했습니다. 신뢰도(IoU)와 안정성 기준을 기반으로 마스크를 필터링하고 NMS(Non-Maximal Suppression)를 적용하여 최종 SA-1B 데이터셋을 구축했습니다.

## 📊 Results
*   **SA-1B 데이터셋**:
    *   $1,100\text{만}$개의 이미지와 $11\text{억}$개 이상의 고품질 세그멘테이션 마스크로 구성되어, 기존의 어떤 세그멘테이션 데이터셋보다 훨씬 큽니다 (가장 큰 Open Images [60]보다 $400\times$ 더 많은 마스크).
    *   이미지 해상도가 높고($\sim 3300 \times 4950$ 픽셀), 마스크 품질은 전문 주석과 비교했을 때 $94\%$의 마스크가 $90\%$ 이상의 IoU를 보였습니다.
    *   마스크 크기 및 모양 분포가 다양하며, 기존 데이터셋에 비해 작은 객체 마스크의 비율이 높습니다.
    *   지리적으로도 다양성을 가지며, 특히 유럽과 아시아 & 오세아니아, 중위 소득 국가의 이미지가 COCO [66]나 Open Images [60]보다 더 많이 포함되어 있습니다.

*   **제로샷 단일 점 유효 마스크 평가**:
    *   새로운 $23$개 세그멘테이션 데이터셋 벤치마크에서 SAM은 RITM [92]을 포함한 기존 강력한 대화형 세그멘테이션 모델보다 뛰어난 mIoU(평균 IoU) 성능을 보였습니다.
    *   SAM의 모호성 인식 기능 덕분에, 가장 적절한 마스크를 선택하는 "오라클" 평가에서는 모든 데이터셋에서 RITM을 능가했습니다.
    *   인간 평가 연구에서 주석가들은 SAM이 생성한 마스크의 품질을 RITM보다 일관되게 높게 평가했습니다 (평균 평점 7-9점).

*   **제로샷 엣지 검출**:
    *   BSDS500 [72] 데이터셋에서 엣지 검출에 특화된 훈련 없이도 Sobel 필터링을 통해 합리적인 엣지 맵을 생성했습니다.
    *   기존 딥러닝 방법인 HED [108]보다 우수하고 Sobel 필터 [13]보다 훨씬 뛰어난 성능을 보였습니다.

*   **제로샷 객체 제안**:
    *   LVIS v1 [44] 데이터셋에서 객체 탐지기 기반의 제안 방식(ViTDet-H [62])에는 못 미치지만, 특히 중대형 객체, 희귀 객체 및 일반 객체에서는 ViTDet-H를 능가하는 우수한 평균 리콜(AR@1000) 성능을 보였습니다.
    *   SAM의 모호성 인식 기능(여러 마스크 출력)이 이 태스크의 성능 향상에 크게 기여했습니다.

*   **제로샷 인스턴스 세그멘테이션**:
    *   ViTDet [62] 객체 탐지기의 바운딩 박스 출력을 SAM의 프롬프트로 사용하여 COCO [66] 및 LVIS v1 [44]에서 인스턴스 세그멘테이션을 수행했습니다.
    *   정량적 마스크 AP(Average Precision) 점수에서는 완전 지도 학습된 ViTDet에 뒤처지지만, 인간 평가에서는 SAM이 더 선명한 경계를 가진 고품질 마스크를 생성한다고 평가되었습니다 (ViTDet보다 높은 인간 평점). 이는 SAM이 데이터셋의 특정 주석 편향을 학습하지 않기 때문일 수 있습니다.

*   **제로샷 텍스트-마스크 예측**:
    *   CLIP [82] 텍스트 임베딩을 프롬프트로 사용하여 텍스트 설명에 따라 객체를 세그멘테이션하는 개념 증명을 보였습니다.
    *   간단한 텍스트 프롬프트뿐만 아니라 미묘한 구절에도 반응했으며, 추가 점 프롬프트와 결합하면 예측 정확도가 향상됨을 보여주었습니다.

*   **어블레이션 연구**:
    *   데이터 엔진의 각 단계(수동-보조, 반자동, 자동)가 모델 성능 향상에 기여함을 확인했으며, 자동으로 생성된 마스크만으로 훈련해도 전체 데이터로 훈련한 것과 거의 동등한 성능을 보였습니다.
    *   훈련 이미지 수를 줄였을 때(11M $\rightarrow$ 1M), 여전히 풀 데이터셋에 필적하는 성능을 유지하며 대규모 데이터의 중요성을 보여주었습니다.
    *   이미지 인코더 스케일(ViT-B $\rightarrow$ ViT-L $\rightarrow$ ViT-H)에 따라 성능이 향상되지만, ViT-L에서 ViT-H로 갈 때는 포화되는 경향을 보였습니다.

## 🧠 Insights & Discussion
*   **기초 모델로서의 의의**: SAM은 이미지 세그멘테이션을 기초 모델 시대로 이끄는 중요한 시도입니다. 광범위한 데이터로 대규모 지도 학습을 통해 훈련되어 다양한 다운스트림 태스크에 적응할 수 있는 능력을 보여주며, 컴퓨터 비전 분야의 새로운 연구 방향을 제시합니다.
*   **구성 가능성 (Compositionality)**: SAM은 다양한 세그멘테이션 프롬프트에 대해 신뢰할 수 있는 마스크를 예측하도록 설계되어, 다른 시스템의 구성 요소로 쉽게 통합될 수 있습니다 (예: 객체 탐지기, 3D 재구성 모델, 웨어러블 장치). 이는 훈련 시에는 상상할 수 없었던 새로운 응용 프로그램을 가능하게 합니다.
*   **한계점**:
    *   **세밀한 구조 및 경계 품질**: SAM은 미세한 구조를 놓치거나 작은 연결되지 않은 구성 요소를 환각하는 경우가 있으며, 계산 집약적인 방법만큼 경계가 선명하지 않을 수 있습니다.
    *   **실시간 성능의 제약**: 마스크 디코더는 실시간이지만, 무거운 이미지 인코더는 전체 추론 시간을 증가시킵니다.
    *   **텍스트-마스크의 견고성**: 텍스트-마스크 태스크는 아직 탐색 단계이며, 완전히 견고하지 않습니다.
    *   **범용성 한계**: 시맨틱 및 파놉틱 세그멘테이션과 같은 특정 태스크에는 단순한 프롬프트 설계가 어려울 수 있으며, 특정 도메인에 특화된 도구(예: 생체 이미지 분석 도구)보다는 성능이 떨어질 수 있습니다.
*   **윤리적 고려사항**: SA-1B 데이터셋의 지리적 및 소득 분포에 대한 분석을 수행했으며, SAM의 사람 세그멘테이션 성능이 다양한 그룹에 걸쳐 유사함을 확인했습니다. 그러나 의류 세그멘테이션에서는 성별 인식에 따른 편향이 일부 발견되었습니다. 연구팀은 사용자들에게 SAM을 특정 사용 사례에 적용할 때 자체적인 공정성 평가를 수행할 것을 권장합니다.
*   **미래 방향**: SAM은 커뮤니티에서 어떻게 활용되는지에 따라 기초 모델의 위상을 얻게 될 것입니다. 본 연구의 관점, 10억 개 이상의 마스크 데이터셋의 공개, 그리고 프롬프트 가능한 세그멘테이션 모델은 향후 연구의 길을 열 것으로 기대됩니다.

## 📌 TL;DR
"Segment Anything" 프로젝트는 이미지 세그멘테이션 분야에 기초 모델 패러다임을 도입합니다. 본 논문은 **프롬프트 가능한 세그멘테이션**이라는 새로운 태스크를 제시하고, 이를 수행하는 효율적인 모델인 **SAM(Segment Anything Model)**을 소개합니다. SAM은 이미지 인코더, 프롬프트 인코더, 경량 마스크 디코더로 구성되어 다양한 프롬프트(점, 박스, 텍스트)에 반응하며 모호성을 인식하여 여러 유효한 마스크를 실시간으로 예측합니다. 이 모델은 **데이터 엔진**을 통해 1,100만 개의 이미지와 11억 개 이상의 마스크를 포함하는 전례 없는 대규모 데이터셋 **SA-1B**를 완전 자동으로 구축하는 데 활용되었습니다. 광범위한 제로샷 전이 실험 결과, SAM은 엣지 검출, 객체 제안, 인스턴스 세그멘테이션 등 다양한 다운스트림 태스크에서 기존 지도 학습 방식에 필적하거나 이를 능가하는 뛰어난 성능을 입증했습니다. 이는 SAM이 컴퓨터 비전 분야의 새로운 기초 모델로서 광범위한 응용 가능성을 가짐을 시사합니다.