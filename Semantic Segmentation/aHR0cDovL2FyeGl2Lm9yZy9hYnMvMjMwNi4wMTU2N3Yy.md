# Segment Anything in High Quality

Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu

## 🧩 Problem to Solve

최근 출시된 Segment Anything Model (SAM)은 방대한 마스크 데이터로 학습되어 인상적인 제로샷(zero-shot) 능력과 유연한 프롬프트 기능을 제공하지만, 마스크 예측 품질, 특히 복잡한 구조를 가진 객체에 대해서는 여전히 미흡한 점이 있습니다. SAM은 거친 마스크 경계를 생성하거나, 얇은 객체 구조를 제대로 분할하지 못하고, 도전적인 경우에 잘못된 예측이나 깨진 마스크를 생성하는 문제가 있습니다. 이러한 품질 문제는 자동 주석 및 이미지/비디오 편집과 같이 매우 정확한 마스크가 필수적인 작업에서 SAM의 적용 가능성을 제한합니다.

## ✨ Key Contributions

- **HQ-SAM 제안**: SAM의 원래 프롬프트 기반 설계, 효율성 및 제로샷 일반화 능력을 유지하면서 모든 객체를 고품질로 정확하게 분할하는 모델을 제안합니다.
- **High-Quality Output Token (HQ-Output Token) 도입**: SAM의 사전 학습된 모델 가중치를 재사용하면서 최소한의 추가 파라미터와 연산만으로 고품질 마스크 예측을 담당하는 학습 가능한 토큰을 설계하여 마스크 디코더에 주입합니다.
- **Global-local Feature Fusion 설계**: 마스크 디코더 특징을 ViT 인코더의 초기 및 최종 특징과 융합하여 마스크 디테일을 향상시키고, 풍부한 글로벌 시맨틱 컨텍스트 및 로컬 경계 정보를 통합합니다.
- **HQSeg-44K 데이터셋 구축**: 44K개의 매우 정교하게 주석 처리된 마스크로 구성된 새로운 데이터셋을 구성하여 HQ-SAM 훈련에 활용합니다. 이 데이터셋을 통해 8개 GPU로 단 4시간 만에 효율적인 학습을 달성했습니다.
- **광범위한 성능 검증**: 10개의 다양한 분할 데이터셋(8개는 제로샷 전송 프로토콜)에서 HQ-SAM의 우수한 성능과 제로샷 능력을 입증했습니다.

## 📎 Related Works

- **고품질 분할 (High-quality Segmentation)**: 기존 고품질 분할 연구(예: 인스턴스/비디오 인스턴스 분할, 시맨틱 분할)는 주로 특정 작업에 대해 닫힌 세계(close-world) 패러다임으로 훈련됩니다. CRF(Conditional Random Field) 또는 별도의 딥 네트워크를 이용한 후처리(post-refinement) 방법도 있으나, 본 연구는 SAM의 제로샷 능력을 보존하며 직접 고품질 마스크를 예측하는 데 중점을 둡니다.
- **파운데이션 모델의 파인튜닝 및 프롬프트 튜닝 (Fine-tuning and Prompt Tuning for Foundation Models)**: GPT 시리즈와 같은 대규모 언어 모델에서 시작된 파운데이션 모델의 파인튜닝 및 프롬프트 기반 학습(CLIP 등)은 하위 작업으로의 일반화를 돕습니다. HQ-SAM은 기존 접근 방식과 달리 학습 가능한 파라미터를 사용하여 컨텍스트 학습을 돕는 것을 넘어, 제안된 HQ-Output Token을 직접 사용하여 정확한 마스크 예측을 수행하며 SAM의 최소한의 적응에 초점을 맞춥니다.

## 🛠️ Methodology

HQ-SAM은 SAM 모델에 단 두 가지 중요한 변경 사항만 도입하여 고품질 제로샷 분할을 달성합니다.

1. **SAM 아키텍처 개요**:

   - **이미지 인코더**: ViT 기반 백본으로 이미지 특징을 $64 \times 64$ 공간 크기로 추출합니다.
   - **프롬프트 인코더**: 입력 점/상자/마스크의 상호 작용적 위치 정보를 인코딩하여 마스크 디코더에 제공합니다.
   - **마스크 디코더**: 추출된 이미지 임베딩과 연결된 출력 및 프롬프트 토큰을 입력으로 받아 최종 마스크 예측을 수행하는 2계층 트랜스포머 기반 디코더입니다.

2. **HQ-SAM의 핵심 구성 요소**:

   - **High-Quality Output Token (HQ-Output Token)**:
     - SAM의 마스크 디코더 내에 학습 가능한 HQ-Output Token($1 \times 256$)을 기존 출력 토큰($4 \times 256$) 및 프롬프트 토큰과 함께 입력으로 주입합니다.
     - HQ-Output Token은 두 개의 디코더 계층을 통과하며 글로벌 이미지 컨텍스트, 프롬프트의 기하학적/유형 정보, 다른 출력 토큰의 마스크 정보에 접근합니다.
     - 업데이트된 HQ-Output Token으로부터 동적 합성곱 커널을 생성하는 새로운 3계층 MLP를 추가합니다. 이 MLP는 융합된 HQ-Feature와 공간적으로 점별 곱셈을 수행하여 고품질 마스크를 생성합니다.
     - 훈련 중에는 사전 학습된 SAM 파라미터 전체를 고정하고, HQ-Output Token, 관련 3계층 MLP, 그리고 작은 특징 융합 블록만 업데이트합니다.
   - **Global-local Feature Fusion**:
     - 정확한 분할을 위해 풍부한 글로벌 시맨틱 컨텍스트와 로컬 경계 디테일을 모두 포함하는 특징을 사용합니다.
     - 새로운 고품질 특징 (HQ-Features)은 SAM 모델의 다른 단계에서 추출된 특징을 융합하여 구성합니다:
       - **초기 계층 로컬 특징**: ViT 인코더의 첫 번째 전역 어텐션 블록 이후의 특징($64 \times 64$)을 추출하여 일반적인 이미지 경계/디테일을 포착합니다.
       - **최종 계층 글로벌 특징**: ViT 인코더의 마지막 블록 출력($64 \times 64$)으로 더 많은 글로벌 이미지 컨텍스트 정보를 가집니다.
       - **SAM 마스크 디코더의 마스크 특징**: $256 \times 256$ 크기로 강력한 마스크 형상 정보를 포함합니다.
     - 이 세 가지 특징을 요소별(element-wise)로 합산하기 위해 초기 계층 및 최종 계층 인코더 특징을 전치 합성곱(transposed convolution)으로 $256 \times 256$ 공간 크기로 업샘플링한 후 간단한 합성곱 처리를 거칩니다.

3. **학습 및 추론**:
   - **학습 데이터**: 44,320개의 매우 정확한 이미지 마스크 주석을 포함하는 HQSeg-44K 데이터셋을 구성하여 사용합니다. 이 데이터셋은 기존의 6개 고품질 데이터셋을 병합한 것입니다.
   - **HQ-SAM 학습**: SAM의 사전 학습된 모델 파라미터는 고정하고, HQ-Output Token, 관련 3계층 MLP 및 3개의 간단한 합성곱만 학습합니다. 바운딩 박스, 랜덤 샘플링된 점, 거친 마스크 등 혼합된 유형의 프롬프트를 샘플링하여 훈련합니다. 학습률은 $0.001$이며, 12 에폭 동안 8개 Nvidia GeForce RTX 3090 GPU에서 총 4시간(16.6K 반복) 훈련합니다.
   - **HQ-SAM 추론**: SAM과 동일한 추론 파이프라인을 따르되, HQ-Output Token의 마스크 예측을 고품질 마스크 예측으로 사용합니다. SAM 마스크(Output Token)와 HQ-SAM 마스크(HQ-Output Token)의 예측 로짓을 공간 해상도 $256 \times 256$에서 합산하여 마스크 보정을 수행한 후, 원래 해상도($1024 \times 1024$)로 업샘플링하여 최종 출력으로 사용합니다.

## 📊 Results

- **정확도 향상**:
  - 10개의 다양한 분할 데이터셋(COCO, UVO, SGinW, LVIS, HQ-YTVIS, BIG, COIFT, HR-SOD 등)에서 SAM 대비 일관되게 우수한 성능을 보였습니다.
  - 4개의 고정밀 분할 데이터셋(DIS, ThinObject-5K, COIFT, HR-SOD)에서 평균 mIoU는 $79.5$에서 $89.1$로, mBIoU는 $71.1$에서 $81.8$로 크게 향상되었습니다.
  - COCO 데이터셋에서 AP$_{B}$가 $1.1$점 증가했으며, LVIS에서 AP$_{strictB75}$가 $0.7$점 증가하여 향상된 마스크 품질과 잘 보존된 제로샷 능력을 보여주었습니다.
  - HQ-YTVIS 벤치마크에서는 Tube Boundary AP$_{B}$가 $3.8$점, Tube Mask AP$_{M}$가 $2.9$점 향상되었습니다.
  - 특히 엄격한 BIoU 임계값($0.9$)에서 SAM과의 성능 격차가 현저하게 증가하여 HQ-SAM의 매우 정확한 마스크 예측 능력을 입증했습니다.
- **효율성**:
  - SAM 대비 $0.5\%$ 미만의 파라미터 증가(ViT-L 기반 SAM의 경우 $5.1$M), GPU 메모리 사용량 및 이미지당 추론 시간의 미미한 증가를 보였습니다.
  - SAM-L과 유사한 추론 속도(HQ-SAM $4.8$ FPS vs SAM $5.0$ FPS)를 유지했습니다.
  - HQSeg-44K 데이터셋과 경량화된 통합 아키텍처 덕분에 8개 RTX 3090 GPU로 단 4시간 만에 훈련을 완료했습니다.
  - MobileSAM에 기반한 Light HQ-SAM은 $41.2$ FPS의 속도로 MobileSAM의 제로샷 COCO AP를 $44.3$에서 $45.0$으로 향상시켰습니다.
- **강건성**: 입력 Ground Truth (GT) 박스 프롬프트에 다양한 수준의 노이즈를 추가했을 때, HQ-SAM은 SAM보다 훨씬 강건한 분할 결과를 보였으며, 노이즈 스케일 증가에 따라 mBIoU 이점이 $10.7$에서 $20.5$로 증가했습니다.
- **시각적 결과**: HQ-SAM은 SAM이 놓쳤던 경계 및 얇은 구조 영역에 효과적으로 주목하며, 깨진 마스크나 큰 오류를 수정하여 훨씬 더 디테일이 보존된 마스크를 생성했습니다.

## 🧠 Insights & Discussion

- HQ-SAM은 SAM의 강력한 제로샷 일반화 능력을 유지하면서도 마스크 품질을 획기적으로 향상시킬 수 있음을 입증했습니다. 이는 파운데이션 모델의 효율적이고 경량적인 적응 전략의 효과를 보여줍니다.
- 새롭게 도입된 HQ-Output Token과 Global-local Feature Fusion 설계는 최소한의 추가 파라미터와 연산만으로 고품질 마스크 예측을 가능하게 하며, 모델 과적합 및 치명적인 망각(catastrophic forgetting) 문제를 성공적으로 방지합니다.
- SAM의 방대한 자동 생성 마스크 데이터셋(SA-1B)에 비해 HQSeg-44K와 같은 수동으로 정교하게 주석된 소규모 데이터셋이 고품질 분할 모델 학습에 훨씬 효과적임을 입증했습니다. 이는 데이터 효율적인 학습의 중요성을 시사합니다.
- 모바일 환경 배포를 위한 Light HQ-SAM (TinyViT 기반) 제안은 고품질 마스크 예측과 실시간 처리 속도 간의 균형을 제공하며, 다양한 응용 분야에서 HQ-SAM의 활용 가능성을 확장합니다.
- HQ-SAM은 여전히 SAM의 무거운 ViT 인코더를 공유하므로, 비디오 처리와 같은 실시간 애플리케이션을 위해서는 Light HQ-SAM과 같은 경량 백본의 필요성을 시사합니다.

## 📌 TL;DR

HQ-SAM은 SAM의 마스크 품질 문제를 해결하기 위해 도입된 경량 모델입니다. 고품질 출력 토큰(High-Quality Output Token)과 글로벌-로컬 특징 융합(Global-local Feature Fusion)을 통해 SAM의 핵심 구조를 최소한으로 수정하며 마스크 디테일을 향상시켰습니다. 44K개의 정교한 마스크 데이터셋(HQSeg-44K)으로 단 4시간 만에 학습하여, SAM의 뛰어난 제로샷 능력을 유지하면서 마스크 경계 정확도와 전반적인 품질을 크게 개선했습니다. 이는 데이터 및 연산 효율적인 방식으로 파운데이션 분할 모델의 성능을 확장하는 효과적인 방법을 제시합니다.
