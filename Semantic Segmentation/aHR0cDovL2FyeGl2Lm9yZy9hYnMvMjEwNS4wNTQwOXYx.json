{
  "url": "http://arxiv.org/abs/2105.05409v1",
  "title": "A Large-Scale Benchmark for Food Image Segmentation",
  "authors": "Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven C. H. Hoi, Qianru Sun",
  "year": 2021,
  "abstract": "Food image segmentation is a critical and indispensible task for developing\nhealth-related applications such as estimating food calories and nutrients.\nExisting food image segmentation models are underperforming due to two reasons:\n(1) there is a lack of high quality food image datasets with fine-grained\ningredient labels and pixel-wise location masks -- the existing datasets either\ncarry coarse ingredient labels or are small in size; and (2) the complex\nappearance of food makes it difficult to localize and recognize ingredients in\nfood images, e.g., the ingredients may overlap one another in the same image,\nand the identical ingredient may appear distinctly in different food images. In\nthis work, we build a new food image dataset FoodSeg103 (and its extension\nFoodSeg154) containing 9,490 images. We annotate these images with 154\ningredient classes and each image has an average of 6 ingredient labels and\npixel-wise masks. In addition, we propose a multi-modality pre-training\napproach called ReLeM that explicitly equips a segmentation model with rich and\nsemantic food knowledge. In experiments, we use three popular semantic\nsegmentation methods (i.e., Dilated Convolution based, Feature Pyramid based,\nand Vision Transformer based) as baselines, and evaluate them as well as ReLeM\non our new datasets. We believe that the FoodSeg103 (and its extension\nFoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to\nfacilitate future works on fine-grained food image understanding. We make all\nthese datasets and methods public at\n\\url{https://xiongweiwu.github.io/foodseg103.html}.",
  "citation": 136
}