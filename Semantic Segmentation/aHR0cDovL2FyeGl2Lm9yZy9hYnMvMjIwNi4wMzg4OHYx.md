# ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical Image Segmentation

Mingxuan Gu, Sulaiman Vesal, Mareike Thies, Zhaoya Pan, Fabian Wagner, Mirabela Rusu, Andreas Maier, Ronak Kosti

## 🧩 Problem to Solve

심층 학습 모델은 도메인에 민감하여 스캐너, 양식, 획득 매개변수 등 다양한 데이터 분포를 가진 새로운 도메인에 적용될 때 성능이 불안정해지는 문제가 있습니다. 특히 의료 영상 분야에서는 데이터 주석 작업이 시간과 비용이 많이 들고 전문가 간/내 편차가 큽니다. 이러한 격차를 해소하기 위해 비지도 도메인 적응(UDA)이 활용되지만, 기존 픽셀 단위의 대조 학습(CL)은 엄청난 메모리 요구 사항으로 인해 의료 영상 분할에 적용하기 어렵습니다. 본 논문은 더 나아가, 레이블이 없는 타겟 도메인 이미지가 **소수(few-shot) 또는 단 하나(one-shot)**만 존재하는 훨씬 더 어려운 UDA 시나리오를 해결하고자 합니다.

## ✨ Key Contributions

* **스타일 변환 모듈 채택:** RAIN(Random Adaptive Instance Normalization) 모듈을 사용하여 소스 샘플의 콘텐츠를 유지하면서 다양하고 난이도가 높은 타겟 유사 이미지를 생성함으로써 타겟 샘플 부족 문제를 완화했습니다.
* **중심점 기반 대조 학습(CCL) 제안:** 픽셀 단위 대조 학습의 계산 부담을 줄이기 위해 이미지 분할에 적합한 클래스별 특징 중심점 기반의 대조 학습을 제안했습니다.
* **중심점 노름 정규화(CNR) 제안:** 대조 손실이 중심점의 방향만 최적화하는 한계를 보완하기 위해 중심점의 크기(magnitude)를 제어하는 정규화 기법을 도입하여 소스와 타겟 중심점을 더욱 가깝게 만들었습니다.
* **다중 파티션 중심점 대조 학습(MPCCL) 제안:** 디코더 특징에 대한 경사 소실 문제를 완화하고 동일 클래스 내 타겟 특징의 분산을 추가로 줄이는 효과적인 방법을 제안했습니다.
* **엄격한 시나리오에서의 검증:** 단 한 개의 타겟 데이터 또는 소수의 타겟 데이터만 사용 가능한 엄격한 few-shot 및 one-shot 설정에서 제안된 UDA 접근 방식의 유효성을 입증했습니다.

## 📎 Related Works

* **적대적 학습 기반 UDA:** 중간 특징 공간 [3], 출력 공간 [4], 엔트로피 공간 [5, 6]을 정렬하여 도메인 불일치를 줄입니다.
* **스타일 변환 기반 UDA:** 실시간 스타일 변환 [7, 8]을 활용하여 소스 도메인 콘텐츠를 타겟 스타일로 변환하여 인공적으로 레이블링된 타겟 도메인 훈련 데이터를 생성합니다.
* **UDA 맥락의 대조 학습:**
  * Singh et al. [9]는 분류 작업을 위해 특징의 평균을 중심점으로 계산하고, 동일 클래스의 도메인 간 중심점을 정렬하는 대조 학습을 제안했습니다.
  * Hu et al. [11]은 블록 대조 학습(BlockCL)을 제안하여 특징을 블록으로 분할한 후 각 블록 내에서만 대조 손실을 적용함으로써 메모리 문제를 해결했습니다. (단점: 음성 쌍에 동일 클래스의 특징이 포함될 수 있음).
* **Few-shot UDA:**
  * Luo et al. [7]은 one-shot 비지도 도메인 적응을 위한 적대적 스타일 마이닝을 다루었습니다.
  * Gu et al. [8]은 다중 모달 심장 영상 분할을 위한 few-shot 비지도 도메인 적응을 연구했습니다.

## 🛠️ Methodology

ConFUDA는 몇 장의 레이블 없는 타겟 샘플만으로 의료 영상 분할을 위한 UDA를 수행하는 것을 목표로 합니다.

1. **Random Adaptive Instance Normalization (RAIN) [7]:**
    * `AdaIN` [12] 기반의 스타일 변환 모듈입니다.
    * `VAE`를 사용하여 단일 이미지의 스타일 분포를 학습하고, `VAE`의 정규 분포 변수를 조정하여 스타일이 적용된 이미지를 생성합니다.
    * 메인 태스크에서 역전파되는 그라디언트의 음수 방향으로 `VAE` 변수를 업데이트하여 점진적으로 더 어려운 스타일이 적용된 이미지를 생성하여 모델의 견고성을 높입니다.
    * 손실 함수: $L_{RAIN} = L_c + \lambda_s L_s + \lambda_{KL} L_{KL} + \lambda_{Rec} L_{Rec}$

2. **Centroid-based Contrastive Learning (CCL):**
    * **클래스별 중심점 계산:**
        * 소스 이미지의 경우, 디코더 특징에 이진 레이블 $y_s$를 곱하여 마스킹한 후, 공간 차원에서 각 클래스별 특징의 평균을 계산하여 클래스별 중심점 $C^s_k$를 생성합니다.
        * $$C^s_k = \frac{\sum_{b=1}^{B} \sum_{h,w=1}^{H,W} (D(E(x_s)) * y^s_k)}{\sum_{b,h,w=1}^{B,H,W} y^s_k}$$
        * 타겟 이미지의 경우, $Softmax(Cls(D(E(x_t))))$를 의사 레이블로 사용하여 디코더 특징을 소프트하게 마스킹한 후 유사하게 중심점을 계산합니다.
        * 소스 중심점 $C_s$는 메모리 뱅크에 유지하며, 지수 이동 평균으로 업데이트합니다.
    * **도메인 간 대조 학습 ($L_{inter}$):**
        * 소스 및 타겟 도메인의 동일 클래스 중심점 $C^t_k, C^s_k$를 긍정 쌍으로, 다른 클래스 중심점을 부정 쌍으로 사용하여 정렬합니다.
        * $$L_{inter} = L_{contrast}(C_t, C_s) = -\sum_{k=1}^{K} \log \frac{h(C^t_k, C^s_k)}{\sum_{d \in s,t} \sum_{i=1, i \neq k}^{K} h(C^t_k, C^d_i)}$$
        * 여기서 $h(u,v) = \exp(\frac{u^T v}{\|u\|_2 \|v\|_2} / \tau)$는 코사인 거리를 측정합니다.
    * **도메인 내 대조 학습 ($L_{intra}$):**
        * 타겟 특징이 결정 경계 근처로 이동하는 문제를 해결합니다.
        * 강하게 증강된 타겟 이미지 $\tilde{x}_t$와 원본 타겟 이미지 $x_t$의 중심점 $C_t, \tilde{C}_t$를 계산하고, $L_{contrast}(C_t, \tilde{C}_t)$를 적용합니다. 이를 통해 두 특징이 고밀도 영역으로 밀려나도록 하여 결정 경계를 저밀도 영역에 두게 됩니다.

3. **Centroid Norm Regularizer (CNR):**
    * 대조 손실이 중심점의 방향만 최적화하는 한계를 극복하기 위해 제안되었습니다.
    * 타겟 중심점의 크기를 소스 중심점의 크기에 가깝게 정규화하여 도메인 간 불일치를 더욱 줄입니다.
    * $$R(C_t) = \frac{1}{K} \sum_{k=1}^{K} (\|C^t_k\|_2 - \|C^s_k\|_2)^2$$

4. **Multi-partition Centroid Contrastive Learning (MPCCL):**
    * 중심점 최적화 시 디코더 특징에 발생하는 경사 소실 문제를 완화합니다.
    * 타겟 이미지의 소프트 예측을 $P$개의 파티션으로 무작위로 분할합니다.
    * 각 파티션에 대해 중심점을 계산하고, 해당 파티션의 중심점에 도메인 간 및 도메인 내 대조 손실을 적용합니다.
    * $$L_{pcontrast}(C_s, C_t, \tilde{C}_t) = \frac{1}{P} \sum_{p=1}^{P} (L_{contrast}(C^p_t, C_s) + L_{contrast}(\tilde{C}_t, C^p_t))$$
    * 이는 계산 부담과 모델 성능 사이의 절충안으로 작용합니다.

5. **전체 손실 함수:**
    * $$L=L_{RAIN} + L_{Seg} + \lambda_{contrast} L_{pcontrast}(C_s, C_t, \tilde{C}_t) + \lambda_{CNR} \sum_{p=1}^{P} R(C^p_t)$$
    * $L_{Seg}$는 cross-entropy와 Jaccard 손실의 조합입니다.

## 📊 Results

* **데이터셋:** MS-CMRSeg 챌린지 데이터셋(45개의 짧은 축 bSSFP, T2-가중, LGE 스캔).
* **평가 지표:** Dice 계수(Dice), Hausdorff 거리(HD95).
* **정량적 결과:**
  * ConFUDA는 베이스라인(W/o UDA) 대비 모든 실험 설정에서 Dice 점수를 0.3 이상 향상시켰습니다.
  * `AdaptSeg` [4], `AdvEnt` [5], `BlockCL` [11], `FUDA` [8] 등 다른 주요 UDA 방법론을 Dice 및 HD95 측면에서 능가했습니다.
  * 특히, one-shot 설정에서 Dice 점수가 0.31, few-shot 설정에서 0.34 향상되었습니다.
  * `BlockCL` 대비 `HD95`를 1.3~7.2 감소시켜 아웃라이어를 효과적으로 줄였습니다.
* **정성적 결과 (Fig. 3):**
  * 제안된 ConFUDA가 비교 방법론들 중 가장 정확하고 조밀한 심장 구조 분할 결과를 생성함을 보여주었습니다.
* **Ablation Study (Table 2, Fig. 4):**
  * 각 구성 요소(`CCL`, `CNR`, `MPCCL`)를 추가할 때마다 Dice 점수가 점진적으로 향상되고 HD95가 감소했습니다.
  * `MPCCL`이 `RV` 및 `MYO`의 오분류를 제거하여 최상의 예측을 이끌어냈습니다.

## 🧠 Insights & Discussion

* **함의:** ConFUDA는 의료 영상 분할을 위한 어려운 few-shot/one-shot UDA 문제를 성공적으로 해결했습니다. 효율적인 대조 학습과 스타일 변환을 활용하여 기존 픽셀 단위 대조 학습의 메모리 한계를 극복하고 특징 정렬 성능을 개선했습니다. 이는 실제 의료 환경에서 레이블링된 데이터가 부족한 상황에 큰 이점을 제공합니다.
* **한계점 및 향후 연구:**
  * 전체 데이터셋(`full`) 설정에서 `MPCCL` 추가 시 성능 향상이 미미하거나 오히려 `HD95`가 증가하는 경우가 있었습니다.
  * 이는 one-shot 설정과의 호환성을 위해 타겟 배치 크기를 1로 설정했기 때문에, 전체 데이터셋의 경우 타겟 샘플당 훈련 횟수가 적어(few-shot/one-shot 대비 200~2000배 적음) 모델이 타겟 도메인에서 충분히 훈련되지 못했기 때문으로 분석됩니다.
  * 향후 연구에서는 전체 데이터셋에 대한 훈련 부족 문제를 해결하고, 다른 다중 모달 데이터셋에 대한 평가를 통해 제안된 방법의 견고성을 더욱 검증할 계획입니다.

## 📌 TL;DR

**문제:** 의료 영상 분할에서 도메인 차이와 레이블링된 데이터 부족, 특히 few-shot/one-shot 비지도 도메인 적응(UDA) 상황이 어렵고, 기존 픽셀 단위 대조 학습은 메모리 비효율적입니다.
**방법:** ConFUDA는 `RAIN` (다양한 타겟 유사 이미지 생성), `CCL` (클래스별 중심점을 이용한 효율적인 도메인 간/내 특징 정렬), `CNR` (중심점 크기 정규화), `MPCCL` (특징 분산 감소 및 경사 소실 완화)를 결합하여 이 문제를 해결합니다.
**발견:** ConFUDA는 베이스라인 및 다른 UDA 방법론 대비 few-shot 및 one-shot 설정에서 심장 영상 분할 성능(Dice 점수)을 크게 향상시키며, 더욱 정확하고 조밀한 구조 분할 결과를 제공함을 입증했습니다.
