{
  "url": "http://arxiv.org/abs/2408.08315v1",
  "title": "Segment Anything for Videos: A Systematic Survey",
  "authors": "Chunhui Zhang, Yawen Cui, Weilin Lin, Guanjie Huang, Yan Rong, Li Liu, Shiguang Shan",
  "year": 2024,
  "abstract": "The recent wave of foundation models has witnessed tremendous success in\ncomputer vision (CV) and beyond, with the segment anything model (SAM) having\nsparked a passion for exploring task-agnostic visual foundation models.\nEmpowered by its remarkable zero-shot generalization, SAM is currently\nchallenging numerous traditional paradigms in CV, delivering extraordinary\nperformance not only in various image segmentation and multi-modal segmentation\n(\\eg, text-to-mask) tasks, but also in the video domain. Additionally, the\nlatest released SAM 2 is once again sparking research enthusiasm in the realm\nof promptable visual segmentation for both images and videos. However, existing\nsurveys mainly focus on SAM in various image processing tasks, a comprehensive\nand in-depth review in the video domain is notably absent. To address this gap,\nthis work conducts a systematic review on SAM for videos in the era of\nfoundation models. As the first to review the progress of SAM for videos, this\nwork focuses on its applications to various tasks by discussing its recent\nadvances, and innovation opportunities of developing foundation models on broad\napplications. We begin with a brief introduction to the background of SAM and\nvideo-related research domains. Subsequently, we present a systematic taxonomy\nthat categorizes existing methods into three key areas: video understanding,\nvideo generation, and video editing, analyzing and summarizing their advantages\nand limitations. Furthermore, comparative results of SAM-based and current\nstate-of-the-art methods on representative benchmarks, as well as insightful\nanalysis are offered. Finally, we discuss the challenges faced by current\nresearch and envision several future research directions in the field of SAM\nfor video and beyond."
}