# 비디오를 위한 Segment Anything: 체계적인 조사
Chunhui Zhang, Yawen Cui, Weilin Lin, Guanjie Huang, Yan Rong, Li Liu, Shiguang Shan

## 🧩 Problem to Solve
최근 Segment Anything Model (SAM)은 이미지 분할 분야에서 뛰어난 제로샷 일반화 능력을 보이며 큰 성공을 거두었지만, 비디오 도메인에서의 SAM 적용에 대한 포괄적이고 심층적인 조사는 부족했습니다. 비디오 데이터는 공간 정보뿐만 아니라 시간적 동역학, 고차원성, 연속성 및 안정성, 높은 처리 시간, 그리고 동작 및 이벤트 인식과 같은 고유한 도전을 제기합니다. 이 연구는 SAM의 비디오 적용에 대한 체계적인 검토를 통해 이러한 격차를 해소하고, 관련 연구자와 실무자에게 도움을 주는 것을 목표로 합니다.

## ✨ Key Contributions
*   파운데이션 모델 시대의 비디오를 위한 SAM 개발 과정을 철저히 검토하고, 비디오 이해, 비디오 생성, 비디오 편집의 세 가지 주요 범주로 분류된 최신 진행 상황에 대한 체계적인 조사를 제공합니다. 이는 이 특정 도메인에 초점을 맞춘 최초의 체계적인 조사입니다.
*   다양한 비디오 작업에 대해 SAM 기반 방법과 현재 최첨단(SOTA) 방법을 대표적인 벤치마크에서 종합적으로 비교하고, 이러한 최첨단 방법의 장단점에 대한 심층 분석을 제공합니다.
*   체계적인 문헌 검토 및 포괄적인 성능 평가를 기반으로 미래의 잠재적인 발전 방향을 제시합니다.

## 📎 Related Works
이 조사는 SAM이 영향을 미친 광범위한 기존 연구 분야와 모델들을 참조합니다.
*   **파운데이션 모델**: 자연어 처리(NLP), 컴퓨터 비전(CV), 기계 학습(ML) 등 다양한 분야에서 혁신을 가져온 대규모 사전 학습 모델을 다룹니다.
*   **Segment Anything Model (SAM) 및 SAM 2**: Meta가 제안한 이미지 분할 파운데이션 모델 [13]과 비디오 분할 기능을 강화한 후속 모델 [21]을 소개합니다.
*   **기존 SAM 관련 조사**: 의료 영상 분할에 중점을 두거나 [9] 비디오 작업을 대략적으로 다루는 [3], [31] 기존 조사들의 한계를 언급하며 본 연구의 차별점을 강조합니다.
*   **비디오 관련 주요 작업**:
    *   **비디오 이해**: 비디오 인식(예: Two-stream ConvNet [44], SlowFast networks [43]), 비디오 로컬라이제이션(예: Two-stage paradigm [45], [46], One-stage paradigm [47], [48]).
    *   **비디오 생성**: 텍스트-비디오 생성(예: VAE 기반 [51], [52], Video Diffusion Models [53], Make-A-Video [54]), 단일 비디오에서 비디오 생성(예: GAN 기반 [56], [57], Patch nearest-neighbour [58], Sinfusion [59]).
    *   **비디오 편집**: 비디오 스타일화, 전파 기반 방법(예: [60], [61]), 비디오 계층 기반 방법(예: Layered neural atlases [62]).
*   **SAM의 연구 방향**: 모델 압축 [28], 모델 견고성 [29], 효율적인 미세 조정 기술 [4] 발전, 혁신적인 응용 프로그램 개발 [9], [30] 등이 있습니다.

## 🛠️ Methodology
이 조사는 SAM의 비디오 적용에 대한 체계적인 문헌 검토를 수행합니다.
1.  **배경 지식 요약**: SAM 및 SAM 2의 작동 방식, 연구 경로, 관련 비디오 연구 도메인(비디오 이해, 생성, 편집)을 간략하게 소개합니다.
    *   **SAM**: 강력한 이미지 인코더(ViT [37]), 프롬프트 인코더, 마스크 디코더로 구성되며, 대규모 데이터셋(1100만 이미지, 10억 개 이상의 마스크)으로 훈련되어 뛰어난 제로샷 일반화 능력을 보입니다.
    *   **SAM 2**: 스트리밍 메모리 구성 요소를 통합한 Transformer 기반 아키텍처를 통해 실시간 비디오 분할 능력을 향상시키고, 더 크고 다양한 SA-V 데이터셋(50.9K 비디오, 642.6K 마스크렛)으로 훈련되었습니다.
2.  **체계적인 분류**: SAM을 활용한 혁신적인 응용 프로그램들을 비디오 이해, 비디오 생성, 비디오 편집의 세 가지 주요 범주로 분류합니다.
3.  **세부 분석**: 각 범주 내에서 기존 방법들의 장점과 한계를 분석하고 요약합니다.
4.  **성능 비교**: SAM 기반 방법과 현재 최첨단(SOTA) 방법의 대표적인 벤치마크에서의 비교 결과를 제시하고, 통찰력 있는 분석을 제공합니다.
5.  **미래 방향 제시**: 현재 연구가 직면한 도전 과제를 논의하고 비디오 분야 및 그 이상의 SAM 연구를 위한 미래 연구 방향을 제시합니다.

## 📊 Results
이 연구는 다양한 비디오 작업에서 SAM 기반 방법과 최신 SOTA 방법의 성능을 비교 분석했습니다.

*   **비디오 객체 분할 (VOS)**:
    *   SAM 기반 방법은 비디오 데이터에 특화된 SOTA 방법(예: XMem [101])에 비해 성능 격차를 보입니다. 이는 SAM이 주로 이미지 데이터로 사전 학습되었기 때문입니다.
    *   "GT Box+SAM" [13]과 같이 고품질 프롬프트(Ground-Truth Bounding Box)를 사용하는 경우 높은 성능을 달성합니다.
    *   비디오 데이터로 훈련된 모델(XMem, VMT)이 이미지 데이터로만 훈련된 모델보다 상당한 이점을 가지며, 다중 모달 데이터 활용(예: [125])이 성능 향상에 기여합니다.

*   **비디오 객체 추적 (VOT)**:
    *   VOTS2023 챌린지에서 SOTA인 DMAOT [128]은 객체별 장기 메모리 사용으로 최고 성능을 달성했습니다.
    *   SAM 기반 MSDeAOT [103] (TREK-150) 및 SAM-DA [26] (NUT-L)는 SAM의 정밀한 마스크 생성 및 고품질 대상 도메인 훈련 샘플 생성 능력 덕분에 특정 시나리오(1인칭 시점, 야간 UAV)에서 최고의 결과를 달성했습니다.
    *   SAM-Track [27]은 클릭 초기화만으로 DAVIS 데이터셋에서 SOTA와 유사한 성능을 보였습니다.
    *   YouTube-VOS 데이터셋에서는 SAM 기반 방법들이 시공간 대응 모듈의 부족으로 SOTA(STCN [132])에 뒤처지는 경향을 보였습니다.

*   **기타 비디오 이해 작업 (딥페이크 감지, 비디오 그림자 감지, 오디오-시각 분할, RVOS, 의료 비디오 등)**:
    *   VOS 및 VOT에 비해, SAM 기반 방법은 딥페이크 감지, 비디오 그림자 감지, RVOS, 다양한 의료 비디오와 같은 다른 비디오 이해 작업에서 더 큰 성공을 거두었습니다.
    *   이러한 작업들은 종종 소규모 데이터셋을 가지며, 광범위한 데이터셋으로 사전 학습된 파운데이션 모델이 제한적이거나 상이한 데이터 분포를 가진 문제 해결에 유망함을 보여줍니다.
    *   오디오-시각 분할에서 SAM 기반 GAVS [71]는 복잡한 다중 소스 하위 집합에서 SOTA와 경쟁할 만한 성능을 보여주며, 파운데이션 모델의 일반화 능력 활용의 중요성을 강조합니다.
    *   3D 포인트 클라우드 분할에서 SAM 기반 CSF [87]는 완전 지도 학습 SparseConvNet [141]과 유사한 결과를 달성하여 2D 파운데이션 모델을 3D 작업에 적용하는 효과를 입증했습니다.

*   **비디오 생성 및 편집**:
    *   비디오 합성 (Dancing Avatar [84], DISCO [85]), 비디오 초고해상도 (SEEM [25]), 일반 비디오 편집 (Make-A-Protagonist [93]), 텍스트 기반 비디오 편집 (2SVE [94])에서 SAM 기반 방법들이 기존 SOTA 방법들에 비해 현저히 우월하거나 경쟁력 있는 성능을 보여주었습니다.
    *   이는 SAM이 본질적으로 이미지 분할 모델임에도 불구하고, 비디오 생성 및 편집 작업에서도 뛰어난 성능을 발휘하는 "무엇이든 분할"이라는 잠재력을 증명합니다.

## 🧠 Insights & Discussion
*   **함의**: SAM은 이미지 중심 모델임에도 불구하고, 비디오 도메인에서 강력한 제로샷 일반화 능력을 보입니다. 특히 소규모 또는 특정 데이터 분포를 가진 비디오 작업에서 도메인 격차를 메우고 우수한 성능을 발휘할 수 있습니다. 이는 "SAM for anything"이라는 개념이 비디오 분야에서도 유효함을 입증합니다.
*   **한계**:
    *   비디오별 최첨단(SOTA) 방법에 비해 직접적인 SAM 적용은 여전히 성능 격차를 보일 수 있으며, 특히 시간적 일관성과 효율성 측면에서 도전이 있습니다.
    *   많은 SAM 기반 방법들은 비디오 데이터의 핵심인 시공간적 대응 관계 모델링이 부족합니다.
*   **향후 연구 방향**:
    *   **대규모 비디오 데이터셋 구축**: SAM을 활용한 자동 마스크 주석 생성 등을 통해 고품질의 밀집 주석이 있는 대규모 비디오 데이터셋 구축이 필요합니다.
    *   **대규모 비디오 파운데이션 모델 구축**: 이미지 수준의 사전 학습 및 적응을 넘어, 비디오의 복잡하고 동적인 특성을 이해하는 비디오 파운데이션 모델 개발이 시급합니다 (예: 의료 비디오 파운데이션 모델).
    *   **파라미터 효율적인 훈련 및 빠른 추론**: 높은 데이터 차원과 계산 오버헤드를 줄이기 위한 어댑터, 프롬프트 학습, 모델 압축 등의 효율적인 훈련 전략 및 추론 방법이 중요합니다.
    *   **더 많은 모달리티 통합**: 시각, 텍스트, 오디오를 넘어 포인트 클라우드, 적외선, 깊이 이미지, 이벤트 스트림 등 다양한 모달리티를 통합하고, 비쌍을 이루는 멀티모달 데이터에서도 작동하는 통합 모델 개발이 필요합니다.
    *   **신뢰할 수 있고 해석 가능한 비디오 파운데이션 모델**: 프라이버시 침해 및 보안 위험에 대비하여 모델의 견고성을 강화하고, 복잡한 모델의 의사결정 과정을 이해할 수 있도록 해석 가능성을 높이는 연구가 중요합니다.
    *   **SAM을 위한 더 많은 혁신 기회**: 비디오 캡션, 비디오 기반 이벤트 감지, 행동 인식, 비디오 요약, 프레임 보간 등 SAM이 아직 다루지 않았거나 충분히 연구되지 않은 비디오 작업들이 많습니다. 또한, 전통적인 기술(지식 증류, 그래프 학습)과 최첨단 기술(비디오 확산 모델, XAI, 체화된 AI)과의 결합을 통해 SAM의 다용성(versatility)을 확장할 수 있습니다.

## 📌 TL;DR
이 논문은 Segment Anything Model (SAM)이 비디오 도메인에서 어떻게 활용되고 있는지를 체계적으로 조사합니다. 주요 문제는 SAM이 이미지 분할에서 뛰어난 성능을 보였음에도 불구하고, 비디오의 고유한 시간적 및 계산적 도전 과제를 다루는 포괄적인 검토가 부족하다는 점입니다. 논문은 비디오 이해, 생성, 편집의 세 가지 주요 영역에서 SAM 기반 방법들을 분류하고, SOTA 방법들과 비교 분석합니다. 핵심 결과는 SAM이 비디오 특정 SOTA 모델에 비해 일부 성능 격차를 보일 수 있지만, 제한된 데이터나 특정 틈새 시장에서는 뛰어난 제로샷 일반화 능력을 발휘하며, 비디오 생성 및 편집에서는 강력한 경쟁력을 보여준다는 것입니다. 미래 방향으로는 대규모 비디오 데이터셋 및 파운데이션 모델 구축, 효율적인 훈련/추론, 다중 모달리티 통합, 모델의 신뢰성 및 해석 가능성 향상, 그리고 다양한 비디오 작업 및 최첨단 기술과의 융합을 제시합니다.