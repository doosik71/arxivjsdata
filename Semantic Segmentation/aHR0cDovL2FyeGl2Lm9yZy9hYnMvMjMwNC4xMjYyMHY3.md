# Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation

Junde Wu, Wei Ji, Yuanpei Liu, Huazhu Fu, Min Xu, Yanwu Xu, Yueming Jin

## 🧩 Problem to Solve

최근 출시된 Segment Anything Model (SAM)은 일반 이미지 분할에서 인상적인 성능을 보여주지만, 의료 도메인에 특화된 지식이 부족하여 의료 영상 분할에서는 기대 이하의 성능을 보입니다. 기존에는 SAM을 의료 영상 데이터에 완전히 파인튜닝하는 방식(MedSAM 등)이 있었으나, 이는 계산 및 메모리 측면에서 비용이 많이 들고, 사전 학습된 모델의 전이 학습 능력을 완전히 활용하지 못한다는 의문이 있습니다. 특히, 2D 이미지에 최적화된 SAM을 CT, MRI와 같은 3D 의료 영상에 적용하고, 사용자 프롬프트를 효과적으로 활용하여 분할 성능을 개선하는 것이 주요 과제입니다.

## ✨ Key Contributions

* **Adaption(어댑션) 접근 방식 도입:** SAM 아키텍처에 간단하면서도 효과적인 어댑터 모듈을 통합하여 의료 영상 분할 성능을 크게 향상시키며, 전체 파라미터의 2%만 업데이트합니다.
* **Space-Depth Transpose (SD-Trans) 제안:** 2D SAM을 3D 의료 영상에 적응시키기 위한 새로운 기법인 SD-Trans를 제안하여, 공간(spatial) 및 깊이(depth) 차원의 상관관계를 효율적으로 학습합니다.
* **Hyper-Prompting Adapter (HyP-Adpt) 제안:** 사용자 프롬프트(예: 클릭, 바운딩 박스) 정보를 어댑터에 조건부로 통합하는 HyP-Adpt를 개발하여, 프롬프트 기반 적응을 가능하게 합니다.
* **광범위한 평가 및 SOTA 달성:** 5가지 다른 모달리티에 걸쳐 17가지 의료 영상 분할 태스크에 대한 광범위한 실험을 통해, Med-SA가 기존 SAM 및 최첨단 의료 영상 분할 방법(nnUNet, Swin-UNetr 등)보다 우수함을 입증했습니다. 특히, BTCV 벤치마크에서 Swin-UNetr보다 2.9%, vanilla SAM보다 34.8%, 완전 파인튜닝된 MedSAM보다 9.4% 더 나은 성능을 달성했습니다.

## 📎 Related Works

* **Interactive Segmentation (대화형 분할):** 초기에는 그래프 컷(Graph Cuts)이나 랜덤 워크(Random Walks)와 같은 최적화 기법에 기반했으나, 딥러닝 도입(DIOS, CDNet, RITM 등)으로 발전했습니다. 최근 SAM은 제로샷 분할에서의 대화형 분할의 중요성을 강조하며 비전 파운데이션 모델에 큰 영향을 미쳤습니다.
* **Parameter-Efficient Fine-Tuning (PEFT, 파라미터 효율적 파인튜닝):** 대규모 사전 학습 모델을 특정 작업에 효율적으로 파인튜닝하는 전략으로, 전체 파라미터의 5% 미만만 학습하여 계산 효율성을 높이고, 파국적 망각(catastrophic forgetting)을 방지하며 일반화 성능을 향상시킵니다. Adaption(Hu et al. 2021)은 NLP뿐만 아니라 컴퓨터 비전 분야에서도 효과적인 PEFT 기법으로 주목받고 있습니다.

## 🛠️ Methodology

Med-SA는 사전 학습된 SAM 모델의 파라미터를 고정하고, Adaption 기술을 활용하여 소수의 추가 파라미터만 학습하는 방식으로 의료 영상 분할에 적응시킵니다.

1. **Med-SA 아키텍처:**
    * **Adapter 모듈:** 병목 현상(bottleneck) 모델로, 다운-프로젝션(MLP), ReLU 활성화, 업-프로젝션(MLP)으로 구성됩니다.
    * **SAM 인코더에 어댑터 통합:** 각 ViT 블록에 두 개의 어댑터를 삽입합니다. 첫 번째 어댑터는 멀티-헤드 어텐션 후에, 두 번째 어댑터는 MLP 레이어의 잔차 경로에 배치됩니다.
    * **SAM 디코더에 어댑터 통합:** 각 ViT 블록에 세 개의 어댑터를 통합합니다. 첫 번째 어댑터는 프롬프트 임베딩을 통합하는 데 사용되는 HyP-Adpt이며, 두 번째와 세 번째 어댑터는 인코더와 유사하게 배치됩니다.

2. **Space-Depth Transpose (SD-Trans):**
    * **목표:** 2D SAM을 3D 의료 영상(CT, MRI)에 적응시키기 위함.
    * **동작 방식:** 3D 샘플 입력($D \times N \times L$)에 대해 어텐션 연산을 공간(space) 브랜치와 깊이(depth) 브랜치로 이분합니다.
        * **공간 브랜치:** $D \times N \times L$을 멀티-헤드 어텐션에 입력하여 $N \times L$에 걸쳐 공간 상관관계를 학습합니다.
        * **깊이 브랜치:** 입력 행렬을 $N \times D \times L$로 전치(transpose)한 후 동일한 멀티-헤드 어텐션에 입력하여 $D \times L$에 걸쳐 깊이 상관관계를 학습합니다.
    * 두 브랜치의 결과를 다시 원래 모양으로 전치하여 합산함으로써 깊이 정보를 통합합니다.

3. **Hyper-Prompting Adapter (HyP-Adpt):**
    * **목표:** 대화형 모델에서 중요한 사용자 제공 시각 프롬프트(클릭, 바운딩 박스)를 어댑터에 통합하여 프롬프트 조건부 적응을 가능하게 합니다.
    * **동작 방식:** HyperNetworks에서 영감을 받아, 프롬프트 임베딩 $e_{\text{prompt}}$를 사용하여 일련의 가중치 맵 $W$를 생성합니다.
        $$W = \text{Re}(\text{M}(e_{\text{prompt}}))$$
        여기서 $\text{Re}$는 재형성(reshape), $\text{M}$은 MLP 레이어입니다.
    * 생성된 가중치 맵 $w_{\text{prompt}}$는 어댑터 임베딩 $e_{\text{down}}$에 직접 행렬 곱셈($\otimes$)으로 적용됩니다.
        $$e_{\text{down}}^{n+1} = \text{ReLU}(\text{Norm}(e_{\text{down}}^n \otimes w_{\text{prompt}}))$$
    * 이를 통해 적은 파라미터로 광범위하고 깊이 있는 피처 레벨 상호작용이 가능하며, 다양한 모달리티와 다운스트림 태스크에 유연하게 대응할 수 있습니다.

4. **학습 전략:**
    * **프롬프트 생성:** 학습 과정에서 클릭 프롬프트와 바운딩 박스(BBox) 프롬프트를 사용합니다.
        * **BBox 프롬프트:** SAM과 동일한 방식을 채택합니다.
        * **클릭 프롬프트:** 포지티브 클릭(전경)과 네거티브 클릭(배경)을 표시하기 위해 무작위 샘플링과 반복적 샘플링 전략을 결합하여 사용합니다. 반복적 샘플링은 실제 사용자 상호작용을 모방하여, 네트워크의 이전 예측에서 잘못된 영역에 새로운 클릭을 배치합니다.

## 📊 Results

* **BTCV 복부 다기관 분할:** Med-SA는 모든 12개 기관에서 SOTA 성능을 달성했으며, BBox 0.75 프롬프트 사용 시 89.8%의 Dice 점수를 기록하여 이전 SOTA인 Swin-UNetr(138M 학습 가능한 파라미터)보다 2.9%p 높은 성능을 보였습니다. Med-SA는 13M 파라미터만 업데이트하여, MedSAM(636M 학습 가능한 파라미터)보다도 우수한 성능을 달성했습니다.
* **다중 모달리티 의료 영상 분할:** 안저 영상(Optic Disc/Cup), 뇌 MRI(뇌종양), 초음파(갑상선 결절), 피부경 검사(멜라노마) 등 다양한 모달리티에서 Med-SA가 SOTA 성능을 달성하며 뛰어난 일반화 능력을 입증했습니다. 특히, 3D 이미지 적응성 덕분에 BraTs 벤치마크에서 Swin-UNetr보다 Dice 점수 2.1%p, HD95 지표 1.86 더 나은 성능을 달성했습니다.
* **파라미터 효율성:** Med-SA는 SAM 전체 파라미터의 2%만 업데이트하여 기존 SOTA 방법들과 완전 파인튜닝된 MedSAM을 능가하는 성능을 보였습니다.
* **정성적 비교:** 시각적 비교에서 Med-SA는 사람의 눈으로도 구분하기 어려운 영역을 정확하게 분할하는 반면, SAM은 경계가 명확한 기관에서도 실패하는 경우가 많았습니다.
* **어블레이션 스터디:**
  * **SD-Trans:** 2D 이미지를 3D 이미지로 처리하는 기능은 3D 데이터셋(BTCV 및 Brain Tumor)에서 성능을 크게 향상시켜 SD-Trans의 효과를 입증했습니다.
  * **HyP-Adpt:** 프롬프트 임베딩을 통합하는 방법으로 HyP-Adpt가 단순한 덧셈이나 연결보다 훨씬 더 뛰어난 성능 향상을 가져왔음을 확인했습니다.

## 🧠 Insights & Discussion

* **의료 도메인 특화 적응의 중요성:** SAM은 일반 이미지에서 뛰어난 성능을 보이지만, 의료 영상의 낮은 대비, 모호한 조직 경계, 작은 병변 영역 등 특수한 특성으로 인해 의료 도메인에 대한 명시적인 적응(adaptation)이 필수적입니다. Med-SA는 이러한 필요성을 효과적으로 입증했습니다.
* **파라미터 효율성의 이점:** Med-SA는 전체 파라미터의 극히 일부(2%)만 업데이트하면서도 SOTA 성능을 달성함으로써, 대규모 파운데이션 모델을 특정 도메인에 적용할 때 완전 파인튜닝이 항상 최적의 솔루션이 아님을 시사합니다. 이는 계산 리소스 제약이 있는 환경에서도 고성능 모델을 배포할 수 있는 가능성을 열어줍니다.
* **3D 데이터 및 프롬프트 활용의 중요성:** SD-Trans와 HyP-Adpt의 효과는 의료 영상 분할에서 3D 공간 정보를 통합하고 사용자 프롬프트를 지능적으로 활용하는 것이 얼마나 중요한지를 보여줍니다. 특히 3D 의료 영상에서 슬라이스 간의 상관관계를 고려하지 않는 기존 2D 기반 접근 방식의 한계를 극복했습니다.
* **향후 연구 방향:** Med-SA는 대화형 의료 영상 분할을 위한 훌륭한 출발점이며, 미래의 파운데이션 의료 모델 개발 및 새로운 파인튜닝 기법 연구에 영감을 줄 것으로 예상됩니다.

## 📌 TL;DR

Med-SA는 일반 이미지용으로 설계된 SAM이 의료 영상에서 낮은 성능을 보이는 문제를 해결하기 위해, SAM 파라미터의 2%만 업데이트하는 효율적인 Adaption 프레임워크를 제안합니다. 이 프레임워크는 2D SAM을 3D 의료 영상에 적응시키는 SD-Trans와 프롬프트 조건부 적응을 가능하게 하는 HyP-Adpt를 핵심으로 하며, 17개 의료 영상 분할 태스크에서 기존 SAM, MedSAM, 그리고 SOTA 의료 분할 모델들을 뛰어넘는 성능을 달성하며 의료 영상 분할에서 대규모 모델의 효율적인 적응 가능성을 입증했습니다.
