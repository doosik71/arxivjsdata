# Applying ViT in Generalized Few-shot Semantic Segmentation

Liyuan Geng, Jinhong Xia, Yuanhe Guo

## 🧩 Problem to Solve

이 논문은 일반화된 소수점 의미론적 분할(Generalized Few-shot Semantic Segmentation, GFSS) 프레임워크에서 기존 모델들의 한계를 다룹니다.

* **기존 의미론적 분할:** 새로운 클래스에 대한 대규모 주석 데이터가 필요하여 확장성이 떨어집니다.
* **소수점 분할(FSS):** 적은 수의 주석 데이터로 새로운 클래스에 적응하려 하지만, 쿼리 이미지의 클래스가 지원 이미지에 완전히 포함되어야 한다는 가정이 실제 시나리오에서는 수동 데이터 준비에 많은 비용을 발생시킵니다. 또한, 기준 클래스(base classes)의 성능 저하를 간과하고 오직 새로운 클래스(novel classes)에 대해서만 평가됩니다.
* **GFSS의 목표:** FSS의 이러한 한계를 극복하여 지원 이미지와 쿼리 이미지 간에 정확한 클래스 일치를 요구하지 않고, 기준 클래스와 새로운 클래스 모두에 대해 모델 성능을 평가합니다.

이 연구는 기존 GFSS 방법들이 주로 ResNet 아키텍처에 기반하고 복잡한 손실 함수를 사용하는 경향이 있음을 지적하며, 대규모 사전 학습된 Vision Transformer(ViT) 모델이 GFSS 작업에서 더 나은 성능을 보일 수 있는지, 그리고 이들의 잠재력과 잠재적인 주의사항(caveats)을 탐색하는 것을 주요 문제로 삼습니다.

## ✨ Key Contributions

* 동일한 데이터셋과 학습 조건에서 ViT 기반 모델(DINO, DINOv2)이 ResNet 기반 모델을 능가하며, 특히 DINOv2는 소수점 분할 작업에서 DINO보다 훨씬 우수함을 입증했습니다.
* DINOv2와 선형 분류기 조합이 1-shot 시나리오에서 ResNet 구조 중 최고 성능보다 116% 더 높은 성능을 보이며 PASCAL-5$_i$ 벤치마크에서 선두를 차지했습니다.
* Mask Transformer 디코더가 DINOv2에 연결될 때 선형 분류기보다 과적합(overfitting)될 가능성이 더 높다는 것을 관찰했습니다. 이는 Mask Transformer가 기준 클래스 지식 보존에는 유리하지만, 새로운 클래스에서는 소수의 지원 이미지만으로 학습할 때 과적합되기 쉽다는 것을 의미합니다.
* 대규모 사전 학습된 ViT 기반 모델이 GFSS 작업에서 큰 잠재력을 가지고 있음을 보여주었으며, 향후 테스트 벤치마크에서 더 많은 개선을 기대할 수 있음을 시사합니다.

## 📎 Related Works

* **Few-shot Segmentation (FSS):** 소수점 학습 패러다임을 의미론적 분할에 확장한 분야입니다. 초기에는 듀얼-브랜치 프레임워크(예: [8, 21]), 프로토타입 학습(예: [15, 29]), 그래프 어텐션 메커니즘(예: [26, 30]) 등이 활용되었으며, 최근에는 비전 트랜스포머(예: [16, 18])가 적용되고 있습니다.
* **Generalized Few-shot Segmentation (GFSS):** FSS의 단점을 해결하기 위해 등장한 새로운 설정입니다. CAPL [24]이 첫 시도였으며, DIaM [11]은 학습 및 추론 단계의 분리 및 복잡한 손실 함수를 통해 성능을 향상시켰습니다.
* **Visual Transformer (ViT):** 이미지 패치를 토큰화하여 트랜스포머 아키텍처 [25]를 컴퓨터 비전 작업에 도입한 모델 [9]입니다. 의미론적 분할(SegFormer [28], Segmenter [22]), 객체 탐지 [3], 다중 객체 추적 [23] 등 다양한 비전 작업에 적용되고 있습니다.
* **ViT 기반 모델의 사전 학습:** DINO [4]는 간단하면서도 효과적인 자기 지도 학습 프레임워크를 제안했으며, DINOv2 [19]는 대규모 비지도 데이터를 사용하여 사전 학습되어 더욱 강화되었습니다.
* **Segmentation에서의 Visual Transformer:** SegFormer [28]는 계층적 트랜스포머 인코더와 경량 All-MLP 디코더를 사용하며, Segmenter [22]는 완전한 트랜스포머 기반 인코더-디코더 아키텍처를 사용하고 Mask Transformer [7]를 디코더로 활용하여 유연한 클래스 임베딩 초기화를 통해 소수점 학습에 적합합니다.

## 🛠️ Methodology

논문은 GFSS 프레임워크를 따르며, 전체 절차를 **기본 학습(Base Training)**과 **추론(Inference)** 두 단계로 나눕니다.

* **모델 구성:** 특징 추출기(인코더) $f_{\phi}$와 분할 맵을 출력하는 디코더로 구성됩니다.

* **기본 학습 단계:**
  * 모델은 기준 클래스 $C_b$만을 분할하도록 표준 학습 절차와 교차 엔트로피 손실을 사용하여 학습됩니다.
  * **ResNet 백본:** ResNet-34 및 ResNet-50 [13] 사전 학습된 백본을 사용합니다. 디코더로는 선형 분류기 또는 UPerNet [27]을 사용하며, 전체 인코더-디코더 아키텍처의 모든 파라미터가 학습됩니다.
  * **ViT 백본:** DINO [4] 및 DINOv2 [19]를 사전 학습된 백본으로 직접 로드합니다. ViT 백본은 **고정**하고, 선형 분류기 또는 Mask Transformer [7] 디코더의 파라미터만 학습합니다.

* **추론 단계:**
  * 특징 추출기 $f_{\phi}$는 고정됩니다. 디코더는 새로운 클래스 $C_n$에 대한 지원 이미지를 사용하여 증강(augment) 또는 적응(adapt)됩니다.
  * **지원 레이블 및 예측 정렬:** 모델 예측과 지원 이미지 레이블 간의 불일치를 해결하기 위해, 조정된 예측은 기존 배경 클래스 및 기준 클래스 확률의 합으로 배경을 설정하고, 기준 클래스 차원을 0으로, 새로운 클래스 차원을 그대로 유지하여 구성됩니다. 이 조정된 예측과 실제 레이블에 교차 엔트로피 손실을 적용합니다.
  * **디코더 증강 방식:**
    * **선형 분류기/UPerNet:** 사전 학습된 기준 분류기 $\theta_b$에 새로운 프로토타입 $\theta_n$을 추가하여 최종 분류기 $[\theta_b; \theta_n]$를 구성합니다. UPerNet [27]의 파라미터는 추론 단계에서 완전히 고정됩니다.
    * **Mask Transformer:** Mask Transformer [7]를 디코더로 사용합니다. $C_n$개의 클래스 임베딩을 무작위로 초기화합니다. 이 새로운 클래스 임베딩과 원본 이미지 패치, 기준 클래스 임베딩이 Mask Transformer에 입력됩니다. 추론 단계에서는 Mask Transformer 내의 클래스 임베딩과 layernorm 파라미터만 지원 이미지로 학습됩니다.

* **실험 설정:**
  * **데이터셋:** PASCAL-5$_i$ [21] (PASCAL VOC 2012 + SDS 주석). 5개의 새로운 클래스와 15개의 기준 클래스로 구성됩니다.
  * **평가 지표:** 기준 클래스와 새로운 클래스 각각에 대한 평균 Intersection-over-Union (mIoU)을 사용합니다. 5회 독립 실행 결과의 평균을 사용합니다.
  * **학습 상세:** 기본 학습은 100 epoch 동안 바닐라 교차 엔트로피 손실을 사용하며, 배치 크기는 8, SGD 옵티마이저를 사용합니다(초기 학습률 $2.5 \times 10^{-4}$). 추론 단계에서는 각 쿼리 이미지에 대해 $|C_n| \times \text{shots}$ 개의 지원 이미지로 디코더를 300회 반복 학습합니다(SGD, 학습률 $1.25 \times 10^{-3}$).

## 📊 Results

* **DINOv2의 압도적인 성능:** DINOv2 [19]를 백본으로 사용했을 때, 1-shot 또는 5-shot 설정 모두에서 ResNet 및 DINO 모델을 월등히 능가합니다. 특히 DINOv2 + 선형 분류기 조합은 1-shot 시나리오에서 PASCAL-5$_i$ 벤치마크에서 기준 ResNet 모델보다 116% 뛰어난 성능을 보였습니다.
* **UPerNet의 특성:** UPerNet [27]은 기본 학습 단계에서 기준 클래스 정보를 잘 보존하지만, 추론 단계에서 새로운 클래스로 일반화하는 데 어려움을 겪습니다.
* **ResNet의 성능 저하:** ResNet 백본 [13]은 5-shot 학습에서 1-shot 학습보다 오히려 성능이 저하되는 반직관적인 결과를 보였습니다. 이는 모델이 배경 픽셀을 다른 클래스로 잘못 예측하도록 제한하지 않아, 부정확한 새로운 클래스 정보가 학습에 통합되어 성능이 저하되기 때문으로 분석됩니다 (그림 3 참조).
* **Mask Transformer의 과적합:** Mask Transformer [7]를 디코더로 사용했을 때, DINO [4] 또는 DINOv2 [19] 백본 모두에서 선형 분류기를 사용한 경우보다 새로운 클래스에 대한 성능이 저조했습니다. 이는 Mask Transformer가 소수의 지원 이미지로 학습할 때 과적합되기 쉽기 때문으로 추정됩니다. 다만, DINOv2 + Mask Transformer 설정에서는 5-shot mIoU가 1-shot보다 현저히 높아, 더 많은 지원 이미지가 과적합 효과를 완화할 수 있음을 보여줍니다.
* **선형 분류기의 새로운 클래스 적응력:** 선형 분류기는 기준 클래스 성능이 크게 감소하더라도 UPerNet보다 새로운 클래스에 더 잘 적응하는 경향을 보였습니다.

## 🧠 Insights & Discussion

* **사전 학습된 ViT의 잠재력:** DINOv2와 같은 대규모 사전 학습된 ViT 모델은 GFSS 작업에서 강력한 일반화 능력을 바탕으로 엄청난 잠재력을 보여줍니다. 이는 복잡한 손실 함수 없이도 우수한 성능을 달성할 수 있음을 시사합니다.
* **기준 지식 보존 vs. 새로운 클래스 적응:** UPerNet과 같은 복잡한 디코더는 기준 클래스 지식 보존에는 탁월하지만, 새로운 클래스에 대한 일반화에는 한계가 있습니다. 반면, 선형 분류기처럼 간단한 디코더는 기준 클래스 성능 저하를 감수하더라도 새로운 클래스에 더 효과적으로 적응합니다.
* **소수점 설정에서의 과적합 문제:** Mask Transformer와 같이 강력한 디코더조차도 1-shot과 같이 극히 제한된 지원 데이터에 노출될 경우 과적합되기 쉽습니다. 이는 복잡한 디코더가 극단적인 소수점 시나리오에서는 더 많은 미세 조정 데이터나 정규화 기법이 필요할 수 있음을 의미합니다.
* **ResNet의 GFSS 한계:** ResNet은 이미지 전반에 걸쳐 의미론적 정보를 효과적으로 일반화하는 데 어려움을 겪으며, 소수점 학습에서 추가(하지만 여전히 불충분한) 샘플이 주어질 때 배경 픽셀을 잘못 분류하는 등 부정확한 정보를 학습할 수 있습니다.
* **단순한 손실 함수의 효과:** 이 연구는 간단한 교차 엔트로피 손실만을 사용하여 강력한 결과를 달성했습니다. 이는 GFSS에서 복잡한 손실 함수보다 백본 및 디코더 아키텍처, 특히 사전 학습된 ViT의 선택이 더 중요하다는 것을 시사합니다.

## 📌 TL;DR

이 논문은 일반화된 소수점 의미론적 분할(GFSS) 프레임워크에서 ViT 기반 모델의 잠재력을 탐구합니다. 연구는 DINOv2 인코더와 선형 분류기 디코더 조합이 PASCAL-5$_i$ 벤치마크에서 기존 ResNet 기반 모델을 크게 능가함을 보여줍니다. 특히, DINOv2는 뛰어난 소수점 학습 능력을 입증하지만, Mask Transformer와 같은 복잡한 디코더는 소수점 시나리오에서 과적합될 수 있으며, ResNet은 배경 픽셀을 새로운 클래스로 오분류하는 등 일반화에 어려움을 겪는 경향을 보입니다. 이는 대규모 사전 학습된 ViT 모델이 GFSS에 큰 잠재력을 가지고 있음을 시사합니다.
