{
  "url": "http://arxiv.org/abs/2112.14757v2",
  "title": "A Simple Baseline for Open-Vocabulary Semantic Segmentation with\n  Pre-trained Vision-language Model",
  "authors": "Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, Xiang Bai",
  "year": 2021,
  "abstract": "Recently, open-vocabulary image classification by vision language\npre-training has demonstrated incredible achievements, that the model can\nclassify arbitrary categories without seeing additional annotated images of\nthat category. However, it is still unclear how to make the open-vocabulary\nrecognition work well on broader vision problems. This paper targets\nopen-vocabulary semantic segmentation by building it on an off-the-shelf\npre-trained vision-language model, i.e., CLIP. However, semantic segmentation\nand the CLIP model perform on different visual granularity, that semantic\nsegmentation processes on pixels while CLIP performs on images. To remedy the\ndiscrepancy in processing granularity, we refuse the use of the prevalent\none-stage FCN based framework, and advocate a two-stage semantic segmentation\nframework, with the first stage extracting generalizable mask proposals and the\nsecond stage leveraging an image based CLIP model to perform open-vocabulary\nclassification on the masked image crops which are generated in the first\nstage. Our experimental results show that this two-stage framework can achieve\nsuperior performance than FCN when trained only on COCO Stuff dataset and\nevaluated on other datasets without fine-tuning. Moreover, this simple\nframework also surpasses previous state-of-the-arts of zero-shot semantic\nsegmentation by a large margin: +29.5 hIoU on the Pascal VOC 2012 dataset, and\n+8.9 hIoU on the COCO Stuff dataset. With its simplicity and strong\nperformance, we hope this framework to serve as a baseline to facilitate future\nresearch. The code are made publicly available\nat~\\url{https://github.com/MendelXu/zsseg.baseline}."
}