{
  "url": "http://arxiv.org/abs/2307.02003v3",
  "title": "Multi-Modal Prototypes for Open-World Semantic Segmentation",
  "authors": "Yuhuan Yang, Chaofan Ma, Chen Ju, Fei Zhang, Jiangchao Yao, Ya Zhang, Yanfeng Wang",
  "year": 2023,
  "abstract": "In semantic segmentation, generalizing a visual system to both seen\ncategories and novel categories at inference time has always been practically\nvaluable yet challenging. To enable such functionality, existing methods mainly\nrely on either providing several support demonstrations from the visual aspect\nor characterizing the informative clues from the textual aspect (e.g., the\nclass names). Nevertheless, both two lines neglect the complementary intrinsic\nof low-level visual and high-level language information, while the explorations\nthat consider visual and textual modalities as a whole to promote predictions\nare still limited. To close this gap, we propose to encompass textual and\nvisual clues as multi-modal prototypes to allow more comprehensive support for\nopen-world semantic segmentation, and build a novel prototype-based\nsegmentation framework to realize this promise. To be specific, unlike the\nstraightforward combination of bi-modal clues, we decompose the high-level\nlanguage information as multi-aspect prototypes and aggregate the low-level\nvisual information as more semantic prototypes, on basis of which, a\nfine-grained complementary fusion makes the multi-modal prototypes more\npowerful and accurate to promote the prediction. Based on an elastic mask\nprediction module that permits any number and form of prototype inputs, we are\nable to solve the zero-shot, few-shot and generalized counterpart tasks in one\narchitecture. Extensive experiments on both PASCAL-$5^i$ and COCO-$20^i$\ndatasets show the consistent superiority of the proposed method compared with\nthe previous state-of-the-art approaches, and a range of ablation studies\nthoroughly dissects each component in our framework both quantitatively and\nqualitatively that verify their effectiveness."
}