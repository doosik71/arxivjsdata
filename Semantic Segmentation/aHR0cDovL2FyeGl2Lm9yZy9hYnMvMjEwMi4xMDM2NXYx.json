{
  "url": "http://arxiv.org/abs/2102.10365v1",
  "title": "Analyzing Overfitting under Class Imbalance in Neural Networks for Image\n  Segmentation",
  "authors": "Zeju Li, Konstantinos Kamnitsas, Ben Glocker",
  "year": 2021,
  "abstract": "Class imbalance poses a challenge for developing unbiased, accurate\npredictive models. In particular, in image segmentation neural networks may\noverfit to the foreground samples from small structures, which are often\nheavily under-represented in the training set, leading to poor generalization.\nIn this study, we provide new insights on the problem of overfitting under\nclass imbalance by inspecting the network behavior. We find empirically that\nwhen training with limited data and strong class imbalance, at test time the\ndistribution of logit activations may shift across the decision boundary, while\nsamples of the well-represented class seem unaffected. This bias leads to a\nsystematic under-segmentation of small structures. This phenomenon is\nconsistently observed for different databases, tasks and network architectures.\nTo tackle this problem, we introduce new asymmetric variants of popular loss\nfunctions and regularization techniques including a large margin loss, focal\nloss, adversarial training, mixup and data augmentation, which are explicitly\ndesigned to counter logit shift of the under-represented classes. Extensive\nexperiments are conducted on several challenging segmentation tasks. Our\nresults demonstrate that the proposed modifications to the objective function\ncan lead to significantly improved segmentation accuracy compared to baselines\nand alternative approaches."
}