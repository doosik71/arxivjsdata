# BiCANet: Bi-directional Contextual Aggregating Network for Image Semantic Segmentation

Quan Zhou, Dechun Cong, Bin Kang, Xiaofu Wu, Baoyu Zheng, Huimin Lu, Longin Jan Latecki

---

## 🧩 Problem to Solve

시맨틱 분할(semantic segmentation)에서 컨볼루션 신경망(CNN)은 높은 수준의 의미를 추상화하는 데 강력한 능력을 보여주지만, 다음과 같은 근본적인 문제점들을 안고 있습니다:

* **공간 정보 손실 및 세부 특징 복원 어려움:** 기존 FCN(Fully Convolutional Networks)은 여러 단계의 공간 풀링(spatial pooling)과 컨볼루션 스트라이드(convolution stride)로 인해 특징 표현의 차원이 크게 줄어들어 미세한 이미지 구조를 많이 잃습니다. 이는 분류 작업에는 유용할 수 있으나, 픽셀 단위 예측을 요구하는 시맨틱 분할에는 성능 저하를 초래합니다. 인코더-디코더 네트워크(EDN) 역시 인코더 단계에서 발생하는 상당한 공간 정보 손실로 인해 저수준 시각 특징의 세부 사항을 복원하기 어렵습니다.
* **특징 공간 중심의 문맥 학습 한계:** 기존 CNN 기반 접근 방식들은 주로 특징 공간(feature space)에서 문맥 표현을 학습하며, **카테고리 관점(categorical perspective)**에서 문맥적 단서를 명시적으로 포착하는 것을 간과합니다. 이는 시맨틱 분할에서 필수적인 역할을 함에도 불구하고 종종 무시됩니다.
* **비적응적인 문맥 집계:** 서로 다른 픽셀이 다른 범위의 의존성을 필요로 함에도 불구하고, 기존 CNN들은 문맥을 비적응적인 방식으로 집계하여 고정된 수용장(receptive field)에 의존하는 경향이 있습니다. 이로 인해 수용장보다 훨씬 크거나 작은 객체는 파편화되거나 잘못 분류될 수 있습니다.
* **다양한 범위 의존성 통합의 어려움:** 다양한 범위의 의존성(단거리, 장거리, 전역 문맥)을 적절하게 통합하는 CNN 아키텍처를 설계하는 것이 여전히 어렵습니다.

## ✨ Key Contributions

* **카테고리 기반 문맥 탐색을 위한 새로운 CCPB(Contextual Condensed Projection Block) 제안:** 시맨틱 분할에서 종종 간과되었던 카테고리 관점에서 문맥을 명시적으로 조사할 수 있도록 합니다.
* **양방향 문맥 교환 네트워크 BCIB(Bi-directional Context Interaction Block) 제안:** CCPB를 기반으로 양방향 건너뛰기 연결(bi-directional skipped-connections)을 통해 중간 컨볼루션 레이어의 문맥 단서를 최대한 활용합니다.
* **다중 스케일 문맥 융합을 위한 MCFB(Multi-scale Contextual Fusion Block) 도입:** 단거리, 장거리 및 전역 의존성을 인코딩하여 서로 다른 픽셀에 적절한 문맥 단서를 할당함으로써 다중 스케일 문맥을 명시적으로 탐색합니다.
* **세 가지 벤치마크 데이터셋에서 SOTA 성능 달성:** PASCAL VOC 2012 (86.7% mIoU), Cityscapes (82.4% mIoU), ADE20K (38.66% mIoU)에서 최신 기술 수준의 네트워크보다 우수한 결과를 달성했습니다.

## 📎 Related Works

이 논문은 시맨틱 분할에서 문맥 정보를 포착하는 기존 심층 신경망을 EDN(Encoder-Decoder Networks), CAN(Context Aggregation Networks), AEN(Attention Embedding Networks)의 세 가지 범주로 분류하고, BiCANet과의 차별점을 설명합니다.

* **인코더-디코더 네트워크 (EDNs):**
  * **주요 특징:** 인코더에서 디코더로 문맥 정보를 전달하기 위해 건너뛰기 연결(skip connections)을 활용합니다 (예: FCN-8s [1], SegNet [5], U-Net [36], RefineNet [10], GCN [2], SDNet [38]). 일부는 디코더에서 문맥 단서를 공식화합니다 (예: Bilinski et al. [15], Tian et al. [39]).
  * **BiCANet과의 차이:** 기존 EDN이 특징 공간에서 문맥을 포착하는 것과 달리, BiCANet은 **카테고리 관점**에서 문맥 단서를 인코딩합니다. 또한 BCIB의 양방향 건너뛰기 연결을 통해 더욱 강력한 문맥 탐색 능력을 가집니다.
* **문맥 집계 네트워크 (CANs):**
  * **주요 특징:** 다중 스케일 특징을 통합하여 문맥 정보를 인코딩합니다 (예: Dilated/Atrous convolution [4, 17], PSPNet [3], ASPP [4], HRNet [40]). 그래프 모델링(graph modeling)을 CNN과 통합하여 장거리 문맥 정보를 집계하기도 합니다 (예: CRF-RNN [19], DSNet [12]).
  * **BiCANet과의 차이:** 기존 CAN이 고비용의 계산이나 제한된 스케일 특징 집계 능력을 가질 수 있는 반면, BiCANet은 어떤 사후 처리(post-processing)도 없이 효율적으로 구현되며, 다중 스케일 문맥을 집계하는 더 강력한 능력을 제공합니다.
* **어텐션 임베딩 네트워크 (AENs):**
  * **주요 특징:** 지역 정보의 분류 모호성을 줄이기 위해 전역 문맥 정보를 학습합니다. 채널별 어텐션(channel-wise attention) (예: ParseNet [46], EncNet [25], PANet [22])과 픽셀별 어텐션(pixel-wise attention) (예: DANet [6], OCNet [47], ACFNet [48])으로 나뉩니다.
  * **BiCANet과의 차이:** 기존 AEN이 단일 문맥 스케일에 의존하는 비합리성을 가질 수 있는 반면, BiCANet의 MCFB는 단거리부터 장거리, 심지어 전체 장면의 전역 의존성까지 픽셀 의미 레이블을 예측하며, 다른 픽셀에 적절한 문맥 단서를 할당하기 위해 다중 스케일 특징을 선택적으로 집계합니다.

## 🛠️ Methodology

제안된 BiCANet은 시맨틱 분할을 위해 카테고리 관점에서 문맥 정보를 포착하는 새로운 프레임워크입니다. 전체 아키텍처는 주로 CCPB, BCIB, MCFB 세 부분으로 구성됩니다.

### Contextual Condensed Projection Block (CCPB)

* **목표:** 계층적 특징을 시맨틱 카테고리 공간으로 투영하고, 객체의 다양한 스케일에 적응하기 위해 수용장을 적응적으로 확장합니다. 기존 FCN의 고정된 수용장 및 부정확한 특징 표현의 한계를 극복합니다.
* **구조:**
  * 입력 특징 $F \in \mathbb{R}^{W \times H \times C}$는 먼저 $1 \times 1$ 컨볼루션을 통해 채널 수가 줄어든 $F' \in \mathbb{R}^{W \times H \times C'}$으로 변환됩니다.
  * $F'$는 $D=3$개의 분기(branch)로 균등하게 분할됩니다.
  * 각 분기 $T_i$는 $1 \times 1$ 컨볼루션을 사용하여 저차원 임베딩을 생성하고, 마지막 두 분기는 $3 \times 3$ 컨볼루션을 연속으로 적용하여 다른 스케일의 수용장을 얻습니다.
  * 모든 분기의 변환된 특징은 연결(concatenation)됩니다: $F(F') = \bigodot_{i=1}^{D} T_i(F', \theta_i)$.
  * 이 집계된 변환은 잔여 함수(residual function)로 사용되어 $\tilde{F} = F' + F(F')$와 같이 압축된 출력을 생성합니다.
  * 최종적으로 $1 \times 1$ 컨볼루션이 $\tilde{F}$를 정의된 카테고리 수와 같은 채널을 가진 시맨틱 공간 특징 $\hat{F} \in \mathbb{R}^{W \times H \times L}$으로 투영합니다.

### Bi-directional Context Interaction Block (BCIB)

* **목표:** 특징 계층에서 서로 다른 해상도의 문맥 정보 상호작용을 강화하고, 세부 정보와 클래스 수준 의미론을 보완하여 성능을 향상시킵니다.
* **구조:**
  * 백본의 stage2부터 stage5까지의 특징을 입력으로 받아 4개의 병렬 경로로 구성됩니다.
  * 각 경로에서는 현재 경로를 제외한 모든 다른 경로의 특징들을 현재 경로의 해상도에 맞게 업샘플링(간단한 bilinear upsampling) 또는 다운샘플링(stride convolutions)합니다.
  * 이렇게 정렬된 특징들은 모두 합산되어 각 경로의 최종 출력 $\hat{F}_i$를 형성합니다:
        $$ \hat{F}_i = F_i + \sum_{m=i+1}^4 U_m(F_m) + \sum_{n=1}^{i-1} D_n(F_n, \theta_n) $$
  * 이 구조는 각 경로에서 입력과 출력 특징 간의 잔여 함수 역할을 하여 end-to-end 학습을 용이하게 합니다.

### Multi-scale Contextual Fusion Block (MCFB)

* **목표:** 픽셀에 적절하고 식별력 있는 문맥 정보를 할당하기 위해 고수준 문맥 특징을 적응적으로 통합합니다.
* **구조:**
  * 입력 특징 $F \in \mathbb{R}^{W \times H \times 4L}$는 세 가지 분기를 통과합니다.
  * **단거리 문맥($F_s$):** $3 \times 3$ 컨볼루션을 통해 지역적 의존성을 포착합니다.
  * **장거리 상호작용($F_l$):** 분해된 컨볼루션(예: $K \times 1$ 및 $1 \times K$ Conv)을 사용하여 넓은 수용장을 가진 장거리 상호작용을 인코딩합니다.
  * **전역 문맥($F_g$):** Maxpooling을 통해 특징을 한 채널로 압축한 후, 더 큰 크기의 분해된 컨볼루션(예: $M \times 1$ 및 $1 \times M$ Conv, $M=\max(W,H)$)을 사용하여 전체 장면의 전역 의존성을 포착합니다. 결과는 시그모이드(Sigmoid) 함수로 정규화된 공간 어텐션 맵으로 사용됩니다.
  * 단거리와 장거리 문맥은 합쳐져 $F_{sl} = F_s(F, \theta_s) + F_l(F, \theta_l)$를 생성합니다.
  * 최종 출력 $\hat{F}$는 $F_{sl}$과 공간 어텐션 맵 $F_g$로 가중치 부여된 $F_{sl}$을 선택적으로 집계합니다: $\hat{F} = F_{sl} + F_g \otimes F_{sl}$ (여기서 $\otimes$는 원소별 곱셈).

### 네트워크 아키텍처

* **백본:** ImageNet으로 사전 학습된 ResNet-101을 사용하여 깊은 특징을 추상화합니다. 마지막 완전 연결 계층과 분류 계층은 제거됩니다.
* **전체 흐름:**
    1. ResNet-101 백본의 stage2부터 stage5까지 특징을 추출합니다.
    2. 이 특징들은 일련의 **CCPB**를 통해 카테고리 공간으로 투영됩니다. 각 CCPB 출력에는 보조 손실 $L_i$가 적용되어 전체 네트워크 학습에 도움을 줍니다.
    3. CCPB의 출력은 **BCIB**에 공급되어 다른 수준의 정보 흐름 및 융합을 가능하게 합니다. 융합된 특징들은 모두 연결됩니다 (다른 해상도는 업샘플링하여 동일 크기로 만듦).
    4. 연결된 특징은 채널별 어텐션 블록을 거친 후 2배 업샘플링되어 입력 이미지 해상도와 일치시킵니다.
    5. 이 풍부한 문맥적 단서가 담긴 특징들은 **MCFB**의 입력으로 사용되어 지역적 주변 환경부터 전역적 의존성까지 픽셀 간의 상호 상관 관계를 탐색합니다.
    6. MCFB의 최종 출력은 채널별 시맨틱 맵으로, Ground Truth 맵으로부터 주 손실 $L_f$의 감독을 받습니다.
* **손실 함수:** 총 손실 $L = L_f + \lambda \sum_i L_i$로 구성됩니다. $L_f$는 최종 출력에 대한 크로스 엔트로피 손실, $L_i$는 각 CCPB 출력에 대한 크로스 엔트로피 손실입니다. 온라인 하드 예제 마이닝(online hard example mining)이 $L_f$에 적용됩니다.

## 📊 Results

BiCANet의 효과를 입증하기 위해 PASCAL VOC 2012, Cityscapes, ADE20K 세 가지 널리 사용되는 시맨틱 분할 데이터셋에서 광범위한 실험을 수행했습니다.

* **PASCAL VOC 2012:**
  * 86.7%의 mIoU(mean Intersection-over-Union) 정확도를 달성하여 최신 기술 네트워크를 능가합니다.
  * 특히, 어떤 사후 처리 기술도 사용하지 않았음에도 불구하고, CRF를 사후 처리로 사용하는 기존 방법(예: DeepLabV3 [42])보다 우수합니다.
  * 20개 객체 카테고리 중 12개에서 최고의 mIoU 점수를 얻었으며, 일부 카테고리(예: Plant에서 7.8%, Motobike에서 2.1%)에서 상당한 성능 향상을 보였습니다.
* **Cityscapes:**
  * 82.4%의 mIoU를 달성하여 최신 기술인 HRNetV2 [40] (81.8% mIoU)보다 0.6% 향상된 성능을 보여줍니다.
  * 19개 객체 카테고리 중 12개에서 최고의 mIoU를 기록했으며, Sidewalk (1.1%), Wall (1.3%), Truck (1.6%) 등에서 큰 폭의 향상이 있었습니다.
  * 다양한 외관의 카테고리(Building, Car)와 다중 스케일 객체(Signs, Pedestrians)에 대해서도 강건함을 입증했습니다.
* **ADE20K:**
  * 73.90%의 픽셀 단위 정확도(Pixel-wise accuracy), 38.66%의 mIoU, 0.5588의 최종 점수를 달성했습니다.
  * 2016년 1위를 차지한 PSPNet [3]을 포함한 모든 기존 베이스라인을 능가합니다. 특히 SegNet [5] 대비 mIoU에서 21.12% 향상되었습니다.
  * 작은 객체(Light, Book, Sign) 식별에 뛰어난 능력을 보여주었습니다.

**Ablation Study (요약)**:

* **BiCANet 구성 요소의 기여:** CCPB, BCIB, MCFB 세 가지 구성 요소 각각이 성능 향상에 일관되게 기여함을 확인했습니다. 특히 MCFB는 PASCAL VOC 2012에서 2.63~3.31% mIoU 향상을 보이며 가장 큰 영향을 미쳤습니다.
* **다양한 백본의 영향:** VGG-16, ResNet-101, ResNext-101, DenseNet-161 등 다양한 사전 학습된 백본을 사용한 결과, 더 강력한 표현 능력을 가진 백본이 성능 향상으로 이어진다는 것을 확인했습니다.
* **보조 손실($L_i$)의 기여:** 보조 손실($L_i$)의 하이퍼파라미터 $\lambda$를 0.1로 설정했을 때 Cityscapes 데이터셋에서 79.9% mIoU로 최적의 성능을 달성했습니다. $\lambda$가 증가할수록 성능이 저하되었습니다.
* **데이터 증강 방법의 영향:** 랜덤 스케일링(RS), 종횡비(AR), 이미지 뒤집기(IF) 등 모든 데이터 증강 방법을 함께 사용했을 때 9.23% mIoU 향상으로 가장 좋은 성능을 보였으며, 각 증강 방법 단독으로도 성능 개선에 기여했습니다.

## 🧠 Insights & Discussion

BiCANet은 시맨틱 분할에서 기존 CNN 기반 접근 방식의 여러 한계를 성공적으로 극복했습니다.

* **명시적인 카테고리 기반 문맥 통합:** 기존 모델이 특징 공간에서 문맥을 암묵적으로 학습한 것과 달리, BiCANet은 **카테고리 관점**에서 문맥을 명시적으로 탐색하고 통합합니다. 이는 그래픽 추론(예: CRF)을 단일 통합 모델로 시뮬레이션하는 효과를 가져와, 픽셀 간의 카테고리 기반 추정을 최적화 문제에 직접 통합할 수 있게 합니다.
* **적응적이고 견고한 문맥 표현 학습:** 각 픽셀에 대한 예측을 지역 정보뿐만 아니라 장거리 주변 및 전체 장면의 전역 문맥까지 기반으로 하는 적응적이고 견고한 문맥 표현을 명시적으로 학습합니다. MCFB는 픽셀마다 맞춤형 특징을 생성하여 더욱 강력한 문맥적 표현을 제공합니다.
* **단순성, 유연성, 효율성:** BiCANet은 단순하고 유연하며 효과적인 구조를 가집니다. 사후 처리 기술(예: CRF)이 필요 없으면서도 최첨단 성능을 달성합니다. 또한 이미지 피라미드, CRF, 추가 분할 손실 등 최신 발전을 통합하여 성능을 더욱 향상시킬 수 있는 여지가 있습니다. VGGNet, ResNet, ResNext, DenseNet과 같은 고급 특징 추상화 백본에도 쉽게 통합될 수 있습니다.
* **세밀한 객체 경계 및 모양 예측:** 정성적 결과는 BiCANet이 객체 경계와 모양을 정확하게 구분하여 더욱 부드러운 출력과 정확한 예측을 생성하는 뛰어난 능력을 보여줍니다. 이는 작은 객체 식별에도 매우 효과적입니다.

**제한 사항 및 향후 연구:**

* **경량화 버전 설계:** 실시간 애플리케이션의 요구 사항을 충족하는 네트워크의 경량 버전 설계에 대한 연구가 필요합니다.
* **다른 시각 작업으로의 전이:** 제안된 방법론이 객체 감지 [59], 중요도 감지 [60], 깊이 추정 [61]과 같은 다른 시각 작업에도 쉽게 전이될 수 있을 것으로 예상하며, 이에 대한 연구도 흥미로운 방향입니다.

## 📌 TL;DR

* **문제:** 기존 시맨틱 분할 CNN은 특징 공간 기반 문맥 포착, 고정된 수용장, 다양한 범위의 의존성에 대한 비적응적 처리로 인해 세부 정보 손실 및 분류 모호성 문제를 겪습니다.
* **제안 방법:** BiCANet은 **카테고리 관점**에서 문맥을 명시적으로 집계하는 새로운 신경망입니다.
  * **CCPB**는 카테고리 기반 투영을 통해 적응형 수용장을 가진 특징을 응축합니다.
  * **BCIB**는 양방향 건너뛰기 연결을 사용하여 중간 계층 간 문맥 상호작용을 강화합니다.
  * **MCFB**는 단거리, 장거리, 전역 의존성을 인코딩하여 픽셀에 적합한 다중 스케일 문맥을 적응적으로 융합합니다.
* **주요 결과:** BiCANet은 PASCAL VOC 2012 (86.7% mIoU), Cityscapes (82.4% mIoU), ADE20K (38.66% mIoU)에서 어떤 사후 처리도 없이 최신 기술 수준의 성능을 달성하며, 객체 경계 및 모양을 정확하게 구분하는 우수한 능력을 보여주었습니다.
