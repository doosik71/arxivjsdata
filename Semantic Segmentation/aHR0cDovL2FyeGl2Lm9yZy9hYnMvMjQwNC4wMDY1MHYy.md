# Deep Instruction Tuning for Segment Anything Model

Xiaorui Huang, Gen Luo, Chaoyang Zhu, Bo Tong, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji

## 🧩 Problem to Solve

Segment Anything Model (SAM)은 포인트 또는 바운딩 박스 안내 분할에서는 강력한 성능을 보이지만, 참조 이미지 분할(Referring Image Segmentation, RIS)과 같은 텍스트 명령 기반 작업에서는 현저히 낮은 성능을 나타냅니다. 이는 SAM의 경량 마스크 디코더 내에서 텍스트 정보에 대한 얕은 융합(shallow fusion) 방식 때문이며, 이로 인해 텍스트의 언어적 모호성을 효과적으로 해결하고 시각 토큰과 깊이 있게 상호작용하는 데 한계가 있습니다.

## ✨ Key Contributions

* SAM의 텍스트 기반 분할에서의 주요 단점을 규명하고, 이러한 문제를 해결하는 데 '깊은 텍스트 인스트럭션 튜닝(deep text instruction tuning)'이 핵심임을 주장합니다.
* SAM을 위한 두 가지 간단하면서도 효과적인 딥 인스트럭션 튜닝(DIT) 방법, 즉 엔드투엔드 DIT(End-to-end DIT, E-DIT)와 레이어 단위 DIT(Layer-wise DIT, L-DIT)를 제안합니다. 이 방법들은 SAM의 구조를 최소한으로 수정하여 SAM의 이미지 인코더를 독립형 시각-언어 학습기로 변환합니다.
* 제안된 DIT-SAM 모델은 RIS 벤치마크에서 기본 SAM보다 월등히 뛰어난 성능을 보였으며, 특히 L-DIT는 훨씬 적은 데이터와 학습 비용으로 기존 SOTA(State-of-the-Art) 방법들과 견줄 만한 경쟁력을 보여줍니다.
* SAM의 멀티모달 확장(multi-modal extension)을 위한 빠르고 실현 가능한 솔루션을 제공합니다.

## 📎 Related Works

* **이미지 분할 (Image Segmentation):** Mask-RCNN, SOLO, IFR, TAR 등 다양한 연구가 진행되었으며, SAM은 방대한 데이터셋으로 학습된 강력하고 범용적인 모델입니다. 하지만 기존 SAM은 텍스트 정보의 복잡성과 디코더의 불충분한 융합으로 인해 텍스트 안내 작업에서 성능이 저하되는 한계가 있습니다.
* **참조 이미지 분할 (Referring Image Segmentation, RIS):** 초기 2단계 방식에서 최근에는 텍스트 특징을 시각 네트워크에 직접 통합하는 1단계 모델(VLT, SeqTR, ReSTR)이 주목받고 있습니다. UniLSeg, LISA, SEEM과 같은 대규모 언어 모델(LLM) 및 대규모 이미지-텍스트 쌍을 활용한 연구도 있지만, 본 연구의 DIT는 SAM의 텍스트 인지 능력 향상에 중점을 둡니다. LAVT 및 ETRIS와 유사하게 두 모달리티 인코더를 사용하지만, DIT는 텍스트 단어를 SAM 이미지 인코더의 시맨틱 공간에 투사하여 단일 전방 네트워크에서 특징 융합 및 학습을 달성한다는 점에서 차이가 있습니다.
* **프롬프트 튜닝 (Prompt Tuning):** 방대한 사전 학습 모델의 다운스트림 작업 적응을 위해 사용됩니다(CoOp, CoCoOp). DIT 방법과 프롬프트 튜닝은 과정상 유사하나, DIT는 예시마다 동적으로 변하는 텍스트 임베딩을 투사하여 SAM의 시맨틱 공간에 언어 정보를 통합하는 반면, 프롬프트 튜닝은 주로 정적인 학습 가능한 토큰을 사용하여 모델이 다운스트림 작업에 적응하도록 돕는다는 점에서 원칙과 설계가 다릅니다.

## 🛠️ Methodology

본 논문은 SAM의 텍스트 이해 능력 부족을 해결하기 위해, SAM의 전체 아키텍처를 유지하면서 이미지 인코더를 강력한 멀티모달 융합 네트워크로 활용하는 **딥 인스트럭션 튜닝(Deep Instruction Tuning, DIT)**을 제안합니다.

* **문제 정의:** RIS는 이미지 $I$와 자연어 표현 $T$가 주어졌을 때 참조 객체의 이진 마스크 $M$을 예측합니다. 기본 SAM은 $F(I,T;\theta) = F_d(F_v(I), F_t(T))$로 표현되며, 여기서 $F_v$, $F_t$는 각각 시각 백본과 언어 인코더, $F_d$는 마스크 디코더입니다. SAM의 경량 마스크 디코더는 단 두 개의 교차-어텐션 레이어만을 포함하여 멀티모달 상호작용이 제한적입니다. DIT는 이를 $F(T,I;\theta) = F_d(F_v(I, \phi(F_t(T))), F_t(T))$로 변환하며, $\phi(\cdot)$는 텍스트 특징을 SAM의 시각 공간에 투사하는 함수입니다.

* **엔드투엔드 DIT (End-to-end DIT, E-DIT):**
    1. 텍스트 명령 $T$를 BERT 언어 인코더를 사용하여 $F_t \in \mathbb{R}^{l \times d}$로 추출합니다. 여기서 $l$은 명령의 길이, $d$는 특징 차원입니다.
    2. $F_t$를 선형 레이어($W_0 \in \mathbb{R}^{d \times d}, b_0 \in \mathbb{R}^{d}$)를 통해 SAM의 시각 시맨틱 공간에 투사하여 $F_t^0 = F_t W_0 + b_0$를 얻습니다.
    3. 이 투사된 텍스트 토큰 $F_t^0$를 패치 임베딩 레이어 이후의 초기 시각 특징 $F_v^0 \in \mathbb{R}^{m \times d}$와 연결합니다: $F_m^0 = [F_t^0; F_v^0]$.
    4. 연결된 시퀀스 $F_m^0$를 SAM 시각 인코더의 Transformer 레이어에 입력하여 깊은 멀티모달 융합을 수행합니다.
    5. 최종 시각 특징 $F_v^n$과 투사된 텍스트 특징 $F_t W_o$ (여기서 $W_o \in \mathbb{R}^{d \times d}$는 투사 가중치)를 경량 마스크 디코더에 입력하여 마스크 $M'$를 예측합니다.

* **레이어 단위 DIT (Layer-wise DIT, L-DIT):**
    1. E-DIT와 달리 텍스트 토큰 $F_t$를 SAM 인코더의 **각 Transformer 레이어**에 독립적으로 투사하여 삽입합니다.
    2. $i$번째 레이어의 입력 시퀀스는 $F_m^i = [F_t^i; F_v^i]$로 정의되며, 여기서 $F_t^i$는 해당 레이어를 위한 투사된 텍스트 토큰입니다.
    3. 이 방식은 모달리티 간의 간극을 각 레이어 수준에서 완화하여 텍스트 인스트럭션 튜닝의 효율성을 높입니다.
    4. 최종 출력 시각 토큰 $F_v^k$와 투사된 텍스트 특징 $F_t'$는 경량 디코더에 입력됩니다.

## 📊 Results

* **데이터셋 및 평가 지표:** RefCOCO, RefCOCO+, RefCOCOg 벤치마크에서 oIoU(overall Intersection-over-Union), mIoU(mean Intersection-over-Union), P@0.5, P@0.7, P@0.9 지표를 사용했습니다.
* **기본 SAM과의 비교:** DIT 방법은 기본 SAM 및 디코더 레이어를 추가한 SAM보다 훨씬 우수한 성능을 보였으며, 특히 이미지 백본이 고정된 설정에서 그 차이가 두드러졌습니다. E-DIT는 RefCOCO val에서 기본 SAM 대비 mIoU를 크게 향상시켰습니다(+11.8%). L-DIT는 RefCOCO val에서 74.73% mIoU를 달성하며 E-DIT(71.46% mIoU)보다 더 나은 성능을 보였습니다.
* **백본 동결의 영향:** 백본이 동결된 L-DIT$^{*}$는 RefCOCO val에서 69.97% mIoU를 기록하여, 백본을 완전히 튜닝한 기본 SAM(57.98% mIoU)보다도 높은 성능을 달성했습니다. 이는 L-DIT가 텍스트 토큰을 SAM의 시맨틱 공간에 더 잘 투사한다는 가설을 뒷받침합니다.
* **프롬프트 튜닝과의 결합:** E-DIT에 학습 가능한 프롬프트 토큰을 추가하면 성능이 약간 향상되었지만, L-DIT의 경우 오히려 성능이 감소했습니다. 이는 L-DIT의 독립적인 투사기가 이미 텍스트-시각 정렬을 잘 수행하고 있음을 시사합니다.
* **텍스트 인코더의 영향:** BERT가 CLIP보다 대부분의 경우에서 약간 더 나은 성능을 보였지만, 데이터셋의 간결한 설명으로 인해 그 차이는 크지 않았습니다.
* **인코더 동결 효과:** 이미지 인코더를 업데이트하는 것이 텍스트 인코더를 업데이트하는 것보다 성능 향상에 더 큰 영향을 미쳤습니다(RefCOCO val에서 +4.88% oIoU). 이는 DIT에서 이미지 인코더가 시각 특징 학습과 교차-모달 상호작용에서 핵심적인 역할을 함을 보여줍니다.
* **텍스트 단어 주입 방식:** L-DIT에서 간단한 선형 투사 방식이 교차-어텐션이나 FFN(Feed-Forward Network)을 사용한 복잡한 방식보다 더 나은 성능을 보였습니다. 이는 SAM의 강력한 ViT 인코더와 함께 직접적인 시맨틱 투사가 교차-모달 정렬을 촉진하기에 충분함을 시사합니다.
* **SAM 튜닝의 영향:** DIT는 SAM이 박스 안내 능력에 근접하는 강력한 텍스트 안내 성능을 달성하도록 돕습니다. 텍스트 토큰 수가 이미지 패치보다 훨씬 적기 때문에 추론 속도에 미치는 영향은 제한적입니다.
* **SOTA와의 비교:** L-DIT$_{B}$는 RefCOCO 데이터셋에서 기존 SOTA 방법들을 능가하는 SOTA 성능을 달성했습니다(예: RefCOCO testA에서 1.56% oIoU 향상). L-DIT$_{H}$는 RefCOCO val에서 새로운 SOTA 성능을 기록하며, 대규모 시각 그라운딩 데이터로 사전 학습된 SOTA RIS 방법들과도 매우 경쟁력 있는 모습을 보였습니다.
* **정성적 분석:** 시각화 결과는 DITs, 특히 L-DIT가 기본 SAM보다 텍스트 이해 및 분할 정확도에서 훨씬 우수함을 보여주었습니다. 복잡한 텍스트 표현이나 이미지 배경에서도 정확한 분할을 수행하며, 각 레이어의 어텐션 맵이 텍스트 명령에 따라 관심 영역을 조정하는 것을 확인했습니다.

## 🧠 Insights & Discussion

* **깊은 텍스트 인스트럭션 튜닝의 중요성:** 이 논문은 텍스트 기능의 깊은 통합(디코더의 얕은 융합이 아닌 시각 인코더 내)이 SAM의 텍스트 인지 능력에 필수적임을 성공적으로 입증했습니다.
* **효율성:** DIT는 SAM 아키텍처에 최소한의 변경을 가하고 제한된 추가 학습 비용으로 강력한 성능을 달성하여, SAM의 멀티모달 기능을 확장하는 실용적인 솔루션임을 보여줍니다.
* **L-DIT의 우수성:** 레이어 단위 주입(layer-wise injection) 방식은 텍스트 토큰을 SAM의 다양한 시맨틱 하위 공간에 더 잘 투사할 수 있게 하여, 특히 백본이 고정된 경우에도 뛰어난 성능을 제공합니다.
* **프롬프트 튜닝과의 차이:** DIT는 *시맨틱 투사* (텍스트를 시각 공간에 매핑하여 깊은 융합)를 목표로 하는 반면, 기존 프롬프트 튜닝은 *작업 적응* (사전 학습과 다운스트림 작업 간의 격차를 정적인 학습 가능 토큰으로 메우는 것)을 목표로 합니다.
* **시각 인코더의 시각-언어 학습기 역할:** DIT는 SAM의 강력한 사전 학습된 시각 인코더를 멀티모달 학습기로 활용하여, 텍스트와 시각 콘텐츠 간의 지속적인 상호작용을 통해 참조 대상을 점진적으로 찾아낼 수 있도록 합니다.

## 📌 TL;DR

* **문제:** SAM은 텍스트 명령 기반 이미지 분할(RIS)에서 취약합니다.
* **해결책:** 본 논문은 SAM의 시각 인코더를 텍스트와 이미지 간의 깊은 상호작용을 가능하게 하는 시각-언어 학습기로 활용하는 두 가지 새로운 딥 인스트럭션 튜닝(DIT) 방법(End-to-end DIT 및 Layer-wise DIT)을 제안합니다.
* **주요 발견:** DIT-SAM은 RIS 벤치마크에서 기본 SAM을 크게 능가하며, 특히 Layer-wise DIT는 최소한의 수정과 학습 비용으로 SOTA 성능을 달성하여 SAM의 텍스트 인식 능력을 크게 향상시킵니다.
