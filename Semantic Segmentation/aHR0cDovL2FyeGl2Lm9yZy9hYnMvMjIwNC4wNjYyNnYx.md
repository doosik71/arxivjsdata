# Adaptive Memory Management for Video Object Segmentation

Ali Pourganjalikhan, Charalambos Poullis

## 🧩 Problem to Solve

기존의 매칭 기반 비디오 객체 분할(VOS) 네트워크는 모든 `k`번째 프레임을 외부 메모리 뱅크에 저장하여 추론에 활용합니다. 이 방법은 현재 프레임의 객체 분할을 위해 과거 프레임의 풍부한 정보를 제공하지만, 비디오 길이가 길어질수록 메모리 뱅크의 크기가 선형적으로 증가하여 추론 속도가 느려지고 임의 길이의 비디오를 처리하는 데 비현실적이라는 문제가 있습니다.

## ✨ Key Contributions

* **적응형 메모리 뱅크 전략 제안:** 매칭 기반 VOS 네트워크를 위한 적응형 메모리 뱅크 전략을 제안하여, 오래된(obsolete) 특징을 버림으로써 임의 길이의 비디오를 처리할 수 있게 합니다.
* **중요도 기반 특징 인덱싱:** 이전 프레임의 객체 세분화에서 특징의 중요도에 따라 특징에 인덱스를 부여하고, 이 인덱스를 기반으로 중요하지 않은 특징을 폐기하여 새로운 특징을 수용합니다.
* **성능 향상:** 'first-and-latest' 전략(고정 크기 메모리 뱅크 사용)보다 우수한 성능을 보이며, 'every-k' 전략(증가 크기 메모리 뱅크 사용)과 유사한 성능을 달성합니다.
* **추론 속도 개선:** 'every-k' 전략 대비 최대 80%, 'first-and-latest' 전략 대비 35%까지 추론 속도를 향상시킵니다.
* **실용성 증대:** 증가하는 메모리 요구 사항 없이 임의 길이의 비디오 시퀀스를 처리할 수 있도록 합니다.

## 📎 Related Works

* **VOS 방법론 분류:** 암시적(implicit) 모델은 네트워크 가중치를 미세 조정하여 객체 표현을 학습하며, 명시적(explicit) 모델은 유사성 매칭을 통해 과거 프레임의 객체 특징을 사용하여 현재 프레임 픽셀을 분류합니다. 후자는 종종 매칭 기반 방법이라고 불립니다.
* **매칭 기반 네트워크:**
  * [18], [16], [25], [26]은 'first-and-latest' 프레임의 정보만을 활용하여 객체를 분할합니다.
  * STM [15]은 `every-κ` 프레임을 메모리 뱅크에 저장하여 중간 프레임 정보를 활용합니다.
  * STCN [33]은 객체에 독립적인(object-agnostic) 키를 사용하여 다중 객체 시나리오에서 계산량을 줄입니다.
* **메모리 뱅크의 효율성:**
  * STM [15] 및 그 확장 버전 [34-40]은 중간 프레임의 키-값 쌍을 메모리 뱅크에 저장합니다.
  * MiVOS [4]는 메모리 특징 중 소수만이 세분화에 의미 있게 기여하며, 나머지 중복 특징은 추론 시 무시되지만 메모리 뱅크에는 여전히 유지됩니다.
  * Adaptive Feature Bank (AFB) [19]는 메모리 뱅크 크기를 고정하기 위한 업데이트 및 제거 전략을 제안하지만, 데이터에 의존적인 임계값과 제한된 거리 측정 방식이 필요합니다.

## 🛠️ Methodology

본 논문은 효과적이면서 최소한의 STCN [33]을 베이스라인으로 선택하여 제안된 특징 샘플링 전략의 범용성을 보여줍니다.

1. **키-값 인코딩 ($K_S, V_S$)**:
    * **키 인코딩:** ResNet50 [42]과 $3 \times 3$ 컨볼루션 레이어를 사용하여 객체에 독립적인 키를 인코딩합니다. `res4` 레이어의 출력을 사용하여 공간 정보를 보존하고 채널 수를 1024에서 64로 줄입니다. 쿼리 및 서포트 키 인코더는 가중치를 공유하므로 쿼리 키는 서포트 키로 재사용될 수 있습니다.
    * **값 인코딩:** 더 가벼운 ResNet18 백본을 사용하여 이미지와 마스크를 인코딩합니다. 값 인코딩은 객체별(object-specific)이며 서포트 프레임에만 사용됩니다. 출력은 해당 키 인코더의 특징 맵과 연결되어 처리됩니다.
2. **메모리 읽기 (Memory Read)**:
    * 서포트 프레임의 키-값 쌍 ($K_S, V_S$)을 연결하여 시공간 키-값을 형성합니다.
    * 서포트 키 $K_{S} \in \mathbb{R}^{T \times HW \times C_K}$와 쿼리 키 $K_{Q} \in \mathbb{R}^{HW \times C_K}$ 간의 유사도(affinity) $d_{ij}$를 계산합니다. 유사도는 음수 제곱 거리 $\text{dist}(.,.) : \mathbb{R}^{C_k} \times \mathbb{R}^{C_k} \to \mathbb{R}$ (즉, $-\|K_{S_i}, K_{Q_j}\|_2^2$)를 기반으로 하며, $d_{ij} = \frac{\text{dist}(K_{S_i}, K_{Q_j})}{\sqrt{C_k}}$로 주어집니다.
    * 유사도 행렬 $D \in \mathbb{R}^{T \times HW \times HW}$는 쿼리 특징 차원을 따라 Softmax로 정규화되고, 서포트 값의 가중 합 계산에 사용됩니다.
3. **디코더 (Decoder)**: STM [15]의 작업을 확장하여 두 개의 연속적인 개선 모듈 [18]을 포함합니다. 특징의 공간 크기를 4배로 업샘플링하고 채널 수를 1024에서 1로 줄입니다. 쿼리 키 인코더의 특징은 스킵-연결을 통해 입력과 연결됩니다. 다중 객체의 경우 soft aggregation [15]을 사용하여 최종 마스크를 얻습니다.
4. **적응형 메모리 뱅크 (Adaptive Memory Bank)**:
    * 메모리 뱅크 크기 증가 문제 해결을 위해, 가장 적게 사용된 (Least-Frequently-Used, LFU) 정책을 기반으로 특징을 제거합니다.
    * LFU 정책: $\text{LFU} = \frac{\text{index}}{\text{age}}$
        * `index`: 해당 특징이 쿼리 특징과 매칭되는 `top-k` 특징에 나타난 횟수입니다.
        * `age`: 해당 특징이 메모리에 존재했던 프레임 수입니다.
    * 새로운 특징을 수용하기 위해 충분한 양의 특징을 제거합니다. 새로운 프레임을 추가할 때 업데이트는 수행하지 않습니다.
5. **구현 상세**:
    * **학습 절차:** 두 단계 학습을 사용합니다. 첫째, 정적 이미지 [45-49]로 300,000회 반복 학습 (배치 크기 16). 둘째, Youtube-VOS [50] 및 DAVIS [44], [51]로 150,000회 반복 학습 (배치 크기 8). 각 반복에서 시간적으로 정렬된 세 프레임을 선택하여 학습합니다.
    * **최적화:** Adam [52] 옵티마이저와 부트스트랩 교차 엔트로피 [4] 손실 함수를 사용합니다. 4P100 GPU로 5일간 학습했습니다.
    * **추론:** GTX1070 GPU에서 실행합니다. 서포트 키가 객체에 독립적이므로, 모든 객체에 공유되는 단일 인덱스 및 연령 카운터를 시퀀스 시작 시 초기화합니다. 새로운 객체가 중간에 나타나는 경우에도 성공적으로 처리할 수 있습니다.

## 📊 Results

* **정량적 결과 (DAVIS 2016 및 DAVIS 2017):**
  * **DAVIS 2016:** 우리의 방법(J&F-Mean 90.9)은 'first-and-latest' (89.5)보다 1.4% 우수하며, 'every-5' (91.7)와 비교할 만한 성능을 보였습니다.
  * **DAVIS 2017:** 우리의 방법(J&F-Mean 84.4)은 'first-and-latest' (82.5)보다 크게 우수하며, 'every-5' (85.3)와 비교할 만한 성능을 달성했습니다.
* **정량적 결과 (Youtube-VOS):**
  * 우리의 방법(J&F 81.1)은 'first-and-latest' (80.5)보다 우수하지만, 'every-5' (84.1)보다는 낮은 성능을 보였습니다. 이는 Youtube-VOS 비디오 길이가 DAVIS2017보다 길어 'every-5'가 훨씬 더 많은 프레임을 메모리에 저장하기 때문입니다.
* **추론 속도 및 메모리 활용도 (DAVIS 2017):**
  * **FPS (초당 프레임 수):** Ours (11.4) > 'first-and-latest' (8.4) > 'every-5' (6.3).
  * 추론 속도가 'every-k' 대비 80%, 'first-and-latest' 대비 35% 향상되었습니다.
  * **메모리 활용도 (저장된 프레임 수):** Ours (2 프레임 상당) = 'first-and-latest' (2 프레임 상당) < 'every-5' (평균 72 프레임).
* **Ablation Study (Top-k 저장 vs. Softmax 가중치 저장):**
  * `top-k` 저장을 사용한 우리의 방법이 Softmax 가중치를 인덱스로 사용한 경우보다 DAVIS 2017과 Youtube-VOS에서 일관되게 더 나은 성능을 보였습니다.

## 🧠 Insights & Discussion

* **LFU 기반 적응형 메모리의 장점:**
  * 객체의 큰 변위와 빠른 변형을 성공적으로 처리하며, 잘못된 이전 예측으로부터 회복하는 능력을 보여줍니다.
  * 데이터에 의존적인 임계값 설정의 필요성을 제거하며, 사용되는 거리 측정 방식에 구애받지 않습니다.
  * 고정된 크기의 메모리 뱅크를 유지하여 임의 길이의 비디오 처리를 가능하게 합니다.
* **한계점:**
  * 객체가 완전히 가려진 후 다시 나타나는 경우, 해당 객체 특징이 메모리에서 제거되었다면 재식별에 실패할 수 있습니다. `first-and-latest` 방식은 첫 프레임의 정보를 항상 유지하므로, 객체 형태가 변하지 않은 경우 재식별에 더 유리할 수 있습니다.
* **`top-k` 인덱스 사용의 효과:** `top-k`를 인덱스로 사용하는 것이 Softmax 가중치를 사용하는 것보다 우수합니다. Softmax 정규화는 가장 가까운 특징에 매우 불균형하게 편향되어, 효과를 잃은 특징을 폐기하는 네트워크의 능력을 저해할 수 있기 때문입니다.
* **메모리 뱅크 크기의 영향:** 메모리 뱅크 용량을 늘리면 `J&F` 점수가 향상됩니다 (예: Youtube-VOS에서 2프레임에서 4프레임으로 두 배 증가 시 `J&F`가 2% 증가). Youtube-VOS 데이터셋의 경우 4프레임 상당의 메모리 뱅크 용량이 추론 속도와 정확도 사이의 최적의 균형을 제공합니다.
* **정규화 방법 (드롭아웃, 특징 재할당):** 보충 자료에 따르면 드롭아웃 및 특징 재할당과 같은 정규화 방법은 다중 객체 데이터셋에서 네트워크 성능을 저하시키고 불필요한 복잡성을 추가할 수 있습니다.

## 📌 TL;DR

* **문제:** 기존 VOS 네트워크는 비디오 길이에 따라 메모리 뱅크가 커져 임의 길이 비디오 처리가 비효율적이고 추론이 느려지는 문제가 있습니다.
* **제안 방법:** 이 논문은 객체 세분화에 기여하는 중요도(top-k 인덱스)를 기준으로 특징을 평가하고, 가장 적게 사용된(LFU) 특징을 제거하는 적응형 메모리 관리 전략을 제안합니다. 이를 통해 메모리 뱅크 크기를 고정하고 새로운 특징을 효율적으로 수용합니다.
* **주요 결과:** 제안된 방식은 고정 크기 메모리를 사용하는 'first-and-latest' 전략보다 성능이 우수하고, 메모리가 증가하는 'every-k' 전략과 유사한 성능을 달성합니다. 특히, 추론 속도가 'every-k' 대비 최대 80%, 'first-and-latest' 대비 35% 향상되어, 제한된 컴퓨팅 자원에서도 임의 길이의 비디오 스트림을 효율적으로 처리할 수 있음을 입증했습니다.
