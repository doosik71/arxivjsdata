# 개방형-어휘 검출 및 분할에 대한 서베이: 과거, 현재, 그리고 미래

Chaoyang Zhu, Long Chen

## 🧩 Problem to Solve

기존의 객체 검출 및 분할 모델은 고비용의 수동 주석 작업으로 인해 학습 데이터셋의 범주가 소규모로 미리 정의되어 있습니다. 이 "폐쇄형-어휘(closed-vocabulary)" 한계는 모델이 학습 시 접하지 못한 새로운 범주(novel categories)의 객체를 식별하고 분류하는 데 실패하게 만듭니다. 이는 자율 주행, 지능형 로봇 공학 등 실제 응용 분야에서 현재 모델의 활용을 크게 제한하는 주요 문제입니다. 인간의 지각 시스템이 임의의 시각적 개념을 개방형 클래스 이름이나 자연어 설명과 연관시킬 수 있는 것과 달리, 기계는 이러한 일반화 능력이 부족합니다.

## ✨ Key Contributions

이 서베이 논문은 개방형-어휘 검출(OVD) 및 분할(OVS) 분야의 최근 발전에 대한 포괄적인 검토를 제공하며, 다음과 같은 핵심 기여를 합니다:

* 개방형-어휘 작업을 체계적으로 분류하고 정리하기 위한 새로운 **분류 체계(taxonomy)**를 제시합니다. 이 분류 체계는 약한 감독 신호(weak supervision signals)의 허용 여부 및 사용 방식을 기준으로 방법론을 구분합니다.
* 주요 방법론을 **시각-의미 공간 매핑(visual-semantic space mapping)**, **새로운 시각 특징 합성(novel visual feature synthesis)**, **영역-인지 학습(region-aware training)**, **의사-레이블링(pseudo-labeling)**, **지식 증류(knowledge distillation)**, 그리고 **전이 학습(transfer learning)**의 여섯 가지 범주로 분류하고 심층적으로 분석합니다.
* 객체 검출, 의미론적/인스턴스/파노라마 분할, 3D 및 비디오 이해를 포함한 다양한 작업 전반에 걸쳐 제안된 분류 체계의 보편성을 보여줍니다.
* 각 방법론의 주요 설계 원칙, 핵심 과제, 개발 경로, 강점 및 약점을 철저히 분석합니다.
* 각 작업 및 방법의 핵심 구성 요소를 벤치마킹하여 제공하며, 미래 연구를 자극하기 위한 유망한 연구 방향을 제시하고 논의합니다.

## 📎 Related Works

개방형-어휘 검출 및 분할은 여러 관련 연구 분야와 연관됩니다.

* **시각적 접지(Visual Grounding)**: 문장의 의미 개념을 이미지 영역에 연결하는 작업으로, OVD/OVS의 영역-단어 대응 학습에 영향을 줍니다.
* **약한 감독 검출 및 분할(Weakly-Supervised Detection and Segmentation)**: 바운딩 박스나 마스크 주석 없이 이미지-수준 클래스 이름만으로 학습하며, OVD/OVS의 약한 감독 신호 활용에 기여합니다.
* **개방형-집합 검출 및 분할(Open-Set Detection and Segmentation)**: 알려진 클래스를 분류하고 단일 "알 수 없는(unknown)" 클래스를 식별하는 데 중점을 둡니다. OVD/OVS는 "알 수 없는" 객체를 특정 클래스로 분류한다는 점에서 차이가 있습니다.
* **개방형-세계 검출 및 분할(Open-World Detection and Segmentation)**: 새로운 클래스를 점진적으로 학습하고 이전에 학습한 클래스를 잊지 않으려 하는 지속 학습 패러다임을 따릅니다.
* **분포-외 검출(Out-of-Distribution Detection)**: 학습 데이터와 다른 분포를 가진 테스트 샘플을 감지하는 데 초점을 맞추며, OVD/OVS와 유사하게 새로운 범주를 다룹니다.
* **표준 폐쇄형-집합 검출기 및 분할기**: Faster R-CNN, DETR, Mask R-CNN, MaskFormer 등 기존의 고성능 모델들이 OVD/OVS 연구의 기반이 됩니다.
* **대규모 시각-언어 모델(Large Vision-Language Models, VLMs)**: CLIP, ALIGN, DINO, MAE, T2I Diffusion Models와 같은 모델들은 대규모 사전 학습을 통해 우수한 제로-샷 전이 능력을 보여주며, OVD/OVS 방법론의 핵심 구성 요소로 활용됩니다.

## 🛠️ Methodology

논문은 약한 감독 신호의 허용 및 사용 여부에 따라 방법론을 분류합니다.

**1. 제로-샷(Zero-Shot) 설정 (약한 감독 신호 비허용)**

* **목표**: 학습 시 주석 없는 새로운 객체가 포함된 이미지에 접근할 수 없는 상황에서 일반화.
* **접근 방식**:
  * **시각-의미 공간 매핑 (Visual-Semantic Space Mapping)**:
    * 학습 가능한 분류기를 Word2Vec, GloVe, BERT와 같은 고정된 의미론적 임베딩으로 대체합니다.
    * 시각 특징을 의미 공간으로, 의미 임베딩을 시각 공간으로, 또는 두 공간의 공동 매핑을 학습하여 교차-모달 유사성을 측정합니다.
    * 주로 기존 검출기에 선형 레이어를 추가하여 시각 특징의 차원을 의미론적 임베딩과 맞춥니다.
    * 주요 과제는 배경 및 보지 못한 클래스 간의 혼란을 해결하는 것입니다.
  * **새로운 시각 특징 합성 (Novel Visual Feature Synthesis)**:
    * 추가적인 생성 모델(예: GAN, VAE)을 사용하여 보지 못한 클래스의 **가짜(fake)** 시각 특징을 합성합니다.
    * 합성된 특징은 "완전 감독(fully-supervised)" 설정처럼 새로운 분류기 학습에 사용됩니다.
    * 이를 통해 보지 못한 객체와 배경 개념 간의 혼란을 완화하고, 학습 데이터 부족 문제를 해결합니다.

**2. 개방형-어휘(Open-Vocabulary) 설정 (약한 감독 신호 허용)**

* **목표**: 모델이 주석 없는 새로운 객체가 포함된 이미지로 학습할 수 있도록 허용.
* **접근 방식**: 약한 감독 신호(이미지-텍스트 쌍, 대규모 사전 학습된 VLM)를 활용합니다. 모든 방법론은 클래스-불가지론적(class-agnostic) 지역화 분기 및 VLM의 텍스트 인코더(VLMs-TE)를 고정된 분류기로 사용합니다.
  * **영역-인지 학습 (Region-Aware Training)**:
    * 이미지-텍스트 쌍(캡션)을 사용하여 영역과 단어 사이에 약한(implicit), 양방향의 정합(grounding) 또는 대조적 손실(contrastive loss)을 통해 암시적인 대응 관계를 설정합니다.
    * $$L_{T \to I} = -\log \frac{\exp(\text{sim}(I,T))}{\sum_{I' \in B} \exp(\text{sim}(I',T))}$$
    * $$L_{I \to T} = -\log \frac{\exp(\text{sim}(I,T))}{\sum_{T' \in B} \exp(\text{sim}(I,T'))}$$
    * 이를 통해 모델이 더 큰 어휘를 접하게 하여 일반화 능력을 향상시킵니다.
  * **의사-레이블링 (Pseudo-Labeling)**:
    * 대규모 VLM 또는 자체 모델(자기 학습)을 "교사(teacher)"로 사용하여 새로운 클래스에 대한 의사 영역-단어 쌍 또는 영역-캡션 쌍을 명시적으로(explicitly) 생성합니다.
    * 이 방법은 학습 시 새로운 클래스 이름을 미리 알아야 합니다.
  * **지식 증류 (Knowledge Distillation)**:
    * CLIP과 같은 VLM의 이미지 인코더(VLMs-IE)에서 시각적 지식을 검출기(학생 모델)로 증류합니다.
    * 이를 통해 학생 모델이 교사의 정렬된 시각-의미 공간에 더 효과적으로 접근하도록 합니다.
    * 증류는 단일 영역, 영역 집합 또는 이미지 영역 수준에서 이루어질 수 있으며, 주로 $L_1$ 손실을 사용합니다.
  * **전이 학습 (Transfer Learning)**:
    * 사전 학습된 VLM의 이미지 인코더(VLMs-IE)를 하류(downstream) 지각 작업에 직접 전이하여 사용합니다.
    * 이는 VLMs-IE를 특징 추출기로 사용하거나, VLMs-IE를 하류 데이터에 미세 조정하거나, 가벼운 어댑터나 시각적 프롬프트를 사용하여 VLMs-IE를 고정시킨 채로 학습하는 방식으로 나뉩니다.

**3. 이미지 외 개방형-어휘 이해 (Open-Vocabulary Beyond Images)**

* 위의 분류 체계는 3D 장면 이해 및 비디오 이해에도 확장 적용됩니다.
* 이러한 분야에서는 대규모 점군-텍스트 또는 비디오-텍스트 쌍이 부족하므로, 이미지 도메인의 대규모 VLM이 점군-텍스트 또는 비디오-텍스트 간의 중간 다리 역할을 합니다.
* 예를 들어, 2D OVD 검출기로 생성된 의사 레이블을 3D 공간으로 역투영하여 감독 신호로 활용합니다.

## 📊 Results

이 서베이는 개방형-어휘 검출 및 분할 분야의 최신 연구들을 포괄적으로 벤치마킹합니다. COCO, LVIS, Pascal VOC, ADE20K, ScanNet, nuScenes, Youtube-VIS 등 다양한 데이터셋에서 제로-샷 및 개방형-어휘 설정에 대한 성능을 AP, mIoU, PQ와 같은 표준 메트릭으로 평가합니다. 특히, 보지 못한(novel) 클래스에 대한 일반화 능력(AP$_N$, mIoU$_N$, hIoU 등)과 학습된(base) 클래스와의 조화 평균(hIoU)에 중점을 둡니다.

결과는 대규모 시각-언어 모델(VLM)의 활용이 이 분야에서 엄청난 성능 향상을 가져왔음을 보여줍니다. 특히 지식 증류 및 전이 학습 방법론은 VLM의 풍부한 지식을 활용하여 폐쇄형-어휘 모델의 한계를 극복하는 데 효과적입니다. 의사-레이블링 및 영역-인지 학습도 약한 감독 신호를 통해 새로운 개념에 대한 학습을 가능하게 합니다. 3D 및 비디오 분야는 데이터 부족으로 인해 2D VLM의 간접적인 활용이 주를 이루며, 여전히 초기 단계에 있음을 보여줍니다.

## 🧠 Insights & Discussion

논문은 OVD 및 OVS의 발전과 함께 다음과 같은 핵심 통찰과 한계, 그리고 미래 방향을 논의합니다.

**도전 과제:**

* **학습된 클래스에 대한 과적합(Overfitting on Base Classes)**: 새로운 주석 부족으로 인해 모델이 학습된 클래스에 편향되어 새로운 객체를 학습된 클래스나 배경으로 오분류하는 경향이 있습니다. 이를 완화하기 위해 독립적인 제안 생성기, 객체성 점수 개선, 검출기와 CLIP 간의 예측 앙상블 등이 시도됩니다.
* **새로운 개념과 배경 개념의 혼란(Confusion on Novel and Background Concepts)**: 학습 시 새로운 객체와 배경이 모두 배경으로 분류되어, 추론 시 새로운 객체가 배경으로 오분류되는 문제가 발생합니다. 배경 텍스트 임베딩의 단순성도 문제의 원인입니다.
* **정확한 영역-단어 대응(Correct Region-Word Correspondence)**: 이미지-텍스트 쌍의 약하고 노이즈가 많은 대응 관계는 "단어 뭉치(bag-of-words)" 분류로 이어질 수 있으며, 캡션이 모든 객체를 설명하지 않아 많은 객체가 일치하는 단어를 찾지 못하는 문제가 있습니다. 의사-레이블의 반복적인 품질 개선이 필요합니다.
* **대규모 VLM 적응(Large VLMs Adaptation)**: 사전 학습(예: CLIP)과 하류 작업(검출/분할) 간의 이미지 해상도, 컨텍스트, 작업 통계의 큰 차이(도메인 갭)가 존재합니다. 또한, VLM 전체를 미세 조정하면 치명적인 망각(catastrophic forgetting)이 발생할 수 있으므로, 가벼운 어댑터나 프롬프트 튜닝이 중요합니다.
* **추론 속도 및 평가 지표(Inference Speed and Evaluation Metrics)**: 현재 OVD/OVS 모델은 대부분 무거운 백본을 사용하여 엣지 디바이스에서 느립니다. 경량 검출기는 새로운 객체의 리콜률을 낮출 수 있으며, 대규모 VLM 지식을 소규모 모델에 증류하는 것은 학습 능력의 한계로 인해 여전히 의문입니다. 또한, 평가 지표가 동의어 예측 등을 고려하지 않아 실제 개방형 세계에서의 성능을 정확히 반영하지 못할 수 있습니다.

**미래 연구 방향:**

* **다른 장면 이해 작업으로의 개방형-어휘 확장**: 3D 장면 이해, 비디오 분석, 행동 인식, 객체 추적, 인간-객체 상호작용 등 현재 덜 탐색된 분야로 개방형-어휘 능력을 확장하는 것이 중요합니다.
* **OVD와 OVS의 통합**: 컴퓨터 비전의 통합은 피할 수 없는 추세입니다. 모든 작업과 데이터셋을 위한 보편적인 기반 모델, 나아가 2D 및 3D 개방형-어휘 지각을 동시에 달성하는 것이 다음 과제입니다.
* **지각을 위한 다중 모달 대규모 언어 모델(LLMs)**: 시각 인코더, 시각 특징을 LLM 입력 공간으로 매핑하는 매퍼, 그리고 원하는 출력을 디코딩하는 LLM을 포함하는 다중 모달 LLM은 언어 컨텍스트 내에서 사용자 의도를 추론하고 대화형 검출 및 분할을 가능하게 할 잠재력을 가집니다.
* **대규모 기반 모델의 결합**: SAM(객체 지역화), CLIP(이미지-텍스트 정렬), DINO(교차-이미지 대응), T2I Diffusion 모델(생성) 등 각기 다른 강점을 가진 기반 모델들을 최적으로 결합하는 방법에 대한 연구가 필요합니다.
* **실시간 OVD 및 OVS**: 현재 모델은 실시간 응용에 부적합하므로, YOLO와 같은 경량 검출기와 개방형-어휘 인식 능력을 결합하여 실시간 성능을 달성하는 것이 유망한 연구 방향입니다.

## 📌 TL;DR

기존 객체 검출 및 분할 모델은 미리 정의된 소수의 클래스에만 작동하는 "폐쇄형-어휘" 문제에 직면해 있습니다. 이 서베이는 미탐지 객체 검출(OVD) 및 분할(OVS) 분야를 종합적으로 검토하며, 약한 감독 신호의 사용 여부 및 유형에 따라 **시각-의미 공간 매핑**, **새로운 시각 특징 합성**, **영역-인지 학습**, **의사-레이블링**, **지식 증류**, **전이 학습**의 여섯 가지 방법론으로 분류하는 새로운 분류 체계를 제안합니다. 이 논문은 각 방법론의 세부 사항, 주요 과제(예: 베이스 클래스에 대한 과적합, 배경과의 혼란, VLM 적응 문제, 추론 속도), 그리고 **다중 모달 LLM 활용**, **기반 모델 통합**, **실시간 성능 확보** 등 미래 연구 방향을 제시하여 분야의 발전을 위한 로드맵을 제공합니다.
