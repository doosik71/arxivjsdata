# Video Object Segmentation using Space-Time Memory Networks

Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim

## 🧩 Problem to Solve

반지도 학습(Semi-supervised) 비디오 객체 분할(VOS)은 주어진 비디오의 첫 프레임에서 객체 마스크가 제공될 때, 나머지 모든 프레임에서 해당 객체의 마스크를 추정하는 작업입니다. 이 작업은 객체의 외형 변화, 가려짐(occlusions), 오류 누적(drifts) 등으로 인해 매우 도전적입니다. 기존 방법들은 중간 예측을 통해 풍부해지는 가용 정보를 충분히 활용하지 못했으며, 종종 특정 프레임(예: 이전 프레임 또는 첫 프레임)에만 의존하거나 계산 비용이 높은 온라인 학습(online learning)을 필요로 했습니다.

## ✨ Key Contributions

- **새로운 시공간 메모리 네트워크(Space-Time Memory Networks, STM) 제안:** 과거 프레임과 객체 마스크를 외부 메모리로 활용하여 현재 프레임을 분할하는 새로운 프레임워크를 제시합니다.
- **풍부한 정보 활용:** 과거의 모든 가용 프레임(객체 마스크 포함)으로부터 관련 정보를 읽어내는 학습을 통해 외형 변화 및 가려짐과 같은 문제에 더 잘 대처할 수 있도록 합니다.
- **밀집 시공간 픽셀 매칭:** 쿼리 프레임과 메모리 프레임 간의 특징 공간에서 모든 시공간 픽셀 위치를 조밀하게 매칭합니다.
- **뛰어난 성능 및 속도:** Youtube-VOS 및 DAVIS 벤치마크에서 기존 최고 성능(state-of-the-art)을 능가하는 정확도와 빠른 실행 속도(0.16초/프레임)를 달성했습니다.
- **오프라인 학습 방식:** 온라인 학습 없이 메모리 네트워크의 순방향 패스(forward pass) 내에서 메모리 읽기가 이루어지므로, 빠른 속도로 실제 적용에 유리합니다.

## 📎 Related Works

- **반지도 학습 비디오 객체 분할:**
  - **전파 기반(Propagation-based) 방법 [26, 14, 11, 18]:** 이전 프레임으로부터 마스크를 전파하여 정제합니다. 종종 객체별 온라인 학습을 필요로 합니다.
  - **탐지 기반(Detection-based) 방법 [2, 21, 42, 1, 3, 12]:** 첫 프레임의 객체 외형을 학습하여 각 프레임에서 객체를 독립적으로 탐지합니다.
  - **하이브리드(Hybrid) 방법 [40, 24]:** 첫 프레임과 이전 프레임의 장점을 모두 활용합니다. 일부는 모든 이전 정보를 활용하기도 합니다 [38, 34].
  - **온라인/오프라인 학습:** 많은 기존 방법들은 테스트 시 온라인 학습(미세 조정)을 사용하지만 이는 계산 비용이 높습니다. 본 논문의 프레임워크는 오프라인 학습 방식에 속합니다.
- **메모리 네트워크(Memory Networks) [30, 22, 16]:**
  - 원래 NLP(질의응답) 분야에서 외부 메모리를 통해 정보를 읽고 쓰는 방식으로 제안되었습니다. 키-값(key-value) 쌍으로 정보를 임베딩합니다.
  - 최근 이미지 캡셔닝 [25], 시각적 추적 [41], 영화 이해 [23] 등 다양한 컴퓨터 비전 문제에 적용되었습니다.
  - 본 연구는 메모리 네트워크 개념을 픽셀 단위 비디오 분할에 맞게 확장하여 픽셀 레벨 정보가 담긴 4D 텐서를 메모리로 활용하는 시공간 메모리 읽기 작업을 제안합니다.

## 🛠️ Methodology

본 논문은 `Space-Time Memory Networks (STM)`이라는 새로운 딥러닝 시스템을 제안합니다.

1. **전반적인 프레임워크:** 메모리 인코더($Enc_M$), 쿼리 인코더($Enc_Q$), 시공간 메모리 읽기 블록(Space-time Memory Read Block), 디코더로 구성됩니다 (그림 2 참조).
2. **키 및 값 임베딩 (Key and Value Embedding):**
   - **쿼리 인코더:** 현재 RGB 프레임을 입력으로 받아, 특징 맵의 병목 레이어를 통해 키($k_Q \in \mathbb{R}^{H \times W \times C/8}$) 및 값($v_Q \in \mathbb{R}^{H \times W \times C/2}$) 맵을 출력합니다. 키는 매칭을 위한 시각적 의미를, 값은 정확한 마스크 추정을 위한 세부 정보를 저장합니다.
   - **메모리 인코더:** 과거 RGB 프레임과 해당 객체 마스크(채널 차원으로 연결)를 입력으로 받아, 키($k_M \in \mathbb{R}^{T \times H \times W \times C/8}$) 및 값($v_M \in \mathbb{R}^{T \times H \times W \times C/2}$) 맵을 출력합니다. 값은 전경/배경 마스크 정보를 인코딩합니다.
   - 백본 네트워크로는 ResNet50 [9]의 stage-4 특징 맵을 사용합니다.
3. **시공간 메모리 읽기 (Space-time Memory Read):**
   - 쿼리 키 맵의 모든 픽셀과 메모리 키 맵의 모든 시공간 위치 간의 유사도를 측정하여 소프트 가중치를 계산합니다. 유사도 함수는 내적(dot-product) 기반의 $\exp(k_Q_i \circ k_M_j)$ 입니다.
   - 메모리의 값 특징 맵은 이 소프트 가중치와 함께 가중 합산되어 검색됩니다.
   - 최종 출력 $y_i$는 검색된 메모리 값과 쿼리 값을 연결(concatenate)하여 얻어집니다: $y_i = [v_Q_i, \frac{1}{Z} \sum_{\forall j} f(k_Q_i,k_M_j)v_M_j]$. 이는 시공간 어텐션 메커니즘으로 작동하며, 현대 딥러닝 플랫폼에서 기본 텐서 연산을 통해 효율적으로 구현됩니다 (그림 3 참조).
4. **디코더 (Decoder):** 읽기 연산의 출력을 받아 현재 프레임의 객체 마스크를 재구성합니다. [24]의 정제 모듈(refinement module)을 기반으로 점진적으로 업스케일링하며, 쿼리 인코더로부터의 스킵 연결(skip-connections)을 활용합니다.
5. **다중 객체 분할 (Multi-object Segmentation):** 각 객체에 대해 모델을 독립적으로 실행한 후, 예측된 마스크 확률 맵을 소프트 통합(soft aggregation) 연산을 통해 병합합니다.
6. **2단계 학습 (Two-stage Training):**
   - **이미지 기반 사전 학습 (Pre-training on images):** 정적 이미지 데이터셋(예: [29, 5, 7, 8, 19])을 사용하여 임의의 아핀 변환(affine transforms)을 적용한 합성 비디오 클립으로 사전 학습을 수행합니다. 이는 모델이 다양한 객체 외형 및 카테고리에 강건하도록 돕습니다.
   - **비디오 기반 본 학습 (Main training on videos):** 사전 학습 후 실제 비디오 데이터셋(Youtube-VOS [38] 또는 DAVIS-2017 [28])을 사용하여 미세 조정합니다. 학습 중 장기적인 외형 변화 학습을 위해 프레임을 무작위로 건너뛰는 전략을 사용합니다.
   - **동적 메모리 업데이트:** 학습 중 네트워크의 이전 출력(확률 맵)을 다음 프레임의 메모리로 동적으로 추가합니다.
7. **추론 (Inference):** GPU 메모리 및 속도 효율성을 위해, 기본적으로 첫 번째 프레임(Ground Truth)과 바로 이전 프레임을 메모리에 저장합니다. 추가적으로 매 $N=5$ 프레임마다 중간 프레임을 메모리에 저장합니다. 이는 온라인 학습 없이도 온라인 적응 효과를 제공합니다.

## 📊 Results

- **Youtube-VOS 벤치마크:** 종합 점수 79.4를 달성하여 모든 경쟁 방법을 크게 능가하며 최고 성능을 기록했습니다 (표 1). 미학습 객체 카테고리(unseen categories)에서도 우수한 일반화 성능을 보였습니다.
- **DAVIS 2016 (단일 객체):** 온라인 학습 없이 경쟁 방법 중 최고의 정확도(JMean 84.8)를 달성했으며, 온라인 학습 기반 방법들과도 경쟁력 있는 결과를 보이면서도 훨씬 빠른 속도(0.16초/프레임)를 가집니다. Youtube-VOS 데이터로 추가 학습 시 모든 방법을 크게 능가하는 JMean 88.7을 기록했습니다 (표 2).
- **DAVIS 2017 (다중 객체):** 온라인 학습 없는 빠른 방법들 중에서 최고의 성능(JMean 69.2)을 보였습니다. Youtube-VOS 데이터를 추가 학습 시, 2018 DAVIS 챌린지 우승자 [20]를 포함한 모든 기존 최신 방법들을 크게 능가하는 JMean 79.2를 달성했습니다 (표 3).
- **정성적 결과:** 가려짐(occlusions) 및 복잡한 움직임에 강건함을 보였습니다 (그림 4).

## 🧠 Insights & Discussion

- **학습 데이터의 중요성:** 정적 이미지를 활용한 사전 학습은 모델의 일반화 능력과 과적합 방지에 매우 중요하며, 심지어 실제 비디오 없이도 Youtube-VOS에서 상당한 성능을 보였습니다. 사전 학습과 본 학습을 모두 사용했을 때 최상의 성능을 달성했습니다.
- **데이터셋 크기:** DAVIS는 네트워크 학습에 너무 작아서 과적합 문제가 발생할 수 있으며, 다른 데이터셋으로의 일반화 능력이 제한적입니다. 반면, Youtube-VOS로 학습된 모델은 DAVIS에서도 좋은 성능을 보여 데이터셋 규모의 중요성을 시사합니다. 이는 온라인 학습 기반 방법들이 대규모 벤치마크에서 좋은 성능을 보이지 못하는 이유 중 하나로 분석됩니다.
- **메모리 관리의 효율성:** 첫 프레임과 이전 프레임을 메모리에 저장하는 것이 가장 중요하며, 이를 통해 이미 최신 수준의 정확도를 달성할 수 있습니다. 추가적으로 중간 프레임을 메모리에 저장하면 특히 어려운 경우(낮은 Jaccard 점수 구간)에 성능이 더욱 향상됨을 확인했습니다 (그림 6, 7). 이는 모델이 풍부한 참조 정보를 활용하여 도전적인 시나리오를 더 잘 처리함을 의미합니다.
- **시공간 매칭 학습:** 메모리 읽기 연산의 시각화는 쿼리 프레임과 메모리 프레임 간에 정확하게 대응하는 픽셀을 매칭하는 것을 보여주며, 제안된 메모리 네트워크가 시공간적으로 관련 정보를 효과적으로 학습함을 입증합니다.
- **제한 사항 및 향후 연구:** 본 논문은 VOS에서 뛰어난 성능을 보였으며, 픽셀 단위 추정 문제에 대한 잠재력이 크다고 언급합니다. 향후 객체 추적, 대화형 이미지/비디오 분할, 인페인팅 등 다른 응용 분야에도 적용될 수 있을 것으로 기대됩니다.

## 📌 TL;DR

- **문제:** 기존의 반지동 비디오 객체 분할(VOS) 방법은 외형 변화, 가려짐 등의 문제에 취약하며, 축적되는 풍부한 정보를 충분히 활용하지 못하고 느린 온라인 학습에 의존합니다.
- **제안 방법:** `공간-시간 메모리 네트워크(Space-Time Memory Networks, STM)`를 제안합니다. 이 네트워크는 과거 프레임과 마스크를 외부 메모리로 사용하고, 현재 프레임을 쿼리로 삼아 메모리에서 관련 정보를 읽어 객체 마스크를 분할합니다. 특히, 쿼리 픽셀과 메모리 픽셀 간의 모든 시공간 위치에서 밀집 매칭을 수행하는 "시공간 메모리 읽기" 연산을 핵심으로 합니다. 정적 이미지 사전 학습과 비디오 데이터 본 학습의 2단계 학습 방식을 사용합니다.
- **주요 결과:** Youtube-VOS 및 DAVIS 벤치마크에서 기존 최고 성능을 크게 능가하는 정확도와 함께 0.16초/프레임의 빠른 추론 속도를 달성했습니다. 이는 온라인 학습 없이도 매우 효율적이며 강건한 VOS 성능을 제공함을 입증합니다.
