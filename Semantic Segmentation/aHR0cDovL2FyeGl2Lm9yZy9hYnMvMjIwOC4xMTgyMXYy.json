{
  "title": "Refine and Represent: Region-to-Object Representation Learning",
  "authors": "Akash Gokul, Konstantinos Kallidromitis, Shufan Li, Yusuke Kato, Kazuki Kozuka, Trevor Darrell, Colorado J Reed",
  "year": 2022,
  "url": "http://arxiv.org/abs/2208.11821v2",
  "abstract": "Recent works in self-supervised learning have demonstrated strong performance\non scene-level dense prediction tasks by pretraining with object-centric or\nregion-based correspondence objectives. In this paper, we present\nRegion-to-Object Representation Learning (R2O) which unifies region-based and\nobject-centric pretraining. R2O operates by training an encoder to dynamically\nrefine region-based segments into object-centric masks and then jointly learns\nrepresentations of the contents within the mask. R2O uses a \"region refinement\nmodule\" to group small image regions, generated using a region-level prior,\ninto larger regions which tend to correspond to objects by clustering\nregion-level features. As pretraining progresses, R2O follows a\nregion-to-object curriculum which encourages learning region-level features\nearly on and gradually progresses to train object-centric representations.\nRepresentations learned using R2O lead to state-of-the art performance in\nsemantic segmentation for PASCAL VOC (+0.7 mIOU) and Cityscapes (+0.4 mIOU) and\ninstance segmentation on MS COCO (+0.3 mask AP). Further, after pretraining on\nImageNet, R2O pretrained models are able to surpass existing state-of-the-art\nin unsupervised object segmentation on the Caltech-UCSD Birds 200-2011 dataset\n(+2.9 mIoU) without any further training. We provide the code/models from this\nwork at https://github.com/KKallidromitis/r2o.",
  "citation": 5
}