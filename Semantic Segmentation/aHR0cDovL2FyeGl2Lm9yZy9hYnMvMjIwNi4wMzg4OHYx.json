{
  "url": "http://arxiv.org/abs/2206.03888v1",
  "title": "ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical\n  Image Segmentation",
  "authors": "Mingxuan Gu, Sulaiman Vesal, Mareike Thies, Zhaoya Pan, Fabian Wagner, Mirabela Rusu, Andreas Maier, Ronak Kosti",
  "year": 2022,
  "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to an unlabeled target domain. Contrastive learning\n(CL) in the context of UDA can help to better separate classes in feature\nspace. However, in image segmentation, the large memory footprint due to the\ncomputation of the pixel-wise contrastive loss makes it prohibitive to use.\nFurthermore, labeled target data is not easily available in medical imaging,\nand obtaining new samples is not economical. As a result, in this work, we\ntackle a more challenging UDA task when there are only a few (fewshot) or a\nsingle (oneshot) image available from the target domain. We apply a style\ntransfer module to mitigate the scarcity of target samples. Then, to align the\nsource and target features and tackle the memory issue of the traditional\ncontrastive loss, we propose the centroid-based contrastive learning (CCL) and\na centroid norm regularizer (CNR) to optimize the contrastive pairs in both\ndirection and magnitude. In addition, we propose multi-partition centroid\ncontrastive learning (MPCCL) to further reduce the variance in the target\nfeatures. Fewshot evaluation on MS-CMRSeg dataset demonstrates that ConFUDA\nimproves the segmentation performance by 0.34 of the Dice score on the target\ndomain compared with the baseline, and 0.31 Dice score improvement in a more\nrigorous oneshot setting.",
  "citation": 3
}