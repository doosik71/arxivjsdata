{
  "url": "http://arxiv.org/abs/1612.03716v4",
  "title": "COCO-Stuff: Thing and Stuff Classes in Context",
  "authors": "Holger Caesar, Jasper Uijlings, Vittorio Ferrari",
  "year": 2016,
  "abstract": "Semantic classes can be either things (objects with a well-defined shape,\ne.g. car, person) or stuff (amorphous background regions, e.g. grass, sky).\nWhile lots of classification and detection works focus on thing classes, less\nattention has been given to stuff classes. Nonetheless, stuff classes are\nimportant as they allow to explain important aspects of an image, including (1)\nscene type; (2) which thing classes are likely to be present and their location\n(through contextual reasoning); (3) physical attributes, material types and\ngeometric properties of the scene. To understand stuff and things in context we\nintroduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset\nwith pixel-wise annotations for 91 stuff classes. We introduce an efficient\nstuff annotation protocol based on superpixels, which leverages the original\nthing annotations. We quantify the speed versus quality trade-off of our\nprotocol and explore the relation between annotation time and boundary\ncomplexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of\nstuff and thing classes in terms of their surface cover and how frequently they\nare mentioned in image captions; (b) the spatial relations between stuff and\nthings, highlighting the rich contextual relations that make our dataset\nunique; (c) the performance of a modern semantic segmentation method on stuff\nand thing classes, and whether stuff is easier to segment than things."
}