# Multi-Modal Prototypes for Open-World Semantic Segmentation

Yuhuan Yang, Chaofan Ma, Chen Ju, Fei Zhang, Jiangchao Yao, Ya Zhang, Yanfeng Wang

## 🧩 Problem to Solve

시맨틱 분할(semantic segmentation) 시스템은 대규모 데이터셋과 딥러닝 기술의 발전으로 큰 진전을 이루었지만, 대부분의 연구는 훈련 및 추론 시 관심 대상 범주가 동일하게 유지되는 '폐쇄 세계(closed-world)' 설정에 초점을 맞추고 있습니다. 그러나 실제 환경에서는 예측 시점에 새로운, '보지 못했던(novel)' 범주가 등장하는 '개방 세계(open-world)' 시나리오가 빈번하며, 이러한 기존 모델들은 유연하게 대응하기 어렵다는 한계가 있습니다.

이 문제에 대응하기 위한 기존의 접근 방식은 주로 시각적 단서(몇 가지 지원 예시를 통한 퓨샷 분할) 또는 텍스트 단서(클래스 이름을 활용한 제로샷 분할)에 의존합니다. 하지만 이 두 가지 접근 방식 모두 저수준 시각 정보와 고수준 언어 정보의 상호 보완적인 특성을 간과합니다. 단순히 두 모달리티를 결합하는 것은 잠재 공간에서의 의미 정렬(semantic alignment) 문제로 인해 비효율적이며, 이러한 간극을 메울 수 있는 통합적인 탐색은 여전히 제한적입니다.

## ✨ Key Contributions

* **새로운 멀티모달 프레임워크 제안:** 시맨틱 분할 성능 향상을 위해 상호 보완적인 시각 및 텍스트 단서를 효과적으로 활용하여 더욱 강력한 멀티모달 프로토타입을 구축하는 새로운 개방 세계 시맨틱 분할 프레임워크를 제시합니다.
* **세분화된 멀티-프로토타입 생성 및 융합 메커니즘 설계:** 텍스트 모달리티와 시각 모달리티의 정보를 효율적으로 병합하고, 멀티모달 프로토타입을 유연하게 통합하여 제로샷(zero-shot), 퓨샷(few-shot), 일반화된 퓨샷(generalized few-shot) 등 다양한 개방 세계 분할 작업을 단일 아키텍처에서 수행할 수 있도록 합니다. 이를 위해 텍스트 분해(대규모 언어 모델(LLMs) 기반)와 시각적 집계(M-Splitting 알고리즘 기반)를 활용하고, 상호 보완적 융합 모듈 및 다중 프로토타입 기반 마스크 예측 파이프라인을 설계했습니다.
* **최첨단 성능 달성:** PASCAL-5$_{i}$ 및 COCO-20$_{i}$ 데이터셋에 대한 광범위한 실험을 통해 다양한 설정에서 기존 최첨단(SOTA) 접근 방식보다 일관되게 우수한 성능을 달성했습니다. 또한, 각 구성 요소의 효과를 정량적 및 정성적으로 검증하는 다양한 어블레이션 연구를 수행했습니다.

## 📎 Related Works

* **시각적 단서 활용 (Few-shot Segmentation, FSS):**
  * **단일 벡터 기반 방법:** 주어진 시각적 예시에서 범주의 핵심 특징을 단일 벡터(프로토타입)로 응축하여 픽셀별 분류기 [13] 또는 디코더에 통합 [20-23] 합니다 (예: PPNet [19], PFENet [12]).
  * **밀집 특징 기반 방법:** 지원 이미지의 밀집 특징(dense features)을 활용하여 4D 컨볼루션 [26] 또는 어텐션 메커니즘 [27]을 사용합니다.
  * **일반화된 퓨샷 분할 (Generalized Few-shot Segmentation, GFSS):** 기본 클래스와 새로운 클래스 모두를 예측할 수 있도록 FSS를 확장합니다 (예: GFSS [28], DIaM [29]).
* **텍스트 단서 활용 (Zero-shot Segmentation, ZS3):**
  * 초기 연구: 단어 임베딩으로부터 시각적 특징 생성 [14] 또는 픽셀과 시맨틱 단어를 위한 공동 임베딩 공간 개발 [32].
  * 최근 연구: CLIP [34]과 같은 사전 학습된 비전-언어 모델(VLMs)의 정렬된 표현을 활용하여 텍스트 클래스 이름을 분류기로 구성 [16, 18] 합니다 (예: LSeg [18], Fusioner [16]).
  * **오픈-어휘 분할 (Open-vocabulary Segmentation):** 기본 범주에 대한 학습을 통해 시각-텍스트 정렬을 학습하고, 이를 통해 훈련에 사용되지 않은 데이터셋의 범주도 분할하는 데 중점을 둡니다 (예: OVSeg [17], OpenSeeD [35], FC-CLIP [36]).
* **프로토타입 기반 학습 (Prototype-based Learning):**
  * 퓨샷 및 제로샷 학습에서 기본 범주와 새로운 범주 간의 간극을 연결하는 데 사용됩니다 [44, 45].
  * 이미지-텍스트 정렬을 강화하기 위해 언어-가이드 분할 작업에서도 사용됩니다 [14, 32, 17, 18].
* **본 연구와의 차이점:** 기존 방법들은 주로 단일 프로토타입 생성에 제한되거나 모달리티를 개별적으로 처리하는 반면, 본 연구는 M-Splitting 및 텍스트 분해를 통해 시각 및 텍스트 모달리티에서 **다수의 프로토타입을 풍부하게 생성**하고, **상호 보완적 융합 모듈**을 설계하며, **다중 프로토타입 기반 마스크 예측 파이프라인**을 통해 복잡한 의미론을 포착합니다.

## 🛠️ Methodology

제안하는 프레임워크는 크게 네 가지 주요 구성 요소로 이루어집니다.

1. **시각 프로토타입 추출기($\Phi_{img}$):**
    * 단일 프로토타입으로는 객체 내의 다양한 시각적 변이를 충분히 반영하기 어렵다는 한계를 극복하기 위해, 지원 이미지의 시각 특징($F_{vis} \in \mathbb{R}^{H \times W \times D}$)과 지원 마스크($M_c$)를 활용하여 객체의 다양한 시각적 외관을 캡슐화하는 여러 개의 시각 프로토타입을 생성합니다.
    * **M-Splitting 알고리즘 (Algorithm 1):** 주어진 바이너리 마스크 $M$을 $n$개의 겹치지 않는 영역($\{M_c^1, ..., M_c^n\}$)으로 분할합니다. 각 분할된 영역에서 특징을 집계하여 $n$개의 시각 프로토타입 $P_{img}^c = [\Psi_{single}(F_{vis}, M_c^i)]_{i=1}^n \in \mathbb{R}^{n \times D}$를 얻습니다.
2. **텍스트 프로토타입 추출기($\Phi_{txt}$):**
    * 클래스 이름의 모호성(예: 'crane')과 약한 연관성('Pomeranian dog'과 실제 시각적 특징) 한계를 해결합니다.
    * **대규모 언어 모델(LLMs, $\Phi_{llm}$):** LLMs(예: GPT-4)에 질의 프롬프트(prompt)를 사용하여 클래스 이름을 여러 측면의 세분화된 설명($T_{txt} = \{T_{txt}^1, ..., T_{txt}^{n-1}\}$)으로 자동 분해합니다.
    * 원래 클래스 이름과 분해된 설명을 CLIP 텍스트 인코더($\Psi_{CLIP}$)에 입력하여 $n$개의 텍스트 프로토타입 $P_{txt}^c = [\Psi_{CLIP}(T), \Psi_{CLIP}(T_{txt}^1), ..., \Psi_{CLIP}(T_{txt}^{n-1})] \in \mathbb{R}^{n \times D}$를 생성합니다.
3. **멀티모달 프로토타입 상호 보완적 융합($\Phi_{fuse}$):**
    * 시각 및 텍스트 프로토타입의 상호 보완적 특성을 활용하여 강력한 멀티모달 프로토타입을 생성하기 위해 교차 어텐션 메커니즘을 사용합니다.
    * 쿼리($Q$)는 시각 및 텍스트 프로토타입을 연결한 $Q = [P_{txt}^c, P_{img}^c] \in \mathbb{R}^{2n \times D}$입니다.
    * 키($K$)와 값($V$)은 전역 이미지 특징($F_{vis}$)을 포함하는 $K=V = [P_{txt}^c, F_{vis}] \in \mathbb{R}^{(n+H \times W) \times D}$로 설정하여 배경 정보를 통합합니다.
    * 배경 정보의 영향을 적절히 조절하기 위해 학습 가능한 가중치 마스크($\alpha \cdot M_c$)를 어텐션 메커니즘에 적용합니다. 최종 멀티모달 프로토타입 $P_c = \sigma \left( \frac{QK^T}{\sqrt{D}} - \alpha \cdot M_c \right) \cdot V \in \mathbb{R}^{2n \times D}$를 계산합니다.
4. **탄성 마스크 예측($\Phi_{mask}$):**
    * 모든 $|C|$ 클래스에 대한 마스크 예측을 위해 다수의 멀티모달 프로토타입 세트를 활용합니다.
    * 각 프로토타입을 독립적인 서브-클래스 분류기로 간주하고, 쿼리 특징($F_q$)과 프로토타입 간의 어텐션 메커니즘을 통해 예측 로짓을 생성합니다: $\hat{y}=\sigma \left( \frac{F_q(W_p \cdot \tilde{P})^T}{\sqrt{D}} \right) \cdot l_P \in \mathbb{R}^{HW \times |C|}$. 여기서 $W_p \in \mathbb{R}^{2n}$는 각 프로토타입의 기여도를 조절하는 학습 가능한 가중치입니다.
    * **다단계 융합 (Multi-Level Fusion):** 시각 인코더의 여러 중간 레이어($L$개)에서 얻은 특징 피라미드를 사용하여 얻은 조악한 마스크 예측($\{\hat{y}_l\}_{l=1}^L$)을 잔여 구조와 스킵 연결(skip connection)을 통해 최종 마스크 예측($\hat{y}_{final}$)으로 융합하여 다양한 스케일의 객체를 처리합니다 (Eq. (8) 참조).

* **학습 및 추론:** 학습 시에는 CLIP 인코더와 LLMs를 고정(freeze)하여 사전 지식을 보존하고 오버피팅을 방지합니다. 추론 시에는 시각 지원 예시와 텍스트 데이터를 사용하여 새로운 클래스($C_{unseen}$)에 대한 멀티모달 프로토타입을 계산하고, 이를 학습된 클래스의 프로토타입과 연결하여 분할을 수행합니다.

## 📊 Results

* **Z/FS (Zero/Few-Shot) 설정에서의 비교:** PASCAL-5$_{i}$ 및 COCO-20$_{i}$ 데이터셋에서 단일 모달리티 버전(Text Only, Image Only)도 기존 모든 베이스라인을 능가했으며, 멀티모달(Full) 버전은 모든 측면에서 가장 우수한 결과를 달성하며 두 모달리티의 결합 효과를 입증했습니다 (Table 2).
* **GFS (Generalized Few-Shot) 설정에서의 비교:** PASCAL-5$_{i}$ 및 COCO-20$_{i}$ 데이터셋 모두에서 최신 DIaM [29] 대비 UnSeen 지표에서 상당한 개선(각각 1-샷 설정에서 4.33%, 2.26% 개선)을 보였으며, 가장 높은 HIoU 점수를 달성하여 보았던 클래스와 보지 못했던 클래스 모두에서 동시에 강력한 분할 성능을 입증했습니다 (Table 3, 4).
* **어블레이션 연구:**
  * **마스크 분할 알고리즘:** M-Splitting은 K-means보다 약 200배 빠르면서도 합리적인 마스크 분할 결과를 제공하여 계산 효율성 측면에서 큰 이점을 가집니다 (Table 5).
  * **마스크 분할 수 및 설명 수:** 시각 프로토타입 및 텍스트 설명 수를 적절히 늘릴 때 성능이 향상되며, 이는 멀티 프로토타입 및 텍스트 분해의 효과를 입증합니다. (Table 6, 7).
  * **다단계 융합:** 가장 깊은 특징($L=1$)이 가장 큰 기여를 하며, 얕은 특징($L=2$)을 추가하면 성능이 향상되지만, 더 얕은 특징($L=3$)의 추가는 미미한 개선을 보입니다 (Table 8).
  * **정보 누락/부정확성에 대한 견고성:** 전체 지원 정보(마스크 주석 + 텍스트 설명)를 제공할 때 최고의 성능을 보였습니다. 텍스트 설명이 없거나, 마스크 주석 대신 바운딩 박스를 사용하거나, 심지어 이미지 정보가 전혀 없는 경우에도 허용 가능한 성능을 보였습니다 (Table 9). 또한, 부정확한 마스크(점진적으로 침식된 마스크)에 대해서도 기존 방법보다 훨씬 견고한 성능을 입증했습니다 (Figure 7).
* **다른 강력한 분할 모델과의 비교:** 'Fantastic Beats' 데이터셋(훈련 데이터에 포함되지 않은 새로운 범주)에서 제로샷(51.6 IoU) 및 원샷(78.5 IoU)으로 FC-CLIP, SEEM, LISA를 포함한 최신 '일반 분할 아키텍처'를 능가하는 성능을 보여, 제한된 훈련 데이터만으로도 새로운 범주를 분할하는 데 멀티모달 프로토타입의 강력한 힘을 입증했습니다 (Table 10). 또한, ADE20k, PASCAL-Context, PASCAL VOC 데이터셋에서 기존 오픈-어휘 방법들을 능가하는 성능을 달성했으며, 작은 백본(ResNet-50)으로도 완전 지도 학습된 LISA의 성능에 필적하는 결과를 보여주었습니다 (Table 11).

## 🧠 Insights & Discussion

* **두 모달리티의 상호 보완성 활용의 중요성:** 본 연구는 시각 및 텍스트 단서의 상호 보완적인 특성을 효과적으로 활용하는 것이 개방 세계 시맨틱 분할에서 상당한 성능 향상을 이끌어낼 수 있음을 입증했습니다. 이는 단일 모달리티에만 의존하거나 단순 결합하는 기존 방식의 한계를 명확히 보여줍니다.
* **세분화된 프로토타입의 효과:** 객체의 복잡한 시각적 특징(M-Splitting)과 클래스 이름의 풍부한 의미론적 측면(LLM 기반 텍스트 분해)을 여러 개의 세분화된 프로토타입으로 표현하고, 이를 상호 보완적으로 융합하는 메커니즘이 모델의 식별 능력과 일반화 능력을 크게 향상시킴을 확인했습니다.
* **유연하고 실용적인 아키텍처:** 제안된 프레임워크는 제로샷, 퓨샷, 일반화된 퓨샷 등 다양한 개방 세계 시나리오에 유연하게 적용될 수 있는 단일 아키텍처를 제공하여 실용적 가치를 높입니다.
* **새로운 범주에 대한 탁월한 일반화 능력:** 훈련 데이터에 포함되지 않은 완전히 새로운 범주에 대해서도 강력한 분할 능력을 보여, 실제 환경에서의 적용 가능성을 시사합니다.
* **향후 연구 방향 제시:** 본 연구는 멀티모달 정보를 활용하여 더 효과적이고 포괄적인 알고리즘 설계에 대한 연구자들의 동기를 부여할 것으로 기대됩니다.

## 📌 TL;DR

**문제:** 기존 시맨틱 분할 모델은 '보지 못했던' 범주를 처리하는 데 한계가 있으며, 시각 기반(퓨샷) 또는 텍스트 기반(제로샷) 단일 모달 접근 방식은 시각-텍스트 정보의 상호 보완성을 간과합니다.

**제안 방법:** 이 논문은 개방 세계 시맨틱 분할을 위한 **멀티모달 프로토타입** 프레임워크를 제안합니다. 이 프레임워크는 대규모 언어 모델(LLMs)을 통해 고수준 텍스트 정보를 다각도 프로토타입으로 분해하고, M-Splitting 알고리즘을 통해 저수준 시각 정보를 시맨틱 프로토타입으로 집계합니다. 이러한 시각 및 텍스트 프로토타입은 상호 보완적 융합 모듈을 통해 통합되며, 탄성 마스크 예측 모듈은 단일 아키텍처에서 제로샷, 퓨샷, 일반화된 퓨샷 과제를 모두 처리할 수 있도록 합니다.

**주요 결과:** PASCAL-5$_{i}$ 및 COCO-20$_{i}$ 데이터셋에서 최첨단 성능을 달성했습니다. 특히, 새로운 범주에 대한 뛰어난 일반화 능력과 불완전한 입력에 대한 견고성을 입증했습니다. 이 멀티모달 접근 방식은 단일 모달 베이스라인 및 기존 오픈-어휘 분할 방법들을 크게 능가합니다.
