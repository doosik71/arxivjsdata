# Directional Deep Embedding and Appearance Learning for Fast Video Object Segmentation
Yingjie Yin, De Xu, Xingang Wang and Lei Zhang

## 🧩 Problem to Solve
최근 준지도 비디오 객체 분할(VOS) 방법들은 첫 프레임의 마스크 또는 이후 프레임의 예측된 마스크를 사용하여 딥 컨볼루션 신경망(DCNN)을 온라인으로 미세 조정하는 방식에 의존합니다. 그러나 이러한 온라인 미세 조정 과정은 일반적으로 시간이 오래 걸려 실제 응용 분야에서의 사용을 제한합니다. 기존 미세 조정 없는 방법들은 유클리드 공간의 픽셀 단위 임베딩 학습에서 유사성 매칭을 위한 많은 계산량으로 인해 느리거나, 고차원 유클리드 공간에서 외형 모델링 시 "차원의 저주" 문제에 직면합니다.

## ✨ Key Contributions
*   빠른 VOS를 위한 새로운 온라인 미세 조정(fine-tuning)이 필요 없는 방법인 **방향성 딥 임베딩 및 외형 학습(DDEAL)**을 제안합니다.
*   레이블링된 첫 프레임으로부터 정적 단서(static cues)를 학습하는 **전역 방향성 매칭 모듈(Global Directional Matching Module, GDMM)**을 제안합니다. 이 모듈은 컨볼루션 연산을 통해 효율적으로 구현됩니다.
*   구형 임베딩 공간에서 von Mises-Fisher (vMF) 분포 기반의 효과적인 **방향성 외형 모델(Directional Appearance Model)**을 제안하여, 비디오의 이후 프레임으로부터 동적 단서(dynamic cues)를 효과적으로 학습하고 대상과 배경을 표현합니다.
*   VOS를 위해 방향성 단서를 학습하는 최초의 연구입니다.
*   DAVIS 2017 및 YouTube-VOS와 같은 벤치마크 데이터셋에서 최신 성능을 달성하면서도 훨씬 빠른 속도를 유지합니다.

## 📎 Related Works
*   **준지도 비디오 객체 분할(Semi-supervised VOS):**
    *   **온라인 미세 조정 의존 방법:** OSVOS, OnAVOS, LucidTracker 등은 높은 정확도를 보이지만 속도가 느립니다.
    *   **미세 조정 없는 방법:** PML, FEELVOS(유클리드 픽셀 단위 임베딩, 느릴 수 있음), A-GAME(고차원 유클리드 공간의 생성 모델, "차원의 저주" 문제 직면) 등이 있습니다.
*   **딥 임베딩 학습(Deep embedding learning):** 얼굴 인식 및 검증, 이미지 분할, 이미지 검색 및 클러스터링, 이미지 이해 등 다양한 분야에서 사용됩니다. 하이퍼스피어 임베딩을 위한 SphereFace가 있습니다.
*   **방향성 통계 기반 학습(Directional statistics-based learning):** 단위 구형 공간의 방향성 특징을 모델링하는 von Mises-Fisher (vMF) 분포가 널리 사용됩니다. 얼굴 검증, 클러스터링, 이미지 분류 및 검색, 기계 번역, 문서 분류 등에 적용되었으며, 본 연구는 이를 VOS에 적용한 최초의 시도입니다.

## 🛠️ Methodology
*   DDEAL은 첫 프레임의 정적 단서와 이후 프레임의 동적 단서를 통합하여 미세 조정이 필요 없는 VOS DCNN을 구현합니다.
*   **아키텍처 개요:** A-GAME과 유사하게 특징 추출기(ResNet50/101), 마스크 전파(mask-propagation), 융합(fusion), 업샘플링(up-sampling) 모듈을 사용합니다. 주요 차이점은 두 가지 방향성 특징 기반 모듈인 **전역 방향성 매칭 모듈(GDMM)**과 **방향성 외형 학습 모듈(DALM)**입니다.
*   **전역 방향성 매칭 모듈(GDMM):**
    *   레이블링된 첫 프레임으로부터 정적 단서를 학습합니다.
    *   기존 유클리드 거리와 달리 구형 임베딩 공간에서 픽셀 단위 임베딩 매칭을 위해 **코사인 유사도**를 사용합니다.
    *   병렬 컨볼루션 연산을 통해 효율적으로 구현됩니다.
    *   첫 프레임에서 추출된 정규화된 특징 맵 $F_0 \in \mathbb{R}^{C \times H \times W}$와 마스크 $M_0 \in \mathbb{R}^{1 \times H \times W}$를 사용하여 대상($K_t \in \mathbb{R}^{HW \times C \times 1 \times 1}$) 및 배경($K_b \in \mathbb{R}^{HW \times C \times 1 \times 1}$) 컨볼루션 커널을 생성합니다.
    *   현재 프레임의 특징 $F_1 \in \mathbb{R}^{C \times H \times W}$을 $K_t$ 및 $K_b$와 컨볼루션하여 유사도 맵 $S_t \in \mathbb{R}^{HW \times H \times W}$ 및 $S_b \in \mathbb{R}^{HW \times H \times W}$를 생성합니다:
        $$S_t = F_1 * K_t$$
        $$S_b = F_1 * K_b$$
    *   최종 출력 $s_t \in \mathbb{R}^{1 \times H \times W}$, $s_b \in \mathbb{R}^{1 \times H \times W}$는 각 채널의 최댓값을 취하여 얻습니다:
        $$s_t(h, w, 1, 1) = \max_{c} S_t(c, h, w, 1, 1)$$
        $$s_b(h, w, 1, 1) = \max_{c} S_b(c, h, w, 1, 1)$$
*   **방향성 통계 기반 외형 학습 모듈(DALM):**
    *   구형 임베딩 공간에서 vMF 분포 기반의 생성적 외형 모델을 학습합니다.
    *   비디오 프레임이 진행됨에 따라 대상/배경 단서를 동적으로 업데이트합니다.
    *   단위 L2-노름 벡터인 방향성 특징 $r$을 모델링하기 위해 vMF 분포 혼합을 사용합니다.
    *   실제로는 4개의 vMF 분포를 사용하며, 2개는 배경(k=0, 2), 2개는 전경(k=1, 3)을 모델링합니다. 0과 1은 기본(base) 구성 요소이고, 2와 3은 보조(supplementary) 구성 요소입니다(잘못 분류된 특징을 위해).
    *   **모델 파라미터 추정:**
        *   파라미터($\mu_k$, $\kappa$)는 첫 프레임의 특징과 마스크에서 추정됩니다.
        *   이후 프레임에서는 네트워크 예측을 소프트 클래스 레이블로 사용하여 학습률 $\lambda$로 $\mu_k$를 동적으로 업데이트합니다:
            $$\mu_k^{(i)} = (1 - \lambda)\mu_k^{(i-1)} + \lambda \frac{\sum_{l \in class_k} \alpha_{lk}^{(i)} r_l^{(i)}}{\sum_{l \in class_k} \alpha_{lk}^{(i)}}$$
        *   $\kappa$는 학습 가능한 스칼라 파라미터입니다.
        *   $\alpha_{lk}^{(i)}$ (소프트 클래스 레이블)는 네트워크 예측과 보조 구성 요소를 위한 기본 구성 요소의 사후 확률로부터 파생됩니다.
*   **학습:** 예측된 거친 분할 및 최종 분할 마스크에 대한 교차 엔트로피 손실을 합산하여 종단간(end-to-end) 방식으로 학습됩니다. DAVIS 2017, YouTube-VOS 및 합성 데이터셋에서 학습되었습니다.

## 📊 Results
*   **속도:** DDEAL(ResNet101)은 단일 NVIDIA TITAN Xp GPU에서 25fps로 실행되며, DDEAL(ResNet50)은 31fps로 실행됩니다.
*   **DAVIS 2016:**
    *   DDEAL-Res101은 85.4%의 $\mathcal{J}\&\mathcal{F}$ Mean을 달성하여 모든 방법 중 3위를 차지하며, OnAVOS보다 0.1% 낮지만 325배 빠릅니다.
    *   미세 조정이 필요 없는 다른 모든 방법들보다 정확도와 속도 면에서 크게 뛰어납니다(예: A-GAME보다 3.3% 높고 1.7배 빠름).
*   **DAVIS 2017:**
    *   DDEAL-Res101 및 DDEAL-Res50은 모든 측정치에서 최고의 결과를 달성합니다.
    *   DDEAL-Res101은 74.8%의 $\mathcal{J}\&\mathcal{F}$ Mean을 달성하여 FEELVOS(3.3%), CINM(4.2%), A-GAME(4.8%)보다 뛰어납니다.
*   **YouTube-VOS:**
    *   DDEAL-Res101 및 DDEAL-Res50은 최고의 전체 성능(각각 71.3%, 70.5%)을 달성하여 A-GAME보다 5.3%, 4.5% 더 높습니다.
    *   온라인 미세 조정 의존 방법보다도 더 나은 일반화 성능을 보여줍니다.
*   **어블레이션 연구(Ablation Study, Table V):**
    *   GDMM과 DALM은 모두 성능에 중요합니다. DALM을 비활성화하면 DAVIS 2017에서 $\mathcal{J}\&\mathcal{F}$ Mean이 2.7% 감소하고, GDMM을 비활성화하면 3.7% 감소합니다.
    *   DALM의 보조 구성 요소(SC) 또한 YouTube-VOS의 전체 성능을 1.2% 향상시키는 데 기여합니다.
*   **정성적 결과:** 큰 스케일 변화, 가려짐, 유사한 외형, 큰 움직임과 같은 어려운 경우에도 정확한 분할을 보여줍니다. 매우 얇은 선(예: 카이트 서핑 줄)에서는 실패하는 경향이 있습니다.

## 🧠 Insights & Discussion
*   von Mises-Fisher (vMF) 분포를 사용하여 **하이퍼스피어 공간에서 방향성 딥 특징**을 사용하는 것이 핵심입니다.
*   이 접근 방식은 유클리드 공간 모델(예: A-GAME의 가우시안 혼합)과 비교하여 전경/배경 외형을 견고하게 표현하며 **"차원의 저주" 영향을 완화**합니다.
*   **효율적인 구현:** 전역 방향성 매칭은 병렬 컨볼루션 연산을 사용하여 효율적으로 구현되어 빠른 속도에 기여합니다.
*   **상보적 단서:** 정적 단서(GDMM, 첫 프레임)와 동적으로 업데이트되는 단서(DALM, 이후 프레임)를 통합함으로써 정확하고 빠른 VOS를 가능하게 합니다.
*   **한계:** 매우 얇은 객체(예: 카이트 서핑 줄)의 경우, 컨볼루션 연산 중 특징이 배경에 "묻혀버리고" 방향 기반 임베딩/외형 모델이 이를 정확히 설명하지 못하여 성능이 저하될 수 있습니다.
*   **실용성:** 미세 조정이 필요 없는 방법으로서, DDEAL은 실시간 VOS 응용 분야에 실용적인 솔루션을 제공합니다.

## 📌 TL;DR
*   **문제:** VOS에서 느린 온라인 미세 조정 및 빠르고 정확한 분할을 위한 유클리드 임베딩의 한계.
*   **방법:** DDEAL(방향성 딥 임베딩 및 외형 학습)은 미세 조정이 필요 없는 접근 방식을 제안합니다. 이는 첫 프레임에서 정적 단서를 코사인 유사도를 통해 학습하는 전역 방향성 매칭 모듈과, 하이퍼스피어에서 vMF 분포를 사용하여 동적으로 업데이트되는 단서를 학습하는 방향성 외형 학습 모듈을 활용합니다. 두 모듈 모두 효율성을 위해 방향성 특징을 사용합니다.
*   **결과:** DAVIS 2017 및 YouTube-VOS에서 최신 정확도를 달성하며, 기존 미세 조정 없는 방법들을 크게 능가하고 온라인 미세 조정 의존 방법보다 훨씬 빠릅니다(예: 25-31 fps). "차원의 저주" 문제를 완화하여 우수한 일반화 및 견고한 외형 모델링을 시연합니다.