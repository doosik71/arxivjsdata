# A Large-Scale Benchmark for Food Image Segmentation

Xiongwei Wu, Xin Fu, Ying Liu, Ee-Peng Lim, Steven C.H. Hoi, Qianru Sun

## 🧩 Problem to Solve

음식 이미지 분할은 음식의 칼로리 및 영양소를 추정하는 등 건강 관련 애플리케이션 개발에 필수적인 작업입니다. 기존 음식 이미지 분할 모델은 다음 두 가지 주요 문제로 인해 성능이 저조합니다:

1. **데이터셋 부족**: 세밀한 재료 라벨과 픽셀 단위 위치 마스크를 포함하는 고품질의 대규모 음식 이미지 데이터셋이 부족합니다. 기존 데이터셋은 재료 라벨이 너무 추상적이거나 크기가 작습니다.
2. **음식의 복잡한 외형**: 음식 재료들은 이미지 내에서 서로 겹치거나, 동일한 재료라도 조리법에 따라 시각적으로 매우 다르게 보일 수 있어 재료를 정확히 분할하고 인식하기 어렵습니다.

## ✨ Key Contributions

* **대규모 음식 이미지 분할 벤치마크 데이터셋 구축**: 9,490개의 이미지와 154개의 재료 클래스에 대한 픽셀 단위 마스크가 주석 처리된 FoodSeg103 (및 확장 버전 FoodSeg154)을 구축했습니다.
* **다중 모달리티 사전 훈련 방법론 제안**: 분할 모델에 풍부하고 의미론적인 음식 지식을 명시적으로 부여하는 ReLeM (Recipe Learning Module)을 제안했습니다. 이는 조리법 정보를 언어 임베딩 형태로 활용하여 시각적 표현의 재료 내 분산(intra-variance)을 줄입니다.
* **광범위한 실험 및 벤치마크 제공**: Dilated Convolution, Feature Pyramid, Vision Transformer 기반의 세 가지 인기 있는 시맨틱 분할 방법을 사용하여 ReLeM의 효과를 검증하고, 새로운 데이터셋 FoodSeg103이 기존 객체 이미지 데이터셋보다 훨씬 도전적인 벤치마크임을 입증했습니다.

## 📎 Related Works

* **음식 이미지 데이터셋**:
  * **요리 분류 및 레시피 생성 목적**: ETH Food101 [1], Recipe1M [41], Recipe1M+ [28], UEC Food100 [30], ISIA Food500 [34] 등. 이들 데이터셋은 재료 수준의 마스크를 제공하지 않습니다.
  * **음식 이미지 분할 목적 (한계점)**: UECFoodPix [13] 및 UECFoodPixComplete [35]는 음식 분할을 위한 유일한 공개 데이터셋이지만, 마스크가 요리 단위(dish-level)로만 주석되어 재료 단위 분할에는 사용할 수 없습니다.
* **시맨틱 분할 방법론**:
  * **FCN (Fully Convolutional Network)** [27]: 딥 컨볼루션 신경망 기반의 초기 시맨틱 분할 프레임워크.
  * **Dilated Convolution 기반**: DeepLab [3, 4], CCNet [17] (criss-cross attention을 사용하여 계산 비용 절감).
  * **Feature Pyramid Network (FPN) 기반**: PSPNet [53] (PPM 모듈), FPN [22].
  * **Transformer 기반**: SeTR [54] (Vision Transformer를 시맨틱 분할에 적용).

## 🛠️ Methodology

본 연구의 음식 이미지 분할 프레임워크는 크게 두 가지 모듈로 구성됩니다: 조리법 학습 모듈(ReLeM)과 인코더-디코더 기반의 이미지 분할기(Segmenter).

1. **조리법 학습 모듈 (ReLeM)**
    * **목표**: 조리법에 따른 재료의 시각적 다양성을 줄이고, 분할 모델에 풍부한 음식 지식을 통합합니다.
    * **다중 모달리티 지식 전이**: 조리법 정보를 언어 임베딩 형태로 음식 이미지의 시각적 표현에 통합합니다. 이를 통해 다른 요리에 나타나는 동일 재료의 시각적 표현이 공통 언어 임베딩(재료 라벨 및 조리 지침에서 추출)을 통해 특징 공간에서 "연결"되도록 합니다.
    * **손실 함수**:
        * **코사인 유사도 손실 ($L_{\text{cosine}}$)**: 시각적 특징과 텍스트 임베딩 간의 유사도를 최대화합니다.
        $$ L_{\text{cosine}}((\mathbf{v}, \mathbf{t}), y) = \begin{cases} 1 - \text{cosine}(\mathbf{v}, \mathbf{t}) & y=1 \\ \text{max}(0, \text{cosine}(\mathbf{v}, \mathbf{t}) - \alpha) & y=-1 \end{cases} $$
        여기서 $\mathbf{v}$는 시각적 표현, $\mathbf{t}$는 텍스트 표현, $y$는 동일한 조리법에서 왔는지 여부, $\alpha$는 마진 매개변수입니다.
        * **시맨틱 손실 ($L_{\text{semantic}}$)**: 시각 및 텍스트 표현을 해당 시맨틱 클래스에 매핑하는 Cross-Entropy(CE) 손실입니다.
        $$ L_{\text{semantic}}((\mathbf{v}, \mathbf{t}), u_v, u_t) = \text{CE}(\mathbf{v}, u_v) + \text{CE}(\mathbf{t}, u_t) $$
    * **전처리**:
        * 재료 및 조리 지침 텍스트에서 불필요한 단어 제거.
        * `word2vec` [32]을 사용하여 재료 표현 학습.
        * 긴 조리 지침은 `skip-instructions` [23]를 사용하여 고정 길이 벡터로 인코딩.
    * **텍스트 인코더**: LSTM 기반 인코더와 트랜스포머 기반 인코더 두 가지를 사용합니다.
    * **시각 인코더**: ResNet-50 [15] (CNN 기반)과 ViT-16/B [12] (Vision Transformer 기반)을 사용하며, 이들은 분할기의 인코더를 초기화하는 데 사용됩니다.

2. **이미지 분할 모듈 (Segmenter)**
    * **구조**: 인코더-디코더 아키텍처를 따르며, 인코더는 ReLeM으로 사전 훈련된 가중치로 초기화되고, 디코더는 무작위로 초기화되어 분할 마스크를 학습합니다.
    * **사용된 모델**:
        * **Dilation 기반**: CCNet [17]
        * **FPN 기반**: FPN [22]
        * **Transformer 기반**: SeTR [54]
    * **최적화**: 픽셀 단위 Cross-Entropy 손실을 사용하여 모델을 최적화합니다.

## 📊 Results

* **ReLeM의 효과**: 모든 기본 시맨틱 분할 모델(CCNet, FPN, SeTR)에서 ReLeM을 통합할 때 mIoU(평균 IoU) 성능이 크게 향상되었습니다.
  * CCNet: 35.5% $\rightarrow$ 36.8% (LSTM 기반 ReLeM)
  * FPN: 27.8% $\rightarrow$ 29.1% (LSTM 기반 ReLeM)
  * SeTR: 41.3% $\rightarrow$ 43.9% (LSTM 기반 ReLeM)
  * 특히 SeTR 모델에서 2.6%의 가장 큰 개선을 보였습니다.
* **텍스트 인코더 비교**: LSTM 기반 ReLeM이 트랜스포머 기반 ReLeM보다 모든 모델 구성에서 일관되게 우수한 성능을 보였습니다.
* **FoodSeg103의 도전성**: Cityscapes [8] 데이터셋과 비교했을 때, 모든 기본 모델의 mIoU가 FoodSeg103에서 현저히 낮아(예: CCNet 79.0% vs 35.0%), 음식 이미지 분할이 일반 객체 분할보다 훨씬 어렵다는 것을 보여주었습니다.
* **크로스-도메인 적응성**: FoodSeg154의 아시아 음식 데이터셋을 사용한 교차 도메인 평가에서, ReLeM은 모델 미세 조정을 하지 않은 경우와 한 경우 모두에서 기준 모델보다 지속적으로 뛰어난 성능을 보였습니다 (예: CCNet 28.6% $\rightarrow$ 29.2%, 미세 조정 후 41.3% $\rightarrow$ 47.1%).
* **질적 결과**: ReLeM-CCNet이 일반 CCNet보다 더 정확하고 세밀한 분할 예측을 생성함을 시각적으로 확인했습니다.

## 🧠 Insights & Discussion

* **ReLeM의 중요성**: ReLeM은 조리법 정보(텍스트)를 시각적 표현에 통합함으로써, 조리 방식에 따라 재료의 시각적 외형이 크게 달라지는 음식 이미지 분할의 고유한 어려움을 효과적으로 해결합니다. 이는 모델이 동일한 재료의 다양한 변형을 인식하는 데 도움을 줍니다.
* **음식 이미지 분할의 난이도**: FoodSeg103 데이터셋에 대한 실험 결과는 음식 이미지 분할이 일반적인 객체 시맨틱 분할보다 훨씬 도전적임을 명확히 보여줍니다. 이는 재료의 시각적 유사성, 겹침, 그리고 긴 꼬리 분포(long-tailed distribution) 때문입니다.
* **모델의 일반성**: ReLeM은 CNN 기반(CCNet, FPN) 및 트랜스포머 기반(SeTR) 분할 프레임워크 모두에 효과적으로 적용될 수 있음을 입증하여 그 일반성을 보여주었습니다.
* **현재 ReLeM의 한계 및 향후 과제**: ReLeM은 현재 마지막 특징 맵만을 조리법 학습에 사용하지만, MLA (Multi-Level Aggregation) 디코더와 같이 여러 수준의 특징 맵을 통합하는 경우 성능이 저하될 수 있음을 보였습니다. 향후에는 다양한 수준의 특징 맵을 활용한 다단계 레시피 학습 연구가 필요합니다. 또한, 음식 이미지의 특성을 고려한 "음식 인식 디코더(food-aware decoders)" 설계도 중요한 연구 방향입니다.

## 📌 TL;DR

본 연구는 고품질의 세밀한 재료 수준 주석이 부족하고 재료의 복잡한 외형으로 인해 어려움을 겪는 음식 이미지 분할 문제를 해결합니다. 이를 위해 9,490개 이미지와 154개 재료 클래스에 대한 60K개 이상의 픽셀 단위 마스크를 포함하는 대규모 벤치마크 데이터셋인 FoodSeg103 (및 FoodSeg154)을 구축했습니다. 또한, 조리법 정보를 활용하여 시각적 특징의 재료 내 분산(intra-variance)을 줄이는 다중 모달리티 사전 훈련 방법인 ReLeM을 제안했습니다. 실험 결과, ReLeM은 기존 CNN 기반 및 트랜스포머 기반 시맨틱 분할 모델의 성능을 일관되게 향상시켰으며, FoodSeg103이 기존 객체 분할 데이터셋보다 훨씬 도전적인 벤치마크임을 입증하여 향후 세밀한 음식 이미지 이해 연구를 위한 기반을 마련했습니다.
