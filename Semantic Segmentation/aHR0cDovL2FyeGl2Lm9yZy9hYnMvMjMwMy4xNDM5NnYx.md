# IFSeg: Image-free Semantic Segmentation via Vision-Language Model

Sukmin Yun, Seong Hyeon Park, Paul Hongsuck Seo, Jinwoo Shin

## 🧩 Problem to Solve

본 논문은 기존 비전-언어 (VL) 모델 기반의 이미지 분할(segmentation) 연구들이 여전히 추가적인 훈련 이미지나 분할 주석(annotation)을 필요로 하는 한계를 지적합니다. 특히 새로운 웹 이미지와 같이 작업별 훈련 데이터가 쉽게 사용 가능하지 않은 시나리오에서 이러한 제약은 큰 부담이 됩니다. 이에 본 논문은 **이미지 없이(image-free)**, 즉 대상 의미론적 카테고리 집합만 주어지고 어떠한 작업별 이미지나 주석 없이 의미론적 분할을 수행하는 새로운 과제를 제시합니다.

## ✨ Key Contributions

* **새로운 이미지 없는 의미론적 분할(Image-Free Semantic Segmentation) 과제 도입**: 대상 의미론적 카테고리만 주어지고 어떠한 이미지나 주석 없이 분할을 수행하는 도전적인 과제를 제시합니다.
* **IFSeg 방법론 제안**: 사전 훈련된 VL 모델을 분할 작업에 적응시키기 위해 VL 모델 기반의 인공 이미지-분할 쌍을 생성하여 활용하는 간단하면서도 효과적인 자기 지도(self-supervised) 프레임워크인 IFSeg를 제안합니다.
* **혁신적인 인공 훈련 데이터 생성**: 사전 훈련된 VL 모델의 공통 임베딩 공간에서 시각 토큰과 텍스트 토큰이 의미를 공유하며 가깝게 위치한다는 점에 착안하여, 의미론적 카테고리의 단어 토큰을 사용하여 인공 이미지를 생성합니다.
* **우수한 성능 달성**: 이미지 없는 학습 환경임에도 불구하고, MaskCLIP+와 같이 118k개의 훈련 이미지를 사용하는 강력한 지도(stronger supervision) 기반의 기존 방법들보다 뛰어난 분할 성능을 달성하여 새로운 과제의 효과적인 기준선을 제시합니다.
* **VL 모델의 광범위한 적용 가능성 강조**: 최근 VL 모델의 잠재력을 이미지 분할 영역으로 확장하여 데이터셋 없이 분할 작업을 수행하는 새로운 연구 방향을 제시합니다.

## 📎 Related Works

* **비전-언어 사전 훈련(Vision-language pre-training)**: CLIP, OFA 등 대규모 이미지-텍스트 데이터로 사전 훈련된 모델들이 제로샷(zero-shot) 및 퓨샷(few-shot) 적응에서 성공적인 결과를 보였습니다. 특히 듀얼 인코더(dual encoders), 멀티모달 인코더(multi-modal encoder), 인코더-디코더(encoder-decoder) 구조들이 탐구되었습니다.
* **전이 가능한 이미지 분할(Transferable image segmentation)**:
  * **비지도 분할(Unsupervised segmentation)**: 이미지의 밀집된(dense) 표현을 클러스터링하고 헝가리안 매칭(Hungarian-matching) 알고리즘을 통해 분할 카테고리를 매칭하는 방식 (IIC, PiCIE+H., TransFGU, STEGO).
  * **VL 기반 비지도 분할**: CLIP의 텍스트 인코더를 활용하여 매칭 프로세스를 대체하여 효율성과 전이 가능성을 높이는 방식 (MaskCLIP, MaskCLIP+).
  * **제로샷 분할(Zero-shot segmentation)**: word2vec, fast-text와 같은 학습된 단어 임베딩을 사용하거나, CLIP과 ALIGN과 같은 VL 모델을 활용하는 방식 (LSeg+, ZSSeg, OpenSeg). 이러한 방법들은 종종 클래스 불가지론적(class-agnostic) 분할 마스크나 클래스별 분할 주석을 필요로 합니다.
  * 본 논문은 이러한 기존 방법들이 필요로 하는 이미지 데이터나 주석 없이 오직 분할 어휘만으로 동작하는 이미지 없는 의미론적 분할이라는 더 현실적인 시나리오를 탐구합니다.

## 🛠️ Methodology

IFSeg는 사전 훈련된 인코더-디코더 VL 모델을 활용하여 이미지 없이 자기 지도 방식으로 의미론적 분할을 수행합니다.

1. **VL 인코더-디코더 아키텍처**:
    * **데이터 형식**: 원본 이미지 $X_I$와 텍스트 $X_T$를 토큰화하여 시퀀스 데이터로 변환합니다. 이미지 백본 $f_{img}$은 이미지를 $H \times W \times C$ 피처 맵으로 변환하고, 이를 평탄화(flatten)하여 개념적 이미지 토큰 $e_I \in \mathbb{R}^{L_I \times D}$를 생성합니다. 텍스트 토큰 $e_T \in \mathbb{R}^{L_T \times D}$와 결합하여 $e_x = [e_I; e_T]$를 구성합니다.
    * **모델 구조**: 트랜스포머 인코더 $f_{enc}$는 $e_x$의 문맥화된 임베딩을 생성하고, 디코더 $f_{dec}$는 이 임베딩에 기반하여 순차적으로 출력을 예측합니다. 최종적으로 임베딩 행렬 $E$를 통해 어휘사전 $V$에 대한 로짓(logit)을 생성합니다.

2. **인코더-디코더를 통한 의미론적 분할**:
    * **작업 공식화**: $M$개의 의미론적 카테고리에 대해 이미지의 각 밀집 영역에 대한 카테고리 단어를 디코딩하는 방식으로 분할을 공식화합니다. 의미론적 카테고리는 서브워드 토큰으로 나뉠 수 있으므로, 각 카테고리를 하나의 고유한 단어 $v' \in V_{seg}$로 처리하며, 서브워드 토큰의 임베딩을 평균하여 사용합니다.
    * **공간 조건부 디코더 출력**: 인코더의 이전 인덱스 출력($f^{(i-1)}_{enc}(e_x)$)을 디코더 입력으로 사용하여 이미지 토큰 $x_I^{(i)}$에 공간적으로 조건화된 출력을 생성합니다.
    * **확률 및 예측**: 디코더 출력에 로짓을 계산하고, $V_{seg}$에 없는 단어를 마스킹하여 $M$개 카테고리에 대한 정규화된 확률 $p \in \mathbb{R}^{L_I \times M}$를 얻습니다. 이를 bilinear interpolation을 통해 원래 이미지 크기로 업샘플링하여 최종 픽셀별 예측 $P(y^{(i)}|x)$를 얻습니다. 손실 함수는 음의 로그 우도(negative log-likelihood) $L_{seg}(x, y_{gt})$를 최소화합니다.
    * **프롬프트 설계**: 텍스트 토큰 $x_T$는 "what is the segmentation of the image? object: giraffe, grass"와 같은 프롬프트 형태로 제공됩니다.

3. **이미지 없는 의미론적 분할 (IFSeg)**:
    * **핵심 아이디어**: VL 사전 훈련 과정에서 실제 이미지 토큰과 해당 의미론적 카테고리 단어 토큰이 공유된 문맥화된 임베딩 공간에서 서로 가깝게 위치하게 학습된다는 점을 활용합니다. 즉, 단어 토큰이 인공 이미지 역할을 할 수 있다는 가정입니다.
    * **인공 이미지 토큰 구성**: $M$개의 고유한 카테고리 단어 집합 $V_{seg}$에서 $U \times V$ 수의 단어를 무작위로 샘플링하여 그리드 맵 $\tilde{v}_{IFSeg}$를 구성합니다. 이 그리드를 이미지 백본의 공간 해상도($H \times W$)로 Nearest Neighbor interpolation을 통해 업스케일링하여 $v_{IFSeg}$를 얻습니다.
    * **훈련**: 생성된 인공 이미지 토큰 $v_{IFSeg}$를 실제 이미지 토큰 대신 사용하여 모델을 훈련하며, $v_{IFSeg}$ 자체를 정답 분할 마스크로 사용합니다 ($y_{gt} = x_I$). 이 과정에서 이미지 백본 $f_{img}$은 고정됩니다.
    * **후처리(Post-processing)**: 훈련과 평가 간의 입력 모달리티 불일치(modality discrepancy)를 해결하기 위해, 이미지 백본 $f_{img}$의 출력을 기반으로 K-최근접 이웃(K-nearest neighbors)의 출력 확률을 평균하는 방식을 적용하여 분할 품질을 크게 향상시킵니다. 이는 실제 이미지의 객체 모양이나 레이블 일관성과 같은 이미지 고유의 사전 지식(priors) 학습의 한계를 보완합니다.

## 📊 Results

* **제로샷 이미지 분할 (COCO Stuff)**: IFSeg는 이미지 없는 시나리오에서 기존 이미지 없는 기준선(MaskCLIP)보다 **+30.8 mIoU**P를, 심지어 118k 훈련 이미지를 사용하는 MaskCLIP+보다 **+6.9 mIoU**P 높은 성능을 달성했습니다.
* **교차 데이터셋 전이 (COCO → ADE20K)**: IFSeg는 이미지 없는 훈련 방식에도 불구하고, 118k 훈련 이미지를 사용하는 OpenSeg보다 **+1.5 mIoU**P 높은 성능을 보였으며, 이미지 없는 모든 기준선보다 우수했습니다.
* **비지도 이미지 분할 (COCO → COCO)**: IFSeg는 기존 비지도 분할 기준선 및 MaskCLIP보다 지속적으로 높은 성능을 달성했습니다 (예: MaskCLIP보다 **+4.2 mIoU**P).
* **자기 지도 학습(Self-training) 효과**: 훈련 이미지와 의사 레이블(pseudo-labels)을 추가로 사용하여 자기 지도 학습을 수행했을 때, IFSeg의 성능은 COCO Stuff의 미분류 카테고리에서 55.6 mIoU에서 **61.6 mIoU**로 크게 향상되었습니다.
* **지도 학습(Supervised learning) 성능**: ADE20K 벤치마크에서 기존 지도 분할 방법론(DenseCLIP)보다 **+2.0 mIoU**P 높은 47.1 mIoU를 달성하여 강력한 지도 학습 시나리오에서도 우수함을 입증했습니다.
* **정성적 결과**: MaskCLIP보다 더 세분화된(fine-grained) 카테고리를 분할하는 능력을 보여주었습니다 (예: `accessory` 카테고리).

## 🧠 Insights & Discussion

* 본 연구는 의미론적 분할을 위한 **새로운 '데이터셋 없는' 접근 방식**을 탐구하며, 사전 훈련된 VL 모델의 잠재력을 최대한 활용할 수 있음을 보여줍니다.
* 의미론적 카테고리 단어를 인공 이미지 토큰으로 활용하는 아이디어는 VL 모델의 **크로스-모달(cross-modal) 임베딩 공간의 정렬(alignment)** 특성을 효과적으로 활용합니다.
* 훈련 시 실제 이미지가 없기 때문에 발생하는 **모달리티 불일치(modality discrepancy)**는 후처리 기법(예: 이미지 피처 기반 이웃 평균)을 통해 효과적으로 완화될 수 있음을 보여주었습니다. 이는 실제 이미지의 객체 모양이나 레이블 일관성과 같은 이미지 고유의 사전 지식(priors) 학습의 한계를 보완합니다.
* IFSeg는 기존 방법들이 강력한 지도(즉, 수많은 이미지와 주석)에 의존하는 것과 달리, **훨씬 약한 지도(오직 카테고리 단어)만으로도 경쟁력 있는 성능**을 달성할 수 있음을 입증하여, 데이터 수집 및 주석 작업의 비용을 크게 줄일 수 있는 가능성을 제시합니다.
* 본 연구는 컴퓨터 비전 분야에서 **이미지 없는 분할**이라는 도전적이지만 잠재적으로 중요한 문제를 새로이 도입하며, 향후 연구 방향에 영감을 줄 것으로 기대됩니다.

## 📌 TL;DR

**문제**: 작업별 이미지나 주석 없이 의미론적 분할을 수행하는 새로운 '이미지 없는' 과제를 해결합니다.
**제안 방법**: IFSeg는 의미론적 카테고리 단어를 사용하여 인공 이미지-분할 쌍을 생성하고, 이를 통해 사전 훈련된 비전-언어 모델을 자기 지도 방식으로 훈련하여 분할 작업을 수행합니다.
**주요 발견**: IFSeg는 이미지 없는 환경에서 효과적인 기준선을 수립했을 뿐만 아니라, 추가 훈련 이미지를 사용하는 기존의 강력한 지도 학습 기반 방법들보다 뛰어난 성능을 달성하여, 데이터 수집 비용을 크게 줄일 수 있는 잠재력을 보여줍니다.
