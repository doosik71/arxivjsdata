{
  "url": "http://arxiv.org/abs/2312.00863v1",
  "title": "EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment\n  Anything",
  "authors": "Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra",
  "year": 2023,
  "abstract": "Segment Anything Model (SAM) has emerged as a powerful tool for numerous\nvision applications. A key component that drives the impressive performance for\nzero-shot transfer and high versatility is a super large Transformer model\ntrained on the extensive high-quality SA-1B dataset. While beneficial, the huge\ncomputation cost of SAM model has limited its applications to wider real-world\napplications. To address this limitation, we propose EfficientSAMs,\nlight-weight SAM models that exhibits decent performance with largely reduced\ncomplexity. Our idea is based on leveraging masked image pretraining, SAMI,\nwhich learns to reconstruct features from SAM image encoder for effective\nvisual representation learning. Further, we take SAMI-pretrained light-weight\nimage encoders and mask decoder to build EfficientSAMs, and finetune the models\non SA-1B for segment anything task. We perform evaluations on multiple vision\ntasks including image classification, object detection, instance segmentation,\nand semantic object detection, and find that our proposed pretraining method,\nSAMI, consistently outperforms other masked image pretraining methods. On\nsegment anything task such as zero-shot instance segmentation, our\nEfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably\nwith a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models."
}