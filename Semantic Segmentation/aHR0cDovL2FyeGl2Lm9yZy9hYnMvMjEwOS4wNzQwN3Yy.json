{
  "url": "http://arxiv.org/abs/2109.07407v2",
  "title": "Semi-supervised Contrastive Learning for Label-efficient Medical Image\n  Segmentation",
  "authors": "Xinrong Hu, Dewen Zeng, Xiaowei Xu, Yiyu Shi",
  "year": 2021,
  "abstract": "The success of deep learning methods in medical image segmentation tasks\nheavily depends on a large amount of labeled data to supervise the training. On\nthe other hand, the annotation of biomedical images requires domain knowledge\nand can be laborious. Recently, contrastive learning has demonstrated great\npotential in learning latent representation of images even without any label.\nExisting works have explored its application to biomedical image segmentation\nwhere only a small portion of data is labeled, through a pre-training phase\nbased on self-supervised contrastive learning without using any labels followed\nby a supervised fine-tuning phase on the labeled portion of data only. In this\npaper, we establish that by including the limited label in formation in the\npre-training phase, it is possible to boost the performance of contrastive\nlearning. We propose a supervised local contrastive loss that leverages limited\npixel-wise annotation to force pixels with the same label to gather around in\nthe embedding space. Such loss needs pixel-wise computation which can be\nexpensive for large images, and we further propose two strategies, downsampling\nand block division, to address the issue. We evaluate our methods on two public\nbiomedical image datasets of different modalities. With different amounts of\nlabeled data, our methods consistently outperform the state-of-the-art\ncontrast-based methods and other semi-supervised learning techniques.",
  "citation": 127
}