# Anchor Diffusion for Unsupervised Video Object Segmentation
Zhao Yang, Qiang Wang, Luca Bertinetto, Weiming Hu, Song Bai, Philip H.S. Torr

## 🧩 Problem to Solve
비지도 비디오 객체 분할(Unsupervised Video Object Segmentation, VOS) 분야에서 기존의 순환 신경망(RNN) 및 광학 흐름(optical flow) 기반 방법들은 다음과 같은 한계를 가집니다:
-   **오류 누적(Drift):** 짧은 시간 의존성을 선호하여 시간이 지남에 따라 정확도가 누적되어 저하됩니다.
-   **장기 의존성 모델링의 어려움:** 긴 시간 범위에 걸친 객체 변화에 적응하기 어렵습니다.
-   **정지된 객체 처리 문제:** 광학 흐름 기반 모델은 정지 상태의 전경 객체에 대해 부정확한 흐름 필드를 생성할 수 있습니다.
-   **복잡성과 비효율성:** 복잡한 구조와 온라인 미세조정(fine-tuning) 필요성으로 인해 추론 속도가 느려질 수 있습니다.

이러한 문제들로 인해 단순한 정적 이미지 분할 모델조차 비지도 VOS 방법들과 경쟁할 만한 성능을 보여주므로, 시간적 의존성을 모델링하는 새로운 접근 방식이 필요합니다.

## ✨ Key Contributions
-   **새로운 모델 아키텍처 제안:** RNN이나 광학 흐름 없이 장기적인 시간 의존성(long-term temporal dependencies)을 모델링하는 간단하고 효과적인 `앵커 확산 네트워크(Anchor Diffusion Network, AD-Net)`를 제안했습니다.
-   **앵커 확산 메커니즘 도입:** 참조 "앵커" 프레임과 현재 프레임 간의 픽셀 임베딩(pixel embeddings) 사이에 조밀한 대응 관계(dense correspondences)를 설정하는 `앵커 확산(Anchor Diffusion)` 기법을 도입했습니다. 이는 중간 프레임에 의존하지 않고 임의의 긴 거리에 걸쳐 쌍별 의존성(pairwise dependencies)을 학습할 수 있게 합니다.
-   **최고 성능 달성:** DAVIS-2016 비지도 VOS 벤치마크에서 81.7%의 평균 IoU(Intersection-over-Union)로 모든 비지도 방법 중 1위를 차지했으며, 최신 온라인 반지도(semi-supervised) 방법들과도 경쟁력 있는 성능을 보여주었습니다.
-   **방법론의 일반성 입증:** FBMS 및 ViSal 데이터셋에서도 최첨단 성능을 달성하여 제안된 방법의 견고함과 일반성(generality)을 입증했습니다.
-   **전경 신호 강화 및 배경 억제:** 제안된 `앵커 확산` 과정이 전경 신호를 크게 강화하고 배경을 약화시켜 정확한 분할을 가능하게 함을 정성적 및 정량적으로 분석하여 보여주었습니다.

## 📎 Related Works
-   **반지도 VOS (Semi-supervised VOS):** 첫 프레임에 마스크가 제공되어 이를 기반으로 온라인 미세조정(e.g., PReMVOS, OSVOS) 또는 더 빠른 온라인 추론(e.g., Pixel-wise metric learning [7], Object part tracking [8])을 수행합니다.
-   **비지도 VOS (Unsupervised VOS):**
    -   **광학 흐름 기반:** 전경 움직임을 예측하거나(MP-Net [58]), 외형(appearance) 기반 특징과 광학 흐름(optical flow) 기반 특징을 통합하여 "투-스트림 모델(two-stream model)"을 구성합니다(LVO [59], SegFlow [9], MotAdapt [53], MBN [34]). 단기 의존성, 합성 데이터 훈련으로 인한 도메인 갭, 그리고 정지 객체 처리의 한계가 있습니다.
    -   **RNN 기반:** Convolutional LSTM(e.g., PDB [54])을 사용하여 시간적 의존성을 모델링합니다. 그러나 긴 시퀀스 모델링의 한계와 기울기 소실/폭발(exploding/vanishing gradients) 문제가 있습니다.
    -   **픽셀 유사성 기반:** 특정 시드 픽셀(seed pixels)과 다른 픽셀 간의 유사성을 학습하여 분할합니다(Fathi et al. [14], IET [33], MBN [34]). 시드 선택, 랭킹, 분류 절차의 복잡성과 다중 스코어(움직임 현저성, 객체성) 조합의 어려움이 있습니다.
-   **본 논문의 차별점:** 기존의 픽셀 유사성 기반 접근법보다 훨씬 간결하며, 여러 구성 요소를 따로 학습하지 않고 단일 네트워크에서 유사성 학습, 특징 전파 및 이진 분할을 수행합니다. 특히, Wang et al. [70]의 비지역(non-local) 연산자를 활용하여 장거리 공간 정보를 효과적으로 통합합니다.

## 🛠️ Methodology
AD-Net(Anchor Diffusion Network)은 단일 모델로 정확한 개별 프레임 분할과 비디오 전체에 걸친 전경 객체의 외형 변화 적응이라는 두 가지 목표를 달성합니다.

-   **특징 인코더:** Fully-convolutional DeepLabv3 [5]를 특징 인코더로 사용하며, ResNet101 [21] 백본은 ImageNet으로 사전 학습된 가중치로 초기화됩니다. 최종 레이어의 출력 채널 수는 $c=128$로 설정됩니다.
-   **파이프라인:** 모델의 입력은 고정된 앵커 프레임 $I_0$와 현재 프레임 $I_t$ 쌍입니다.
    1.  **특징 인코딩:** $I_0$와 $I_t$는 특징 인코더를 통해 각각 픽셀 임베딩 $X_0 \in \mathbb{R}^{\text{hw} \times \text{c}}$와 $X_t \in \mathbb{R}^{\text{hw} \times \text{c}}$로 인코딩됩니다.
    2.  **병렬 브랜치:** $X_t$는 다음 세 가지 병렬 브랜치로 입력됩니다.
        -   **스킵 연결(Skip Connection):** $X_t$에 항등 매핑(identity mapping)을 적용합니다.
        -   **프레임 내 브랜치(Intra-frame Branch):** $X_t$에 비지역 연산(non-local operation)을 적용하여 현재 프레임 내의 장거리 공간 정보를 활용하여 분할 정확도를 높입니다.
        -   **앵커 확산 브랜치(Anchor-diffusion Branch):** $X_0$와 $X_t$를 사용하여 조밀한 대응 관계를 계산합니다.
            -   **전이 행렬(Transition Matrix) $P$ 계산:** $X_0$와 $X_t$의 각 픽셀 임베딩 쌍 간의 유사성($X_0 X_t^T$)을 계산하고, 이를 $z = \sqrt{c}$로 스케일링한 후 $\text{softmax}$ 함수를 통해 정규화하여 전이 행렬 $P$를 얻습니다.
            $$ P = \text{softmax}\left(\frac{1}{z} X_0 X_t^T\right) $$
            -   **특징 확산:** $P$를 사용하여 $X_t$를 새로운 인코딩 $\tilde{X}_t$로 매핑합니다. 이 과정은 앵커 프레임과의 유사성에 따라 픽셀 임베딩에 가중치를 부여하여 전경 신호를 강화하고 배경을 약화시킵니다.
            $$ \tilde{X}_t = P X_t $$
    3.  **결과 연결 및 분류:** 세 브랜치에서 나온 특징들을 채널 차원을 따라 연결(concatenate)한 후, 1x1 컨볼루션 레이어를 거쳐 분류 레이어로 전달되어 최종 분할 마스크를 생성합니다.
-   **훈련:** 전체 네트워크는 이진 교차 엔트로피 손실(binary cross-entropy loss)로 종단 간(end-to-end) 훈련됩니다. 훈련 시 첫 프레임을 앵커로, 비디오에서 무작위로 샘플링된 프레임을 현재 프레임으로 사용합니다. 데이터 증강(무작위 크기 자르기, 무작위 회전)이 적용됩니다.
-   **추론:** 테스트 시 앵커 프레임의 특징은 한 번만 계산되어 비디오 전체에서 재사용됩니다. 최종 성능 향상을 위해 멀티 스케일(0.75, 1.00, 1.50) 및 수평 뒤집기(horizontal flip) 입력이 활용됩니다.
-   **인스턴스 가지치기(Instance Pruning):** DAVIS-2016 데이터셋의 다중 객체 문제에 대응하기 위한 선택적 후처리 단계입니다. 사전 학습된 객체 탐지 모델로 객체 궤적을 추적하여 작은 객체나 비디오 일부에만 존재하는 "비-전경(non-foreground)" 객체를 식별하고 필터링합니다.

## 📊 Results
-   **DAVIS-2016 벤치마크:**
    -   **비지도 VOS 최고 성능:** 인스턴스 가지치기 후, 평균 IoU (J) 81.7%, 윤곽 정확도 (F) 80.5%를 달성하여 DAVIS-2016의 모든 비지도 방법 중 최고 성능을 기록했습니다. 이는 2위 MotAdapt [53]보다 J에서 4.5%p, F에서 3.1%p 높은 수치입니다.
    -   인스턴스 가지치기 없이도 J 79.4%, F 78.2%를 기록하여 기존 최첨단 비지도 방법들을 능가했습니다.
    -   일부 반지도(semi-supervised) 방법들보다도 우수한 성능을 보여주었습니다.
-   **FBMS 및 ViSal 데이터셋:**
    -   FBMS 데이터셋에서 F-measure 81.2%를 달성하며 최첨단 성능을 기록했습니다.
    -   ViSal 비디오 현저성(saliency) 데이터셋에서도 MAE 0.030, F-measure 90.4%로 매우 경쟁력 있는 결과를 보여주었습니다.
-   **어블레이션 스터디(Ablation Studies):**
    -   `앵커 확산` 브랜치가 기준선 DeepLabv3 대비 J에서 2.02%p의 가장 큰 성능 향상을 가져왔습니다.
    -   프레임 내 브랜치와 앵커 확산 브랜치를 모두 포함하는 AD-Net 최종 모델이 모든 구성 요소를 통합하여 가장 높은 성능(J 78.26%)을 달성했습니다.
-   **시간적 일관성:** AD-Net은 시간이 지남에 따라 전경 픽셀 임베딩의 코사인 거리가 DeepLabv3 기준선보다 훨씬 안정적으로 유지됨을 보여주어 장기적인 시간 일관성이 뛰어남을 입증했습니다 (그림 3).
-   **추론 속도:** 인스턴스 가지치기 없이 NVIDIA TITAN X GPU에서 854x480 해상도 기준으로 초당 4프레임(4 FPS)으로 온라인 작동이 가능합니다.

## 🧠 Insights & Discussion
-   **단순함의 효율성:** 기존의 복잡한 RNN이나 광학 흐름 기반 모델이 가지고 있던 오류 누적, 장기 의존성 모델링의 한계, 정지 객체 처리 문제 등을 단순한 `앵커 확산` 메커니즘으로 효과적으로 해결했습니다. 이는 비지도 VOS에서 광학 흐름이 필수적인 구성 요소가 아닐 수도 있음을 시사합니다.
-   **장기 의존성 모델링:** 앵커 프레임과의 직접적인 픽셀 임베딩 유사성 학습 및 확산을 통해, 중간 프레임에 대한 의존성 없이 임의로 긴 시간 거리에 걸쳐 정보 전파 및 일관성 유지가 가능함을 입증했습니다. 이는 시간에 따른 전경 임베딩의 안정성으로 나타났습니다.
-   **전경-배경 분리 강화:** 제안된 확산 과정은 앵커 프레임과의 강한 유사성을 보이는 픽셀 신호(대부분 전경 객체)를 강화하고, 유사성이 약한 픽셀 신호(대부분 배경)를 약화시킴으로써 전경-배경 분할을 효과적으로 수행합니다.
-   **경쟁력 있는 성능:** 비지도 학습임에도 불구하고 많은 반지도 방법론을 능가하는 성능은, 첫 프레임 마스크에 기반한 복잡한 온라인 미세조정 없이도 높은 정확도를 달성할 수 있다는 것을 보여줍니다.
-   **한계 및 보완:** DAVIS-2016 데이터셋의 특성상 다중 객체가 존재하는 경우 인스턴스 개념이 부족한 semantic segmentation 기반 모델의 한계를 보완하기 위해 `인스턴스 가지치기`라는 후처리 단계를 사용했습니다. 이는 모델 자체의 한계라기보다는 데이터셋 특정적 보완책으로 볼 수 있습니다.

## 📌 TL;DR
-   **문제:** 기존 비지도 VOS 방법은 RNN이나 광학 흐름을 사용하여 시간에 따른 오류 누적과 장기적인 시간 의존성 모델링에 취약했습니다.
-   **방법:** `AD-Net`은 "앵커 확산"이라는 단순하면서도 효과적인 방법을 제안합니다. 이 방법은 참조 앵커 프레임과 현재 프레임 간의 픽셀 임베딩 유사성을 학습하고 이를 확산하여 전경 신호를 강화하고 배경을 억제합니다. 또한, 프레임 내 비지역 연산을 결합하여 장거리 의존성을 효율적으로 모델링합니다.
-   **결과:** AD-Net은 DAVIS-2016 비지도 VOS 벤치마크에서 81.7%의 IoU로 최고 성능을 달성했으며, FBMS 및 ViSal 데이터셋에서도 뛰어난 결과를 보였습니다. 이는 복잡한 기존 방법론의 한계를 뛰어넘어 비지도 비디오 객체 분할에서 새로운 최첨단 성능을 제시합니다.