{
  "url": "http://arxiv.org/abs/2311.11969v1",
  "title": "SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20\n  Million masks",
  "authors": "Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Min Zhu, Shaoting Zhang, Junjun He, Yu Qiao",
  "year": 2023,
  "abstract": "Segment Anything Model (SAM) has achieved impressive results for natural\nimage segmentation with input prompts such as points and bounding boxes. Its\nsuccess largely owes to massive labeled training data. However, directly\napplying SAM to medical image segmentation cannot perform well because SAM\nlacks medical knowledge -- it does not use medical images for training. To\nincorporate medical knowledge into SAM, we introduce SA-Med2D-20M, a\nlarge-scale segmentation dataset of 2D medical images built upon numerous\npublic and private datasets. It consists of 4.6 million 2D medical images and\n19.7 million corresponding masks, covering almost the whole body and showing\nsignificant diversity. This paper describes all the datasets collected in\nSA-Med2D-20M and details how to process these datasets. Furthermore,\ncomprehensive statistics of SA-Med2D-20M are presented to facilitate the better\nuse of our dataset, which can help the researchers build medical vision\nfoundation models or apply their models to downstream medical applications. We\nhope that the large scale and diversity of SA-Med2D-20M can be leveraged to\ndevelop medical artificial intelligence for enhancing diagnosis, medical image\nanalysis, knowledge sharing, and education. The data with the redistribution\nlicense is publicly available at https://github.com/OpenGVLab/SAM-Med2D.",
  "citation": 58
}