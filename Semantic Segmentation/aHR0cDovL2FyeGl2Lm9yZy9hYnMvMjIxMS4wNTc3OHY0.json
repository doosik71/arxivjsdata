{
  "url": "http://arxiv.org/abs/2211.05778v4",
  "title": "InternImage: Exploring Large-Scale Vision Foundation Models with\n  Deformable Convolutions",
  "authors": "Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao",
  "year": 2022,
  "abstract": "Compared to the great progress of large-scale vision transformers (ViTs) in\nrecent years, large-scale models based on convolutional neural networks (CNNs)\nare still in an early state. This work presents a new large-scale CNN-based\nfoundation model, termed InternImage, which can obtain the gain from increasing\nparameters and training data like ViTs. Different from the recent CNNs that\nfocus on large dense kernels, InternImage takes deformable convolution as the\ncore operator, so that our model not only has the large effective receptive\nfield required for downstream tasks such as detection and segmentation, but\nalso has the adaptive spatial aggregation conditioned by input and task\ninformation. As a result, the proposed InternImage reduces the strict inductive\nbias of traditional CNNs and makes it possible to learn stronger and more\nrobust patterns with large-scale parameters from massive data like ViTs. The\neffectiveness of our model is proven on challenging benchmarks including\nImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved\na new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming\ncurrent leading CNNs and ViTs. The code will be released at\nhttps://github.com/OpenGVLab/InternImage.",
  "citation": 1351
}