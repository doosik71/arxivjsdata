{
  "title": "Pixel-wise object tracking",
  "authors": "Yilin Song, Chenge Li, Yao Wang",
  "year": 2017,
  "url": "http://arxiv.org/abs/1711.07377v2",
  "abstract": "In this paper, we propose a novel pixel-wise visual object tracking framework\nthat can track any anonymous object in a noisy background. The framework\nconsists of two submodels, a global attention model and a local segmentation\nmodel. The global model generates a region of interests (ROI) that the object\nmay lie in the new frame based on the past object segmentation maps, while the\nlocal model segments the new image in the ROI. Each model uses a LSTM structure\nto model the temporal dynamics of the motion and appearance, respectively. To\ncircumvent the dependency of the training data between the two models, we use\nan iterative update strategy. Once the models are trained, there is no need to\nrefine them to track specific objects, making our method efficient compared to\nonline learning approaches. We demonstrate our real time pixel-wise object\ntracking framework on a challenging VOT dataset"
}