{
  "url": "http://arxiv.org/abs/2201.10737v5",
  "title": "Class-Aware Adversarial Transformers for Medical Image Segmentation",
  "authors": "Chenyu You, Ruihan Zhao, Fenglin Liu, Siyuan Dong, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S. Duncan",
  "year": 2022,
  "abstract": "Transformers have made remarkable progress towards modeling long-range\ndependencies within the medical image analysis domain. However, current\ntransformer-based models suffer from several disadvantages: (1) existing\nmethods fail to capture the important features of the images due to the naive\ntokenization scheme; (2) the models suffer from information loss because they\nonly consider single-scale feature representations; and (3) the segmentation\nlabel maps generated by the models are not accurate enough without considering\nrich semantic contexts and anatomical textures. In this work, we present\nCASTformer, a novel type of adversarial transformers, for 2D medical image\nsegmentation. First, we take advantage of the pyramid structure to construct\nmulti-scale representations and handle multi-scale variations. We then design a\nnovel class-aware transformer module to better learn the discriminative regions\nof objects with semantic structures. Lastly, we utilize an adversarial training\nstrategy that boosts segmentation accuracy and correspondingly allows a\ntransformer-based discriminator to capture high-level semantically correlated\ncontents and low-level anatomical features. Our experiments demonstrate that\nCASTformer dramatically outperforms previous state-of-the-art transformer-based\napproaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in\nDice over previous models. Further qualitative experiments provide a more\ndetailed picture of the model's inner workings, shed light on the challenges in\nimproved transparency, and demonstrate that transfer learning can greatly\nimprove performance and reduce the size of medical image datasets in training,\nmaking CASTformer a strong starting point for downstream medical image analysis\ntasks.",
  "citation": 216
}