# 2018 Robotic Scene Segmentation Challenge

M. Allan, S. Kondo, S. Bodenstedt, S. Leger, R. Kadkhodamohammadi, I. Luengo, F. Fuentes, E. Flouty, A. Mohammed, M. Pedersen, A. Kori, V. Alex, G. Krishnamurthi, D. Rauber, R. Mendel, C. Palm, S. Bano, G. Saibro, C. S. Shih, H. A. Chiang, J. Zhuang, J. Yang, V. Iglovikov, A. Dobrenkii, X. Liu, C. Gao, M. Unberath, M. Reddiboina, A. Reddy, M. Kim, C. Kim, C. Kim, H. Kim, G. Lee, I. Ullah, M. Luna, S. H. Park, M. Azizian, D. Stoyanov, L. Maier-Hein, S. Speidel

## 🧩 Problem to Solve

로봇 보조 최소 침습 수술(MIS) 분야에서 외과의사의 인식을 향상시키기 위해 내시경 영상에서 수술 도구와 해부학적 구조를 픽셀 단위로 정확하게 분할하는 것이 중요합니다. 이는 사전/수술 중 의료 영상과 내시경 시야를 융합하여 외과의사의 시야를 명확하게 하고 핵심 정보를 지능적으로 제공하는 데 필수적입니다. 그러나 의료 영상, 특히 수술 영상의 경우 고품질의 레이블링된 데이터가 부족하여 딥러닝 기반의 최신 컴퓨터 비전 기술을 효과적으로 적용하고 평가하는 데 어려움이 있습니다. 본 챌린지는 이러한 데이터 부족 문제를 해결하고 실제 수술 환경에 적합한 영상 분할 기술을 평가하는 것을 목표로 합니다.

## ✨ Key Contributions

* **챌린지 확장:** 2017년 로봇 도구 분할 챌린지에서 더 나아가, 다빈치(da Vinci) 기기 외에 다양한 의료 기기(초음파 프로브, 봉합 바늘, 봉합사, 흡인-세척 기기, 수술 클립)와 해부학적 구조(신장 실질, 덮인 신장, 소장)를 포함한 훨씬 더 복잡한 장면 분할 과제를 제시했습니다.
* **새로운 데이터셋 구축:** 확장된 클래스를 포함하는 돼지 조직 기반의 내시경 영상 데이터셋(15개 훈련, 4개 테스트 시퀀스)을 구축하고 상세한 의미론적 주석을 제공했습니다.
* **해부학적 주석의 어려움 강조:** 해부학적 데이터의 주석 작업이 기기 주석보다 훨씬 복잡하고 일관성을 유지하기 어렵다는 점(예: '덮인 신장'과 같은 모호한 경계)을 시사하고, 이에 대한 추가적인 연구 필요성을 제기했습니다.
* **최신 방법론 평가:** 총 18개 팀이 참가하여 다양한 딥러닝 기반 분할 방법론(U-Net, DeepLab 등 CNN 아키텍처 변형)을 테스트하고, 실제 수술 환경에서의 성능 한계를 파악했습니다.

## 📎 Related Works

본 논문에서 언급되거나 참가 팀들이 사용한 주요 관련 기술 및 데이터셋은 다음과 같습니다.

* **이전 EndoVis 챌린지:** 2015년 및 2017년 EndoVis 워크숍에서 진행된 로봇 도구 분할 챌린지 [1].
* **신경망 아키텍처:**
  * U-Net [8]: 생의학 영상 분할을 위한 컨볼루션 네트워크.
  * DeepLab V3+ [3], DeepLab V3 [16]: 시맨틱 이미지 분할을 위한 인코더-디코더 구조.
  * ResNet [12], ResNeXt-101 [4], VGG 19 [9], VGG 16: 강력한 이미지 인식 백본 네트워크.
  * PSPNet [14]: 피라미드 장면 파싱 네트워크.
  * Pix2Pix [17]: 조건부 적대적 네트워크를 이용한 이미지-투-이미지 변환.
  * StreoScenNet [11]: 수술 스테레오 로봇 장면 분할을 위해 설계된 네트워크.
* **구성 요소 및 기법:**
  * Squeeze-Excitation blocks [6]: 채널 간의 상호 의존성을 명시적으로 모델링하여 특징 재조정.
  * ImageNet [7]: 대규모 계층적 이미지 데이터베이스로, 많은 모델의 사전 훈련에 사용됨.
  * Conditional Random Field (CRF): 후처리 단계에서 분할 결과를 개선하는 데 사용됨.

## 🛠️ Methodology

이 챌린지는 참가 팀들의 다양한 접근 방식을 평가했습니다. 참가자들이 주로 사용한 방법론은 다음과 같습니다.

* **기본 아키텍처:** 대부분의 팀은 U-Net 변형, DeepLab V3/V3+ 또는 ResNet/ResNeXt/VGG와 같은 강력한 CNN 백본을 기반으로 하는 아키텍처를 사용했습니다. 일부 팀은 PSPNet이나 Pix2Pix와 같은 다른 첨단 모델을 채택했습니다.
* **사전 훈련 (Pre-training):**
  * 많은 팀이 ImageNet [7]으로 인코더를 사전 훈련하여 도메인 불일치(domain shift)로 인한 성능 손실을 방지했습니다.
  * 일부 팀은 2017년 EndoVis 기기 분할 챌린지 데이터를 기기 클래스 사전 훈련에 활용했습니다.
* **데이터 증강 (Data Augmentation):**
  * 일반적인 기법으로는 이미지 크기 조정, 회전, 수평/수직 뒤집기, 자르기, 밝기/대비 조절, 색상(Hue, Saturation, Brightness) 조절 등이 사용되었습니다.
  * National Taiwan University 팀은 여러 이미지의 일부를 결합하여 새로운 훈련 샘플을 만드는 '콜라주(collage)' 기법을 도입했습니다.
* **손실 함수 (Loss Functions):**
  * 가중 교차 엔트로피(weighted cross-entropy), Dice 손실(Dice loss), Jaccard 인덱스, Focal 손실(Focal loss), L1 손실, 적대적 손실(adversarial loss) 등이 사용되었습니다. 일부 팀은 여러 손실 함수를 조합하여 사용했습니다.
* **최적화 (Optimization):** 대부분의 팀은 Adam 또는 모멘텀을 사용한 확률적 경사 하강법(SGD)을 최적화 기법으로 채택했습니다.
* **입력 데이터 처리:**
  * 대부분의 팀은 단일 이미지(좌측 눈)를 입력으로 사용했습니다.
  * Norwegian University of Science and Technology 팀은 스테레오(좌/우) 프레임 쌍을 네트워크의 입력으로 사용했습니다.
* **후처리 (Post-processing) 및 앙상블 (Ensemble):**
  * 일부 팀은 조건부 랜덤 필드(CRF) 추론을 사용하여 거짓 양성(false positives)을 줄이는 후처리를 수행했습니다.
  * Yale University 팀은 4개의 모델 앙상블을 훈련하고 다수결 투표(majority voting)로 최종 예측을 얻었습니다.
  * Johns Hopkins University 팀은 무작위 공간 및 광학적 교란을 적용하여 새로운 데이터를 시뮬레이션한 후, 예측에 변환을 되돌리고 여러 출력을 앙상블 투표로 결합했습니다.
  * Rediminds Inc. 팀은 배경, 도구, 신장 등의 상위 클래스를 예측하는 3개의 U-Net 모델을 별도로 훈련하고 최종적으로 앙상블했습니다.

## 📊 Results

참가 팀들의 방법론은 **평균 교차 결합(mean Intersection over Union, mIoU)** 메트릭을 사용하여 평가되었습니다. IoU는 다음과 같이 정의됩니다:

$$ IOU = TP / (TP+FP+FN) $$

여기서 $TP$는 참 양성(True Positive), $FP$는 거짓 양성(False Positive), $FN$은 거짓 음성(False Negative)입니다. 각 프레임에 존재하는 모든 클래스의 IoU 산술 평균을 계산한 후, 전체 프레임에 대해 평균을 내어 데이터셋별 점수를 산출했습니다.

* **전반적인 성능:** 모든 4개 테스트 데이터셋과 클래스를 통틀어 계산된 평균 mIoU는 **0.478**이었습니다.
* **최고 성과:**
  * **OTH Regensburg** 팀이 전체적으로 6개 클래스에서 가장 높은 점수를 획득하며 전반적인 선두를 차지했습니다.
  * **NCT** 팀은 3개 클래스에서, **Satoshi Kondo** 팀은 1개 클래스에서 가장 높은 점수를 기록했습니다.
* **클래스별 성능:**
  * **신장 실질(Kidney Parenchyma)**은 '덮인 신장(Covered Kidney)'에 비해 더 정확하게 분할되었습니다.
  * **'덮인 신장'** 클래스는 종종 배경으로 잘못 분류되거나 그 반대인 경우가 많아 분할이 매우 어려웠습니다.
    0.216의 낮은 평균 IoU를 기록했습니다.
  * **봉합 바늘(Needle)** 및 **초음파 프로브(US Probe)**와 같은 작거나 희귀한 의료 기기 클래스는 대부분의 팀에서 매우 낮은 점수(종종 0.000)를 기록하며 분할이 극히 어려웠습니다.
  * **기기 부품(Instrument Shaft, Clasper, Wrist)**은 다른 의료 기기나 해부학적 구조보다 상대적으로 높은 분할 정확도를 보였습니다.

* **데이터셋별 특성:**
  * 테스트 데이터셋 1: 간, 위, 신장 등 다양한 해부학적 구조가 등장하며 탐색적 시퀀스 초반과 캐뉼라에 의한 시야 방해로 어려움이 있었습니다.
  * 테스트 데이터셋 2: '덮인 신장' 분할이 가장 큰 난관이었습니다.
  * 테스트 데이터셋 3: 노출된 신장 실질의 클로즈업 시퀀스가 많았으나, 표면이 심하게 덮여 있어 점수가 좋지 않았습니다.
  * 테스트 데이터셋 4: 신장이 근막과 지방으로 가려져 있고 소장과 간이 많이 보이는 등 가장 복잡하여 신장 클래스의 결과가 가장 저조했습니다.

## 🧠 Insights & Discussion

* **해부학적 주석의 본질적 난이도:** 수술 도구에 비해 해부학적 구조(특히 '덮인 신장'과 같이 경계가 모호한 경우)의 픽셀 단위 주석은 훨씬 더 복잡하고 일관성을 유지하기 어렵습니다. 이는 주석 프로토콜의 설계와 주석자의 숙련도에 크게 의존합니다.
* **데이터셋 및 주석 도구의 한계:** 해부학적 구조를 정확히 식별하려면 다양한 각도와 거리에서 확장된 시퀀스를 보아야 하는 경우가 많지만, 기존의 이미지 단위 주석 도구는 이러한 비디오 시퀀스 보기를 쉽게 지원하지 않아 주석 작업의 복잡성을 가중시킵니다.
* **최신 CNN 모델의 한계:** 2017년 챌린지에서 기기 분할은 어느 정도 성공을 보였지만, 2018년 챌린지에서 추가된 새로운 해부학적 클래스(특히 '덮인 신장')와 작고 희귀한 의료 기기(바늘, 초음파 프로브)는 최신 CNN 모델들에게도 여전히 큰 도전 과제임을 보여주었습니다. 이는 현재 방법론이 이러한 복잡하고 미묘한 의료 영상의 특징을 완전히 포착하지 못하고 있음을 시사합니다.
* **향후 연구 방향:** 전반적인 mIoU 점수(0.478)는 해결된 문제라고 보기 어렵습니다. 실제 수술 환경에서 필요한 높은 정확도를 달성하기 위해서는 더욱 견고한 분할 기술과 더불어, 해부학적 구조의 변화무쌍한 특성을 포착할 수 있는 더 방대하고 질 높은 데이터(예: 3D 데이터, 장기 시퀀스) 및 고급 주석 프로토콜이 필요합니다. 이러한 발전은 수술 시 데이터 융합 및 외과의사 인식 향상에 필수적입니다.

## 📌 TL;DR

이 논문은 2018년 로봇 수술 장면 분할 챌린지 결과를 요약합니다. **문제**는 로봇 보조 수술에서 내시경 영상 내 도구와 해부학적 구조(예: 신장, 소장)를 픽셀 단위로 정밀하게 분할하는 것입니다. **제안된 방법**은 아니지만, 참가 팀들은 주로 U-Net 및 DeepLab V3+와 같은 최신 컨볼루션 신경망(CNN) 아키텍처를 ImageNet 사전 훈련, 다양한 데이터 증강 기법, 그리고 다양한 손실 함수와 함께 활용했습니다. **주요 발견**은 로봇 도구 분할은 어느 정도 성공적이었으나, 새롭게 추가된 해부학적 클래스(특히 '덮인 신장')와 작거나 희귀한 의료 기기(바늘, 초음파 프로브)는 모든 팀에게 매우 도전적이었으며, 이는 현재 기술로도 실제 수술 환경의 복잡성을 완전히 해결하기 어렵다는 것을 시사합니다.
