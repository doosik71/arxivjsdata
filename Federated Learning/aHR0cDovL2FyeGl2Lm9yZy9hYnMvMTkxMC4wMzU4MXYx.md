# FedMD: Heterogenous Federated Learning via Model Distillation

Daliang Li, Junpu Wang

## 🧩 Problem to Solve

분산된 환경에서 각 참여자가 개인 데이터뿐만 아니라 **고유하게 설계된 모델 아키텍처**를 가지고 있는 상황에서 연합 학습(Federated Learning)을 가능하게 하는 것이 주요 문제입니다. 기존 연합 학습 프레임워크는 모든 참여자가 동일한 모델 아키텍처를 사용한다고 가정합니다. 그러나 의료, 금융, AI 서비스와 같은 실제 응용 분야에서는 지적 재산권 및 특정 작업 요구사항으로 인해 모델 세부 정보를 공유하기 어렵기 때문에 이러한 모델 이질성(heterogeneity) 문제가 발생합니다. 본 연구는 개인 데이터나 모델 아키텍처를 공유하지 않고도 이러한 이질적인 모델들 간의 협력 및 지식 전달을 어떻게 촉진할 것인가에 초점을 맞춥니다.

## ✨ Key Contributions

* **FedMD 프레임워크 제안**: 참여자들이 독립적으로 모델을 설계할 수 있는 새로운 연합 학습 프레임워크 FedMD를 제안했습니다.
* **블랙박스 통신**: 중앙 서버가 참여 모델의 아키텍처를 제어하지 않으며, 제한적인 블랙박스 접근만 필요로 합니다.
* **지식 증류 기반 통신 모듈**: 전이 학습(transfer learning)과 지식 증류(knowledge distillation)를 활용하여 이질적인 모델 간 지식 번역 및 통신 프로토콜을 구현했습니다.
* **광범위한 실험 검증**: FEMNIST 및 CIFAR10/CIFAR100 데이터셋의 다양한 작업(i.i.d. 및 non-i.i.d.)에서 프레임워크의 효과를 검증했으며, 협력하지 않을 때보다 로컬 모델 성능에서 상당한 개선을 확인했습니다.

## 📎 Related Works

* **기존 연합 학습**: 사용자 데이터를 장치에 보관하면서 중앙 집중식 모델을 훈련하는 방식 [1, 2, 3, 4]으로, 통신 효율성과 대규모 사용자 처리에 중점을 둡니다.
* **이질성 문제**:
  * **시스템 이질성**: 각 참여자의 하드웨어 및 네트워크 자원 차이 [6, 7, 8].
  * **통계적 이질성 (non-i.i.d. 문제)**: 클라이언트 데이터 분포의 비독립동일분포(non-i.i.d.) 문제 [9, 10, 11, 12, 13, 14]. 다중 작업 학습, 베이지안, 메타 학습, 전이 학습 기반 접근 방식으로 다뤄지지만, 일반적으로 로컬 모델 설계에 대한 중앙 통제가 필요합니다.
* **전이 학습 (Transfer Learning)**: 개인 데이터 부족 문제를 해결하는 주요 프레임워크로, 본 연구에서는 협력 이전에 모델을 대규모 공개 데이터셋으로 사전 학습하는 데 활용됩니다.
* **지식 증류 (Knowledge Distillation)**: 모델의 아키텍처에 관계없이(model agnostic) 학습된 정보를 효과적으로 전달하는 기술 [15]로, FedMD에서 이질적인 모델 간의 통신 프로토콜 구현에 핵심적으로 사용됩니다.

## 🛠️ Methodology

FedMD (Algorithm 1)는 다음 단계로 구성됩니다.

1. **전이 학습 (Transfer Learning)**:
    * 각 참여자 $k$는 협력 단계 이전에 자신의 모델 $f_k$를 대규모 공개 데이터셋 $D_0$에서 완전히 훈련한 후, 자신의 작은 개인 데이터셋 $D_k$에서 추가로 훈련하여 초기 성능 기준을 설정합니다.

2. **협력 훈련 반복 ($j=1,2...P$ do)**:
    * **통신 (Communicate)**:
        * 각 참여자 $k$는 자신의 모델 $f_k$를 사용하여 공개 데이터셋 $D_0$의 샘플 $x_i^0$에 대한 클래스 점수 $f_k(x_i^0)$를 계산합니다. (실제로는 $D_0$의 작은 무작위 부분집합 $d_j \subset D_0$를 사용하여 통신 오버헤드를 줄입니다.)
        * 이 점수들을 중앙 서버로 전송합니다.
    * **집계 (Aggregate)**:
        * 중앙 서버는 모든 참여자가 전송한 클래스 점수들을 평균하여 합의(consensus) $\tilde{f}(x_i^0) = \frac{1}{m}\sum_k f_k(x_i^0)$를 계산합니다.
    * **배포 (Distribute)**:
        * 각 참여자는 업데이트된 합의 $\tilde{f}(x_i^0)$를 중앙 서버로부터 다운로드합니다.
    * **소화 (Digest)**:
        * 각 참여자는 다운로드한 합의 $\tilde{f}$를 소프트 타겟(soft target)으로 사용하여, 공개 데이터셋 $D_0$에서 자신의 모델 $f_k$를 훈련합니다. 이 과정은 지식 증류를 통해 다른 모델들의 집단 지식을 흡수하는 단계입니다.
    * **재방문 (Revisit)**:
        * 각 참여자는 자신의 모델 $f_k$를 개인 데이터 $D_k$에서 몇 에포크(epoch) 동안 다시 훈련하여 개인 데이터에 대한 성능을 미세 조정합니다.

이 프레임워크는 지식 증류를 통해 모델 아키텍처나 개인 데이터를 직접 공유하지 않고도 이질적인 모델들이 서로의 지식을 효과적으로 이해하고 통합할 수 있도록 합니다.

## 📊 Results

* **테스트 환경**: MNIST/FEMNIST 및 CIFAR10/CIFAR100 데이터셋에서 i.i.d.(독립동일분포) 및 non-i.i.d.(비독립동일분포) 시나리오 모두에서 검증되었습니다.
* **참여자 구성**: 각 환경에서 10명의 참여자가 채널 수, 레이어 수 등이 다른 고유한 Convolutional Neural Network (CNN) 아키텍처를 사용했습니다.
* **성능 향상**:
  * 협력 훈련 단계에서 모든 참여 모델의 테스트 정확도가 **강력하고 빠른 개선**을 보이며, 개별적으로 전이 학습(공개 데이터 + 개인 데이터)만 사용했을 때의 기준선(baseline)을 빠르게 능가했습니다.
  * 10개 참여 모델의 최종 테스트 정확도는 협력 없이 달성 가능한 성능 대비 **평균 20% 향상**을 기록했습니다.
  * 모든 개인 데이터셋을 합쳐(pooled) 각 모델을 훈련했을 때의 성능과 비교했을 때, FedMD 프레임워크를 통한 성능은 **불과 몇 퍼센트 낮은 수준**으로 나타나 매우 효율적인 협업 효과를 입증했습니다.
* **특이 사례**: 일부 경우(예: CIFAR non-i.i.d. 시나리오의 Model-0)에서는 FedMD로 훈련된 모델이 모든 개인 데이터를 통합하여 훈련된 동일 모델보다 **일관되게 더 나은 성능**을 보이기도 했습니다.

## 🧠 Insights & Discussion

* **모델 이질성 문제 해결**: FedMD는 모델 아키텍처에 대한 중앙 통제 없이도 이질적인 모델 간의 연합 학습을 가능하게 하는 실용적인 프레임워크를 제공합니다. 이는 의료, 금융, AI 서비스와 같이 지적 재산권 및 프라이버시가 중요한 분야에서 핵심적인 도구가 될 수 있습니다.
* **효율적인 지식 전달**: 공개 데이터셋에 대한 클래스 점수를 공유하고 이를 지식 증류로 통합하는 통신 프로토콜은 개인 데이터나 모델 아키텍처를 직접 공유하지 않고도 모델 간의 효과적인 지식 교환을 가능하게 합니다.
* **우수한 성능 입증**: FedMD는 다양한 이질적 데이터셋 및 모델 환경에서 기준선을 크게 상회하는 성능을 보였으며, 모든 데이터를 한곳에 모아 훈련한 경우에 근접하는 높은 정확도를 달성했습니다. 특정 모델에서는 오히려 모든 데이터를 통합한 경우보다 더 나은 성능을 보여 협력의 시너지 효과를 입증했습니다.
* **향후 연구 방향**:
  * 특징 변환(feature transformations)이나 긴급 통신 프로토콜(emergent communication protocols)과 같은 더욱 정교한 통신 모듈을 탐색하여 프레임워크 성능을 더욱 개선할 수 있습니다.
  * NLP 및 강화 학습과 같은 다른 영역의 작업으로 프레임워크를 확장할 수 있습니다.
  * 데이터 양, 모델 용량, 로컬 작업의 큰 차이를 포함하는 극단적인 이질성 사례에 대한 프레임워크 확장이 중요합니다.
  * 성능이 낮은 모델의 기여도를 조절하는 가중치 부여 방식이 전체 성능 향상에 중요한 역할을 할 수 있습니다.

## 📌 TL;DR

FedMD는 각 참여자가 고유한 모델 아키텍처와 개인 데이터를 가질 때 연합 학습을 가능하게 하는 프레임워크입니다. 이 문제는 기존 연합 학습의 한계이자 지적 재산권 및 프라이버시가 중요한 산업에서 대두되는 핵심 과제입니다. FedMD는 공개 데이터셋에 대한 모델의 클래스 점수(logits)를 공유하고 이를 지식 증류를 통해 통합하는 방식으로 작동하며, 각 모델은 집단 지식을 학습하고 자신의 개인 데이터로 미세 조정하는 과정을 반복합니다. 실험 결과, 이 프레임워크는 협력 없이 개별적으로 학습한 모델에 비해 평균 20%의 성능 향상을 가져왔으며, 모든 개인 데이터를 합쳐 훈련한 모델에 준하는 높은 정확도를 달성했습니다. 이는 모델 아키텍처나 개인 데이터를 공유하지 않고도 이질적인 시스템 간의 효율적인 협업을 가능하게 하는 강력한 솔루션을 제시합니다.
