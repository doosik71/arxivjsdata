{
  "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language\n  Models",
  "authors": "Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, Jianshu Chen",
  "year": 2022,
  "url": "http://arxiv.org/abs/2210.16433v3",
  "abstract": "Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models."
}