{
  "title": "Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey",
  "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley",
  "year": 2022,
  "url": "http://arxiv.org/abs/2201.09267v1",
  "abstract": "This is a tutorial and survey paper on metric learning. Algorithms are\ndivided into spectral, probabilistic, and deep metric learning. We first start\nwith the definition of distance metric, Mahalanobis distance, and generalized\nMahalanobis distance. In spectral methods, we start with methods using scatters\nof data, including the first spectral metric learning, relevant methods to\nFisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant\nComponent Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric\nlearning, imbalanced metric learning, locally linear metric adaptation, and\nadversarial metric learning are covered. We also explain several kernel\nspectral methods for metric learning in the feature space. We also introduce\ngeometric metric learning methods on the Riemannian manifolds. In probabilistic\nmethods, we start with collapsing classes in both input and feature spaces and\nthen explain the neighborhood component analysis methods, Bayesian metric\nlearning, information theoretic methods, and empirical risk minimization in\nmetric learning. In deep learning methods, we first introduce reconstruction\nautoencoders and supervised loss functions for metric learning. Then, Siamese\nnetworks and its various loss functions, triplet mining, and triplet sampling\nare explained. Deep discriminant analysis methods, based on Fisher discriminant\nanalysis, are also reviewed. Finally, we introduce multi-modal deep metric\nlearning, geometric metric learning by neural networks, and few-shot metric\nlearning."
}