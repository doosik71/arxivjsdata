# A Probabilistic Theory of Deep Learning
Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk

## 🧩 Problem to Solve
기존 딥러닝 알고리즘, 특히 시각 및 음성 인식과 같은 지각 추론 작업에서 뛰어난 성능을 보이는 딥러닝(Deep Convolutional Networks, DCNs 및 Random Decision Forests, RDFs) 시스템이 **왜 작동하는지**에 대한 근본적인 질문에 답할 일관된 이론적 프레임워크가 부족합니다. 객체의 위치, 방향, 크기 또는 음성 발음, 피치, 속도와 같은 **교란 변수(nuisance variables)**의 높은 변동성에도 불구하고 딥러닝이 어떻게 성공적으로 작동하는지 설명하고, 그 성공과 한계를 이해하며, 향후 개선을 위한 원칙적인 설계 경로를 제시해야 합니다.

## ✨ Key Contributions
*   **새로운 확률적 프레임워크 제시:** 교란 변수로 인한 변화를 명시적으로 포착하는 베이즈 생성 확률 모델인 **렌더링 모델(Rendering Model, RM)** 및 **심층 렌더링 모델(Deep Rendering Model, DRM)**을 개발했습니다.
*   **DCN 및 RDF와의 통일:** DRM이 두 가지 주요 딥러닝 시스템인 DCN과 RDF를 포괄하며, 각각을 특정 교란 구조(가우시안 변환 교란 또는 진화적 가산 교란)를 가진 Max-Sum 메시지 전달 네트워크로 해석할 수 있음을 수학적으로 증명했습니다.
*   **딥러닝의 작동 원리 통찰 제공:**
    *   DCN이 완전한 확률적 의미론을 가지며, 합성곱, Max-Pooling, ReLU 등의 연산이 DRM의 요인 그래프(factor graph)에서 특정 메시지 전달 과정(Max-Sum 추론)에 해당함을 밝혔습니다.
    *   DCN의 분류기는 잠재 교란 변수를 포착하는 차별적 분류기이며, 표적(target)의 지도 학습이 잠재 교란 변수의 비지도 학습과 얽혀있음을 보였습니다.
    *   Max-Pooling이 교란 변수에 대한 확률적 주변화(marginalization)를 구현하는 핵심 연산임을 강조했습니다.
*   **새로운 학습 및 추론 알고리즘 제안:** DRM 기반의 EM(기대-최대화) 알고리즘이 기존 역전파(backpropagation) 학습보다 효율적이고 정확한 대안이 될 수 있음을 시사하며, 상향식(bottom-up) 및 하향식(top-down) 추론이 가능한 새로운 DCN 구조를 제안했습니다.

## 📎 Related Works
*   **Mixture of Factor Analyzers (MFA) / Deep Gaussian Mixture Model (DGMM):** DRM은 계층적 모델이라는 점에서 MFA 기반 모델과 관련이 있지만, DRM은 노이즈 선택 및 아핀 변환의 다채널 이미지 적용 방식에서 차이가 있으며, DCN의 Max-Pooling 및 ReLU와 직접적으로 연결됩니다.
*   **i-Theory:** 감각 피질에서 영감을 받은 불변 표현 이론으로, 교란 변수에 대한 불변성 학습을 목표로 합니다. DRM은 교란 변수에 반군(semi-group) 구조를 강요하지 않고, 표현이 추론 과정에서 자연스럽게 나타난다는 점에서 i-Theory와 차별화됩니다.
*   **Scattering Transform (ST):** 웨이블릿 변환을 통해 불변성(translations, rotations, scalings)을 명시적으로 설계하는 접근 방식입니다. ST는 학습 없이 강한 모델 편향을 가지는 반면, DRM은 데이터로부터 특징을 학습하는 데 중점을 둡니다.
*   **Learning Deep Architectures via Sparsity:** 최적의 아키텍처를 데이터에서 직접 학습하려는 시도로, 희소 신경망을 가정합니다. DRM 또한 희소 생성 모델로 볼 수 있지만, DRM은 교란 변수를 처리하는 데 적합한 특정 아키텍처 클래스를 도출하는 데 중점을 둡니다.
*   **Google FaceNet:** 잘 분리된(well-separated) 표현 학습의 중요성을 보여주는 DCN 아키텍처로, `triplet finding`이라는 새로운 학습 목표를 사용합니다. DRM 관점에서는 이 목표가 노이즈가 없는 결정론적 렌더링 과정에서 클러스터가 잘 분리되어야 함을 의미합니다.
*   **Renormalization Theory:** 물리학의 재규격화 이론과 딥러닝 사이의 유사성을 탐구하며, Restricted Boltzmann Machine (RBM)과 블록-스핀 재규격화 사이의 정확한 대응 관계를 제시합니다. DRM은 다중 추상화 수준에서 불필요한 특징을 제거하는 것이 목표이므로, 새로운 실공간(real-space) 재규격화 스킴으로 해석될 수 있습니다.

## 🛠️ Methodology
본 논문은 **심층 렌더링 모델(DRM)**이라는 새로운 확률적 생성 모델을 제안하고, 이를 통해 딥러닝 시스템의 작동 원리를 설명합니다.

1.  **렌더링 모델 (RM):**
    *   주어진 객체 클래스($c$)와 교란 변수($g$)를 사용하여 이미지를 생성하는 **렌더링 함수 $R(c,g)$**를 정의하고, 여기에 노이즈를 추가하여 이미지를 생성합니다: $I|c,g = R(c,g) + \text{noise}$.
    *   노이즈는 지수족 분포를 따르며, 특히 **가우시안 렌더링 모델(GRM)**은 $I|c,g \sim N(I|\mu_{cg}=R(c,g), \Sigma_{cg}=\sigma^2\mathbf{1})$ 형태로 표현됩니다.
    *   교란 변수 $g$를 주변화하기 위해 **Sum-Product RM 분류기(SP-RMC)**와 **Max-Sum RM 분류기(MS-RMC)** 두 가지 접근 방식을 탐색합니다. MS-RMC가 DCN과 밀접하게 연관됨을 보입니다.
    *   패치(patch) 기반으로 국소적인 클래스 $c(x)$와 교란 $g(x)$를 모델링합니다.

2.  **심층 렌더링 모델 (DRM):**
    *   RM을 계층적으로 확장하여 다중 추상화 수준($\ell$)에서 이미지를 생성하는 모델입니다. 가장 추상적인 수준에서 시작하여 아핀 변환($\Lambda(g)$)의 곱을 통해 점진적으로 세부 정보를 추가합니다: $\mu(c_L, g) = \Lambda_1(g_1) \cdots \Lambda_L(g_L) \cdot \mu(c_L)$.
    *   이 계층적 구조는 파라미터 수를 기하급수적으로 줄여 효율적인 추론과 학습을 가능하게 합니다.

3.  **DRM의 추론:**
    *   DRM에서의 추론은 Max-Sum 메시지 전달 알고리즘을 통해 이루어집니다. 특히 이미지 분류를 위한 상향식(fine-to-coarse) 패스는 DCN의 순전파(feedforward propagation)와 정확히 일치합니다.
    *   **DCN 구성 요소 도출:**
        *   **가우시안 노이즈:** 이미지 정규화, 선형 템플릿 매칭($\langle w_{cg}|I \rangle + b_{cg}$)에 해당합니다.
        *   **변환 교란($g$):** 이미지 패치에 대한 Max-Pooling을 설명합니다.
        *   **스위칭 변수($a \in \{ON, OFF\}$):** 렌더러의 활성/비활성 상태를 모델링하며, Max-marginalization을 통해 DCN의 **ReLU(Rectified Linear Unit)** 활성화 함수를 도출합니다: $\max_{a \in A} a(\langle w_{cg}|I \rangle + b_{cg}) + b_{cga} = \text{ReLU}(\langle w_{cg}|I \rangle + b_{cg})$.

4.  **DRM의 학습 (EM 알고리즘):**
    *   DRM은 잠재 변수를 포함하는 그래픽 모델이므로 EM(기대-최대화) 알고리즘을 사용하여 파라미터를 학습합니다.
    *   **E-단계:** 주어진 데이터와 현재 파라미터로 잠재 변수(클래스 및 교란)의 사후 확률($\gamma_{n\tau}$)을 추론합니다. "하드 EM"에서는 가장 가능성이 높은 구성($\tau^*$ )을 찾습니다.
    *   **M-단계:** 추론된 잠재 변수를 사용하여 파라미터(가중치, 편향, 공분산 등)를 업데이트합니다. 이 과정은 DCN의 가중치 및 편향 학습과 유사합니다.
    *   **Batch Normalization 및 Dropout과의 연결:** 구글의 Batch Normalization이 EM 알고리즘의 진정한 정규화 단계에 대한 근사치임을 보이고, Dropout이 완전히 무작위적인 누락 데이터(missing data)가 있는 GRM에 대한 EM 알고리즘에서 파생될 수 있음을 보여줍니다.

5.  **생성 모델에서 판별 모델로의 이완(Discriminative Relaxation):**
    *   생성 모델(DRM)의 파라미터 제약(예: Eq. 8)을 완화하여 판별 모델(DCN)의 자유로운 파라미터를 얻는 과정을 수학적으로 정의합니다. 이는 $\max_\theta L_{\text{gen}}(\theta)$ 목표를 $\max_\eta L_{\text{dis}}(\eta)$ 목표로 변환하는 과정입니다.
    *   이 이완을 통해 DRM은 DCN과 정확히 일치하며, DCN의 역전파 학습은 DRM의 배치 EM 학습에 대한 판별적 이완으로 재해석됩니다.

6.  **진화적 심층 렌더링 모델 (E-DRM)과 RDF:**
    *   DRM의 변형인 E-DRM을 정의하여 계층적 범주(예: 생물 분류군)를 생성하는 **가산적 돌연변이(additive mutation) 교란 과정**을 모델링합니다.
    *   E-DRM에서의 Max-Sum 추론은 단일 **의사결정 트리(decision tree)**를 도출하며, 각 노드는 입력 이미지에 대한 질문에 해당합니다.
    *   과적합(overfitting)을 방지하기 위한 **부트스트랩 집계(Bootstrap Aggregation, Bagging)**는 단일 트리를 **랜덤 포레스트(Random Forest)**로 확장합니다.
    *   E-DRM의 EM 학습은 **InfoMax 원리**로 이어지며, 이는 훈련 레이블과 추론된 렌더링 경로 간의 상호 정보(mutual information)를 최대화하는 방식입니다.

## 📊 Results
*   **DCN과 DRM의 정확한 대응 관계:** DCN의 모든 주요 구성 요소(합성곱, Max-Pooling, ReLU)가 DRM의 확률적 추론(Max-Sum 메시지 전달, 교란 변수 주변화)에서 직접적으로 파생됨을 보였습니다. 이는 DCN의 구조가 임의적이지 않으며, 정교한 확률적 가정에서 비롯됨을 의미합니다.
*   **분리(Disentanglement)의 시각화 및 정량화:** 합성적으로 렌더링된 자동차와 비행기 이미지로 DCN을 훈련시킨 후, 훈련된 DCN의 계층에서 표적(객체 정체성)과 잠재 교란 변수(기울기, 기울기, 위치, 깊이 등)에 대한 선형적으로 분리 가능한 정보량을 측정했습니다 (Fig. 6).
    *   객체 정체성에 대한 정보는 계층 깊이에 따라 증가하고, 교란 변수에 대한 정보는 역 U자형 곡선을 따름을 확인했습니다. 이는 DCN이 다중 계층을 통해 잠재 교란 변수를 명시적으로 훈련하지 않고도 점진적으로 분리(disentangle)함을 강력하게 시사합니다.
    *   교란의 복잡성이 증가할수록 성공적인 분리를 위해 더 많은 계층이 필요함을 입증하여, 깊이(depth)의 필요성을 이론적으로 뒷받침했습니다.
*   **클래스 외형 모델(Class Appearance Models):** 활성도 최대화(activity maximization) 기법을 통해 DCN이 학습한 클래스 외형 모델이 Eq. 27이 예측하는 바와 같이, 동일 객체의 다양한 포즈(pose)를 중첩한 형태로 저장됨을 Fig. 5를 통해 보여주었습니다. 이는 DRM이 가정하는 교란 변수에 대한 혼합(mixture) 모델과 일치합니다.

## 🧠 Insights & Discussion
*   **딥러닝의 확률적 기초:** DCN은 단순한 휴리스틱이 아니라, 잘 정의된 확률적 모델(DRM)에서 파생될 수 있으며, 이는 DCN이 완전한 확률적 의미론을 가진다는 것을 의미합니다. DCN의 연산들은 DRM 요인 그래프에서의 표준 메시지 전달 작업에 해당합니다.
*   **지도 학습과 비지도 학습의 융합:** DCN의 지도 학습은 사실상 잠재된 교란 변수(예: 객체의 자세, 조명)를 비지도 방식으로 학습하는 것과 불가피하게 연결됩니다. 이는 DCN이 데이터 매니폴드의 내재적 차원을 학습하고 분리하도록 설계되었음을 의미합니다.
*   **Max-Pooling의 중요성:** Max-Pooling은 교란 변수에 대한 확률적 주변화(Max-Marginalization)를 구현하는 핵심 연산입니다. 이는 DCN이 교란 변수를 효과적으로 제거하는 능력의 근간입니다. ReLU 또한 ON/OFF 스위칭 변수에 대한 Max-Pooling으로 해석됩니다.
*   **생성 모델의 이점:** DCN의 한계는 기본 생성 모델이 알려지지 않았다는 사실에서 비롯됩니다. DRM과 같은 생성 모델을 통해 샘플링, 모델 개선, 하향식 추론, 더 빠른 학습, 모델 선택, 레이블 없는 데이터로부터의 학습 등 다양한 중요한 작업을 수행할 수 있습니다.
*   **향후 개선 방향:**
    *   **더 현실적인 렌더링 모델:** 3D 기하학, 변환, 회전, 스케일링, 원근법, 비강체 변형 등 컴퓨터 그래픽스 기반의 더 풍부한 교란 변수를 모델링하여 DCN의 성능을 개선할 수 있습니다.
    *   **새로운 추론 알고리즘:** Max-Sum 대신 "Soft 추론"(예: Sum-Product 또는 Variational Bayes EM)을 도입하여 측정 불확실성이 높은 상황에 더 잘 대처하고, 하향식(Top-Down) 메시지 전달을 통해 폐색(occlusion)이나 혼란(clutter)과 같은 시각 작업에서의 성능을 향상시킬 수 있습니다.
    *   **새로운 학습 알고리즘:** DRM의 EM 알고리즘은 기존 역전파보다 빠르고 정확하며, 본질적으로 병렬화가 가능합니다. 또한, 비디오 데이터로부터 학습하는 동적 DRM은 학습 속도를 가속화할 수 있으며, 레이블된 데이터와 레이블 없는 데이터를 모두 활용하는 하이브리드 학습 방식도 가능합니다.

## 📌 TL;DR
본 논문은 딥러닝(DCN, RDF)이 성공적으로 작동하는 이유를 설명하기 위해 **심층 렌더링 모델(DRM)**이라는 새로운 베이즈 생성 확률 프레임워크를 제안합니다. DRM은 계층적 교란 변수 모델링을 통해 DCN의 합성곱, Max-Pooling, ReLU 등의 연산이 Max-Sum 메시지 전달이라는 확률적 추론 과정임을 수학적으로 증명합니다. 또한, DCN의 학습이 DRM EM 알고리즘의 판별적 이완임을 보여주며, DCN이 표적 학습과 함께 잠재 교란 변수를 비지도 방식으로 분리(disentangle)한다는 핵심 통찰을 제공합니다. 이 통일된 프레임워크는 딥러닝의 한계를 극복하고 개선하기 위한 새로운 방향을 제시합니다.