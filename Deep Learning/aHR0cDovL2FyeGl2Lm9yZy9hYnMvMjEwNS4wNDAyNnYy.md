# The Modern Mathematics of Deep Learning

Julius Berner, Philipp Grohs, Gitta Kutyniok, Philipp Petersen

## 🧩 Problem to Solve

이 논문은 딥러닝의 성공적인 실제 적용 뒤에 있는 수학적, 이론적 근거를 설명합니다. 특히, 고전적인 통계 학습 이론으로는 설명하기 어려운 다음과 같은 핵심 연구 질문들을 다룹니다:

- **과적합(overfitting) 없는 일반화:** 엄청난 수의 매개변수를 가진 과잉 매개변수화된(overparametrized) 신경망이 훈련 데이터에 완벽하게 적합함에도 불구하고 어떻게 훌륭하게 일반화될 수 있는가? (일반화 퍼즐)
- **깊이(depth)의 역할:** 얕은(shallow) 아키텍처와 비교하여 깊은(deep) 아키텍처의 구체적인 수학적 이점은 무엇인가?
- **차원의 저주(curse of dimensionality) 극복:** 딥러닝 방법이 왜 고차원 환경에서 차원의 저주에 시달리지 않는 것처럼 보이는가?
- **최적화 성공:** 비볼록성(non-convexity)과 비선형성에도 불구하고 확률적 경사 하강법(SGD)이 어떻게 좋은 지역 최소값(local minima)을 찾아내는지?
- **아키텍처의 영향:** 신경망 아키텍처의 특정 측면이 학습 작업의 성능에 어떤 영향을 미치는가?
- **특징 학습(feature learning):** 딥 아키텍처가 데이터에서 어떤 특징을 학습하는가?
- **자연 과학 분야의 효과:** 딥러닝이 왜 전문화된 수치 도구보다 자연 과학 문제에서 뛰어나거나 동등한 성능을 보이는가?

## ✨ Key Contributions

이 논문은 위 질문들에 대한 부분적인 답을 제공하는 현대적인 접근 방식들을 개괄하며, 주요 기여는 다음과 같습니다:

- **과잉 매개변수화된 신경망의 일반화 설명:** 커널 체제(kernel regime, 신경망이 무한히 넓어질 때 선형 커널 모델처럼 동작)와 암묵적 정규화(implicit regularization, SGD가 최소 L2-norm 또는 최대 마진 해를 찾는 경향)를 통해 과잉 매개변수화가 일반화 성능에 미치는 긍정적인 영향을 설명합니다. "이중 하강(double descent)" 현상을 다룹니다.
- **깊이의 표현력(Expressivity) 증명:** 깊은 신경망이 특정 함수(예: 방사형 함수, Sobolev 정규 함수)를 얕은 신경망보다 훨씬 효율적으로 근사할 수 있음을 수학적으로 증명합니다. ReLU 신경망의 깊이가 선형 조각(affine linear pieces)을 기하급수적으로 더 많이 생성할 수 있음을 보여줍니다.
- **차원의 저주 극복 메커니즘 제시:** 데이터가 저차원 매니폴드(manifold)에 놓여있다는 가정, Barron 함수 공간과 같은 특정 함수 공간에서의 차원 독립적 근사 능력, 그리고 편미분 방정식(PDE) 솔루션의 확률적 표현을 통한 차원 독립적 근사 능력을 통해 고차원 문제에서 딥러닝이 성공하는 이유를 설명합니다.
- **최적화 동역학 분석:** 손실 지형(loss landscape) 분석(스핀 글라스 비유, 연결된 레벨 세트)과 "느슨한 훈련(lazy training)"(매개변수가 초기화 근처에 머무르면서 커널 모델처럼 동작) 개념을 통해 SGD가 비볼록 문제에서 좋은 해를 찾는 이유를 설명하고 수렴 보장을 제시합니다.
- **특수 아키텍처의 정량적 효과 분석:** CNN(지역 수용장, 매개변수 공유), Residual Network(잔차 블록을 통한 그라디언트 흐름 개선), U-Net(프레임렛 및 인코더-디코더 구조), Batch Normalization(손실 지형 완화), 희소 신경망(가지치기, 로터리 티켓 가설), 순환 신경망(시퀀스 데이터 처리)과 같은 특정 아키텍처 요소가 성능에 미치는 이론적 영향을 탐구합니다.
- **학습된 특징의 이해:** 스캐터링 변환(scattering transform)을 통해 변환 불변성(translation invariance) 및 변형 불변성(deformation invariance)을 갖는 특징 추출을 모델링하고, 계층적 희소 표현(hierarchical sparse representations)과 CNN 간의 연관성을 탐구합니다.
- **자연 과학 분야에서의 딥러닝 적용:** 역문제(inverse problems) 및 고차원 편미분 방정식(예: 슈뢰딩거 방정식, 블랙-숄즈 모델) 해결에 딥러닝이 어떻게 활용되며 성공적인 결과를 내는지 설명합니다.

## 📎 Related Works

이 논문은 딥러닝의 현대 수학적 분석을 위해 광범위한 고전 및 현대 연구들을 참조합니다:

- **고전 통계 학습 이론:** Vapnik의 통계 학습 이론, 학습 이론의 기초, 일반화 및 복잡성 측정(예: VC-차원, Rademacher 복잡성).
- **근사 이론:** 신경망의 만능 근사자(universal approximators) 능력 (Cybenko, Funahashi 등), Sobolev 공간에서의 근사 속도, 차원의 저주 개념(Bellman).
- **최적화 이론:** 경사 하강법 및 확률적 경사 하강법의 수렴 이론, 비볼록 최적화 문제.
- **수학적 도구:** Hahn–Banach 정리, Riesz–Markov–Kakutani 표현 정리, Hoeffding 및 McDiarmid의 농도 부등식, 푸리에 해석, 카라테오도리 보조정리(Carathéodory Lemma).
- **물리적 모델:** 스핀 글라스(spin glass) 모델의 해밀토니안(Hamiltonian)을 손실 지형 분석에 적용.
- **확률론적 방법:** Feynman–Kac 공식(PDE 솔루션의 확률적 표현).
- **신호 처리:** 웨이블릿 변환, 프레임 이론, 희소 코딩(sparse coding) 이론.

## 🛠️ Methodology

논문에서 다루는 주요 방법론은 다음과 같습니다:

- **무한 너비 극한(Infinite Width Limit) 및 신경망 접선 커널(Neural Tangent Kernel, NTK) 분석:**
  - 매개변수가 무작위로 초기화되고 신경망의 너비가 무한대로 가는 극한에서 SGD 훈련이 어떻게 고정된 커널 함수에 대한 선형 회귀와 유사하게 작동하는지 분석합니다.
  - 이때 신경망은 실제로는 비선형이지만, 훈련 과정에서 동작이 선형 모델과 유사해지는 "느슨한 훈련(lazy training)" 현상을 설명합니다.
  - $K_{\theta}(x_1, x_2) = (\nabla_{\theta} \Phi(x_1, \theta))^T \nabla_{\theta} \Phi(x_2, \theta)$를 사용하여 매개변수 공간에서의 그라디언트 흐름(gradient flow)을 함수 공간에서의 흐름으로 변환합니다.
- **규범 기반(Norm-based) 복잡성 및 마진 이론:**
  - 매개변수 수에 직접적으로 의존하지 않고, 신경망 가중치 행렬의 프로베니우스 노름($\left\| \cdot \right\|_{F}$) 또는 스펙트럼 노름($\left\| \cdot \right\|_{\text{op}}$)과 같은 규범을 사용하여 Rademacher 복잡성을 제한합니다.
  - 분류 작업에서 훈련 데이터에 대한 예측 마진(margin)을 최대화하는 것이 일반화 성능 향상으로 이어질 수 있음을 보여줍니다.
- **손실 지형(Loss Landscape) 분석:**
  - 스핀 글라스(spin glass) 모델의 해밀토니안과의 비유를 통해 최적점에서 멀리 떨어진 임계점(critical points)이 불안정하다는 것을 보여줍니다.
  - 선형 부분 공간(linear subspace) 및 연결된 레벨 세트(connected level sets) 개념을 사용하여 초기화 지점에서 전역 최소점(global minimum)으로 이어지는 감소하는 위험(risk) 경로의 존재 가능성을 탐구합니다.
- **깊이의 표현력 분석 (함수 구성):**
  - 특정 함수(예: 방사형 함수)를 구성하여 얕은 신경망으로는 효율적으로 근사하기 어렵지만, 깊은 신경망으로는 효율적으로 근사할 수 있음을 보여줍니다.
  - ReLU 활성화 함수가 선형 조각(affine linear pieces)을 기하급수적으로 많이 생성할 수 있는 능력을 활용하여 깊이의 이점을 정량화합니다.
- **차원의 저주 극복 (가정 기반 접근):**
  - **매니폴드 가정(Manifold Assumption):** 고차원 데이터가 실제로는 저차원 매니폴드에 놓여있다고 가정하고, 신경망이 이러한 매니폴드의 지역 좌표 변환을 학습하여 효과적인 차원 축소를 수행할 수 있음을 보입니다.
  - **무작위 샘플링(Random Sampling) / Barron 공간:** 푸리에 변환의 특정 유한 모멘트 조건을 만족하는 함수(Barron 정규 함수)를 무작위 샘플링을 통해 차원 독립적인 속도로 근사할 수 있음을 보여줍니다.
  - **PDE 가정(PDE Assumption):** 함수가 편미분 방정식의 해로 주어졌을 때, 페인만-카츠 공식(Feynman-Kac formula)과 확률적 미분 방정식(SDE) 시뮬레이션을 통해 차원의 저주 없이 근사할 수 있음을 보입니다.
- **특수 아키텍처 분석:**
  - **CNNs:** 지역 수용장(local receptive fields), 매개변수 공유, 동변 표현(equivariant representations) 개념을 수학적으로 정식화하고 보편 근사자로서의 특성을 분석합니다.
  - **U-Net (프레임렛):** 프레임 이론적 관점(프레임렛)에서 CNN을 분석하여 인코더-디코더(encoder-decoder) 구조의 이론적 토대와 스킵 연결(skip connection)의 역할을 설명합니다.
  - **Recurrent NNs:** SGD의 소실/폭주 그라디언트 문제를 극복하기 위한 동역학적 시스템으로서의 연결성을 강조합니다.
- **특징 학습 (신호 처리 및 희소 표현):**
  - **스캐터링 변환(Scattering Transform):** 고정된 매개변수를 가진 특정 CNN 아키텍처가 번역 불변성(translation invariance) 및 변형 불변성(deformation invariance)과 같은 바람직한 특성을 갖는 특징 기술자(feature descriptor)를 생성함을 보입니다.
  - **계층적 희소 표현(Hierarchical Sparse Representations):** 다층 컨볼루션 희소 코딩(ML-CSC) 모델과 CNN 간의 관계를 탐구하며, CNN의 커널이 희소 사전(sparsifying dictionaries)으로 작용함을 설명합니다.

## 📊 Results

주요 연구 결과는 다음과 같이 요약할 수 있습니다:

- **과잉 매개변수화의 일반화 능력:**
  - 무한 너비의 신경망은 훈련 데이터에 완벽하게 적합하면서도 최소 커널-노름(kernel-norm) 보간법(interpolator)에 수렴하여 훌륭한 일반화 성능을 보여줍니다.
  - 경사 하강법은 선형 모델과 선형 분리 가능한 데이터에 대해 최소 노름 또는 최대 마진 해로 수렴하는 암묵적 정규화 효과를 가집니다.
  - "이중 하강" 현상은 과잉 매개변수화 영역에서 테스트 오류가 보간 임계값 이후에도 다시 감소할 수 있음을 보여주며, 이는 고전적인 편향-분산(bias-variance) 상충 관계를 넘어선 새로운 이해를 제시합니다.
- **깊이의 근사 효율성:**
  - 깊은 ReLU 신경망은 특정 함수의 근사에서 얕은 신경망보다 매개변수 수 측면에서 기하급수적으로 더 효율적인 근사율을 달성할 수 있습니다.
  - 예를 들어, Sobolev-정규 함수 근사에서 깊은 네트워크는 $O(\epsilon^{-d/(k-s)}\log(1/\epsilon))$ 매개변수를 사용하는 반면, 얕은 네트워크는 이보다 훨씬 많은 매개변수를 요구합니다.
  - 신경망 레이어를 통과하는 궤적 길이(trajectory length)는 깊이에 따라 기하급수적으로 증가하며, 이는 깊이의 표현력 증가와 관련이 있습니다.
- **고차원 문제에서의 차원의 저주 극복:**
  - 데이터가 저차원 매니폴드에 국한되거나, 함수가 Barron 공간에 속하거나, 편미분 방정식의 해로 주어지는 경우, 신경망은 입력 차원에 독립적인 근사율을 달성하여 차원의 저주를 극복할 수 있습니다.
  - 예를 들어, Barron 정규 함수는 $n$개의 숨겨진 뉴런을 가진 2계층 신경망으로 $O(1/\sqrt{n})$의 근사 오차를 가집니다.
- **비볼록 최적화의 성공:**
  - 손실 지형 분석은 전역 최소값에서 멀리 떨어진 임계점이 일반적으로 불안정하다는 것을 시사하여 SGD가 이러한 지점에서 벗어날 수 있음을 암시합니다.
  - 충분히 과잉 매개변수화된 신경망에서 "느슨한 훈련" 체제는 경사 하강법이 초기화 근처에 머무르면서도 임의로 작은 훈련 오류에 기하급수적으로 빠르게 수렴함을 보장합니다.
- **특수 아키텍처의 정량적 이점:**
  - CNN은 지역성(locality), 매개변수 공유(parameter sharing), 등변성(equivariance)을 활용하여 이미지 처리에서 효율적인 특징 학습을 가능하게 합니다.
  - Residual Network의 스킵 연결은 소실/폭주 그라디언트 문제를 완화하고 깊이 증가에 따른 성능 저하(degradation)를 해결하는 데 기여합니다.
  - Batch Normalization은 손실 함수의 그라디언트 크기를 줄여 최적화 문제를 매끄럽게(smoothen) 만듭니다.
  - 가지치기(pruning)는 Carathéodory 보조정리를 통해 매우 넓은 신경망이 희소한 부분망으로도 잘 근사될 수 있음을 보여줍니다.
- **학습된 특징의 구조:**
  - 스캐터링 변환은 입력 데이터의 번역 불변성과 변형 불변성을 효과적으로 추출하는 특징을 제공하며, 이는 기존 신호 처리 이론과 연결됩니다.
  - CNN은 계층적 컨볼루션 희소 코딩 모델과 유사하게 작동하여 데이터의 희소 표현을 학습합니다.
- **자연 과학 문제 해결:**
  - 딥러닝은 MRI 재구성(inverse problems)과 같은 역문제에서 뛰어난 성능을 보이며, 고전적인 모델 기반 방법과 데이터 기반 방법을 결합하는 하이브리드 접근 방식이 개발되었습니다.
  - 고차원 슈뢰딩거 방정식(Schrödinger equation)과 같은 PDE 문제에 대한 신경망 기반 솔루션은 차원의 저주를 극복하고 물리적 지식(Pauli 원리, cusp 조건)을 통합하여 높은 정확도를 달성할 수 있습니다.

## 🧠 Insights & Discussion

이 논문은 딥러닝의 성공이 단순히 경험적인 것을 넘어, 강력한 수학적 토대 위에 서 있음을 강조합니다. 주요 통찰과 논의는 다음과 같습니다:

- **고전 이론의 한계와 새로운 패러다임의 필요성:** 고전적인 통계 학습 이론, 특히 편향-분산 상충 관계와 VC-차원 기반 일반화 한계는 현대 딥러닝의 "일반화 퍼즐" (과잉 매개변수화된 모델의 좋은 일반화)을 설명하는 데 불충분합니다. 이는 딥러닝에 특화된 새로운 이론적 프레임워크가 필요함을 시사합니다.
- **최적화와 일반화의 상호 작용:** SGD와 같은 최적화 알고리즘의 동역학이 신경망의 일반화 성능에 결정적인 영향을 미칩니다. 암묵적 정규화 효과는 명시적인 정규화 항 없이도 모델의 복잡성을 제어하여 좋은 일반화를 가능하게 합니다.
- **깊이의 본질적 이점:** 깊이는 단순한 추가 레이어가 아니라, 특정 클래스의 함수를 근사하는 데 있어 폭(width)보다 훨씬 효율적인 기하급수적인 표현력 이점을 제공합니다. 이는 신경망이 입력 공간을 복잡하게 접고(folding) 추상적인 특징을 계층적으로 학습하는 능력과 관련됩니다.
- **데이터 구조의 중요성:** 딥러닝이 차원의 저주를 극복하는 능력은 종종 학습 대상 함수나 데이터 자체의 내재된 저차원 구조(매니폴드, Barron 공간, PDE 솔루션)에 대한 가정을 기반으로 합니다. 신경망은 이러한 구조를 효과적으로 포착하고 활용할 수 있습니다.
- **아키텍처 설계의 이론적 근거:** CNN의 지역성 및 불변성, ResNet의 그라디언트 흐름 개선, Batch Normalization의 손실 지형 완화와 같은 특수 아키텍처 요소들은 단순한 경험적 발견이 아니라, 수학적으로 정량화 가능한 장점을 가집니다. 이는 미래 아키텍처 설계에 대한 이론적 지침을 제공할 수 있습니다.
- **딥러닝의 설명 가능성 (XAI)을 향한 발걸음:** 신경망이 학습하는 특징을 스캐터링 변환이나 희소 코딩과 같은 기존 이론적 프레임워크와 연결하려는 시도는 딥러닝 모델의 내부 작동 방식에 대한 이해를 높이고 설명 가능성을 향한 중요한 단계를 제공합니다.
- **과학 컴퓨팅의 혁신:** 딥러닝은 고차원 역문제 및 PDE 문제 해결에 새로운 가능성을 열었으며, 특히 물리적 불변성과 도메인 지식을 신경망 아키텍처에 통합하는 맞춤형 설계가 필수적입니다. 그러나 이러한 응용 분야에서 높은 정확도와 안정적인 훈련을 달성하는 것은 여전히 많은 이론적 및 계산적 과제를 안고 있습니다.

## 📌 TL;DR

- **문제:** 고전 학습 이론이 딥러닝의 뛰어난 일반화, 깊이의 역할, 차원의 저주 회피, 비볼록 최적화 성공 등 핵심 현상들을 설명하지 못하는 "일반화 퍼즐"을 제기합니다.
- **방법:** 이 논문은 과잉 매개변수화된 신경망을 커널 모델(NTK)로 분석, 규범 기반 복잡성 측정, 손실 지형 분석, 깊이의 표현력(예: ReLU 네트워크의 기하급수적 효율성) 연구, 매니폴드 가정 및 PDE 솔루션의 확률적 표현을 통한 차원의 저주 극복, 그리고 CNN, ResNet, U-Net 등 특수 아키텍처 및 특징 학습 메커니즘에 대한 수학적 분석을 통해 이러한 질문들에 답합니다.
- **주요 발견:** 과잉 매개변수화된 신경망은 암묵적 정규화와 느슨한 훈련을 통해 좋은 일반화를 보이고, 깊이는 특정 함수 클래스에 대해 기하급수적인 표현력 이점을 제공하며, 구조화된 데이터/함수에 대해 차원의 저주를 극복할 수 있습니다. SGD는 복잡한 손실 지형에도 불구하고 효율적으로 좋은 최소값을 찾으며, 특수 아키텍처는 정량화 가능한 수학적 이점을 제공하고, 신경망은 계층적이고 불변적인 특징을 학습하여 자연 과학 분야에서 강력한 도구로 활용됩니다.
