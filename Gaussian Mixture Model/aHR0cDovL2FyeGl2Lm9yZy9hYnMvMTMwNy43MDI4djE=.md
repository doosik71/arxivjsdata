# INFINITE MIXTURES OF MULTIVARIATE GAUSSIAN PROCESSES
SHILIANG SUN

## 🧩 Problem to Solve
가우시안 프로세스(Gaussian Processes, GP)는 강력한 베이지안 비모수 모델이지만, 두 가지 중요한 한계점을 가지고 있습니다. 첫째, GP의 본질적인 단일 모드성(unimodality) 때문에 실제 데이터에서 흔히 나타나는 다중 모드(multimodal) 데이터를 효과적으로 모델링할 수 없습니다. 둘째, $N$개의 훈련 예제와 $M$개의 출력 차원을 가진 다변량 가우시안 프로세스(multivariate Gaussian process)의 경우, $NM \times NM$ 공분산 행렬의 역행렬을 계산해야 하므로 추론에 계산 복잡도 $O((NM)^3)$가 발생하여 대규모 데이터에 적용하기 어렵습니다. 기존에는 단일 출력 GP를 위한 무한 혼합 모델이 제시되었으나, 벡터 값 함수 학습 및 다중 작업 학습에 필요한 다변량 GP를 위한 혼합 모델은 제시된 바가 없습니다.

## ✨ Key Contributions
*   **새로운 모델 제안**: 무한 혼합 다변량 가우시안 프로세스(Infinite Mixtures of Multivariate Gaussian Processes, IMMGP)라는 새로운 모델을 제시하여 벡터 값 함수 학습 및 다중 작업 학습에 적용했습니다.
*   **다중 모드 데이터 모델링**: 기존 GP의 단일 모드성 한계를 극복하고 다중 모드 데이터를 효과적으로 모델링할 수 있습니다.
*   **계산 복잡도 완화**: 다변량 GP의 높은 계산 복잡도를 완화하여 대규모 데이터 처리 가능성을 높였습니다.
*   **자동 구성 요소 수 추론**: 디리클레 프로세스(Dirichlet Process) 사전 분포를 사용하여 혼합 구성 요소의 수(무한할 수도 있음)를 훈련 데이터로부터 자동으로 추론합니다. 이는 모델 선택 문제를 회피하게 해줍니다.
*   **MCMC 기반 추론**: 마르코프 체인 몬테 카를로(MCMC) 샘플링 기법을 사용하여 모델의 파라미터 및 잠재 변수를 추론합니다.
*   **다변량 회귀 적용 및 검증**: 제안된 모델을 다변량 회귀 문제에 적용하여 그 실현 가능성과 우수한 성능을 입증했습니다.

## 📎 Related Works
*   **가우시안 프로세스**: 함수 학습을 위한 베이지안 비모수 접근법으로, 회귀 및 분류에 강력한 성능을 보여줍니다 [1, 2].
*   **다변량/다중 출력 가우시안 프로세스**: 벡터 값 함수 학습 및 다중 작업 학습에 대한 관심이 증가하며 연구가 활발히 진행되었으며, 관련 연구로는 [4, 5, 6] 등이 있습니다.
*   **가우시안 프로세스 혼합 모델**: GP의 단일 모드성과 계산 복잡도를 완화하기 위해 도입되었으며 [8], 특히 디리클레 프로세스 기반의 무한 혼합 모델 [9]은 구성 요소 수를 데이터로부터 직접 추론할 수 있어 널리 사용됩니다.
*   **단일 출력 GP의 무한 혼합 모델**: 데이터 모델링 및 예측에 큰 성공을 거두었으며 [2, 7, 10], 본 연구는 이를 다변량 GP로 확장합니다.
*   **MCMC 및 디리클레 프로세스**: 깁스 샘플링 [13], 하이브리드 몬테 카를로 [14], 디리클레 프로세스의 통계적 특성 [15] 등이 모델 추론에 활용됩니다.
*   **다중 작업 학습 신경망 (MTLNN)**: 실험에서 비교 대상으로 사용된 다중 작업 학습 기법입니다 [16].

## 🛠️ Methodology
본 논문은 무한 혼합 다변량 가우시안 프로세스(IMMGP) 모델을 제안하며, 훈련 데이터 $D=\{x_i, y_i\}_{i=1}^N$에 대한 그래픽 모델은 Figure 1에 제시됩니다.

1.  **모델 정의**:
    *   **입력 공간 모델**: 각 혼합 구성 요소 $r$의 입력 공간은 정밀도(역공분산) 행렬 $R_r$을 가진 가우시안 분포 $p(x|z=r, \mu_r, R_r) = \mathcal{N}(x|\mu_r, R_r^{-1})$로 모델링됩니다. $\mu_r$과 $R_r$에는 각각 가우시안 분포와 위샤트 분포(Wishart distribution) 사전 분포가 사용됩니다.
    *   **출력 공간 모델**: 각 구성 요소 $r$의 잠재 함수 $\{f_{r\ell}\}_{l=1}^M$에는 가우시안 프로세스 사전 분포가 적용됩니다. 관측치 $y_{r\ell}(x)$는 $y_{r\ell}(x) \sim \mathcal{N}(f_{r\ell}(x), \sigma_{r\ell})$를 따릅니다. 공분산 함수는 $E(f_{r\ell}(x)f_{rk}(x')) = \sigma_{r0} K_r(\ell, k)k_r(x, x')$로 정의되며, 여기서 $K_r$은 작업 간 유사도를 나타내는 양의 반정부호 행렬, $k_r(\cdot, \cdot)$은 입력 공간의 공분산 함수, $\sigma_{r\ell}$은 노이즈 분산입니다. $K_r$, $\sigma_{r0}$, $\sigma_{r\ell}$, 및 $k_r$의 가중치 $w_{rd}$에는 각각 위샤트 분포 및 감마/로그-정규 사전 분포가 주어집니다.
    *   **디리클레 프로세스**: 혼합 구성 요소의 수 추론을 위해 디리클레 프로세스 사전 분포가 사용되며, 농도 파라미터 $\alpha$는 감마 분포 $G(\alpha|a_0, b_0)$를 따릅니다.

2.  **추론 (Inference)**:
    정확한 사후 분포 $p(Z, \Theta|D)$ 추론이 불가능하므로, 마르코프 체인 몬테 카를로(MCMC) 샘플링 기법, 특히 깁스 샘플링(Gibbs sampling)을 사용하여 $L$개의 샘플 $\{Z^j, \Theta^j\}_{j=1}^L$을 얻어 사후 분포를 근사합니다. 깁스 샘플링은 다음 단계로 구성됩니다:
    *   **(1) 지시 변수 $z_i$ 업데이트**: 각 훈련 데이터 $i$에 대해 $p(z_i|Z_{-i}, \Theta, D)$를 기반으로 $z_i$를 샘플링합니다. 새로운 구성 요소를 탐색하기 위해 필요에 따라 사전을 통해 파라미터를 샘플링합니다.
    *   **(2) 입력 공간 파라미터 $\{\mu_r\}, \{R_r\}$ 업데이트**: 켤레 사전 분포(conjugate priors)를 사용하여 $\mu_r$과 $R_r$의 사후 조건부 분포로부터 직접 샘플링합니다.
    *   **(3) 출력 공간 파라미터 $\{\sigma_{r0}\}, \{K_r\}, \{w_{rd}\}, \{\sigma_{r\ell}\}$ 업데이트**:
        *   $\sigma_{r0}$는 하이브리드 몬테 카를로(Hybrid Monte Carlo)를 사용하여 업데이트합니다.
        *   $K_r$, $\{w_{rd}\}$, $\{\sigma_{r\ell}\}$는 메트로폴리스-해스팅스(Metropolis-Hastings) 알고리즘을 사용하여 업데이트하며, 제안 분포로 해당 사전 분포를 사용합니다.

3.  **예측 (Prediction)**:
    새로운 테스트 입력 $x^*$에 대한 예측 출력 $f^*$의 예측 분포 $p(f^*|x^*, D)$는 MCMC 샘플 $\{Z^j, \Theta^j\}_{j=1}^L$을 사용하여 근사됩니다. 최종 예측 $\hat{f}^*$는 각 샘플에서 계산된 기댓값을 평균하여 얻습니다:
    $$ \hat{f}^* = \frac{1}{L} \sum_{i=1}^L \left[ \sum_{z^*} p(z^*|x^*, Z^i, \Theta^i) E(f^*|z^*, Z^i, \Theta^i, x^*, D) \right] $$
    여기서 $p(z^*|x^*, Z^i, \Theta^i)$는 $x^*$가 기존 구성 요소에 속할 확률 또는 새로운 구성 요소에 속할 확률을 나타냅니다. 새로운 구성 요소의 경우 $E(f^*|\ldots)=0$이며, 기존 구성 요소의 경우 표준 가우시안 프로세스 회귀 공식이 사용됩니다.

## 📊 Results
제안된 IMMGP 모델의 성능을 평가하기 위해 합성 데이터셋을 사용한 다변량 회귀 실험을 수행했습니다.
*   **데이터셋**: 입력 및 출력 차원이 모두 2인 500개의 합성 예제를 사용했습니다. 이 중 400개는 훈련 데이터로, 100개는 테스트 데이터로 사용되었습니다.
*   **비교 대상**: 앙상블 학습이 없는 다중 작업 학습 신경망(Multitask Learning Neural Networks, MTLNN) [16]과 비교했습니다.
*   **샘플링**: 4000개의 MCMC 샘플을 얻었으며, 마지막 2000개의 샘플만 예측에 사용했습니다.
*   **결과**:
    *   IMMGP1: 샘플에 반영된 기존 GP 구성 요소만을 고려한 모델.
    *   IMMGP2: 새로운 구성 요소를 선택하는 경우도 고려한 모델.

| Method | RMSE (Root Mean Squared Error) |
| :------- | :----------------------------- |
| MTLNN    | 2.0659                         |
| IMMGP1   | 0.7963                         |
| IMMGP2   | 0.7964                         |

결과는 IMMGP 모델이 MTLNN보다 훨씬 우수한 예측 성능을 보임을 나타냅니다. 또한 IMMGP1과 IMMGP2 간의 성능 차이는 매우 작아, 새로운 구성 요소를 고려하는 것이 성능에 미치는 영향이 미미함을 시사합니다.

## 🧠 Insights & Discussion
*   **모델의 유효성 입증**: 제안된 IMMGP 모델은 다변량 회귀 문제에서 기존 다중 작업 학습 신경망(MTLNN)보다 훨씬 낮은 RMSE를 달성하며 우수한 성능을 보여, 모델의 실현 가능성과 효과를 입증했습니다. 이는 다중 모드 데이터 모델링 및 계산 복잡도 완화라는 목표가 성공적으로 달성되었음을 시사합니다.
*   **자동 구성 요소 수 추론의 장점**: 디리클레 프로세스 기반의 무한 혼합 접근 방식은 혼합 구성 요소의 수를 수동으로 지정할 필요 없이 데이터로부터 자동으로 추론함으로써 모델 선택의 어려움을 해결합니다. 이는 실제 적용에서 큰 이점을 제공합니다.
*   **계산 복잡도 완화 효과**: 논문은 다변량 GP의 전체 $MN \times MN$ 공분산 행렬 역행렬 대신, 각 구성 요소의 $MN_r \times MN_r$ 행렬 역행렬로 문제를 분할함으로써 계산 비용을 줄였다고 언급합니다. 이는 대규모 데이터셋에 대한 적용 가능성을 높입니다.
*   **한계점 및 미래 방향**: 현재 모델은 합성 데이터셋에서만 평가되었으며, 대규모 실제 데이터셋에 대한 적용과 분류 문제로의 확장, 그리고 MCMC 샘플링 대신 더 빠르고 결정론적인 근사 추론 기법 개발이 향후 연구 방향으로 제시됩니다. 복잡한 MCMC 샘플링 과정의 수렴성 및 효율성도 고려되어야 할 부분입니다.

## 📌 TL;DR
이 논문은 다변량 가우시안 프로세스의 단일 모드성 및 높은 계산 복잡도 문제를 해결하기 위해, 디리클레 프로세스를 활용한 **무한 혼합 다변량 가우시안 프로세스(IMMGP)** 모델을 제안합니다. 이 모델은 마르코프 체인 몬테 카를로(MCMC) 깁스 샘플링을 통해 파라미터와 구성 요소 수를 자동으로 추론하며, 합성 다변량 회귀 데이터셋에서 기존 MTLNN보다 **월등히 낮은 예측 오차(RMSE)**를 달성하며 그 효용성을 입증했습니다.