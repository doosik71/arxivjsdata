{
  "title": "Transferability in Deep Learning: A Survey",
  "authors": "Junguang Jiang, Yang Shu, Jianmin Wang, Mingsheng Long",
  "year": 2022,
  "url": "http://arxiv.org/abs/2201.05867v1",
  "abstract": "The success of deep learning algorithms generally depends on large-scale\ndata, while humans appear to have inherent ability of knowledge transfer, by\nrecognizing and applying relevant knowledge from previous learning experiences\nwhen encountering and solving unseen tasks. Such an ability to acquire and\nreuse knowledge is known as transferability in deep learning. It has formed the\nlong-term quest towards making deep learning as data-efficient as human\nlearning, and has been motivating fruitful design of more powerful deep\nlearning algorithms. We present this survey to connect different isolated areas\nin deep learning with their relation to transferability, and to provide a\nunified and complete view to investigating transferability through the whole\nlifecycle of deep learning. The survey elaborates the fundamental goals and\nchallenges in parallel with the core principles and methods, covering recent\ncornerstones in deep architectures, pre-training, task adaptation and domain\nadaptation. This highlights unanswered questions on the appropriate objectives\nfor learning transferable knowledge and for adapting the knowledge to new tasks\nand domains, avoiding catastrophic forgetting and negative transfer. Finally,\nwe implement a benchmark and an open-source library, enabling a fair evaluation\nof deep learning methods in terms of transferability."
}