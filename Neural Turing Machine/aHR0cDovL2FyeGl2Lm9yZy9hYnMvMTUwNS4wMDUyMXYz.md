# REINFORCEMENT LEARNING NEURAL TURING MACHINES - REVISED

Wojciech Zaremba, Ilya Sutskever

## 🧩 Problem to Solve

기존의 신경 튜링 기계(NTM)는 미분 가능하고 연속적인 외부 메모리와 상호작용하도록 설계되어 미분 가능한 인터페이스에 대해서만 역전파(Backpropagation)를 통해 훈련할 수 있었습니다. 그러나 구글 검색 엔진과 같은 많은 중요한 실제 인터페이스들은 본질적으로 이산적(discrete)이며, 이러한 이산적 인터페이스는 미분 가능하지 않아 표준 역전파로는 직접 훈련할 수 없다는 문제가 있습니다. 이 논문은 모델이 이산적 인터페이스(예: 메모리, 데이터베이스, 검색 엔진)와 상호작용하는 방법을 학습하는 것이 가능한지, 특히 강화 학습(Reinforcement Learning) 알고리즘을 사용하여 이 문제를 해결할 수 있는지를 탐구합니다.

## ✨ Key Contributions

- **이산적 인터페이스와의 상호작용 학습 가능성 입증:** 리인포스(Reinforce) 알고리즘을 사용하여 신경망이 이산적 인터페이스(입력, 메모리, 출력 테이프)와 상호작용하도록 훈련될 수 있음을 보여주었습니다.
- **RL-NTM 모델 제안:** 이산적 인터페이스 접근을 리인포스로, 콘텐츠 쓰기를 역전파로 훈련하는 새로운 모델인 RL-NTM(Reinforcement Learning Neural Turing Machine)을 제시했습니다.
- **튜링 완전성 달성 (원칙적으로):** 모델이 임의의 수의 단계 동안 예측을 연기하고 무한한 메모리에 접근할 수 있도록 허용함으로써, 원칙적으로 튜링 완전(Turing complete)한 모델을 만들 수 있음을 보였습니다.
- **리인포스 기울기 확인 절차 개발:** 리인포스 알고리즘을 사용하는 거의 모든 모델에 적용 가능한 간단하고 효율적인 기울기 확인(gradient checking) 절차를 개발하여, 복잡한 RL 모델 디버깅의 어려움을 해소했습니다.
- **다양한 알고리즘적 태스크 성공:** 시퀀스 복사, 반전 등 여러 간단한 알고리즘적 태스크에서 RL-NTM의 성공적인 성능을 입증했습니다.

## 📎 Related Works

- **신경 튜링 기계 (NTM)** (Graves et al., 2014a,b): 미분 가능한 외부 메모리를 가진 모델로, 역전파를 통해 훈련됩니다. 본 연구의 가장 직접적인 선행 연구이며, NTM의 연속적인 인터페이스와 달리 본 연구는 이산적인 인터페이스를 다룹니다.
- **메모리 기반 모델:** NTM 이후 제안된 Weakly supervised Memory Network (Sukhbaatar et al., 2015), Stack RNN (Joulin & Mikolov, 2015), Neural DeQue (Grefenstette et al., 2015) 등 인터페이스-컨트롤러 추상화를 따르는 여러 모델들이 언급됩니다. 이들 역시 주로 연속적/미분 가능한 인터페이스를 다룹니다.
- **리인포스(Reinforce) 알고리즘** (Williams, 1992): 본 논문에서 이산적 인터페이스 학습에 사용된 고전적인 강화 학습 알고리즘으로, 정책 경사(policy gradient) 방법을 통해 미래 보상의 합을 최대화합니다. 시각적 주의(visual attention) 메커니즘 등 다양한 분야에 적용되어 왔습니다.
- **튜링 완전 모델:** Schmidhuber (2012, 2004) 등이 제안한 튜링 완전 모델들입니다. 본 연구의 RL-NTM도 원칙적으로 튜링 완전하지만, 학습의 어려움으로 인해 실질적인 능력이 제한됩니다.

## 🛠️ Methodology

1. **인터페이스-컨트롤러 아키텍처:** 모델은 `컨트롤러(Controller)`와 `인터페이스(Interfaces)`로 구성됩니다. 컨트롤러는 시스템에서 유일하게 학습하는 부분이며, 인터페이스는 외부 환경과의 통로 역할을 합니다.
2. **RL-NTM 구성:**
   - **컨트롤러:** LSTM 또는 "직접 접근(direct access)" 컨트롤러를 사용합니다. 직접 접근 컨트롤러는 입력 심볼을 메모리나 출력으로 직접 복사하는 내장 메커니즘을 포함하여, 심볼 재배열 태스크에 더 적합하도록 설계되었습니다.
   - **인터페이스:** `입력 테이프`, `메모리 테이프`, `출력 테이프`의 세 가지 1차원 이산적 인터페이스를 가집니다. 각 테이프의 헤드는 이동(앞/뒤, 혹은 고정)할 수 있으며, 출력 테이프는 쓰기 전용입니다.
3. **하이브리드 훈련 전략:**
   - **이산적 결정 (위치 조정):** 입력/메모리 테이프 헤드 이동($[-1, 0, 1]$), 출력 테이프 예측 여부($[0, 1]$)와 같은 이산적 결정들은 `리인포스(Reinforce)` 알고리즘을 통해 훈련됩니다.
   - **연속적 출력 (콘텐츠):** 메모리에 저장할 벡터, 출력할 심볼 예측과 같은 연속적인 출력은 `역전파(Backpropagation)`를 통해 훈련됩니다.
   - **목표 함수:** 리인포스($p_{\text{reinforce}}$)와 역전파($p_{\text{bp}}$)가 결합된 형태의 기대 로그 확률 합을 최대화합니다:
     $$ \sum*{[a_1, ..., a_n] \in A^{\dagger}} p*{\text{reinforce}}(a*1, ..., a_n | \theta) \left[ \sum*{i=1}^n \log(p\_{\text{bp}}(y_i | x_1, ..., x_i, a_1, ...a_i, \theta)) \right] $$
4. **기울기 추정기 분산 감소:** 리인포스 알고리즘의 높은 분산을 줄이기 위해 다음과 같은 기법을 사용합니다.
   - **미래 보상 역전파(Future rewards backpropagation):** 현재 시점 이후의 보상만 해당 시점의 행동에 영향을 미치는 것으로 간주하여 기울기를 계산합니다.
   - **온라인/오프라인 기준선 예측(Online/Offline baseline prediction):** 보상에 기준선(baseline)을 빼서 분산을 줄입니다. 오프라인 기준선은 별도의 LSTM이 전체 입력을 먼저 처리하여 문제의 난이도를 예측하고 더 나은 기준선을 제공합니다.
5. **기울기 확인 절차:** 리인포스 구현의 정확성을 검증하기 위해, 다항 분포에서 샘플링하는 함수를 결정론적 함수로 오버라이드하여 가능한 모든 행동 시퀀스를 순차적으로 생성하고, 이를 통해 예상 기울기를 수치적으로 검증하는 방법을 개발했습니다.
6. **커리큘럼 학습(Curriculum Learning):** 어려운 문제 인스턴스로 바로 훈련하는 대신, 점진적으로 복잡도를 높여가며 훈련하는 커리큘럼 학습 전략을 사용했습니다. 모델 성능이 특정 임계값을 넘으면 다음 단계의 더 어려운 문제 인스턴스로 전환합니다.

## 📊 Results

- **성공적인 태스크:** RL-NTM은 시퀀스 복사(Copy), 중복 입력(DuplicatedInput), 반전(Reverse), 반복 복사(RepeatCopy), 전진 반전(ForwardReverse)과 같은 간단한 알고리즘적 태스크들을 성공적으로 해결했습니다. Copy 태스크의 경우 임의의 길이로 일반화하는 능력을 보였습니다.
- **컨트롤러 의존성:** LSTM 컨트롤러만으로는 Reverse, ForwardReverse, RepeatCopy와 같은 태스크들을 해결하지 못했으나, 태스크에 더 적합한 "직접 접근(Direct Access)" 컨트롤러를 사용했을 때 성공했습니다. 이는 모델의 성공이 컨트롤러 아키텍처에 크게 의존함을 시사합니다.
- **커리큘럼 학습의 중요성:** 커리큘럼 학습 없이는(짧은 시퀀스 제외) 어떤 문제도 해결할 수 없었으며, 커리큘럼 학습이 모델 훈련에 필수적임을 확인했습니다.
- **어려운 태스크:** 정렬(sorting)이나 긴 정수 덧셈(long integer addition), 입력 테이프가 앞으로만 이동 가능한 RepeatCopy와 같은 더 복잡한 태스크에서는 제한적인 성공을 거두거나 실패했습니다. 정렬 태스크에서는 임시방편적인 알고리즘을 학습하고 컨트롤러 메모리를 과도하게 사용하는 경향을 보였습니다.
- **모든 구성 요소의 중요성:** RL-NTM의 모든 구성 요소(Reinforce, 분산 감소 기법, 커리큘럼 학습 등)가 문제 해결에 필수적이라는 것을 실험적으로 확인했습니다.

## 🧠 Insights & Discussion

- **이산적 인터페이스 학습의 가능성과 도전:** 이 논문은 리인포스 알고리즘이 NTM 스타일 모델을 이산적 인터페이스와 상호작용하도록 훈련시킬 수 있음을 성공적으로 입증했습니다. 그러나 리인포스를 이용한 메모리 접근 패턴 학습은 여전히 어렵고 효율적이지 않을 수 있습니다.
- **모델의 일반성 한계:** RL-NTM은 원칙적으로 튜링 완전하지만, 실제로는 비교적 간단한 문제만 해결할 수 있습니다. 특히, 범용적인 LSTM 컨트롤러만으로는 많은 태스크를 해결하기 어려웠으며, 태스크에 특화된 "직접 접근" 컨트롤러가 필요했다는 점은 모델의 일반성 또는 학습 효율성에 한계가 있음을 보여줍니다. 이는 모델이 다양한 관심 태스크에서 성능을 향상시키기 위해 도메인 독립적이지 않은 아키텍처 수정이 필요할 수 있음을 의미합니다.
- **훈련 안정성 및 효율성:** 강화 학습 훈련의 어려움과 높은 분산을 줄이기 위한 기법(미래 보상 역전파, 기준선 예측)과 커리큘럼 학습이 모델의 성공에 결정적인 역할을 했습니다.
- **기울기 확인 절차의 가치:** 개발된 리인포스 기울기 확인 절차는 모델의 복잡성으로 인해 발생할 수 있는 구현 오류를 발견하고 수정하는 데 매우 유용했습니다. 이는 본 연구의 독립적인 기여이자, 향후 다른 강화 학습 구현에도 적용될 수 있는 중요한 도구입니다.

## 📌 TL;DR

본 연구는 신경 튜링 기계(NTM)가 미분 불가능한 **이산적 인터페이스**와 상호작용하도록 학습할 수 있는지 탐구합니다. 저자들은 **RL-NTM**이라는 새로운 모델을 제안하는데, 이는 **리인포스(Reinforce) 알고리즘**을 사용하여 이산적인 행동(예: 테이프 헤드 이동)을 학습하고, **역전파**를 사용하여 연속적인 출력(예: 메모리 내용)을 학습합니다. 모델은 `입력 테이프`, `메모리 테이프`, `출력 테이프` 같은 이산적 인터페이스를 가집니다. **커리큘럼 학습**과 **기울기 분산 감소 기법**이 훈련 안정성에 필수적임을 발견했으며, **리인포스 기울기 확인 절차**라는 새로운 디버깅 도구를 개발했습니다. 결과적으로 RL-NTM은 시퀀스 복사 및 반전과 같은 간단한 알고리즘 태스크들을 성공적으로 해결했지만, 컨트롤러 아키텍처와 커리큘럼 학습에 크게 의존했으며, 복잡한 태스크에는 어려움을 겪었습니다.
