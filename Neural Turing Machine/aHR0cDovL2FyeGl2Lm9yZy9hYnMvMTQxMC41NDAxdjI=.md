# Neural Turing Machines

Alex Graves, Greg Wayne, Ivo Danihelka

## 🧩 Problem to Solve

기존 신경망, 특히 순환 신경망(RNN)은 복잡한 데이터 모델링에 성공적이었지만, 대부분 논리적 흐름 제어와 외부 메모리 사용을 간과했습니다. RNN은 이론적으로 튜링 완전(Turing-Complete)하지만, 실제로는 알고리즘적 작업을 학습하고 수행하는 데 어려움이 많습니다. 예를 들어, 긴 시간 동안 정보를 저장하고 접근해야 하는 작업이나 가변 길이 구조에서 변수 바인딩이 필요한 작업 등에서 제약이 있었습니다. 이 논문은 표준 RNN의 기능을 확장하여 알고리즘적 작업을 더 쉽게 해결할 수 있도록 하는 것을 목표로 합니다.

## ✨ Key Contributions

- 신경망을 외부 메모리 자원과 결합한 Neural Turing Machine (NTM) 아키텍처를 제안했습니다.
- NTM은 튜링 머신 또는 폰 노이만 아키텍처와 유사하지만, 전체 시스템이 엔드-투-엔드(end-to-end)로 미분 가능하여 경사 하강법으로 효율적인 학습이 가능합니다.
- NTM이 입력-출력 예시로부터 복사, 정렬, 연관 기억과 같은 간단한 알고리즘을 추론할 수 있음을 입증했습니다.
- 학습 데이터 범위를 훨씬 넘어선(예: 훈련에 사용된 것보다 훨씬 긴 시퀀스) 강력한 일반화 능력을 보여주었으며, 이는 NTM이 단순히 패턴을 암기하는 것이 아니라 실제 "알고리즘"을 학습했음을 시사합니다.
- 콘텐츠 기반(content-based) 주소 지정과 위치 기반(location-based) 주소 지정을 결합한 미분 가능한 "흐릿한(blurry)" 메모리 접근 메커니즘을 개발했습니다.

## 📎 Related Works

- **심리학 및 신경과학:** 작업 기억(working memory) 개념(Baddeley et al., Miller), 전전두엽 피질의 역할(Goldman-Rakic), 생물리학적 회로 모델(Wang, Hazy et al.), 뇌 기능에서 주소 지정의 중요성(Gallistel & King, Marcus).
- **인지 과학 및 언어학:** 정보/기호 처리 메타포(Chomsky, Miller), 병렬 분산 처리(PDP) 또는 연결주의(Rumelhart et al.), Fodor & Pylyshyn의 신경망 한계 주장(변수 바인딩, 가변 길이 구조 처리) 및 이에 대한 대응 연구(Hinton, Smolensky, Touretzky, Pollack, Plate, Kanerva).
- **순환 신경망(RNN):** 튜링 완전성(Siegelmann & Sontag), 장단기 기억(LSTM) 네트워크를 통한 기울기 소실/폭발 문제 완화(Hochreiter & Schmidhuber), 가변 길이 구조 처리 능력(Graves et al., Sutskever et al.).
- **기타 선행 연구:** 미분 가능한 주의(attention) 모델(Graves, Bahdanau et al.), RNN을 사용한 프로그램 검색(Hochreiter et al., Das et al.).

## 🛠️ Methodology

NTM 아키텍처는 **신경망 컨트롤러**와 **메모리 뱅크**의 두 가지 기본 구성 요소로 이루어집니다.

- **메모리 구조**:

  - 메모리는 $N \times M$ 크기의 행렬 $M_{t}$로 표현되며, $N$은 메모리 위치의 수, $M$은 각 위치의 벡터 크기입니다.

- **미분 가능한 읽기(Reading) 작업**:

  - 읽기 헤드(read head)는 $N$개 메모리 위치에 대한 가중치 벡터 $w_{t}$를 생성합니다.
  - 가중치 $w_{t}(i)$는 다음 제약 조건을 따릅니다: $\sum_{i} w_{t}(i) = 1$, $0 \le w_{t}(i) \le 1$.
  - 헤드가 반환하는 길이 $M$의 읽기 벡터 $r_{t}$는 메모리 행렬 $M_{t}(i)$의 행 벡터들의 볼록 조합(convex combination)으로 정의됩니다:
    $$r_{t} \leftarrow \sum_{i} w_{t}(i)M_{t}(i)$$
  - 이는 메모리와 가중치 모두에 대해 미분 가능합니다.

- **미분 가능한 쓰기(Writing) 작업**:

  - 각 쓰기 작업은 지우기(erase)와 추가(add)의 두 부분으로 분해됩니다.
  - 쓰기 헤드가 생성한 가중치 $w_{t}$와 $(0,1)$ 범위의 $M$개 요소로 구성된 지우기 벡터 $e_{t}$를 사용하여 메모리 벡터 $M_{t-1}(i)$를 다음과 같이 수정합니다:
    $$\tilde{M}_{t}(i) \leftarrow M_{t-1}(i) [1 - w_{t}(i)e_{t}]$$
  - 각 쓰기 헤드는 또한 길이 $M$의 추가 벡터 $a_{t}$를 생성하며, 이는 지우기 단계 후에 메모리에 추가됩니다:
    $$M_{t}(i) \leftarrow \tilde{M}_{t}(i) + w_{t}(i)a_{t}$$
  - 지우기 및 추가 작업은 모두 미분 가능하므로, 복합 쓰기 작업도 미분 가능합니다.

- **주소 지정 메커니즘 (가중치 $w_{t}$ 생성 방식)**:

  1. **콘텐츠 기반 주소 지정(Content-based Addressing)**:
     - 각 헤드는 키 벡터 $k_{t}$를 생성하여 메모리 $M_{t}(i)$의 각 벡터와 유사도 $K[ \cdot, \cdot ]$를 비교합니다 (주로 코사인 유사도 사용).
     - 콘텐츠 기반 가중치 $w^{c}_{t}$는 다음 공식으로 생성됩니다. 여기서 $\beta_{t}$는 초점의 정밀도를 조절하는 키 강도입니다:
       $$w^{c}_{t}(i) \leftarrow \frac{\exp(\beta_{t} K[k_{t},M_{t}(i)])}{\sum_{j} \exp(\beta_{t} K[k_{t},M_{t}(j)])}$$
  2. **위치 기반 주소 지정(Location-based Addressing)**:
     - **보간(Interpolation)**: 각 헤드는 스칼라 보간 게이트 $g_{t} \in (0,1)$를 생성하여 이전 시간 단계의 가중치 $w_{t-1}$와 현재 시간 단계의 콘텐츠 기반 가중치 $w^{c}_{t}$를 혼합합니다:
       $$w^{g}_{t} \leftarrow g_{t}w^{c}_{t} + (1-g_{t})w_{t-1}$$
     - **시프트(Shift)**: 각 헤드는 허용된 정수 시프트에 대한 정규화된 분포를 정의하는 시프트 가중치 $s_{t}$를 생성합니다. 이는 다음 순환 합성곱(circular convolution)으로 적용됩니다:
       $$\tilde{w}_{t}(i) \leftarrow \sum_{j=0}^{N-1} w^{g}_{t}(j)s_{t}(i-j \pmod{N})$$
     - **선명화(Sharpening)**: 각 헤드는 최종 가중치를 선명하게 만드는 스칼라 $\gamma_{t} \ge 1$를 생성합니다:
       $$w_{t}(i) \leftarrow \frac{ \tilde{w}_{t}(i)^{\gamma_{t}}}{\sum_{j} \tilde{w}_{t}(j)^{\gamma_{t}}}$$

- **컨트롤러 네트워크**: 피드포워드(feedforward) 또는 순환(recurrent, 예: LSTM) 신경망이 사용될 수 있습니다. 순환 컨트롤러는 자체적인 내부 메모리를 가지며, 이는 프로세서의 레지스터와 유사하게 동작합니다.

## 📊 Results

- **복사(Copy) 작업**:
  - NTM은 LSTM 단독보다 훨씬 빠르게 학습했으며, 더 낮은 비용으로 수렴했습니다.
  - 특히, 20까지의 시퀀스 길이로 훈련된 NTM은 100 길이의 시퀀스에도 잘 일반화하는 반면, LSTM은 길이 20을 넘어가면 빠르게 성능이 저하되었습니다. NTM은 "배열 생성 및 반복"과 같은 알고리즘을 학습했음을 시사합니다.
- **반복 복사(Repeat Copy) 작업**:
  - NTM은 LSTM보다 훨씬 빠르게 이 작업을 학습하여 완벽하게 해결했습니다.
  - NTM은 훈련 데이터 범위를 넘어선 더 긴 시퀀스와 더 많은 반복 횟수에 일반화하는 능력을 보여주었지만, 정확한 반복 횟수 계산에는 한계를 보였습니다. 이는 NTM이 간단한 중첩 함수를 학습했음을 시사합니다.
- **연관 기억(Associative Recall) 작업**:
  - NTM은 LSTM보다 훨씬 빠르게 학습했으며(3만 에피소드 내에 거의 0 비용 도달 vs. 100만 에피소드 후에도 0에 미달), 더 긴 항목 시퀀스에 훨씬 잘 일반화했습니다.
  - 이는 NTM의 외부 메모리가 "간접 참조"가 필요한 데이터 구조에 더 효과적임을 나타냅니다. NTM은 항목의 압축된 표현을 저장하고, 콘텐츠 기반 조회 후 시프트를 통해 다음 항목을 검색하는 알고리즘을 학습했습니다.
- **동적 N-그램(Dynamic N-Grams) 작업**:
  - NTM은 LSTM에 비해 작지만 유의미한 성능 우위를 보였으며, 최적의 베이즈 추정기(Bayesian estimator)에 근접했습니다.
  - NTM은 메모리를 사용하여 다른 컨텍스트에서 관찰된 1과 0의 수를 세어 전환 통계(transition statistics)를 추적하는 알고리즘을 구현한 것으로 분석되었습니다.
- **우선순위 정렬(Priority Sort) 작업**:
  - NTM은 피드포워드 및 LSTM 컨트롤러 모두에서 LSTM을 크게 능가했습니다.
  - NTM은 우선순위를 사용하여 각 쓰기 위치를 결정하고, 정렬된 시퀀스를 순차적으로 읽는 방식으로 정렬 알고리즘을 학습했습니다.

**전반적인 결과**: NTM은 모든 실험에서 LSTM에 비해 훨씬 빠른 학습 속도와 뛰어난 일반화 능력을 보였습니다. 이는 NTM이 입력-출력 예시로부터 실제 알고리즘을 성공적으로 추론하고 적용했음을 강력하게 지지합니다.

## 🧠 Insights & Discussion

- NTM은 신경망을 외부 주소 지정 가능한 메모리와 효과적으로 결합함으로써 신경망이 간단한 알고리즘을 학습할 수 있는 길을 열었습니다.
- 엔드-투-엔드 미분 가능성은 NTM이 경사 하강법으로 훈련될 수 있는 실용적인 메커니즘을 제공하는 핵심 요소입니다.
- 콘텐츠 기반 및 위치 기반 주소 지정의 조합은 메모리와 유연하게 상호 작용할 수 있는 능력을 부여합니다 (예: 시퀀스 시작점으로 점프, 반복, 오프셋 조회 등).
- 학습 데이터 범위를 훨씬 넘어선 강력한 일반화 능력은 NTM이 단순한 패턴을 암기하는 것이 아니라 내재된 "알고리즘"을 학습했음을 의미합니다. 이는 신경망의 근본적인 한계에 대한 주요 비판에 대한 중요한 반박입니다.
- 한계점으로는 반복 횟수와 같이 수치적으로 표현되는 정보의 경우, NTM이 훈련 범위를 넘어 일반화하는 데 여전히 어려움을 겪을 수 있다는 점이 지적되었습니다.
- NTM은 미분 가능한 컴퓨터와 유사하게 작동하여 특정 작업을 위해 "스스로 프로그래밍"하는 능력을 학습할 수 있는 가능성을 보여주며, 이는 기계 학습이 더 복잡하고 알고리즘적인 문제를 해결하는 새로운 방향을 제시합니다.

## 📌 TL;DR

**문제**: 기존 신경망은 명시적인 외부 메모리 및 논리적 흐름 제어의 부재로 인해 알고리즘적 작업을 학습하고 일반화하는 데 어려움이 있었습니다.

**방법**: Neural Turing Machines (NTM)은 신경망 컨트롤러와 외부 메모리 행렬을 결합한 아키텍처입니다. 이 시스템은 엔드-투-엔드로 미분 가능하여 경사 하강법으로 훈련될 수 있습니다. NTM은 콘텐츠 기반 및 위치 기반 주소 지정을 혼합하는 "흐릿한" 읽기/쓰기 작업을 통해 주의 메커니즘으로 메모리와 상호 작용합니다.

**결과**: NTM은 복사, 정렬, 연관 기억과 같은 간단한 알고리즘을 LSTM보다 훨씬 빠르게 학습했으며, 훈련된 시퀀스 길이보다 훨씬 긴 미지의 시퀀스에 대해 월등한 일반화 능력을 보여주었습니다. 이는 NTM이 알고리즘 프로그램을 추론하고 실행하여 사실상 학습 가능한 미분 컴퓨터를 생성하는 능력을 입증합니다.
